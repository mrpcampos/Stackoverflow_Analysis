Id,PostTypeId,AcceptedAnswerId,ParentId,CreationDate,DeletionDate,Score,ViewCount,Body,OwnerUserId,OwnerDisplayName,LastEditorUserId,LastEditorDisplayName,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount,ClosedDate,CommunityOwnedDate,ContentLicense,isPandas
"33054527","1","33054552","","2015-10-10 13:28:09","","648","1269290","<p>I've very recently migrated to Py 3.5.
This code was working properly in Python 2.7:</p>

<pre><code>with open(fname, 'rb') as f:
    lines = [x.strip() for x in f.readlines()]

for line in lines:
    tmp = line.strip().lower()
    if 'some-pattern' in tmp: continue
    # ... code
</code></pre>

<p>After upgrading to 3.5, I'm getting the:</p>

<pre><code>TypeError: a bytes-like object is required, not 'str'
</code></pre>

<p>error on the last line (the pattern search code).</p>

<p>I've tried using the <code>.decode()</code> function on either side of the statement, also tried:</p>

<pre><code>if tmp.find('some-pattern') != -1: continue
</code></pre>

<p>- to no avail.</p>

<p>I was able to resolve almost all 2:3 issues quickly, but this little statement is bugging me.</p>
","1656343","","100297","","2019-08-21 11:30:44","2020-09-12 09:40:46","TypeError: a bytes-like object is required, not 'str' when writing to a file in Python3","<python><python-3.x><string><file><byte>","9","4","101","","","CC BY-SA 4.0","0"
"29846087","1","","","2015-04-24 11:16:44","","271","772140","<p>I've installed Python 3.5 and while running </p>

<pre><code>pip install mysql-python
</code></pre>

<p>it gives me the following error </p>

<pre><code>error: Microsoft Visual C++ 14.0 is required (Unable to find vcvarsall.bat)
</code></pre>

<p>I have added the following lines to my Path</p>

<pre><code>C:\Program Files\Python 3.5\Scripts\;
C:\Program Files\Python 3.5\;

C:\Windows\System32;
C:\Program Files (x86)\Microsoft Visual Studio 12.0\VC;
C:\Program Files (x86)\Microsoft Visual Studio 11.0\VC
</code></pre>

<p>I have a 64bit win 7 setup in my PC.</p>

<p>What could be the solution for mitigating this error and installing the modules correctly via <code>pip</code>.</p>
","268598","","3989608","","2019-12-11 14:07:29","2020-10-23 23:33:38","Microsoft Visual C++ 14.0 is required (Unable to find vcvarsall.bat)","<python><python-3.x><visual-c++>","30","11","121","","","CC BY-SA 4.0","0"
"42339876","1","42340744","","2017-02-20 08:43:18","","187","629114","<p><a href=""https://github.com/affinelayer/pix2pix-tensorflow/tree/master/tools"" rel=""noreferrer"">https://github.com/affinelayer/pix2pix-tensorflow/tree/master/tools</a></p>

<p>An error occurred when compiling ""process.py"" on the above site.</p>

<pre><code> python tools/process.py --input_dir data --            operation resize --outp
ut_dir data2/resize
data/0.jpg -&gt; data2/resize/0.png
</code></pre>

<p>Traceback (most recent call last):</p>

<pre><code>File ""tools/process.py"", line 235, in &lt;module&gt;
  main()
File ""tools/process.py"", line 167, in main
  src = load(src_path)
File ""tools/process.py"", line 113, in load
  contents = open(path).read()
      File""/home/user/anaconda3/envs/tensorflow_2/lib/python3.5/codecs.py"", line 321, in decode
  (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode     byte 0xff in position 0: invalid start byte
</code></pre>

<p>What is the cause of the error?
Python's version is 3.5.2.</p>
","7580060","","1281485","","2017-02-20 09:28:03","2020-09-23 23:09:28","error UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte","<python><python-3.x><utf-8>","17","0","41","","","CC BY-SA 3.0","0"
"45137395","1","45138817","","2017-07-17 06:33:37","","125","616691","<p>I have a Python 2.7.11 installed on one of my LAB stations. I would like to upgrade Python to at least 3.5.</p>

<p>How should I do that ?
Should I prefer to completely uninstall 2.7.11 and than install the new one ?
Is there a way to update it ? Is an update a good idea ?</p>
","7589661","","","","","2020-05-26 09:28:06","How do I upgrade the Python installation in Windows 10?","<python><python-3.x>","5","2","40","","","CC BY-SA 3.0","0"
"43728431","1","43938991","","2017-05-02 00:40:41","","204","506042","<p>This is the first time I've really sat down and tried python 3, and seem to be failing miserably. I have the following two files:</p>
<ol>
<li>test.py</li>
<li>config.py</li>
</ol>
<p>config.py has a few functions defined in it as well as a few variables. I've stripped it down to the following:</p>
<p><strong>config.py</strong></p>
<pre><code>debug = True
</code></pre>
<p><strong>test.py</strong></p>
<pre><code>import config
print (config.debug)
</code></pre>
<p>I also have an <code>__init__.py</code></p>
<p>However, I'm getting the following error:</p>
<pre><code>ModuleNotFoundError: No module named 'config'
</code></pre>
<p>I'm aware that the py3 convention is to use absolute imports:</p>
<pre><code>from . import config
</code></pre>
<p>However, this leads to the following error:</p>
<pre><code>ImportError: cannot import name 'config'
</code></pre>
<p>So I'm at a loss as to what to do here... Any help is greatly appreciated. :)</p>
","788054","","4518341","","2020-06-27 03:29:13","2020-10-01 04:05:02","Relative imports - ModuleNotFoundError: No module named x","<python><python-3.x><package><python-import><relative-import>","12","7","63","","","CC BY-SA 4.0","0"
"43189302","1","43191021","","2017-04-03 16:02:59","","55","485075","<p>I am getting error while running this part of the code. tried some of the existing solutions, none of them helped</p>

<pre><code>elec_and_weather = pd.read_csv(r'C:\HOUR.csv', parse_dates=True,index_col=0)
# Add historic DEMAND to each X vector
 for i in range(0,24):
    elec_and_weather[i] = np.zeros(len(elec_and_weather['DEMAND']))
    elec_and_weather[i][elec_and_weather.index.hour==i] = 1
# Set number of hours prediction is in advance
n_hours_advance = 24

# Set number of historic hours used
n_hours_window = 24

for k in range(n_hours_advance,n_hours_advance+n_hours_window):
    elec_and_weather['DEMAND_t-%i'% k] = np.zeros(len(elec_and_weather['DEMAND']))'
</code></pre>

<p>I am always getting this error</p>

<pre><code>for i in range(0,24):
File ""&lt;ipython-input-29-db3022a769d1&gt;"", line 1
for i in range(0,24):
                     ^
SyntaxError: unexpected EOF while parsing

File ""&lt;ipython-input-25-df0a44131c36&gt;"", line 1
    for k in range(n_hours_advance,n_hours_advance+n_hours_window):
                                                                   ^
SyntaxError: unexpected EOF while parsing
</code></pre>
","6916820","","6233714","","2019-02-01 11:22:11","2020-05-15 09:19:50","SyntaxError: unexpected EOF while parsing","<python><python-3.x><python-3.6><eof><lint>","6","6","13","","","CC BY-SA 4.0","0"
"30418481","1","30418498","","2015-05-23 23:14:54","","458","428632","<p>I'm trying to use NetworkX to read a Shapefile and use the function <code>write_shp()</code> to generate the Shapefiles that will contain the nodes and edges, but when I try to run the code it gives me the following error:</p>

<pre><code>Traceback (most recent call last):   File
""C:/Users/Felipe/PycharmProjects/untitled/asdf.py"", line 4, in
&lt;module&gt;
    nx.write_shp(redVial, ""shapefiles"")   File ""C:\Python34\lib\site-packages\networkx\readwrite\nx_shp.py"", line
192, in write_shp
    for key, data in e[2].iteritems(): AttributeError: 'dict' object has no attribute 'iteritems'
</code></pre>

<p>I'm using Python 3.4 and installed NetworkX via pip install.</p>

<p>Before this error it had already given me another one that said ""xrange does not exist"" or something like that, so I looked it up and just changed <code>xrange</code> to <code>range</code> in the nx_shp.py file, which seemed to solve it.</p>

<p>From what I've read it could be related to the Python version (Python2 vs Python3).</p>
","4926318","","2535611","","2020-05-05 14:42:21","2020-05-05 14:42:21","Error: "" 'dict' object has no attribute 'iteritems' ""","<python><python-3.x><dictionary>","6","5","68","","","CC BY-SA 4.0","0"
"28583565","1","28583969","","2015-02-18 12:20:05","","199","406012","<p>Here is my code:</p>

<pre><code>import imaplib
from email.parser import HeaderParser

conn = imaplib.IMAP4_SSL('imap.gmail.com')
conn.login('example@gmail.com', 'password')
conn.select()
conn.search(None, 'ALL')
data = conn.fetch('1', '(BODY[HEADER])')
header_data = data[1][0][1].decode('utf-8')
</code></pre>

<p>at this point I get the error message </p>

<pre><code>AttributeError: 'str' object has no attribute 'decode'
</code></pre>

<p>Python 3 doesn't have decode anymore, am I right? how can I fix this? </p>

<p>Also, in:</p>

<pre><code>data = conn.fetch('1', '(BODY[HEADER])')
</code></pre>

<p>I am selecting only the 1st email. How do I select all?</p>
","","user4530588","100297","user4530588","2018-02-09 10:13:36","2020-03-19 15:51:28","'str' object has no attribute 'decode'. Python 3 error?","<python><python-3.x><imaplib>","7","0","31","","","CC BY-SA 3.0","0"
"37372603","1","37372690","","2016-05-22 09:31:05","","176","402765","<p>I have a set of strings <code>set1</code>, and all the strings in <code>set1</code> have a two specific substrings which I don't need and want to remove. <br/>
Sample Input:
<code>set1={'Apple.good','Orange.good','Pear.bad','Pear.good','Banana.bad','Potato.bad'}</code><br/>
So basically I want the <code>.good</code> and <code>.bad</code> substrings removed from all the strings. <br/>What I tried:</p>

<pre><code>for x in set1:
    x.replace('.good','')
    x.replace('.bad','')
</code></pre>

<p>But this doesn't seem to work at all. There is absolutely no change in the output and it is the same as the input. I tried using <code>for x in list(set1)</code> instead of the original one but that doesn't change anything.  </p>
","6311520","","","","","2020-08-11 12:54:59","How to remove specific substrings from a set of strings in Python?","<python><python-3.x>","9","0","23","","","CC BY-SA 3.0","0"
"34475051","1","","","2015-12-26 20:40:15","","104","391648","<p>I'm running Python 3.5.1 for Mac. I want to use <code>urllib2</code> module. I tried installing it but I was told that it's been split into <code>urllib.request</code> and <code>urllib.error</code> for Python 3. </p>

<p>My command (running from the framework bin directory for now because it's not in my path):</p>

<pre><code>sudo ./pip3 install urllib.request
</code></pre>

<p>Returns this:</p>

<pre><code>Could not find a version that satisfies the requirement urllib.request (from versions: )
No matching distribution found for urllib.request
</code></pre>

<p>I got the same error before when I tried to install <code>urllib2</code> in one fell swoop.</p>
","845642","","12860895","","2020-06-03 00:17:53","2020-06-03 06:46:33","Need to install urllib2 for Python 3.5.1","<python><python-3.x><urllib2>","3","4","15","","","CC BY-SA 4.0","0"
"33412974","1","35524522","","2015-10-29 11:27:43","","235","384770","<p>There is a <code>--user</code> option for pip which can install a Python package per user:</p>

<pre><code>pip install --user [python-package-name]
</code></pre>

<p>I used this option to install a package on a server for which I do not have root access. What I need now is to uninstall the installed package on the current user. I tried to execute this command:</p>

<pre><code>pip uninstall --user [python-package-name]
</code></pre>

<p>But I got:</p>

<pre><code>no such option: --user
</code></pre>

<p>How can I uninstall a package that I installed with <code>pip install --user</code>, other than manually finding and deleting the package?</p>

<p>I've found this article </p>

<p><a href=""https://stackoverflow.com/questions/33412974/uninstall-python-package-per-user"">pip cannot uninstall from per-user site-packages directory</a></p>

<p>which describes that uninstalling packages from user directory does not supported. According to the article if it was implemented correctly then with</p>

<pre><code>pip uninstall [package-name]
</code></pre>

<p>the package that was installed will be also searched in user directories. But a problem still remains for me. What if the same package was installed both system-wide and per-user?
What if someone needs to target a specific user directory?</p>
","546822","","2303761","","2019-07-09 08:34:33","2020-03-28 09:00:39","How to uninstall a package installed with pip install --user","<python><python-3.x><pip><virtualenv>","7","5","59","","","CC BY-SA 3.0","0"
"41950021","1","41950042","","2017-01-31 05:06:25","","39","382014","<p>I'm learning python and working on exercises. One of them is to code a voting system to select the best player between 23 players of the match using lists. </p>

<p>I'm using <code>Python3</code>.</p>

<p>My code:</p>

<pre><code>players= [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
vote = 0
cont = 0

while(vote &gt;= 0 and vote &lt;23):
    vote = input('Enter the name of the player you wish to vote for')
    if (0 &lt; vote &lt;=24):
        players[vote +1] += 1;cont +=1
    else:
        print('Invalid vote, try again')
</code></pre>

<p>I get</p>

<blockquote>
  <p>TypeError: '&lt;=' not supported between instances of 'str' and 'int'</p>
</blockquote>

<p>But I don't have any strings here, all variables are integers.</p>
","7493136","","4720935","","2017-07-19 14:15:45","2020-06-30 11:57:45","TypeError: '<=' not supported between instances of 'str' and 'int'","<python><python-3.x>","4","0","10","2019-03-27 21:39:56","","CC BY-SA 3.0","0"
"41570359","1","","","2017-01-10 13:44:59","","130","381805","<p>I'm trying to convert a fairly simple Python program to an executable and couldn't find what I was looking for, so I have a few questions (I'm running Python 3.6):</p>
<p>The methods of doing this that I have found so far are as follows</p>
<ol>
<li>downloading an old version of Python and using <code>pyinstaller/py2exe</code></li>
<li>setting up a virtual environment in Python 3.6 that will allow me to do 1.</li>
<li>downloading a Python to C++ converter and using that.</li>
</ol>
<p>Here is what I've tried/what problems I've run into.</p>
<ul>
<li>I installed <code>pyinstaller</code> before the required download before it (pypi-something) so it did not work. After downloading the prerequisite file, <code>pyinstaller</code> still does not recognize it.</li>
<li>If I'm setting up a virtualenv in Python 2.7, do I actually need to have Python 2.7 installed?</li>
<li>similarly, the only python to C++ converters I see work only up until Python 3.5 - do I need to download and use this version if attempting this?</li>
</ul>
","7396807","","63550","","2020-06-22 14:25:40","2020-09-22 05:44:37","How can I convert a .py to .exe for Python?","<python><python-3.x><exe><py2exe><python-3.6>","6","2","88","","","CC BY-SA 4.0","0"
"48720833","1","51831928","","2018-02-10 12:35:44","","190","364143","<p>I installed the latest version of Python <code>(3.6.4 64-bit)</code> and the latest version of <code>PyCharm (2017.3.3 64-bit)</code>. Then I installed some modules in PyCharm (Numpy, Pandas, etc), but when I tried installing Tensorflow it didn't install, and I got the error message: </p>

<blockquote>
  <p>Could not find a version that satisfies the requirement TensorFlow (from versions: )
      No matching distribution found for TensorFlow.</p>
</blockquote>

<p>Then I tried installing TensorFlow from the command prompt and I got the same error message.
I did however successfully install tflearn. </p>

<p>I also installed Python 2.7, but I got the same error message again. I googled the error and tried some of the things which were suggested to other people, but nothing worked (this included installing Flask). </p>

<p>How can I install Tensorflow? Thanks.</p>
","5538535","","11717381","","2020-02-09 18:02:30","2020-10-11 14:56:25","Could not find a version that satisfies the requirement tensorflow","<python><python-3.x><python-2.7><tensorflow><pip>","17","7","22","","","CC BY-SA 4.0","0"
"41986507","1","41986843","","2017-02-01 17:57:06","","172","351638","<p>I was trying to set default python version to <code>python3</code> in <code>Ubuntu 16.04</code>. By default it is <code>python2</code> (2.7). I followed below steps :  </p>

<pre><code>update-alternatives --remove python /usr/bin/python2
update-alternatives --install /usr/bin/python python /usr/bin/python3
</code></pre>

<p>but I'm getting the following error for the second statement,    </p>

<pre><code>rejeesh@rejeesh-Vostro-1015:~$ update-alternatives --install /usr/bin/python python /usr/bin/python3
update-alternatives: --install needs &lt;link&gt; &lt;name&gt; &lt;path&gt; &lt;priority&gt;

Use 'update-alternatives --help' for program usage information.   
</code></pre>

<p>I'm new to Ubuntu and Idon't know what I'm doing wrong.</p>
","4093585","","","","","2020-10-20 11:20:44","Unable to set default python version to python3 in ubuntu","<python><python-3.x><ubuntu><installation><ubuntu-16.04>","18","6","49","","","CC BY-SA 3.0","0"
"41276067","1","41276151","","2016-12-22 05:02:47","","107","317911","<p><strong>Before you mark it as  duplicate</strong> please read my problem:</p>
<p>I am trying to import a class from a file from a subdirectory</p>
<pre><code>&gt; main.py
&gt; ---&gt;folder/
&gt; -----&gt;file.py
</code></pre>
<p>and in <code>file.py</code> i have a class imlpemented ( <code>Klasa</code>)
What have I tried:</p>
<p>putting in main.py:</p>
<pre><code>from folder import file
from file import Klasa
</code></pre>
<p>I am getting the error:</p>
<blockquote>
<p>from file import Klasa</p>
<p>ImportError: No module named 'file'</p>
</blockquote>
<p>When I try to use just:</p>
<pre><code>from folder import file
</code></pre>
<p>I get this error:</p>
<blockquote>
<p>tmp = Klasa()</p>
<p>NameError: name 'Klasa' is not defined</p>
</blockquote>
<p>I have put an empty <code>__init__.py</code> in the subfolder and it still does not work, and I have put in the <code>__init__.py</code>  : <code>from file import Klasa</code> and still doesnt work.</p>
<p>If main and file are in the same folder this work:</p>
<p><code>from file import Klasa</code></p>
<p>but i want them to be in separate files.</p>
<p>Can someone tell me what i am doing wrong?</p>
","6337767","","-1","","2020-06-20 09:12:55","2016-12-22 05:16:04","Importing class from another file","<python><python-3.x>","1","0","22","2016-12-22 05:28:54","","CC BY-SA 3.0","0"
"38613316","1","","","2016-07-27 12:42:32","","74","312174","<p>I want to use python3.5 to develop basically, but many times when I install the module for the python 3.5, it always failed. And the terminal told me that higher version is available, it did not work when I upgrade it.
<img src=""https://i.stack.imgur.com/75IDN.png"" alt=""enter image description here""></p>
","5617121","","10679649","","2019-09-25 22:50:11","2020-08-19 18:06:37","How to upgrade pip3?","<python><linux><python-3.x>","10","5","26","","","CC BY-SA 4.0","0"
"30492623","1","30493051","","2015-05-27 20:34:41","","259","303855","<p>I use IPython notebooks and would like to be able to select to create a 2.x or 3.x python notebook in IPython.</p>

<p>I initially had Anaconda.  With Anaconda a global environment variable had to be changed to select what version of python you want and then IPython could be started.  This is not what I was looking for so I uninstalled Anaconda and now have set up my own installation using MacPorts and PiP.  It seems that I still have to use</p>

<pre><code>port select --set python &lt;python version&gt; 
</code></pre>

<p>to toggle between python 2.x and 3.x. which is no better than the anaconda solution.</p>

<p>Is there a way to select what version of python you want to use after you start an IPython notebook, preferably with my current MacPorts build?</p>
","3749393","","137794","","2015-10-09 18:24:08","2019-11-07 12:54:39","Using both Python 2.x and Python 3.x in IPython Notebook","<python><python-2.7><python-3.x><ipython><ipython-notebook>","11","1","210","","","CC BY-SA 3.0","0"
"34573159","1","","","2016-01-03 03:51:46","","161","289930","<p>I'm trying to install pip3, but I'm not having any luck. Also, I tried <code>sudo install</code> and it did not work. How could I install pip3 on my Mac?</p>

<pre class=""lang-none prettyprint-override""><code>sudo easy_install pip3
Password:
Searching for pip3
Reading https://pypi.python.org/simple/pip3/
Couldn't find index page for 'pip3' (maybe misspelled?)
Scanning index of all packages (this may take a while)
Reading https://pypi.python.org/simple/

No local packages or download links found for pip3
error: Could not find suitable distribution for Requirement.parse('pip3')
</code></pre>
","3697597","","2911458","","2018-05-28 21:13:20","2020-01-07 12:56:55","How to install pip3 on my Mac?","<python><python-3.x><pip>","11","4","28","","","CC BY-SA 4.0","0"
"41501636","1","41501815","","2017-01-06 08:19:35","","47","277941","<p>I use python 2.7, 3.5.2 and 3.6. Normally <code>pip3</code> is installed with python3.x. But ...</p>

<p>The code:</p>

<pre><code>where pip3
</code></pre>

<p>gives me:</p>

<pre><code>INFO: Could not find files for the given pattern(s).
</code></pre>

<p>How can I fix this?</p>
","237934","","6782707","","2020-05-25 06:14:22","2020-06-22 12:29:36","How to install pip3 on Windows?","<python><windows><python-3.x><pip>","3","1","4","","","CC BY-SA 3.0","0"
"31058055","1","31060836","","2015-06-25 18:29:14","","121","272642","<p>I have read in an XML email attachment with</p>

<pre><code>bytes_string=part.get_payload(decode=False)
</code></pre>

<p>The payload comes in as a byte string, as my variable name suggests.</p>

<p>I am trying to use the recommended Python 3 approach to turn this string into a usable string that I can manipulate.</p>

<p>The example shows:</p>

<blockquote>
<pre><code>str(b'abc','utf-8')
</code></pre>
</blockquote>

<p>How can I apply the <code>b</code> (bytes) keyword argument to my variable <code>bytes_string</code> and use the recommended approach?</p>

<p>The way I tried doesn't work:</p>

<pre><code>str(bbytes_string, 'utf-8')
</code></pre>
","3842512","","1013393","","2020-09-22 11:35:34","2020-10-11 06:17:38","How do I convert a Python 3 byte-string variable into a regular string?","<python-3.x><string><type-conversion>","4","1","22","2020-10-22 21:18:23","","CC BY-SA 3.0","0"
"28190534","1","29860484","","2015-01-28 11:00:07","","170","253000","<p>I am trying to install python and a series of packages onto a 64bit windows 7 desktop. I have installed Python 3.4, have Microsoft Visual Studio C++ installed, and have successfully installed numpy, pandas and a few others. I am getting the following error when trying to install scipy;</p>

<pre><code>numpy.distutils.system_info.NotFoundError: no lapack/blas resources found
</code></pre>

<p>I am using pip install offline, the install command I am using is;</p>

<pre><code>pip install --no-index --find-links=""S:\python\scipy 0.15.0"" scipy
</code></pre>

<p>I have read the posts on here about requiring a compiler which if I understand correctly is the VS C++ compiler. I am using the 2010 version as I am using Python 3.4. This has worked for other packages.</p>

<p>Do I have to use the window binary or is there a way I can get pip install to work?</p>

<p>Many thanks for the help</p>
","4402407","","660921","","2016-12-17 03:07:38","2020-10-26 13:20:28","Windows Scipy Install: No Lapack/Blas Resources Found","<python><windows><python-3.x><numpy><pip>","16","6","49","","","CC BY-SA 3.0","0"
"52584907","1","","","2018-10-01 05:14:54","","85","252277","<p>I'm trying to install tensorflow but it needs a Python 3.6 installation and I only have Python 3.7 installed. I tried to switch using brew and pyenv but it doesn't work. </p>

<p>Does anyone know of a way to solve this problem?</p>
","10439149","","9805238","","2018-12-21 15:28:21","2020-07-14 10:22:12","How to downgrade python from 3.7 to 3.6","<python><python-3.x>","8","3","18","","","CC BY-SA 4.0","0"
"30362600","1","30362669","","2015-05-21 00:29:53","","48","247334","<p>I have both Python 2.7 and 3.4 installed on my Ubuntu 14.04 machine.  I want to install the 'requests' module so it is accessible from Py3.4.</p>

<p>When I issued <code>pip install requests</code> on my terminal cmd line I got back:</p>

<blockquote>
  <p>""Requirement already satisfied (use --upgrade to upgrade): requests in /usr/lib/python2.7/dist-packages""</p>
</blockquote>

<p>How can I direct pip to install requests for 3.4 even though it is already in 2.7?</p>
","1955720","","1307166","","2015-05-21 01:14:17","2020-07-19 03:01:05","How to install requests module in Python 3.4, instead of 2.7","<python-3.x><pip><python-requests>","5","0","14","","","CC BY-SA 3.0","0"
"44597662","1","44597801","","2017-06-16 20:40:54","","109","246821","<p>I installed Anaconda 4.4.0 (Python 3.6 version) on Windows 10 by following the instructions here: <a href=""https://www.continuum.io/downloads"" rel=""noreferrer"">https://www.continuum.io/downloads</a>. However, when I open the Command prompt window and try to write </p>

<pre><code>conda list
</code></pre>

<p>I get the </p>

<blockquote>
  <p>'conda' command is not recognized...</p>
</blockquote>

<p>error. </p>

<p>I tried to run</p>

<pre><code>set PATH=%PATH%;C:\Users\Alex\Anaconda3
</code></pre>

<p>but it didn't help. I also read that I might need to edit my <code>.bashrc</code> file, but I don't know how to access this file, and how I should edit it. </p>
","8173713","","6214491","","2018-08-19 00:33:51","2019-11-01 16:35:30","Conda command is not recognized on Windows 10","<python><python-3.x><windows-10><anaconda><conda>","11","6","39","","","CC BY-SA 4.0","0"
"29558007","1","29558077","","2015-04-10 09:34:50","","69","228331","<p>Say if you had a number input <code>8</code> in python and you wanted to generate a list of consecutive numbers up to <code>8</code> like</p>

<pre><code>[0, 1, 2, 3, 4, 5, 6, 7, 8]
</code></pre>

<p>How could you do this?</p>
","4773048","","1903116","","2015-04-10 14:56:54","2020-09-07 16:36:53","How can I generate a list of consecutive numbers?","<python><list><python-3.x>","7","2","12","","","CC BY-SA 3.0","0"
"42988977","1","42989020","","2017-03-23 23:44:40","","210","223669","<p>From <code>pip install --help</code>:</p>

<pre><code> --user      Install to the Python user install directory for your platform. Typically ~/.local/, or %APPDATA%\Python on
             Windows. (See the Python documentation for site.USER_BASE for full details.)
</code></pre>

<p>The documentation for site.USER_BASE is a terrifying wormhole of interesting *NIX subject matter that I don't understand.  </p>

<p><strong>What is the purpose of <code>--user</code> in plain english?</strong> Why would intalling the package to <code>~/.local/</code> matter? Why not just put an executable somewhere in my $PATH?</p>
","6293857","","676001","","2020-04-11 08:03:50","2020-06-29 20:15:57","What is the purpose of ""pip install --user ...""?","<python><python-3.x><pip><virtualenv>","8","2","50","","","CC BY-SA 4.0","0"
"36965507","1","36965528","","2016-05-01 10:19:24","","86","221145","<p>I have a dictionary and am trying to write it to a file.</p>

<pre><code>exDict = {1:1, 2:2, 3:3}
with open('file.txt', 'r') as file:
    file.write(exDict)
</code></pre>

<p>I then have the error</p>

<pre class=""lang-none prettyprint-override""><code>file.write(exDict)
TypeError: must be str, not dict
</code></pre>

<p>So I fixed that error but another error came</p>

<pre><code>exDict = {111:111, 222:222}
with open('file.txt', 'r') as file:
    file.write(str(exDict))
</code></pre>

<p>The error:</p>

<pre class=""lang-none prettyprint-override""><code>file.write(str(exDict))
io.UnsupportedOperation: not writable
</code></pre>

<p>I have no idea what to do as I am still a beginner at python.
If anyone knows how to resolve the issue, please provide an answer.</p>

<p>NOTE: I am using python 3, not python 2</p>
","6277504","","3656904","","2019-01-11 08:28:43","2020-09-04 16:03:16","Writing a dictionary to a text file?","<python-3.x><file><dictionary><file-writing>","10","0","23","","","CC BY-SA 4.0","0"
"36434764","1","","","2016-04-05 18:54:22","","29","218271","<p>I'm getting this error :</p>

<pre><code>Exception in Tkinter callback
Traceback (most recent call last):
File ""C:\Python34\lib\tkinter\__init__.py"", line 1538, in __call__
return self.func(*args)
File ""C:/Users/Marc/Documents/Programmation/Python/Llamachat/Llamachat/Llamachat.py"", line 32, in download
with open(place_to_save, 'wb') as file:
PermissionError: [Errno 13] Permission denied: '/goodbye.txt'
</code></pre>

<p>When running this :</p>

<pre><code>def download():
    # get selected line index
    index = films_list.curselection()[0]
    # get the line's text
    selected_text = films_list.get(index)
    directory = filedialog.askdirectory(parent=root, 
                                        title=""Choose where to save your movie"")
    place_to_save = directory + '/' + selected_text
    print(directory, selected_text, place_to_save)
    with open(place_to_save, 'wb') as file:
        connect.retrbinary('RETR ' + selected_text, file.write)
    tk.messagebox.showwarning('File downloaded', 
                              'Your movie has been successfully downloaded!' 
                              '\nAnd saved where you asked us to save it!!')
</code></pre>

<p>Can someone tell me what I am doing wrong ?
Thanks</p>

<p>Specs :
Python 3.4.4 x86
Windows 10 x64</p>
","5930384","","344286","","2016-04-05 20:05:07","2020-09-14 10:11:58","PermissionError: [Errno 13] Permission denied","<windows><python-3.x><tkinter><permission-denied>","12","10","10","","","CC BY-SA 3.0","0"
"30081275","1","30081318","","2015-05-06 15:32:43","","2223","216912","<p>It is my understanding that the <code>range()</code> function, which is actually <a href=""https://docs.python.org/3/library/stdtypes.html#typesseq-range"" rel=""noreferrer"">an object type in Python 3</a>, generates its contents on the fly, similar to a generator. </p>

<p>This being the case, I would have expected the following line to take an inordinate amount of time, because in order to determine whether 1 quadrillion is in the range, a quadrillion values would have to be generated: </p>

<pre><code>1000000000000000 in range(1000000000000001)
</code></pre>

<p>Furthermore: it seems that no matter how many zeroes I add on, the calculation more or less takes the same amount of time (basically instantaneous). </p>

<p>I have also tried things like this, but the calculation is still almost instant: </p>

<pre><code>1000000000000000000000 in range(0,1000000000000000000001,10) # count by tens
</code></pre>

<p>If I try to implement my own range function, the result is not so nice!! </p>

<pre><code>def my_crappy_range(N):
    i = 0
    while i &lt; N:
        yield i
        i += 1
    return
</code></pre>

<p>What is the <code>range()</code> object doing under the hood that makes it so fast? </p>

<hr>

<p><a href=""https://stackoverflow.com/a/30081318/2437514"">Martijn Pieters' answer</a> was chosen for its completeness, but also see <a href=""https://stackoverflow.com/a/30081894/2437514"">abarnert's first answer</a> for a good discussion of what it means for <code>range</code> to be a full-fledged <em>sequence</em> in Python 3, and some information/warning regarding potential inconsistency for <code>__contains__</code> function optimization across Python implementations. <a href=""https://stackoverflow.com/a/30088140/2437514"">abarnert's other answer</a> goes into some more detail and provides links for those interested in the history behind the optimization in Python 3 (and lack of optimization of <code>xrange</code> in Python 2). Answers <a href=""https://stackoverflow.com/a/30081467/2437514"">by poke</a> and <a href=""https://stackoverflow.com/a/30081470/2437514"">by wim</a> provide the relevant C source code and explanations for those who are interested. </p>
","2437514","","2437514","","2018-02-22 19:43:05","2020-10-18 18:42:45","Why is ""1000000000000000 in range(1000000000000001)"" so fast in Python 3?","<python><performance><python-3.x><range><python-internals>","11","13","436","","","CC BY-SA 3.0","0"
"38132755","1","39097003","","2016-06-30 20:33:45","","114","213691","<p>I recently reinstalled ubuntu and did upgrade to 16.04 and cannot use python:</p>

<pre><code>$ python manage.py runserver
Could not find platform independent libraries &lt;prefix&gt;
Could not find platform dependent libraries &lt;exec_prefix&gt;
Consider setting $PYTHONHOME to &lt;prefix&gt;[:&lt;exec_prefix&gt;]
Fatal Python error: Py_Initialize: Unable to get the locale encoding
ImportError: No module named 'encodings'
Aborted
</code></pre>

<p>At this point, python itself doesn't work</p>

<pre><code>$ python
Could not find platform independent libraries &lt;prefix&gt;
Could not find platform dependent libraries &lt;exec_prefix&gt;
Consider setting $PYTHONHOME to &lt;prefix&gt;[:&lt;exec_prefix&gt;]
Fatal Python error: Py_Initialize: Unable to get the locale encoding
ImportError: No module named 'encodings'
Aborted
</code></pre>

<p>Even this suggestion is no longer working:</p>

<pre><code>unset PYTHONHOME
unset PYTHONPATH
</code></pre>

<p>Every every I fix it one way, it comes back again. Several answers help to fix it temporarily, but not for good. I've reinstalled python and python3 several times. What can I do from here? Thank you</p>
","3282434","","953553","","2019-10-08 12:49:06","2020-08-10 13:21:55","ImportError: No module named 'encodings'","<python><python-3.x><virtualenv><development-environment><macos-catalina>","13","7","15","","","CC BY-SA 3.0","0"
"38836795","1","38838589","","2016-08-08 19:09:01","","32","211458","<p>I having trouble passing a function as a parameter to another function. This is my code:</p>

<p><strong>ga.py:</strong></p>

<pre><code>def display_pageviews(hostname):
    pageviews_results = get_pageviews_query(service, hostname).execute()
    if pageviews_results.get('rows', []):
        pv = pageviews_results.get('rows')
        return pv[0]
    else:
        return None


def get_pageviews_query(service, hostname):  
    return service.data().ga().get(
        ids=VIEW_ID,
        start_date='7daysAgo',
        end_date='today',
        metrics='ga:pageviews',
        sort='-ga:pageviews',
        filters='ga:hostname==%s' % hostname,)
</code></pre>

<p><strong>models.py:</strong></p>

<pre><code>class Stats(models.Model):
    user = models.OneToOneField('auth.User')
    views = models.IntegerField()
    visits = models.IntegerField()
    unique_visits = models.IntegerField()
</code></pre>

<p><strong>updatestats.py:</strong></p>

<pre><code>class Command(BaseCommand):

    def handle(self, *args, **options):
        users = User.objects.all()
        try:
            for user in users:
                hostname = '%s.%s' % (user.username, settings.NETWORK_DOMAIN)
                stats = Stats.objects.update_or_create(
                    user=user,
                    views=display_pageviews(hostname),
                    visits=display_visits(hostname),
                    unique_visits=display_unique_visits(hostname),)
        except FieldError:
            print ('There was a field error.')
</code></pre>

<p>When I run this: <code>python manage.py updatestats</code> I get the error:</p>

<blockquote>
  <p>TypeError: int() argument must be a string, a bytes-like object or a
  number, not 'list'</p>
</blockquote>

<p>I don't know what's causing this. I've tried converting it to a string, but I get the same error. Any ideas?</p>

<p><strong>Full traceback:</strong></p>

<pre><code>Traceback (most recent call last):
  File ""manage.py"", line 20, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/core/management/__init__.py"", line 353, in execute_from_command_line
    utility.execute()
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/core/management/__init__.py"", line 345, in execute
    self.fetch_command(subcommand).run_from_argv(self.argv)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/core/management/base.py"", line 348, in run_from_argv
    self.execute(*args, **cmd_options)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/core/management/base.py"", line 399, in execute
    output = self.handle(*args, **options)
  File ""/Users/myusername/project/Dev/project_files/project/main/management/commands/updatestats.py"", line 23, in handle
    unique_visits=display_unique_visits(hostname),)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/manager.py"", line 122, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/query.py"", line 480, in update_or_create
    obj = self.get(**lookup)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/query.py"", line 378, in get
    clone = self.filter(*args, **kwargs)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/query.py"", line 790, in filter
    return self._filter_or_exclude(False, *args, **kwargs)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/query.py"", line 808, in _filter_or_exclude
    clone.query.add_q(Q(*args, **kwargs))
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/sql/query.py"", line 1243, in add_q
    clause, _ = self._add_q(q_object, self.used_aliases)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/sql/query.py"", line 1269, in _add_q
    allow_joins=allow_joins, split_subq=split_subq,
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/sql/query.py"", line 1203, in build_filter
    condition = self.build_lookup(lookups, col, value)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/sql/query.py"", line 1099, in build_lookup
    return final_lookup(lhs, rhs)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/lookups.py"", line 19, in __init__
    self.rhs = self.get_prep_lookup()
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/lookups.py"", line 57, in get_prep_lookup
    return self.lhs.output_field.get_prep_lookup(self.lookup_name, self.rhs)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/fields/__init__.py"", line 1860, in get_prep_lookup
    return super(IntegerField, self).get_prep_lookup(lookup_type, value)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/fields/__init__.py"", line 744, in get_prep_lookup
    return self.get_prep_value(value)
  File ""/Users/myusername/project/Dev/lib/python3.4/site-packages/django/db/models/fields/__init__.py"", line 1854, in get_prep_value
    return int(value)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'list'
</code></pre>

<p>Edit:</p>

<p>Alright, I understand what the issue is. I used the shell to get the type of function output:</p>

<pre><code>&gt;&gt;&gt; type(display_pageviews('test.domain.com'))
&lt;class 'list'&gt;
</code></pre>

<p><strong>I tried with this but it is still considered as a list:</strong></p>

<pre><code>pv = pageviews_results.get('rows')[0]
    return pv
</code></pre>
","6660913","","6660913","","2016-08-08 19:24:14","2019-07-04 05:18:37","TypeError: int() argument must be a string, a bytes-like object or a number, not 'list'","<python><django><python-3.x>","1","4","4","","","CC BY-SA 3.0","0"
"45554008","1","45554153","","2017-08-07 19:02:14","","79","210056","<p>I'm following <a href=""https://pythonprogramming.net/linear-svc-example-scikit-learn-svm-python"" rel=""noreferrer"">this tutorial</a> to make this ML prediction:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style

style.use(""ggplot"")
from sklearn import svm

x = [1, 5, 1.5, 8, 1, 9]
y = [2, 8, 1.8, 8, 0.6, 11]

plt.scatter(x,y)
plt.show()

X = np.array([[1,2],
             [5,8],
             [1.5,1.8],
             [8,8],
             [1,0.6],
             [9,11]])

y = [0,1,0,1,0,1]
X.reshape(1, -1)

clf = svm.SVC(kernel='linear', C = 1.0)
clf.fit(X,y)

print(clf.predict([0.58,0.76]))
</code></pre>

<p>I'm using Python 3.6 and I get error ""Expected 2D array, got 1D array instead:""
I think the script is for older versions, but I don't know how to convert it to the 3.6 version.</p>

<p>Already try with the:</p>

<pre><code>X.reshape(1, -1)
</code></pre>
","8430537","","6296561","","2019-04-28 10:49:57","2019-10-16 20:54:50","Error in Python script ""Expected 2D array, got 1D array instead:""?","<python><python-3.x><machine-learning><predict>","9","9","13","","","CC BY-SA 4.0","0"
"37220055","1","37228413","","2016-05-13 22:09:49","","86","205753","<p>I installed python 3.5.1 via ampps and it's working. However, when i try to use pip, i get the following message:</p>

<pre><code>Fatal error in launcher: Unable to create process using '""'
</code></pre>

<p>I already reinstalled ampps into a path which doesn't include any whitespaces. Note that the ""python -m pip"" workaround doesn't work for me too, since i get the following message everytime i use it:</p>

<pre><code>C:\Users\MyUserName\Desktop\Ampps\python\python.exe: Error while finding spec for 'pip.__main__' (&lt;class 'ImportError'&gt;: No module named 'queue'); 'pip' is a package and cannot be directly executed
</code></pre>

<p>How do i get pip to work properly? I hope, there is a way to use the pip command itself without the preceding python command.</p>

<p>EDIT: This is what happens, if i try to run <code>python -c ""import pip.__main__""</code>:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\MyUserName\Desktop\Ampps\python\lib\site-packages\pip\compat\__init__.py"", line 11, in &lt;module&gt;
    from logging.config import dictConfig as logging_dictConfig
  File ""C:\Users\MyUserName\Desktop\Ampps\python\lib\logging\config.py"", line 30, in &lt;module&gt;
    import logging.handlers
  File ""C:\Users\MyUserName\Desktop\Ampps\python\lib\logging\handlers.py"", line 28, in &lt;module&gt;
    import queue
ImportError: No module named 'queue'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
  File ""C:\Users\MyUserName\Desktop\Ampps\python\lib\site-packages\pip\__init__.py"", line 13, in &lt;module&gt;
    from pip.utils import get_installed_distributions, get_prog
  File ""C:\Users\MyUserName\Desktop\Ampps\python\lib\site-packages\pip\utils\__init__.py"", line 18, in &lt;module&gt;
    from pip.compat import console_to_str, stdlib_pkgs
  File ""C:\Users\MyUserName\Desktop\Ampps\python\lib\site-packages\pip\compat\__init__.py"", line 13, in &lt;module&gt;
    from pip.compat.dictconfig import dictConfig as logging_dictConfig
  File ""C:\Users\MyUserName\Desktop\Ampps\python\lib\site-packages\pip\compat\dictconfig.py"", line 22, in &lt;module&gt;
    import logging.handlers
  File ""C:\Users\MyUserName\Desktop\Ampps\python\lib\logging\handlers.py"", line 28, in &lt;module&gt;
    import queue
ImportError: No module named 'queue'
</code></pre>
","6312338","","6312338","","2016-05-14 08:39:20","2020-05-20 07:31:22","Pip - Fatal error in launcher: Unable to create process using '""'","<python><windows><python-3.x><pip><ampps>","31","7","27","","","CC BY-SA 3.0","0"
"34283178","1","34283957","","2015-12-15 07:20:35","","179","202551","<blockquote>
  <p>TypeError: a bytes-like object is required, not 'str' </p>
</blockquote>

<p>getting above error while Executing below python code to save the HTML table data in Csv file. don't know how to get rideup.pls help me.</p>

<pre><code>import csv
import requests
from bs4 import BeautifulSoup

url='http://www.mapsofindia.com/districts-india/'
response=requests.get(url)
html=response.content

soup=BeautifulSoup(html,'html.parser')
table=soup.find('table', attrs={'class':'tableizer-table'})
list_of_rows=[]
for row in table.findAll('tr')[1:]:
    list_of_cells=[]
    for cell in row.findAll('td'):
        list_of_cells.append(cell.text)
    list_of_rows.append(list_of_cells)
outfile=open('./immates.csv','wb')
writer=csv.writer(outfile)
writer.writerow([""SNo"", ""States"", ""Dist"", ""Population""])
writer.writerows(list_of_rows)
</code></pre>

<p>on above the last line.</p>
","2846920","","4197269","","2020-08-10 13:35:52","2020-08-10 13:35:52","TypeError: a bytes-like object is required, not 'str' in python and CSV","<python><python-3.x><csv><beautifulsoup><html-table>","5","2","25","","","CC BY-SA 3.0","0"
"50997928","1","50997969","","2018-06-23 04:20:29","","54","192677","<p>I want to write a function that randomly picks elements from a training set, based on the <strong>bin probabilities</strong> provided. I <strong>divide the set indices to 11 bins</strong>, then create <strong>custom probabilities</strong> for them.</p>

<pre><code>bin_probs = [0.5, 0.3, 0.15, 0.04, 0.0025, 0.0025, 0.001, 0.001, 0.001, 0.001, 0.001]

X_train = list(range(2000000))

train_probs = bin_probs * int(len(X_train) / len(bin_probs)) # extend probabilities across bin elements
train_probs.extend([0.001]*(len(X_train) - len(train_probs))) # a small fix to match number of elements
train_probs = train_probs/np.sum(train_probs) # normalize
indices = np.random.choice(range(len(X_train)), replace=False, size=50000, p=train_probs)
out_images = X_train[indices.astype(int)] # this is where I get the error
</code></pre>

<p>I get the following error:</p>

<pre><code>TypeError: only integer scalar arrays can be converted to a scalar index with 1D numpy indices array
</code></pre>

<p>I find this weird, since I already checked the array of indices that I have created. It is <strong>1-D</strong>, it is <strong>integer</strong>, and it is <strong>scalar</strong>.</p>

<p>What am I missing?</p>

<p>Note : I tried to pass <code>indices</code> with <code>astype(int)</code>. Same error.</p>
","776348","","776348","","2020-05-15 10:55:16","2020-07-17 15:20:19","TypeError: only integer scalar arrays can be converted to a scalar index with 1D numpy indices array","<python><python-3.x><numpy>","3","0","5","","","CC BY-SA 4.0","0"
"34009653","1","34009711","","2015-11-30 23:04:29","","82","192282","<p>I'm currently working on an encryption/decryption program and I need to be able to convert bytes to an integer. I know that:</p>

<pre><code>bytes([3]) = b'\x03'
</code></pre>

<p>Yet I cannot find out how to do the inverse. What am I doing terribly wrong?</p>
","5129275","","6243352","","2020-03-23 03:54:43","2020-05-07 17:16:48","Convert bytes to int?","<python><python-3.x><int><type-conversion><byte>","3","2","12","","","CC BY-SA 4.0","0"
"44841470","1","","","2017-06-30 07:56:26","","35","189672","<p>Anaconda (listed as ""Python 3.6.0 (Anaconda 4.3.1 64 bit)"" ) is in my programs and features list, but there is seeming <strong>no Anaconda Navigator desktop app, as their seems to be no icon on my desktop and I am unable to search for it through ""Start""</strong>. Could this be because I have the 32-bit version of Anaconda downloaded and I have a 64-bit OS (I thought I should do this because Python on my computer was 64-bit) or because I downloaded Anaconda under ""users"" instead of Desktop. I also downloaded Anaconda twice, if that could be causing some of the problem. I have a Windows 10 laptop, if that is any help.</p>
","8232942","","8232942","","2017-06-30 10:50:49","2020-03-15 10:34:56","Anaconda Installed but Cannot Launch Navigator","<python><windows><python-3.x><windows-10><anaconda>","24","7","14","","","CC BY-SA 3.0","0"
"30170468","1","30170469","","2015-05-11 14:33:47","","102","186210","<p>I have been using Spyder installed with with Anaconda distribution which uses Python 2.7 as default. Currently I need to set up a development virtual environment with Python 3.4. </p>

<p>Top two suggestions after research online are:</p>

<ol>
<li>to set up virtual environment first and to point change the preferences of Spyder , e.g <a href=""https://stackoverflow.com/questions/28190500/virtualenv-ipython-in-spyder-not-working"">here</a>; </li>
<li>to install all Spyder dependencies, like PyQt4, in the virtual environment itself, e. g. <a href=""https://stackoverflow.com/q/28190500/3052217"">here</a> ;</li>
</ol>

<p>Both recommendations are cumbersome and do not look like smart options for development.</p>

<p>Is there a solution that would allow to run Spyder with required Python version automatically after activating the required virtual environment?</p>
","3052217","","-1","","2017-05-23 11:54:46","2020-10-26 07:35:26","How to run Spyder in virtual environment?","<python><python-3.x><virtualenv><anaconda><spyder>","9","0","47","","","CC BY-SA 3.0","0"
"33003498","1","","","2015-10-07 22:29:22","","63","185520","<p>The following is the code that tries to modify the input supplied by a user by using sockets:</p>

<pre><code>from socket import *

serverName = '127.0.0.1'
serverPort = 12000
clientSocket = socket(AF_INET, SOCK_DGRAM)
message = input('Input lowercase sentence:')
clientSocket.sendto(message,(serverName, serverPort))
modifiedMessage, serverAddress = clientSocket.recvfrom(2048)
print (modifiedMessage)
clientSocket.close()
</code></pre>

<p>When I execute it and supply input the following error occurs:</p>

<pre><code>Input lowercase sentence:fdsgfdf
Traceback (most recent call last):
  File ""C:\srinath files\NETWORKS\UDPclient.py"", line 6, in &lt;module&gt;
    clientSocket.sendto(message,(serverName, serverPort))
TypeError: a bytes-like object is required, not 'str'
</code></pre>

<p>What can I do to solve this?</p>
","5420593","","229044","","2019-06-12 12:02:13","2019-10-01 15:48:51","TypeError: a bytes-like object is required, not 'str'","<python><python-3.x><sockets>","5","6","13","","","CC BY-SA 3.0","0"
"34952651","1","34953020","","2016-01-22 17:29:13","","46","183432","<p>I am implementing fft as part of my homework. My problem lies in the implemention of shuffling data elements using bit reversal. I get the following warning:</p>
<blockquote>
<p>DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future.</p>
<p>data[x], data[y] = data[y], data[x]</p>
</blockquote>
<p>And the auto grading system (provided by university) returns the following:</p>
<blockquote>
<p>error: only integers, slices (<code>:</code>), ellipsis (<code>...</code>), numpy.newaxis (<code>None</code>) and integer or boolean arrays are valid indices.</p>
</blockquote>
<p>My code is:</p>
<pre><code>def shuffle_bit_reversed_order(data: np.ndarray) -&gt; np.ndarray:
    &quot;&quot;&quot;
    Shuffle elements of data using bit reversal of list index.

    Arguments:
    data: data to be transformed (shape=(n,), dtype='float64')

    Return:
    data: shuffled data array
    &quot;&quot;&quot;

    # implement shuffling by reversing index bits

    size = data.size

    half = size/2;

    for x in range(size):
        xx = np.int(x)
        n = np.int(half)

        y = 0

        while n &gt; 0:
            y += n * np.mod(xx,2)
            n /= 2
            xx = np.int(xx /2)

        if (y &gt; x):

            data[x], data[y] = data[y], data[x]

    return data
</code></pre>
<p>I have already implemented the function for fft but it won't work until I get this shuffling function working. I think the problem is my data is of type 'float64' and I may have used it as an integer but I don't know how I can solve it.</p>
","3612693","","-1","","2020-06-20 09:12:55","2020-09-20 12:52:30","only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices","<python><python-3.x><numpy><fft><dft>","3","1","8","","","CC BY-SA 3.0","0"
"34097988","1","34975902","","2015-12-04 21:44:47","","64","181041","<p>I am trying to work on neural networks in Python using the following Keras packages:</p>

<pre><code>from keras.utils import np_utils
from keras.layers.core import Dense, Activation, Dropout
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.optimizers import SGD
</code></pre>

<p>But, I am getting the following error:</p>

<pre><code> 15 import theano
 ---&gt; 16 from theano import gof
 17 from theano.compat.python2x import partial
 18 import theano.compile.mode
 ImportError: cannot import name gof
</code></pre>

<p>Installing installed <code>conda install keras</code>. Later I tried to use <code>pip install Theano</code>, but it did not work. I Tried to install using <code>pip install git</code>, but I am getting this error: <code>cannot find command git.</code> So I installed Git and I set the environment variables.</p>

<p>So, is there any procedure to install these packages?</p>
","5641188","","400589","","2017-02-24 22:22:55","2019-04-19 09:13:44","How do I install Keras and Theano in Anaconda Python on Windows?","<python-2.7><python-3.x><anaconda><theano><keras>","8","3","56","","","CC BY-SA 3.0","0"
"36394101","1","36394262","","2016-04-04 03:24:45","","239","180848","<p>Full stacktrace:  </p>

<pre><code>➜  ~ pip install virtualenv
Traceback (most recent call last):
  File ""/usr/bin/pip"", line 11, in &lt;module&gt;
    sys.exit(main())
  File ""/usr/lib/python3.4/site-packages/pip/__init__.py"", line 215, in main
    locale.setlocale(locale.LC_ALL, '')
  File ""/usr/lib64/python3.4/locale.py"", line 592, in setlocale
    return _setlocale(category, locale)
locale.Error: unsupported locale setting
</code></pre>

<p>On the same server, I successfully ran <code>pip install virtualenv</code> with python 2.7.x.  </p>

<p>Now, I've just installed python3.4 using <code>curl https://bootstrap.pypa.io/get-pip.py | python3.4</code>.  </p>

<pre><code>➜  ~ pip --version
pip 8.1.1 from /usr/lib/python3.4/site-packages (python 3.4)
</code></pre>

<p><code>pip uninstall virtualenv</code> throws the same error too</p>
","541624","","541624","","2018-04-01 06:31:23","2020-06-09 15:13:53","pip install - locale.Error: unsupported locale setting","<python><python-3.x><centos><pip>","9","4","84","","","CC BY-SA 3.0","0"
"31019854","1","31019855","","2015-06-21 05:24:49","","117","169633","<p>I am trying to learn how to automatically fetch urls from a page. In the following code I am trying to get the title of the webpage:</p>

<pre><code>import urllib.request
import re

url = ""http://www.google.com""
regex = r'&lt;title&gt;(,+?)&lt;/title&gt;'
pattern  = re.compile(regex)

with urllib.request.urlopen(url) as response:
   html = response.read()

title = re.findall(pattern, html)
print(title)
</code></pre>

<p>And I get this unexpected error:</p>

<pre><code>Traceback (most recent call last):
  File ""path\to\file\Crawler.py"", line 11, in &lt;module&gt;
    title = re.findall(pattern, html)
  File ""C:\Python33\lib\re.py"", line 201, in findall
    return _compile(pattern, flags).findall(string)
TypeError: can't use a string pattern on a bytes-like object
</code></pre>

<p>What am I doing wrong?</p>
","4949315","Inspired_Blue","1222951","","2019-04-09 11:29:12","2020-05-10 23:55:37","TypeError: can't use a string pattern on a bytes-like object in re.findall()","<python><python-3.x><web-crawler>","2","1","17","","","CC BY-SA 4.0","0"
"39179948","1","39180068","","2016-08-27 10:13:59","","65","169520","<p>I have a script that requires PIL to run. Other than downgrading my Python, I couldn't find anyway to install PIL on my Python 3.6</p>

<p>Here are my attempts:</p>

<pre><code>pip install pil
Collecting pil
  Could not find a version that satisfies the requirement pil (from versions: )
No matching distribution found for pil

pip install Pillow
Collecting Pillow
  Using cached Pillow-3.3.1.zip
Installing collected packages: Pillow
  Running setup.py install for Pillow ... error
    Complete output from command c:\python\python36\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\ABDULR~1\\AppData\\Local\\Temp\\pip-build-rez5zpri\\Pillow\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record C:\Users\ABDULR~1\AppData\Local\Temp\pip-a5bugnjo-record\install-record.txt --single-version-externally-managed --compile:
    Single threaded build for windows
    running install
    running build
    running build_py
    creating build
    creating build\lib.win-amd64-3.6
    creating build\lib.win-amd64-3.6\PIL
    copying PIL\......................
    ..................................
    ..................................
    running egg_info
    writing Pillow.egg-info\PKG-INFO
    writing dependency_links to Pillow.egg-info\dependency_links.txt
    writing top-level names to Pillow.egg-info\top_level.txt
    warning: manifest_maker: standard file '-c' not found

    reading manifest file 'Pillow.egg-info\SOURCES.txt'
    reading manifest template 'MANIFEST.in'
    warning: no files found matching '*.sh'
    no previously-included directories found matching 'docs\_static'
    warning: no previously-included files found matching '.coveragerc'
    warning: no previously-included files found matching '.editorconfig'
    warning: no previously-included files found matching '.landscape.yaml'
    warning: no previously-included files found matching 'appveyor.yml'
    warning: no previously-included files found matching 'build_children.sh'
    warning: no previously-included files found matching 'tox.ini'
    warning: no previously-included files matching '.git*' found anywhere in distribution
    warning: no previously-included files matching '*.pyc' found anywhere in distribution
    warning: no previously-included files matching '*.so' found anywhere in distribution
    writing manifest file 'Pillow.egg-info\SOURCES.txt'
    copying PIL\OleFileIO-README.md -&gt; build\lib.win-amd64-3.6\PIL
    running build_ext
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""C:\Users\ABDULR~1\AppData\Local\Temp\pip-build-rez5zpri\Pillow\setup.py"", line 753, in &lt;module&gt;
        zip_safe=not debug_build(), )
      File ""c:\python\python36\lib\distutils\core.py"", line 148, in setup
        dist.run_commands()
      File ""c:\python\python36\lib\distutils\dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""c:\python\python36\lib\distutils\dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""c:\python\python36\lib\site-packages\setuptools\command\install.py"", line 61, in run
        return orig.install.run(self)
      File ""c:\python\python36\lib\distutils\command\install.py"", line 539, in run
        self.run_command('build')
      File ""c:\python\python36\lib\distutils\cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""c:\python\python36\lib\distutils\dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""c:\python\python36\lib\distutils\command\build.py"", line 135, in run
        self.run_command(cmd_name)
      File ""c:\python\python36\lib\distutils\cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""c:\python\python36\lib\distutils\dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""c:\python\python36\lib\distutils\command\build_ext.py"", line 338, in run
        self.build_extensions()
      File ""C:\Users\ABDULR~1\AppData\Local\Temp\pip-build-rez5zpri\Pillow\setup.py"", line 521, in build_extensions
        ' using --disable-%s, aborting' % (f, f))
    ValueError: zlib is required unless explicitly disabled using --disable-zlib, aborting

    ----------------------------------------
Command ""c:\python\python36\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\ABDULR~1\\AppData\\Local\\Temp\\pip-build-rez5zpri\\Pillow\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record C:\Users\ABDULR~1\AppData\Local\Temp\pip-a5bugnjo-record\install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in C:\Users\ABDULR~1\AppData\Local\Temp\pip-build-rez5zpri\Pillow\
</code></pre>

<p>Didn't know to add argument <code>--disable-zlib</code>, <code>pip install Pillow --disable-zlib</code> wasn't correct. </p>

<p>Couldn't find what matches my system here: <a href=""https://pypi.python.org/pypi/Pillow/3.0.0"" rel=""noreferrer"">https://pypi.python.org/pypi/Pillow/3.0.0</a></p>

<p>64-bit Windows 10 &amp; Python 3.6</p>
","6580128","","1954886","","2016-08-27 10:59:55","2018-01-01 00:26:42","How do I install PIL/Pillow for Python 3.6?","<python-3.x><python-imaging-library><pillow>","3","0","9","","","CC BY-SA 3.0","0"
"42612002","1","42612820","","2017-03-05 18:05:41","","29","166992","<p>I am trying to create a program that will open a port on the local machine and let others connect into it via netcat. My current code is.</p>

<pre><code>s = socket.socket()
host = '127.0.0.1'
port = 12345
s.bind((host, port))

s.listen(5)
while True:
    c, addr = s.accept()
    print('Got connection from', addr)
    c.send('Thank you for connecting')
    c.close()
</code></pre>

<p>I am new to Python and sockets. But when I run this code it will allow me to send a netcat connection with the command:</p>

<pre><code>nc 127.0.0.1 12345
</code></pre>

<p>But then on my Python script I get the error for the c.send:</p>

<pre><code>TypeError: a bytes-like object is required, not 'str'
</code></pre>

<p>I am basically just trying to open a port, allow netcat to connect and have a full shell on that machine.</p>
","6101406","","","","","2020-05-20 11:19:58","Python sockets error TypeError: a bytes-like object is required, not 'str' with send function","<python><python-3.x>","4","2","3","","","CC BY-SA 3.0","0"
"30227466","1","30228308","","2015-05-14 00:36:10","","124","166766","<p>I am trying to horizontally combine some JPEG images in Python.</p>
<h2>Problem</h2>
<p>I have 3 images - each is 148 x 95 - see attached. I just made 3 copies of the same image - that is why they are the same.</p>
<p><img src=""https://i.stack.imgur.com/KVMyX.jpg"" alt=""enter image description here"" /><img src=""https://i.stack.imgur.com/hlGpu.jpg"" alt=""enter image description here"" /><img src=""https://i.stack.imgur.com/MgAyo.jpg"" alt=""enter image description here"" /></p>
<h2>My attempt</h2>
<p>I am trying to horizontally join them using the following code:</p>
<pre><code>import sys
from PIL import Image

list_im = ['Test1.jpg','Test2.jpg','Test3.jpg']
new_im = Image.new('RGB', (444,95)) #creates a new empty image, RGB mode, and size 444 by 95

for elem in list_im:
    for i in xrange(0,444,95):
        im=Image.open(elem)
        new_im.paste(im, (i,0))
new_im.save('test.jpg')
</code></pre>
<p>However, this is producing the output attached as <code>test.jpg</code>.</p>
<p><img src=""https://i.stack.imgur.com/69hcL.jpg"" alt=""enter image description here"" /></p>
<h2>Question</h2>
<p>Is there a way to horizontally concatenate these images such that the sub-images in test.jpg do not have an extra partial image showing?</p>
<h3>Additional Information</h3>
<p>I am looking for a way to horizontally concatenate n images. I would like to use this code generally so I would prefer to:</p>
<ul>
<li>not to hard-code image dimensions, if possible</li>
<li>specify dimensions in one line so that they can be easily changed</li>
</ul>
","4057186","","4057186","","2020-08-10 16:24:26","2020-08-10 16:24:26","Combine several images horizontally with Python","<python-3.x><python-2.7><python-imaging-library><python-2.x><paste>","11","4","55","","","CC BY-SA 4.0","0"
"28210060","1","28210081","","2015-01-29 08:28:11","","29","166404","<p>Often I am checking if a number variable <code>number</code> has a value with <code>if number</code> but sometimes the number could be zero. So I solve this by <code>if number or number == 0</code>.</p>

<p>Can I do this in a smarter way? I think it's a bit ugly to check if value is zero separately.</p>

<h1>Edit</h1>

<p>I think I could just check if the value is a number with</p>

<pre><code>def is_number(s):
    try:
        int(s)
        return True
    except ValueError:
        return False
</code></pre>

<p>but then I will still need to check with <code>if number and is_number(number)</code>.</p>
","2378710","","","","","2020-08-13 16:32:42","Check if value is zero or not null in python","<python><python-3.x>","4","3","9","","","CC BY-SA 3.0","0"
"43139081","1","43139407","","2017-03-31 11:02:29","","183","165560","<p>I am working on Django project where I need to create a form for inputs. I tried to import <code>reverse</code> from <code>django.core.urlresolvers</code>. I got an error:</p>

<pre><code>line 2, in from django.core.urlresolvers import reverse ImportError: No module named 'django.core.urlresolvers'
</code></pre>

<p>I am using Python 3.5.2, Django 2.0 and MySQL.</p>
","6670668","","1839439","","2020-03-16 11:58:27","2020-10-23 08:28:12","ImportError: No module named 'django.core.urlresolvers'","<django><python-3.x><python-3.5><django-2.0>","10","1","19","","","CC BY-SA 4.0","0"
"27745500","1","28305183","","2015-01-02 16:47:00","","56","155945","<p>Say I have the list score=[1,2,3,4,5] and it gets changed whilst my program is running. How could I save it to a file so that next time the program is run I can access the changed list as a list type? </p>

<p>I have tried:</p>

<pre><code>score=[1,2,3,4,5]

with open(""file.txt"", 'w') as f:
    for s in score:
        f.write(str(s) + '\n')

with open(""file.txt"", 'r') as f:
    score = [line.rstrip('\n') for line in f]


print(score)
</code></pre>

<p>But this results in the elements in the list being strings not integers.</p>
","3704411","","3704411","","2015-01-04 14:29:39","2020-07-24 06:56:19","How to save a list to a file and read it as a list type?","<python><list><file><python-3.x><pickle>","9","1","24","","","CC BY-SA 3.0","0"
"35013726","1","35016330","","2016-01-26 12:19:58","","50","155458","<p>I am creating bag of words representation of the sentence. Then taking the words that exist in the sentence to compare to the file ""vectors.txt"", in order to get their embedding vectors. After getting vectors for each word that exists in the sentence, I am taking average of the vectors of the words in the sentence. This is my code: </p>

<pre><code>import nltk
import numpy as np
from nltk import FreqDist
from nltk.corpus import brown


news = brown.words(categories='news') 
news_sents = brown.sents(categories='news') 

fdist = FreqDist(w.lower() for w in news) 
vocabulary = [word for word, _ in fdist.most_common(10)] 
num_sents = len(news_sents) 

def averageEmbeddings(sentenceTokens, embeddingLookupTable):
    listOfEmb=[]
    for token in sentenceTokens:
        embedding = embeddingLookupTable[token] 
        listOfEmb.append(embedding)

return sum(np.asarray(listOfEmb)) / float(len(listOfEmb))

embeddingVectors = {}

with open(""D:\\Embedding\\vectors.txt"") as file: 
    for line in file:
       (key, *val) = line.split()
       embeddingVectors[key] = val

for i in range(num_sents): 
    features = {}
    for word in vocabulary: 
        features[word] = int(word in news_sents[i])        
    print(features) 
    print(list(features.values()))  
sentenceTokens = [] 
for key, value in features.items(): 
    if value == 1:
       sentenceTokens.append(key)
sentenceTokens.remove(""."")    
print(sentenceTokens)        
print(averageEmbeddings(sentenceTokens, embeddingVectors))

print(features.keys()) 
</code></pre>

<p>Not sure why, but I get this error:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-4-643ccd012438&gt; in &lt;module&gt;()
 39     sentenceTokens.remove(""."")
 40     print(sentenceTokens)
---&gt; 41     print(averageEmbeddings(sentenceTokens, embeddingVectors))
 42 
 43 print(features.keys()) 

&lt;ipython-input-4-643ccd012438&gt; in averageEmbeddings(sentenceTokens, embeddingLookupTable)
 18         listOfEmb.append(embedding)
 19 
---&gt; 20     return sum(np.asarray(listOfEmb)) / float(len(listOfEmb))
 21 
 22 embeddingVectors = {}

TypeError: ufunc 'add' did not contain a loop with signature matching types dtype('&lt;U9') dtype('&lt;U9') dtype('&lt;U9')
</code></pre>

<p>P.S. Embedding Vector looks like:</p>

<pre><code>the 0.011384 0.010512 -0.008450 -0.007628 0.000360 -0.010121 0.004674 -0.000076 
of 0.002954 0.004546 0.005513 -0.004026 0.002296 -0.016979 -0.011469 -0.009159 
and 0.004691 -0.012989 -0.003122 0.004786 -0.002907 0.000526 -0.006146 -0.003058 
one 0.014722 -0.000810 0.003737 -0.001110 -0.011229 0.001577 -0.007403 -0.005355 
in -0.001046 -0.008302 0.010973 0.009608 0.009494 -0.008253 0.001744 0.003263 
</code></pre>

<p>After using np.sum I get this error:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-13-8a7edbb9d946&gt; in &lt;module&gt;()
 40     sentenceTokens.remove(""."")
 41     print(sentenceTokens)
---&gt; 42     print(averageEmbeddings(sentenceTokens, embeddingVectors))
 43 
 44 print(features.keys()) 

&lt;ipython-input-13-8a7edbb9d946&gt; in averageEmbeddings(sentenceTokens, embeddingLookupTable)
 18         listOfEmb.append(embedding)
 19 
---&gt; 20     return np.sum(np.asarray(listOfEmb)) / float(len(listOfEmb))
 21 
 22 embeddingVectors = {}

C:\Anaconda3\lib\site-packages\numpy\core\fromnumeric.py in sum(a, axis, dtype, out, keepdims)
   1829     else:
   1830         return _methods._sum(a, axis=axis, dtype=dtype,
-&gt; 1831                              out=out, keepdims=keepdims)
   1832 
   1833 

C:\Anaconda3\lib\site-packages\numpy\core\_methods.py in _sum(a, axis, dtype, out, keepdims)
 30 
 31 def _sum(a, axis=None, dtype=None, out=None, keepdims=False):
---&gt; 32     return umr_sum(a, axis, dtype, out, keepdims)
 33 
 34 def _prod(a, axis=None, dtype=None, out=None, keepdims=False):

TypeError: cannot perform reduce with flexible type
</code></pre>
","5533112","","5533112","","2016-01-26 14:12:58","2016-01-26 14:34:48","TypeError: ufunc 'add' did not contain a loop with signature matching types","<python><python-3.x>","1","2","7","","","CC BY-SA 3.0","0"
"36470343","1","64213625","","2016-04-07 08:12:41","","52","154415","<p>I cannot find a way to draw an arbitrary line with <code>matplotlib</code> Python library.  It allows to draw horizontal and vertical lines (with <code>matplotlib.pyplot.axhline</code> and <code>matplotlib.pyplot.axvline</code>, for example), but i do not see how to draw a line through two given points <code>(x1, y1)</code> and <code>(x2, y2)</code>. Is there a way? Is there a simple way?</p>
","898649","","2875563","","2017-10-18 10:40:01","2020-10-18 14:10:55","How to draw a line with matplotlib?","<python><python-3.x><matplotlib>","4","3","16","","","CC BY-SA 3.0","0"
"33727149","1","","","2015-11-16 01:19:01","","113","153325","<p>While traversing a graph in Python, a I'm receiving this error:</p>

<blockquote>
  <p>'dict' object has no attribute 'has_key'</p>
</blockquote>

<p>Here is my code:</p>

<pre><code>def find_path(graph, start, end, path=[]):
    path = path + [start]
    if start == end:
        return path
    if not graph.has_key(start):
        return None
    for node in graph[start]:
        if node not in path:
            newpath = find_path(graph, node, end, path)
            if newpath: return newpath
    return None
</code></pre>

<p>The code aims to find the paths from one node to others. Code source: <a href=""http://cs.mwsu.edu/~terry/courses/4883/lectures/graphs.html"" rel=""noreferrer"">http://cs.mwsu.edu/~terry/courses/4883/lectures/graphs.html</a></p>

<p>Why am I getting this error and how can I fix it?</p>
","5565695","","2840103","","2017-08-01 16:34:42","2020-06-25 20:39:17","'dict' object has no attribute 'has_key'","<python><python-3.x><dictionary>","6","2","20","","","CC BY-SA 3.0","0"
"39327032","1","39327156","","2016-09-05 08:58:23","","137","151546","<p>I need to get the latest file of a folder using python. While using the code:</p>

<pre><code>max(files, key = os.path.getctime)
</code></pre>

<p>I am getting the below error:  </p>

<p><code>FileNotFoundError: [WinError 2] The system cannot find the file specified: 'a'</code></p>
","4641383","","355230","","2019-08-30 15:08:03","2020-06-23 14:48:16","How to get the latest file in a folder using python","<python><python-3.x><python-2.7>","9","2","37","","","CC BY-SA 4.0","0"
"33372054","1","33372147","","2015-10-27 15:28:21","","85","151081","<p>In Python what command should I use to get the name of the folder which contains the file I'm working with?</p>
<p><code>&quot;C:\folder1\folder2\filename.xml&quot;</code></p>
<p>Here <code>&quot;folder2&quot;</code> is what I want to get.</p>
<p>The only thing I've come up with is to use <code>os.path.split</code> twice:</p>
<pre class=""lang-py prettyprint-override""><code>folderName = os.path.split(os.path.split(&quot;C:\folder1\folder2\filename.xml&quot;)[0])[1]
</code></pre>
<p>Is there any better way to do it?</p>
","5494179","","9724138","","2020-09-03 21:23:34","2020-09-03 21:23:34","Get folder name of the file in Python","<python><python-3.x><directory>","6","1","15","","","CC BY-SA 4.0","0"
"52283840","1","","","2018-09-11 20:35:41","","33","148995","<p>I have a Windows 10 PC and I want to install pyaudio to use it with my chatbot, powered by chatterbot.</p>

<p>I tried 2 different ways to install pyaudio.</p>

<p>The first way is doing this on the command prompt:</p>

<pre><code>python -m pip install PyAudio
</code></pre>

<p>This is the result:</p>

<pre><code>   C:\Users\Waaberi&gt;python -m pip install PyAudio
Collecting PyAudio
  Using cached https://files.pythonhosted.org/packages/ab/42/b4f04721c5c5bfc196ce156b3c768998ef8c0ae3654ed29ea5020c749a6b/PyAudio-0.2.11.tar.gz
Installing collected packages: PyAudio
  Running setup.py install for PyAudio ... error
    Complete output from command C:\Users\Waaberi\AppData\Local\Programs\Python\Python37-32\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\Waaberi\\AppData\\Local\\Temp\\pip-install-e5le61j0\\PyAudio\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\Waaberi\AppData\Local\Temp\pip-record-adj3zivl\install-record.txt --single-version-externally-managed --compile:
    running install
    running build
    running build_py
    creating build
    creating build\lib.win32-3.7
    copying src\pyaudio.py -&gt; build\lib.win32-3.7
    running build_ext
    building '_portaudio' extension
    error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual C++ Build Tools"": http://landinghub.visualstudio.com/visual-cpp-build-tools

    ----------------------------------------
Command ""C:\Users\Waaberi\AppData\Local\Programs\Python\Python37-32\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\Waaberi\\AppData\\Local\\Temp\\pip-install-e5le61j0\\PyAudio\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\Waaberi\AppData\Local\Temp\pip-record-adj3zivl\install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in C:\Users\Waaberi\AppData\Local\Temp\pip-install-e5le61j0\PyAudio\
</code></pre>

<p>The second way is entering this command:</p>

<pre><code>python pip install python-pyaudio
</code></pre>

<p>and it does nothing.</p>
","10247298","","2745495","","2020-03-28 08:17:20","2020-06-23 13:28:49","I can't install pyaudio on Windows? How to solve ""error: Microsoft Visual C++ 14.0 is required.""?","<python><python-3.x><windows><pyaudio>","11","3","14","","","CC BY-SA 4.0","0"
"36575890","1","36575951","","2016-04-12 14:14:08","","21","146411","<p>I'm programming a little game with tkinter and briefly, I'm stuck.</p>

<p>I have a kind od starting menu, in which are two buttons and one label.</p>

<p>If I just create the frame everything is fine, it has the size 500x500 pixels</p>

<p>I want the background not to change when I create the buttons and the labe, but it adapts the size whatever I do. Here is my code:</p>

<pre><code>import tkinter as tk

def startgame():
    pass

mw = tk.Tk()              #Here I tried (1)
mw.title('The game')

back = tk.Frame(master=mw, width=500, height=500, bg='black')
back.pack()

go = tk.Button(master=back, text='Start Game', bg='black', fg='red',
                     command=lambda:startgame()).pack()
close = tk.Button(master=back, text='Quit', bg='black', fg='red',
                     command=lambda:quit()).pack()
info = tk.Label(master=back, text='Made by me!', bg='red',
                         fg='black').pack()

mw.mainloop()
</code></pre>

<p>I've searched around on stackoverflow and didn't get anything useful!
I've found just one question a bit similar to mine but the answer didn't work. I tried this:</p>

<p>(1)  mw.resizable(width=False, height=False)</p>

<p>I can't imagine what is the problem, I'm really desperate.</p>
","6181722","","","","","2020-08-10 19:45:24","How to set a tkinter window to a constant size","<python><python-3.x><tkinter>","4","1","12","","","CC BY-SA 3.0","0"
"41752946","1","","","2017-01-19 22:32:50","","39","144933","<p>How can I replace a character in a string from a certain index? For example, I want to get the middle character from a string, like abc, and if the character is not equal to the character the user specifies, then I want to replace it.</p>

<p>Something like this maybe?</p>

<pre><code>middle = ? # (I don't know how to get the middle of a string)

if str[middle] != char:
    str[middle].replace('')
</code></pre>
","7014322","","6862601","","2019-05-31 21:46:44","2020-09-26 03:01:46","Replacing a character from a certain index","<python><python-3.x><string>","5","4","5","2020-09-26 12:21:00","","CC BY-SA 4.0","0"
"34579327","1","34579607","","2016-01-03 17:19:07","","24","144021","<p>I am receiving this error in Python 3.5.1.</p>

<blockquote>
  <p>json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)</p>
</blockquote>

<p>Here is my code:</p>

<pre><code>import json
import urllib.request

connection = urllib.request.urlopen('http://python-data.dr-chuck.net/comments_220996.json')

js = connection.read()

print(js)

info = json.loads(str(js))
</code></pre>

<p><a href=""https://i.stack.imgur.com/sfOl3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sfOl3.png"" alt=""image""></a></p>
","4679487","","5106955","","2018-09-18 08:55:03","2020-08-15 15:47:48","JSONDecodeError: Expecting value: line 1 column 1","<json><python-3.x>","2","0","6","","","CC BY-SA 4.0","0"
"34980251","1","34980283","","2016-01-24 19:17:53","","32","142914","<p>If I wanted to print multiple lines of text in Python without typing <code>print('')</code> for every line, is there a way to do that? I'm using this for ASCII art.</p>

<p>(python 3.5.1)</p>
","5833159","","1169233","","2018-07-10 19:10:41","2019-05-28 12:58:00","How to print multiple lines of text with python","<python><python-3.x>","4","3","11","","","CC BY-SA 4.0","0"
"36244380","1","","","2016-03-27 06:03:22","","82","141090","<p>I know we use <code>enumerate</code> for iterating a list but I tried it in a dictionary and it didn't give an error.</p>

<p>CODE:</p>

<pre><code>enumm = {0: 1, 1: 2, 2: 3, 4: 4, 5: 5, 6: 6, 7: 7}

for i, j in enumerate(enumm):
    print(i, j)
</code></pre>

<p>OUTPUT:</p>

<pre><code>0 0

1 1

2 2

3 4

4 5

5 6

6 7
</code></pre>

<p>Can someone explain the output?</p>
","6013645","","1222951","","2018-08-13 09:01:30","2020-10-03 16:47:45","enumerate() for dictionary in python","<python><python-3.x><dictionary><enumerate>","11","1","9","","","CC BY-SA 3.0","0"
"45310254","1","45310389","","2017-07-25 17:41:20","","374","139168","<p>Is there an easy way with Python f-strings to fix the number of digits after the decimal point? (Specifically f-strings, not other string formatting options like .format or %)</p>

<p>For example, let's say I want to display 2 digits after the decimal place. </p>

<p>How do I do that? Let's say that</p>

<pre><code>a = 10.1234
</code></pre>
","6432812","","10908375","","2020-04-19 16:07:08","2020-08-11 16:44:03","Fixed digits after decimal with f-strings","<python><python-3.x><f-string>","5","0","56","","","CC BY-SA 4.0","0"
"40832533","1","40832702","","2016-11-27 18:42:03","","128","136528","<p>I have a Macbook with OS X El Captain. I think that <code>Python 2.7</code> comes preinstalled on it. However, I installed <code>Python 3.5</code> too. When I started using <code>Python 3</code>, I read that if I want to install a package, I should type:</p>

<pre><code>pip3 install some_package
</code></pre>

<p>Anyway, now when I use</p>

<pre><code>pip install some_package
</code></pre>

<p>I get <code>some_package</code> installed for <code>Python 3</code>. I mean I can import it and use it without problems. Moreover, when I type just <code>pip3</code> in <code>Terminal</code>, I got this message about the usage:</p>

<pre><code>Usage:   
  pip &lt;command&gt; [options]
</code></pre>

<p>which is the same message I get when I type just <code>pip</code>.</p>

<p>Does it mean that in previos versions, things were different, and now <code>pip</code> and <code>pip3</code> can be used interchangeably? If so, and for the sake of argument, how can I install packages for <code>Python 2</code> instead of <code>Python 3</code>?</p>
","2282785","","","","","2020-10-29 08:33:30","pip or pip3 to install packages for Python 3?","<python><macos><python-2.7><python-3.x><pip>","9","0","38","","","CC BY-SA 3.0","0"
"37619848","1","37620170","","2016-06-03 17:00:29","","14","136098","<p>Given the following list</p>

<pre><code>a = [0, 1, 2, 3]
</code></pre>

<p>I'd like to create a new list <code>b</code>, which consists of elements for which the current and next value of <code>a</code> are summed. It will contain <strong>1</strong> less element than <code>a</code>.</p>

<p>Like this:</p>

<pre><code>b = [1, 3, 5]
</code></pre>

<p>(from 0+1, 1+2, and 2+3)</p>

<p>Here's what I've tried:</p>

<pre><code>b = []
for i in a:
   b.append(a[i + 1] - a[i])
b
</code></pre>

<p>The trouble is I keep getting this error:</p>

<pre><code>IndexError: list index out of range
</code></pre>

<p>I'm pretty sure it occurs because by the time I get the the last element of a (3), I can't add it to anything because doing so goes outside of the value of it (there is no value after 3 to add). So I need to tell the code to stop at 2 while still referring to 3 for the calculation.</p>
","5569314","","1909378","","2018-08-09 12:47:27","2018-08-09 12:47:27","Python Loop: List Index Out of Range","<python><list><loops><python-3.x>","4","1","2","","","CC BY-SA 4.0","0"
"36571560","1","36571602","","2016-04-12 11:10:46","","69","135725","<p>Is there a way to save all of the print output to a txt file in python? Lets say I have the these two lines in my code and I want to save the print output to a file named <code>output.txt</code>.</p>

<pre><code>print (""Hello stackoverflow!"")
print (""I have a question."")
</code></pre>

<p>I want the <code>output.txt</code> file to to contain</p>

<pre class=""lang-none prettyprint-override""><code>Hello stackoverflow!
I have a question.
</code></pre>
","6130540","","355230","","2020-06-27 17:03:52","2020-06-27 18:32:19","Directing print output to a .txt file","<python><python-3.x>","7","0","23","","","CC BY-SA 4.0","0"
"34680228","1","","","2016-01-08 15:13:38","","57","135667","<p>I generally use Python 2.7 but recently installed Python 3.5 using Miniconda on Mac OS X. Different libraries have been installed for these two versions of python. Now, the entering either of the keywords 'python' or 'python3' in terminal invokes python 3.5, and 'python2' returns '-bash: python2: command not found'. How can I now invoke them specifically using aliases 'python2' and 'python3' respectively?</p>

<p>I am currently using OS X El Capitan.</p>
","3915069","","3915069","","2016-01-08 20:18:41","2018-12-11 07:29:48","Switch between python 2.7 and python 3.5 on Mac OS X","<python><macos><python-2.7><python-3.x><terminal>","7","4","17","","","CC BY-SA 3.0","0"
"37442494","1","37442560","","2016-05-25 16:08:38","","101","133384","<p>I am using currently Anaconda with Python 2.7, but I will need to use Python 3.5. Is it ok to have them installed both in the same time? Should I expect some problems?<br>
I am on a 64-bit Win8.</p>
","5875325","","4551041","","2019-06-18 16:51:01","2019-10-30 12:40:06","Is it ok having both Anacondas 2.7 and 3.5 installed in the same time?","<python><python-3.x><python-2.7><anaconda><virtualenv>","6","4","39","","","CC BY-SA 4.0","0"
"59382579","1","59391121","","2019-12-17 21:46:50","","5","131857","<p>I am looking to do the opposite of what has been done here:</p>

<pre><code>import re

text = '1234-5678-9101-1213 1415-1617-1819-hello'

re.sub(r""(\d{4}-){3}(?=\d{4})"", ""XXXX-XXXX-XXXX-"", text)

output = 'XXXX-XXXX-XXXX-1213 1415-1617-1819-hello'
</code></pre>

<p><em><a href=""https://stackoverflow.com/questions/16327590/partial-replacement-with-re-sub"">Partial replacement with re.sub()</a></em></p>

<p>My overall goal is to replace all <code>XXXX</code> within a text using a neural network. <code>XXXX</code> can represent names, places, numbers, dates, etc. that are in a .csv file.</p>

<p>The end result would look like:</p>

<pre><code>XXXX went to XXXX XXXXXX
</code></pre>

<p>Sponge Bob went to Disney World.</p>

<p>In short, I am unmasking text and replacing it with a generated dataset using fuzzy.</p>
","7297511","","63550","","2020-01-26 16:38:47","2020-01-26 16:49:31","Replace specific text with a redacted version using Python","<python-3.x><nlp><lstm>","1","0","","","","CC BY-SA 4.0","0"
"29188757","1","29188910","","2015-03-21 22:25:26","","60","130106","<p>I am trying to set the format to two decimal numbers in a matplotlib subplot environment. Unfortunately, I do not have any idea how to solve this task.</p>

<p>To prevent using scientific notation on the y-axis I used <code>ScalarFormatter(useOffset=False)</code> as you can see in my snippet below. I think my task should be solved by passing further options/arguments to the used formatter. However, I could not find any hint in matplotlib's documentation.</p>

<p>How can I set two decimal digits or none (both cases are needed)? I am not able to provide sample data, unfortunately.</p>

<hr>

<p>-- SNIPPET --</p>

<pre><code>f, axarr = plt.subplots(3, sharex=True)

data = conv_air
x = range(0, len(data))

axarr[0].scatter(x, data)
axarr[0].set_ylabel('$T_\mathrm{air,2,2}$', size=FONT_SIZE)
axarr[0].yaxis.set_major_locator(MaxNLocator(5))
axarr[0].yaxis.set_major_formatter(ScalarFormatter(useOffset=False))
axarr[0].tick_params(direction='out', labelsize=FONT_SIZE)
axarr[0].grid(which='major', alpha=0.5)
axarr[0].grid(which='minor', alpha=0.2)

data = conv_dryer
x = range(0, len(data))

axarr[1].scatter(x, data)
axarr[1].set_ylabel('$T_\mathrm{dryer,2,2}$', size=FONT_SIZE)
axarr[1].yaxis.set_major_locator(MaxNLocator(5))
axarr[1].yaxis.set_major_formatter(ScalarFormatter(useOffset=False))
axarr[1].tick_params(direction='out', labelsize=FONT_SIZE)
axarr[1].grid(which='major', alpha=0.5)
axarr[1].grid(which='minor', alpha=0.2)

data = conv_lambda
x = range(0, len(data))

axarr[2].scatter(x, data)
axarr[2].set_xlabel('Iterationsschritte', size=FONT_SIZE)
axarr[2].xaxis.set_major_locator(MaxNLocator(integer=True))
axarr[2].set_ylabel('$\lambda$', size=FONT_SIZE)
axarr[2].yaxis.set_major_formatter(ScalarFormatter(useOffset=False))
axarr[2].yaxis.set_major_locator(MaxNLocator(5))
axarr[2].tick_params(direction='out', labelsize=FONT_SIZE)
axarr[2].grid(which='major', alpha=0.5)
axarr[2].grid(which='minor', alpha=0.2)
</code></pre>
","3991125","","4576447","","2020-04-19 20:05:20","2020-04-19 20:05:20","Matplotlib: Specify format of floats for tick labels","<python><python-3.x><matplotlib><scipy>","4","0","18","","","CC BY-SA 4.0","0"
"53204916","1","56956783","","2018-11-08 09:33:53","","112","129930","<p>This is a truly popular question here at SO, but none of the many answers I have looked at, clearly explain what this error really mean, and why it occurs. </p>

<p>One source of confusion, is that when (for example) you do <code>pip install pycparser</code>, you first get the error:</p>

<p><strong><code>Failed building wheel for pycparser</code></strong></p>

<p>which is then followed by the message that the package was: </p>

<p><strong><code>Successfully installed pycparser-2.19</code></strong>.</p>

<hr>

<pre class=""lang-none prettyprint-override""><code># pip3 install pycparser

Collecting pycparser
  Using cached https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz
Building wheels for collected packages: pycparser
  Running setup.py bdist_wheel for pycparser ... error
  Complete output from command /usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-install-g_v28hpp/pycparser/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d /tmp/pip-wheel-__w_f6p0 --python-tag cp36:
  Traceback (most recent call last):
    File ""&lt;string&gt;"", line 1, in &lt;module&gt;
    ...
    File ""/usr/lib/python3.6/site-packages/pkg_resources/__init__.py"", line 2349, in resolve
      module = __import__(self.module_name, fromlist=['__name__'], level=0)
  ModuleNotFoundError: No module named 'wheel.bdist_wheel'

  ----------------------------------------
  Failed building wheel for pycparser
  Running setup.py clean for pycparser
Failed to build pycparser
Installing collected packages: pycparser
  Running setup.py install for pycparser ... done
Successfully installed pycparser-2.19
</code></pre>

<p><strong>What is going on here?</strong></p>

<p>(I would like to understand how something can fail but still get installed and whether you can trust this package functioning correctly?)</p>

<p>So far the best partial explanation I have found is <a href=""https://stackoverflow.com/a/37693431/"">this</a>.</p>
","1147688","","4464702","","2020-05-16 14:27:10","2020-11-01 00:27:42","What is the meaning of ""Failed building wheel for X"" in pip install?","<python><python-3.x><pip><python-wheel>","11","7","17","","","CC BY-SA 4.0","0"
"51279791","1","","","2018-07-11 07:40:26","","47","128850","<p>I have python3.5 already in linux ubuntu. I have downloaded the python3.7.tar <a href=""http://www.python.org"" rel=""noreferrer"">www.python.org</a>.</p>

<p>But i have no idea how to install it. How to upgrade to python 3.7? Thanks.</p>
","","user9885031","","","","2019-06-14 11:40:12","How to upgrade Python version to 3.7?","<python><linux><python-3.x><ubuntu>","2","3","7","2018-07-11 09:16:36","","CC BY-SA 4.0","0"
"39980323","1","39980744","","2016-10-11 14:59:23","","511","127315","<p>Dictionaries are ordered in Python 3.6 (under the CPython implementation at least) unlike in previous incarnations. This seems like a substantial change, but it's only a short paragraph in the <a href=""https://docs.python.org/3.6/whatsnew/3.6.html#new-dict-implementation"" rel=""noreferrer"">documentation</a>. It is described as a CPython implementation detail rather than a language feature, but also implies this may become standard in the future.</p>
<p>How does the new dictionary implementation perform better than the older one while preserving element order?</p>
<p>Here is the text from the documentation:</p>
<blockquote>
<p><code>dict()</code> now uses a “compact” representation <a href=""https://morepypy.blogspot.com/2015/01/faster-more-memory-efficient-and-more.html"" rel=""noreferrer"">pioneered by PyPy</a>. The memory usage of the new dict() is between 20% and 25% smaller compared to Python 3.5. <a href=""https://www.python.org/dev/peps/pep-0468"" rel=""noreferrer"">PEP 468</a> (Preserving the order of **kwargs in a function.) is implemented by this. The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5). (Contributed by INADA Naoki in <a href=""https://bugs.python.org/issue27350"" rel=""noreferrer"">issue 27350</a>. Idea <a href=""https://mail.python.org/pipermail/python-dev/2012-December/123028.html"" rel=""noreferrer"">originally suggested by Raymond Hettinger</a>.)</p>
</blockquote>
<p>Update December 2017: <code>dict</code>s retaining insertion order is <a href=""https://mail.python.org/pipermail/python-dev/2017-December/151283.html"" rel=""noreferrer"">guaranteed</a> for Python 3.7</p>
","6260170","","777225","","2020-08-28 19:36:43","2020-10-26 20:30:22","Are dictionaries ordered in Python 3.6+?","<python><python-3.x><dictionary><python-internals><python-3.6>","5","9","156","","","CC BY-SA 4.0","0"
"46391721","1","46396136","","2017-09-24 15:28:45","","125","126874","<p>I'm new to Python development and attempting to use pipenv. I ran the command <code>pip install pipenv</code>, which ran successfully:</p>

<pre><code>...
Successfully built pipenv pathlib shutilwhich pythonz-bd virtualenv-clone
Installing collected packages: virtualenv, pathlib, shutilwhich, backports.shutil-get-terminal-size, pythonz-bd, virtualenv-clone, pew, first, six, click, pip-tools, certifi, chardet, idna, urllib3, requests, pipenv
...
</code></pre>

<p>However, when I run the command <code>pipenv install</code> in a fresh root project directory I receive the following message: <code>-bash: pipenv: command not found</code>. I suspect that I might need to modify my .bashrc, but I'm unclear about what to add to the file or if modification is even necessary.</p>
","6090736","","","","","2020-10-15 23:47:22","Pipenv: Command Not Found","<python><python-3.x><pip><pipenv>","15","3","38","","","CC BY-SA 3.0","0"
"28014241","1","","","2015-01-18 20:04:25","","24","126686","<p>Here is how, this is the best way, I have found:</p>

<pre><code>x = int(raw_input(""Enter an integer: ""))
for ans in range(0, abs(x) + 1):
    if ans ** 3 == abs(x):
        break
if ans ** 3 != abs(x):
    print x, 'is not a perfect cube!'
else:
    if x &lt; 0:
        ans = -ans
    print 'Cube root of ' + str(x) + ' is ' + str(ans)
</code></pre>

<p>Is there a better way, preferably one that avoids having to iterate over candidate values?</p>
","1476145","","367273","","2015-01-18 20:12:05","2015-01-19 15:21:15","How to find cube root using Python?","<python><python-3.x>","3","0","4","2015-01-20 17:57:38","","CC BY-SA 3.0","0"
"33837717","1","","","2015-11-20 23:23:32","","98","126504","<p>I have the following directory:</p>

<pre><code>myProgram
└── app
    ├── __init__.py
    ├── main.py 
    └── mymodule.py
</code></pre>

<p>mymodule.py:</p>

<pre><code>class myclass(object):

def __init__(self):
    pass

def myfunc(self):
    print(""Hello!"")
</code></pre>

<p>main.py:</p>

<pre><code>from .mymodule import myclass

print(""Test"")
testclass = myclass()
testclass.myfunc()
</code></pre>

<p>But when I run it, then I get this error:</p>

<pre><code>Traceback (most recent call last):
  File ""D:/Users/Myname/Documents/PycharmProjects/myProgram/app/main.py"", line 1, in &lt;module&gt;
    from .mymodule import myclass
SystemError: Parent module '' not loaded, cannot perform relative import
</code></pre>

<p>This works:</p>

<pre><code>from mymodule import myclass
</code></pre>

<p>But I get no auto completion when I type this in and there is a message: ""unresolved reference: mymodule"" and ""unresolved reference: myclass"".
And in my other project, which I am working on, I get the error: ""ImportError: No module named 'mymodule'.</p>

<p>What can I do?</p>
","4697157","","5445670","","2018-04-10 03:28:17","2018-04-10 03:28:17","SystemError: Parent module '' not loaded, cannot perform relative import","<python><python-3.x>","4","5","15","2016-09-14 06:54:04","","CC BY-SA 3.0","0"
"44951456","1","44953739","","2017-07-06 14:15:04","","44","125482","<p>I just ran the following command:</p>
<pre><code>pip install -U steem
</code></pre>
<p>and the installation worked well until it failed to install <code>pycrypto</code>.
Afterwards I did the</p>
<pre><code>pip install cryptography
</code></pre>
<p>command because I thought it was the missing package.
So my question is, how I can install <code>steem</code> without the pycrypto-error (or the pycrypto-package in addition) and how to uninstall the cryptography-Package which I don't need.
(I'm using Windows 7 and Python 3)</p>
<pre><code>Requirement already up-to-date: python-dateutil in c:\users\***\appdata\lo
cal\programs\python\python36\lib\site-packages (from dateparser-&gt;maya-&gt;steem)
...
Installing collected packages: urllib3, idna, chardet, certifi, requests, pycryp
to, funcy, w3lib, voluptuous, diff-match-patch, scrypt, prettytable, appdirs, la
ngdetect, ruamel.yaml, humanize, tzlocal, regex, dateparser, pytzdata, pendulum,
maya, ecdsa, pylibscrypt, ujson, toolz, steem
Running setup.py install for pycrypto ... error
Complete output from command c:\users\***\appdata\local\programs\pytho
n\python36\python.exe -u -c &quot;import setuptools, tokenize;__file__='C:\\Users\\
***~1\\AppData\\Local\\Temp\\pip-build-k6flhu5k\\pycrypto\\setup.py';f=getattr(
tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();
exec(compile(code, __file__, 'exec'))&quot; install --record C:\Users\***N~1\AppDat
a\Local\Temp\pip-igpkll6u-record\install-record.txt --single-version-externally-
managed --compile:
running install
running build
running build_py
...
building 'Crypto.Random.OSRNG.winrandom' extension
error: Microsoft Visual C++ 14.0 is required. Get it with &quot;Microsoft Visual
C++ Build Tools&quot;: http://landinghub.visualstudio.com/visual-cpp-build-tools

----------------------------------------
Command &quot;c:\users\***\appdata\local\programs\python\python36\python.exe -u
-c &quot;import setuptools, tokenize;__file__='C:\\Users\\***N~1\\AppData\\Local\\
Temp\\pip-build-k6flhu5k\\pycrypto\\setup.py';f=getattr(tokenize, 'open', open)(
__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code,   __fil
e__, 'exec'))&quot; install --record C:\Users\***N~1\AppData\Local\Temp\pip-igpkll6
u-record\install-record.txt --single-version-externally-managed --compile&quot;   faile
d with error code 1 in C:\Users\***N~1\AppData\Local\Temp\pip-build-    k6flhu5k\p
ycrypto\
</code></pre>
","8113806","","2745495","","2020-10-02 00:15:17","2020-10-02 00:15:17","Pip error: Microsoft Visual C++ 14.0 is required","<python><python-3.x><python-3.6><pycrypto>","8","2","10","","","CC BY-SA 4.0","0"
"28991015","1","28991449","","2015-03-11 15:39:05","","175","124270","<p>What is the BEST way to clear out all the <code>__pycache__</code>  folders and <code>.pyc/.pyo</code> files from a python3 project. I have seen multiple users suggest the <code>pyclean</code> script bundled with Debian, but this does not remove the folders. I want a simple way to clean up the project before pushing the files to my DVS.</p>
","2411063","","","","","2020-05-15 13:57:20","Python3 project remove __pycache__ folders and .pyc files","<python><python-3.x><python-3.4><delete-file>","12","0","54","","","CC BY-SA 3.0","0"
"32557920","1","32558710","","2015-09-14 05:37:33","","274","123411","<p>One of the most talked about features in Python 3.5 is <strong>type hints</strong>.</p>

<p>An example of <strong>type hints</strong> is mentioned in <a href=""http://lwn.net/Articles/650904/"" rel=""noreferrer"">this article</a> and <a href=""http://lwn.net/Articles/640359/"" rel=""noreferrer"">this one</a> while also mentioning to use type hints responsibly. Can someone explain more about them and when they should be used and when not?</p>
","1934182","","4518341","","2019-10-13 22:15:26","2020-07-23 00:19:24","What are type hints in Python 3.5?","<python><python-3.x><python-3.5><type-hinting>","5","4","123","","","CC BY-SA 4.0","0"
"28906859","1","28906913","","2015-03-06 20:13:04","","114","122575","<p>When I try to follow the <a href=""https://docs.python.org/2/library/urllib.html#urllib.urlencode"" rel=""noreferrer"">Python Wiki's example</a> related to URL encoding:</p>

<pre><code>&gt;&gt;&gt; import urllib
&gt;&gt;&gt; params = urllib.urlencode({'spam': 1, 'eggs': 2, 'bacon': 0})
&gt;&gt;&gt; f = urllib.urlopen(""http://www.musi-cal.com/cgi-bin/query"", params)
&gt;&gt;&gt; print f.read()
</code></pre>

<p>An error is raised on the second line:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'module' object has no attribute 'urlencode'
</code></pre>

<p>What am I missing?</p>
","3367446","","6332918","","2017-09-26 22:10:38","2019-10-08 06:36:57","'module' has no attribute 'urlencode'","<python><python-3.x><urllib>","3","1","13","","","CC BY-SA 3.0","0"
"33308781","1","33308988","","2015-10-23 18:05:38","","73","122485","<p>I've installed django rest framework using <code>pip install djangorestframework</code> yet I still get this error when I run ""python3 manage.py sycndb"":</p>

<blockquote>
  <p>ImportError: No module named 'rest_framework'</p>
</blockquote>

<p>I'm using python3, is this my issue?</p>
","3853339","","4060551","","2015-10-24 09:55:33","2020-10-20 06:07:08","Django Rest Framework -- no module named rest_framework","<python><django><python-3.x><pip><django-rest-framework>","20","6","9","","","CC BY-SA 3.0","0"
"40697845","1","40698307","","2016-11-19 20:59:57","","140","122015","<p>I want to check my environment for the existence of a variable, say <code>""FOO""</code>, in Python. For this purpose, I am using the <code>os</code> standard library. After reading the library's documentation, I have figured out 2 ways to achieve my goal:</p>

<p>Method 1:  </p>

<pre><code>if ""FOO"" in os.environ:
    pass
</code></pre>

<p>Method 2:</p>

<pre><code>if os.getenv(""FOO"") is not None:
    pass
</code></pre>

<p>I would like to know which method, if either, is a good/preferred conditional and why.</p>
","4927751","","4952130","","2016-11-19 22:35:28","2020-10-22 15:26:21","What is a good practice to check if an environmental variable exists or not?","<python><python-2.7><python-3.x><if-statement><environment-variables>","6","10","10","","","CC BY-SA 3.0","0"
"33446347","1","33446502","","2015-10-30 23:34:19","","53","121642","<p>I'm trying to use PyMySQL on Ubuntu.</p>

<p>I've installed <code>pymysql</code> using both <code>pip</code> and <code>pip3</code> but every time I use <code>import pymysql</code>, it returns <code>ImportError: No module named 'pymysql'</code></p>

<p>I'm using Ubuntu 15.10 64-bit and Python 3.5.</p>

<p>The same .py works on Windows with Python 3.5, but not on Ubuntu.</p>
","4621575","","530873","","2015-10-31 00:42:15","2020-08-01 02:49:21","No module named 'pymysql'","<python><python-3.x><ubuntu><pymysql>","14","7","5","","","CC BY-SA 3.0","0"
"38423360","1","38423419","","2016-07-17 16:22:14","","16","121545","<p>I just recently joined the python3 HypeTrain. However I just wondered how you can use an if statement onto a boolean. Example:  </p>

<pre><code>RandomBool = True
# and now how can I check this in an if statement? Like the following:
if RandomBool == True:
    #DoYourThing
</code></pre>

<p>And also, can I just switch the value of a boolean like this?</p>

<pre><code>RandomBool1 == True   #Boolean states True
if #AnyThing:
    RandomBool1 = False   #Boolean states False from now on?
</code></pre>
","6581025","","1783163","","2020-05-27 18:04:41","2020-05-27 18:04:41","Syntax for an If statement using a boolean","<python><python-3.x><if-statement><boolean>","1","9","1","","","CC BY-SA 4.0","0"
"40206569","1","40206661","","2016-10-23 18:44:56","","132","121346","<p>I'm at wit's end. After a dozen hours of troubleshooting, probably more, I thought I was finally in business, but then I got:</p>

<pre><code>Model class django.contrib.contenttypes.models.ContentType doesn't declare an explicit app_label 
</code></pre>

<p>There is SO LITTLE info on this on the web, and no solution out there has resolved my issue. Any advice would be tremendously appreciated.</p>

<p>I'm using Python 3.4 and Django 1.10.</p>

<p>From my settings.py:</p>

<pre><code>INSTALLED_APPS = [
    'DeleteNote.apps.DeletenoteConfig',
    'LibrarySync.apps.LibrarysyncConfig',
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
]
</code></pre>

<p>And my apps.py files look like this:</p>

<pre><code>from django.apps import AppConfig


class DeletenoteConfig(AppConfig):
    name = 'DeleteNote'
</code></pre>

<p>and</p>

<pre><code>from django.apps import AppConfig


class LibrarysyncConfig(AppConfig):
    name = 'LibrarySync'
</code></pre>
","6025788","","6025788","","2016-10-23 19:09:48","2020-10-26 14:20:43","Django model ""doesn't declare an explicit app_label""","<python><django><python-3.x>","28","17","14","","","CC BY-SA 3.0","0"
"46753393","1","46768243","","2017-10-15 08:58:25","","128","120834","<p>I am running this code with python, selenium, and firefox but still get 'head' version of firefox:</p>

<pre><code>binary = FirefoxBinary('C:\\Program Files (x86)\\Mozilla Firefox\\firefox.exe', log_file=sys.stdout)
binary.add_command_line_options('-headless')
self.driver = webdriver.Firefox(firefox_binary=binary)
</code></pre>

<p>I also tried some variations of binary:</p>

<pre><code>binary = FirefoxBinary('C:\\Program Files\\Nightly\\firefox.exe', log_file=sys.stdout)
        binary.add_command_line_options(""--headless"")
</code></pre>
","4454657","","1033581","","2018-11-02 11:37:49","2020-09-19 20:49:36","How to make firefox headless programmatically in Selenium with python?","<python><python-3.x><selenium><selenium-webdriver><firefox-headless>","6","1","42","","","CC BY-SA 4.0","0"
"38364404","1","","","2016-07-14 02:08:02","","11","119794","<p>I have windows 10 (64 bit). I want to utilize the <code>Openpyxl</code> package to start learning how to interact with excel and other spreadsheets. </p>

<p>I installed Python with <code>""windowsx86-64web-basedinstaller""</code> I have a 64 bit OS, was I mistaken when trying to install this version? </p>

<p><a href=""https://i.stack.imgur.com/EvNYW.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/EvNYW.png"" alt=""Python Details in Command Prompt""></a>
<a href=""https://i.stack.imgur.com/1zJaF.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1zJaF.png"" alt=""Python Installation Options""></a>
<a href=""https://i.stack.imgur.com/9A0fL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/9A0fL.png"" alt=""I found some pip.exes in the script folder of python""></a></p>
","1193857","","4952130","","2016-07-14 06:03:20","2020-10-13 09:47:53","How to install Openpyxl with pip","<python><python-3.x><pip><openpyxl>","7","1","2","","","CC BY-SA 3.0","0"
"29909330","1","29910249","","2015-04-28 02:38:09","","51","119491","<p>I know that there is a <a href=""http://www.microsoft.com/en-gb/download/details.aspx?id=44266"">""Microsoft Visual C++ Compiler for Python 2.7""</a> but is there, currently or planned, a Microsoft Visual C++ Compiler for Python 3.4 or eve Microsoft Visual C++ Compiler for Python 3.x for that matter?  It would be supremely beneficial if I didn't have to install a different version of visual studio on my entire lab.</p>
","1470373","","284795","","2015-07-03 21:40:13","2017-06-21 05:42:02","Microsoft Visual C++ Compiler for Python 3.4","<python><windows><python-3.x><compilation>","3","1","26","","","CC BY-SA 3.0","0"
"56974927","1","57168165","","2019-07-10 16:23:38","","103","118932","<p>Seems as though an update on Windows 10 overnight broke Python. Just trying to run <code>python --version</code> returned a ""Permission Denied"" error.  None of the three updates; KB4507453, KB4506991, or KB4509096 look like they'd be the culprit but the timing of the issue is suspicious. Rather than messing with rolling back, I'm hoping there's a simpler fix that I'm missing.</p>

<p>The permissions on python are ""-rwxr-xr-x"" and I haven't changed anything besides letting the Windows update reboot machine after installing last night's patches.</p>

<p>According to the System Information, I'm running 10.0.18362</p>

<p>Should also note that this is happening whether I (try) to execute Python from git-bash using ""run as administrator"" or not, and if I try using PowerShell, it just opens the Windows store as if the app isn't installed so I'm thinking it can't see the contents of my <code>/c/Users/david/AppData/Local/Microsoft/WindowsApps/</code> folder for some reason.</p>

<p>I've also tried to reinstall Python 3.7.4, but that didn't help either. Is there something else I should be looking at?</p>
","83101","","1710956","","2019-08-11 08:53:53","2020-07-01 14:01:28","""Permission Denied"" trying to run Python on Windows 10","<python-3.x><windows><windows-store-apps><file-permissions><git-bash>","10","6","20","","","CC BY-SA 4.0","0"
"40416072","1","40416154","","2016-11-04 05:48:08","","77","118667","<p>Say I have a python project that is structured as follows:</p>

<pre><code>project
    /data
        test.csv
    /package
        __init__.py
        module.py
    main.py
</code></pre>

<p><code>__init__.py</code>:</p>

<pre><code>from .module import test
</code></pre>

<p><code>module.py</code>:</p>

<pre><code>import csv

with open(""..data/test.csv"") as f:
    test = [line for line in csv.reader(f)]
</code></pre>

<p><code>main.py</code>:</p>

<pre><code>import package

print(package.test)
</code></pre>

<p>When I run <code>main.py</code> I get the following error:</p>

<pre><code> C:\Users\Patrick\Desktop\project&gt;python main.py
Traceback (most recent call last):
  File ""main.py"", line 1, in &lt;module&gt;
    import package
  File ""C:\Users\Patrick\Desktop\project\package\__init__.py"", line 1, in &lt;module&gt;
    from .module import test
  File ""C:\Users\Patrick\Desktop\project\package\module.py"", line 3, in &lt;module&gt;
    with open(""../data/test.csv"") as f:
FileNotFoundError: [Errno 2] No such file or directory: '../data/test.csv'
</code></pre>

<p>However, if I run <code>module.py</code> from the <code>package</code> directory I get no errors. So it seems that the relative path used in <code>open(...)</code> is only relative to where the originating file is being run from (i.e <code>__name__ == ""__main__""</code>)? I don't want to use absolute paths. What are some ways to deal with this?</p>
","2593236","","","","","2020-10-01 20:23:19","Reading file using relative path in python project","<python><python-3.x><io><relative-path><python-import>","6","1","21","","","CC BY-SA 3.0","0"
"46610689","1","63610984","","2017-10-06 17:06:26","","35","118640","<p>I'm using Windows, and I'm trying to install package cv2 for python3.</p>

<p>I did a <code>pip3 install opencv-python</code> and it reports successful:</p>

<p><a href=""https://i.stack.imgur.com/7nsl4.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7nsl4.png"" alt=""opencv-python3 is installed""></a></p>

<p>But when I do the <code>import cv2</code> from python3, it's not found and I get weird errors:</p>

<p><a href=""https://i.stack.imgur.com/xNFfw.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xNFfw.png"" alt=""enter image description here""></a></p>

<p>What am I doing wrong?</p>
","8353066","","445131","","2018-10-16 14:26:22","2020-08-27 07:16:54","How to import cv2 in python3?","<python-3.x><opencv><numpy>","6","2","5","","","CC BY-SA 4.0","0"
"39910730","1","41492852","","2016-10-07 06:24:16","","71","118228","<p>I am using Python 3.5.2 version on Windows 7 and tried using <code>python3 app.py</code>. I am getting this error message:</p>
<pre><code>'python3' is not recognized as an internal or external command,
operable program or batch file. 
</code></pre>
<p>Is there any specific cause about why the <code>python3</code> command is not working?</p>
<p>I also verified that the PATH is added to environment variables.</p>
","2160547","","2745495","","2020-07-04 04:02:48","2020-07-04 04:02:48","'python3' is not recognized as an internal or external command, operable program or batch file","<python-3.x><python-3.5>","8","0","19","","","CC BY-SA 4.0","0"
"40659212","1","","","2016-11-17 15:46:33","","99","118053","<p>I am using Pandas <code>0.19.1</code> on Python 3. I am getting a warning on these lines of code. I'm trying to get a list that contains all the row numbers where string <code>Peter</code> is present at column <code>Unnamed: 5</code>.</p>

<pre><code>df = pd.read_excel(xls_path)
myRows = df[df['Unnamed: 5'] == 'Peter'].index.tolist()
</code></pre>

<p>It produces a Warning:</p>

<pre><code>""\Python36\lib\site-packages\pandas\core\ops.py:792: FutureWarning: elementwise 
comparison failed; returning scalar, but in the future will perform 
elementwise comparison 
result = getattr(x, name)(y)""
</code></pre>

<p>What is this FutureWarning and should I ignore it since it seems to work.  </p>
","","user411103","445131","","2020-05-04 18:13:24","2020-08-25 21:02:20","FutureWarning: elementwise comparison failed; returning scalar, but in the future will perform elementwise comparison","<python><python-3.x><pandas><numpy><matplotlib>","11","0","31","","","CC BY-SA 4.0","1"
"44144584","1","44820979","","2017-05-23 20:39:41","","54","117112","<p>Trying to run two different functions at the same time with shared queue and get an error...how can I run two functions at the same time with a shared queue? This is Python version 3.6 on Windows 7. </p>

<pre><code>from multiprocessing import Process
from queue import Queue
import logging

def main():
    x = DataGenerator()
    try:
        x.run()
    except Exception as e:
        logging.exception(""message"")


class DataGenerator:

    def __init__(self):
        logging.basicConfig(filename='testing.log', level=logging.INFO)

    def run(self):
        logging.info(""Running Generator"")
        queue = Queue()
        Process(target=self.package, args=(queue,)).start()
        logging.info(""Process started to generate data"")
        Process(target=self.send, args=(queue,)).start()
        logging.info(""Process started to send data."")

    def package(self, queue): 
        while True:
            for i in range(16):
                datagram = bytearray()
                datagram.append(i)
                queue.put(datagram)

    def send(self, queue):
        byte_array = bytearray()
        while True:
            size_of__queue = queue.qsize()
            logging.info("" queue size %s"", size_of_queue)
            if size_of_queue &gt; 7:
                for i in range(1, 8):
                    packet = queue.get()
                    byte_array.append(packet)
                logging.info(""Sending datagram "")
                print(str(datagram))
                byte_array(0)

if __name__ == ""__main__"":
    main()
</code></pre>

<p>The logs indicate an error, I tried running console as administrator and I get the same message...</p>

<pre><code>INFO:root:Running Generator
ERROR:root:message
Traceback (most recent call last):
  File ""test.py"", line 8, in main
    x.run()
  File ""test.py"", line 20, in run
    Process(target=self.package, args=(queue,)).start()
  File ""C:\ProgramData\Miniconda3\lib\multiprocessing\process.py"", line 105, in start
    self._popen = self._Popen(self)
  File ""C:\ProgramData\Miniconda3\lib\multiprocessing\context.py"", line 223, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File ""C:\ProgramData\Miniconda3\lib\multiprocessing\context.py"", line 322, in _Popen
    return Popen(process_obj)
  File ""C:\ProgramData\Miniconda3\lib\multiprocessing\popen_spawn_win32.py"", line 65, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\ProgramData\Miniconda3\lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
TypeError: can't pickle _thread.lock objects
</code></pre>
","3131062","","","","","2018-05-18 10:48:22","TypeError: can't pickle _thread.lock objects","<python-3.x>","3","2","9","","","CC BY-SA 3.0","0"
"42259166","1","42259375","","2017-02-15 20:15:21","","10","116470","<p>I have a problem with my Python 3 program. I use Mac OS X. This code is running properly.</p>

<pre><code># -*- coding: utf-8 -*-
#! python3
# sendDuesReminders.py - Sends emails based on payment status in spreadsheet.

import openpyxl, smtplib, sys


# Open the spreadsheet and get the latest dues status.
wb = openpyxl.load_workbook('duesRecords.xlsx')
sheet = wb.get_sheet_by_name('Sheet1')

lastCol = sheet.max_column
latestMonth = sheet.cell(row=1, column=lastCol).value

# Check each member's payment status.
unpaidMembers = {}
for r in range(2, sheet.max_row + 1):
payment = sheet.cell(row=r, column=lastCol).value
if payment != 'zaplacone':
    name = sheet.cell(row=r, column=2).value
    lastname = sheet.cell(row=r, column=3).value
    email = sheet.cell(row=r, column=4).value
    unpaidMembers[name] = email


# Log in to email account.
smtpObj = smtplib.SMTP_SSL('smtp.gmail.com', 465)
smtpObj.ehlo()
smtpObj.login('abc@abc.com', '1234')


# Send out reminder emails.
for name, email in unpaidMembers.items()
body = ""Subject: %s - przypomnienie o platnosci raty za treningi GIT Parkour. "" \
       ""\n\nPrzypominamy o uregulowaniu wplaty za uczestnictwo: %s w treningach GIT Parkour w ."" \
       ""\n\nRecords show  that you have not paid dues for %s. Please make "" \
       ""this payment as soon as possible.""%(latestMonth, name, latestMonth)
print('Sending email to %s...' % email)
sendmailStatus = smtpObj.sendmail('abc@abc.com', email, body)

if sendmailStatus != {}:
    print('There was a problem sending email to %s: %s' % (email,
    sendmailStatus))
smtpObj.quit()enter code here
</code></pre>

<p>Problems starts when I am trying to add next value to the for loop.</p>

<pre><code># Send out reminder emails.
for name, lastname, email in unpaidMembers.items()
body = ""Subject: %s - przypomnienie o platnosci raty za treningi GIT Parkour. "" \
       ""\n\nPrzypominamy o uregulowaniu wplaty za uczestnictwo: %s %s w treningach GIT Parkour w ."" \
       ""\n\nRecords show  that you have not paid dues for %s. Please make "" \
       ""this payment as soon as possible.""%(latestMonth, name, lastname, latestMonth)
print('Sending email to %s...' % email)
sendmailStatus = smtpObj.sendmail('abc@abc.com', email, body)
</code></pre>

<p>Terminal shows error:</p>

<pre><code>Traceback (most recent call last):
    File ""sendDuesEmailReminder.py"", line 44, in &lt;module&gt;
        for name, email, lastname in unpaidMembers.items():
ValueError: not enough values to unpack (expected 3, got 2)
</code></pre>
","6845295","","541208","","2017-02-15 20:18:42","2019-12-12 08:51:48","Python 3 - ValueError: not enough values to unpack (expected 3, got 2)","<python><python-3.x>","5","3","2","","","CC BY-SA 3.0","0"
"47022070","1","47022213","","2017-10-30 18:34:36","","102","116071","<p>I want to show all columns in a dataframe in a Jupyter Notebook. Jupyter shows some of the columns and adds dots to the last columns like in the following picture:</p>

<p><a href=""https://i.stack.imgur.com/gfcqV.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gfcqV.jpg"" alt=""Juputer Screenshot""></a></p>

<p>How can I display all columns? </p>
","2618461","","2673029","","2017-10-30 18:37:21","2020-08-27 04:42:03","Display all dataframe columns in a Jupyter Python Notebook","<python><python-3.x><dataframe><jupyter-notebook>","6","0","20","","","CC BY-SA 3.0","0"
"39371772","1","","","2016-09-07 13:50:47","","51","114007","<p>I would like to know how to install the latest Anaconda version from Continuum on my Raspberry Pi 3 model B. Any help would be appreciated...</p>
","1707348","","","","","2020-06-24 09:05:08","How to install Anaconda on RaspBerry Pi 3 Model B","<python-3.x><anaconda><raspberry-pi3>","4","2","20","","","CC BY-SA 3.0","0"
"27920837","1","27921146","","2015-01-13 11:14:09","","56","113880","<p>This is my python code:</p>

<pre><code>import subprocess
subprocess.check_output(""ls"",shell=True,stderr=subprocess.STDOUT)

import subprocess
subprocess.check_output(""yum"",shell=True,stderr=subprocess.STDOUT)
</code></pre>

<p>The first <code>.check_output()</code> works well, but the second returns this:</p>

<pre><code>Traceback (most recent call last):
File ""/usr/lib/x86_64-linux-gnu/gedit/plugins/pythonconsole/console.py"", line 378, in __run
r = eval(command, self.namespace, self.namespace)
File ""&lt;string&gt;"", line 1, in &lt;module&gt;
File ""/usr/lib/python3.4/subprocess.py"", line 616, in check_output
raise CalledProcessError(retcode, process.args, output=output)
subprocess.CalledProcessError: Command 'yum' returned non-zero exit status 1
</code></pre>

<p>Why does this happen? Is it because <code>ls</code> is the original shell command but <code>yum</code> is the new package? How can I solve this problem? </p>
","4220419","","7117003","","2019-03-25 21:05:18","2020-07-22 10:49:33","Subprocess check_output returned non-zero exit status 1","<python><python-3.x><subprocess>","2","0","13","","","CC BY-SA 4.0","0"
"41274007","1","41274348","","2016-12-22 00:19:52","","143","113040","<p>How can I make anaconda environment file which could be use on other computers?</p>

<p>I exported my anaconda python environment to YML using <code>conda env export &gt; environment.yml</code>. The exported <code>environment.yml</code> contains this line <code>prefix: /home/superdev/miniconda3/envs/juicyenv</code> which maps to my anaconda's location which will be different on other's pcs.</p>
","6304774","","","","","2020-04-23 14:13:32","Anaconda export Environment file","<python><python-3.x><anaconda><conda>","5","2","47","","","CC BY-SA 3.0","0"
"40884668","1","40891721","","2016-11-30 09:32:08","","28","112075","<p>I'm trying to install <a href=""https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html#pip-installation-on-windows"" rel=""noreferrer"">TensorFlow on Windows</a>.</p>

<p>I tried to install it with <code>pip</code>, but I always get the same error message:</p>

<pre><code>... is not a supported wheel on this platform.
</code></pre>

<p>I first tried it with Python 3.5.1, now I upgraded to <strong>3.6.0b4</strong>, but it makes no difference.</p>

<hr />

<p>Python:</p>

<pre><code>Python 3.6.0b4 (default, Nov 22 2016, 05:30:12) [MSC v.1900 64 bit (AMD64)] on win32
</code></pre>

<p>pip:</p>

<pre><code>pip 9.0.1 from ...\python\lib\site-packages (python 3.6)
</code></pre>

<hr />

<p>To be exact, I tried the following two commands:</p>

<pre><code>pip install --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl
pip install --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-0.12.0rc0-cp35-cp35m-win_amd64.whl
</code></pre>

<p>they output the following:</p>

<pre><code>&gt; tensorflow-0.12.0rc0-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.
&gt; tensorflow_gpu-0.12.0rc0-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.
</code></pre>

<p>Does anyone know how to solve this problem? I'm not sure where I'm making a mistake.</p>

<p>Thanks!</p>

<p><hr />
<strong>Edit 1</strong></p>

<p>Btw, I also tried <code>pip install tensorflow</code> and <code>pip install tensorflow-gpu</code> like suggested <a href=""https://stackoverflow.com/a/38900276/6459948"">here</a>. I got the following output:</p>

<pre><code>&gt; Could not find a version that satisfies the requirement tensorflow (from versions: ) No matching distribution found for tensorflow
&gt; Could not find a version that satisfies the requirement tensorflow-gpu (from versions: ) No matching distribution found for tensorflow-gpu
</code></pre>
","6459948","","-1","","2017-05-23 11:47:31","2019-07-31 00:34:15","Installing TensorFlow on Windows (Python 3.6.x)","<python><python-3.x><installation><tensorflow>","22","3","16","","","CC BY-SA 3.0","0"
"29022377","1","29022395","","2015-03-12 23:40:21","","8","109791","<p>I'm writing an assignment to count the number of vowels in a file, currently in my class we have only been using code like this to check for the end of a file:</p>

<pre><code>vowel=0
f=open(""filename.txt"",""r"",encoding=""utf-8"" )
line=f.readline().strip()
while line!="""":
    for j in range (len(line)):
        if line[j].isvowel():
            vowel+=1

    line=f.readline().strip()
</code></pre>

<p>But this time for our assignment the input file given by our professor is an entire essay, so there are several blank lines throughout the text to separate paragraphs and whatnot, meaning my current code would only count until the first blank line.</p>

<p>Is there any way to check if my file has reached its end other than checking for if the line is blank? Preferably in a similar fashion that I have my code in currently, where it checks for something every single iteration of the while loop</p>

<p>Thanks in advance</p>
","2502019","","","","","2018-12-15 12:56:19","How to while loop until the end of a file in Python without checking for empty line?","<python><loops><python-3.x><file-io>","3","1","5","","","CC BY-SA 3.0","0"
"47563013","1","47563055","","2017-11-29 22:37:39","","46","106876","<p>Tried to run command:</p>

<pre><code>from django.urls import path
</code></pre>

<p>Getting error:</p>

<blockquote>
  <p>Traceback (most recent call last):   File ""&lt; stdin >"", line 1, in
   ImportError: cannot import name 'path'</p>
</blockquote>

<p>I am using django version 1.11</p>
","3476943","","3476943","","2018-02-18 03:31:24","2020-09-06 21:19:17","Unable to import path from django.urls","<python><django><python-3.x><django-views>","12","0","8","","","CC BY-SA 3.0","0"
"35623776","1","50725224","","2016-02-25 09:52:24","","22","105130","<p>I'm trying to import numpy on Pycharm.</p>

<p>Using the Pycharm terminal and Miniconda I've launched the command:</p>

<pre><code>conda install numpy
</code></pre>

<p>And this was the output</p>

<pre><code>Fetching package metadata: ....
Solving package specifications: ....................
# All requested packages already installed.
# packages in environment at C:\Users\...\Miniconda3:
#
numpy                     1.10.4                   py35_0
</code></pre>

<p>So I run my project but the terminal said</p>

<pre><code>ImportError: No module named 'numpy'
</code></pre>

<p>On my project bar I can see two different folders, the one with my project and another one with the external libraries.</p>

<p>Under External libraries -> Extendend definitions there is a numpy folder so I guess that the installation goes well.</p>

<p>Can you please help me ?</p>
","3289652","","","","","2020-02-24 12:40:48","Import numpy on pycharm","<python><python-3.x><numpy><pycharm>","5","1","9","","","CC BY-SA 3.0","0"
"50655738","1","","","2018-06-02 10:05:27","","66","104692","<p>I am trying to use pytesseract in Python but I always end up with the following error:</p>

<pre><code>    raise TesseractNotFoundError()
pytesseract.pytesseract.TesseractNotFoundError: tesseract is not installed or it's not in your path
</code></pre>

<p>However, pytesseract and Tesseract are installed on my system.</p>

<p>Example code that produces this error:</p>

<pre class=""lang-py prettyprint-override""><code>import cv2
import pytesseract

img = cv2.imread('1d.png')
print(pytesseract.image_to_string(img))
</code></pre>

<p>How do I resolve this TesseractNotFoundError?</p>
","9884640","","10795151","","2019-11-29 18:56:37","2020-09-05 11:16:00","How do I resolve a TesseractNotFoundError?","<python><python-3.x><tesseract><python-tesseract>","21","1","19","","","CC BY-SA 4.0","0"
"31827012","1","31827113","","2015-08-05 08:17:28","","114","104486","<p>I would like to use <code>urllib.quote()</code>. But python (python3) is not finding the module.
Suppose, I have this line of code:</p>

<pre><code>print(urllib.quote(""châteu"", safe=''))
</code></pre>

<p>How do I import urllib.quote?</p>

<p><code>import urllib</code> or
<code>import urllib.quote</code> both give</p>

<pre><code>AttributeError: 'module' object has no attribute 'quote'
</code></pre>

<p>What confuses me is that <code>urllib.request</code> is accessible via <code>import urllib.request</code></p>
","4592067","","4592067","","2017-04-01 22:41:15","2020-09-01 22:26:26","Python: Importing urllib.quote","<python><python-3.x><import><urllib>","5","0","24","","","CC BY-SA 3.0","0"
"42662104","1","44254088","","2017-03-08 02:54:06","","51","104377","<p>I'd like to start by pointing out that this question may seem like a duplicate, but it isn't. All the questions I saw here were regarding pip for Python 3 and I'm talking about Python 3.6. The steps used back then don't work for Python 3.6.</p>

<ol>
<li>I got a clear Ubuntu 16.10 image from the <a href=""https://store.docker.com/images/414e13de-f1ba-40d0-9867-08f2e5884b3f"" rel=""noreferrer"">official docker store</a>.</li>
<li>Run <code>apt-get update</code></li>
<li>Run <code>apt-get install python3.6</code></li>
<li>Run <code>apt-get install python3-pip</code></li>
<li>Run <code>pip3 install requests bs4</code></li>
<li>Run <code>python3.6 script.py</code></li>
</ol>

<p>Got <code>ModuleNotFoundError</code> below:</p>

<pre><code> Traceback (most recent call last):
    File ""script.py"", line 6, in &lt;module&gt;
     import requests
 ModuleNotFoundError: No module named 'requests'
</code></pre>

<p>Python's and pip's I have in the machine:</p>

<pre><code>python3
python3.5
python3.5m
python3.6
python3m
python3-config
python3.5-config
python3.5m-config
python3.6m
python3m-config  

pip
pip3
pip3.5
</code></pre>
","1812319","","4518341","","2018-03-03 12:50:09","2019-08-03 17:59:36","How to install pip for Python 3.6 on Ubuntu 16.10?","<python-3.x><ubuntu><pip><installation>","4","1","33","","","CC BY-SA 3.0","0"
"47730259","1","","","2017-12-09 15:46:38","","23","104067","<p>I would like to import urllib to use the function 'request'. However, I encountered an error when trying to do so. I tried pip install urllib but still had the same error. I am using Python 3.6. Really appreciate any help.</p>

<p>i do import urllib.request using this code:</p>

<pre><code>import urllib.request, urllib.parse, urllib.error 
fhand = urllib.request.urlopen('data.pr4e.org/romeo.txt') 
counts = dict()
for line in fhand: 
    words = line.decode().split() 
for word in words: 
    counts[word] = counts.get(word, 0) + 1 
print(counts) 
</code></pre>

<p>but it gives me this error: ModuleNotFoundError: No Module named 'urllib.parse'; 'urllib' is not a package </p>

<p><a href=""https://i.stack.imgur.com/WTrfl.png"" rel=""noreferrer"">here is a screenshot for the error</a></p>
","9076413","","9076413","","2017-12-09 17:06:12","2020-09-27 02:22:14","installing urllib in Python3.6","<python><python-3.x><urllib>","5","6","3","","","CC BY-SA 3.0","0"
"42370339","1","42400980","","2017-02-21 14:39:55","","25","103708","<p>Is there a way to install the win32api module for python 3.6 or do I have to change my version of python?
Everytime I try to install it using pip I get the following error:</p>

<pre><code> Could not find a version that satisfies the requirement win32api (from versions: )
No matching distribution found for win32api 
</code></pre>
","7421048","","6296561","","2019-06-04 17:28:20","2019-09-10 13:38:04","Python 3.6 install win32api?","<python><python-3.x>","2","1","10","","","CC BY-SA 4.0","0"
"31996367","1","32032784","","2015-08-13 19:00:21","","29","103319","<p>I'm using OpenCV 3.0.0 and Python 3.4.3 to process a very large RGB image (107162,79553,3). While I'm trying to resize it using the following code: </p>

<pre><code>import cv2
image = cv2.resize(img, (0,0), fx=0.5, fy=0.5, interpolation=cv2.INTER_AREA)
</code></pre>

<p>I had this error message coming up :<br>
""cv2.error: C:\opencv-3.0.0\source\modules\imgproc\src\imgwarp.cpp:3208: error: (-215) ssize.area() > 0 in function cv::resize""</p>

<p>I'm certain there is image content in the image array because I can save them into small tiles in jpg format. When I try to resize just a small part of the image, there is no problem and I end up with correctly resized image. (Taking a rather big chunk (50000,50000,3) still won't work, but it will work on a (10000,10000,3) chunk)  </p>

<p>I'm wondering what could cause this problem and how I can solve this? </p>

<p>Thanks</p>
","3667217","","3667217","","2015-08-13 19:10:34","2020-06-11 10:52:28","OpenCV resize fails on large image with ""error: (-215) ssize.area() > 0 in function cv::resize""","<image><opencv><python-3.x><image-processing><opencv3.0>","13","4","3","","","CC BY-SA 3.0","0"
"39402795","1","","","2016-09-09 02:12:17","","72","103036","<p>I'm trying to make <code>length = 001</code> in Python 3 but whenever I try to print it out it truncates the value without the leading zeros (<code>length = 1</code>). How would I stop this happening without having to cast <code>length</code> to a string before printing it out?</p>
","6776396","","6381711","","2016-09-09 04:31:06","2019-11-23 06:06:37","How to pad a string with leading zeros in Python 3","<python><python-3.x><math><rounding>","5","1","10","2019-09-12 06:30:27","","CC BY-SA 3.0","0"
"33193374","1","","","2015-10-18 01:25:52","","38","102613","<p>I've just installed python 3.5, ran <code>Python 3.5 (32-bit)</code> and typed</p>

<pre><code>pip
</code></pre>

<p>and received the message:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;pyshell#2&gt;"", line 1, in &lt;module&gt;
    pip
NameError: name 'pip' is not defined
</code></pre>

<p>I don't see any scripts directories in my path, but I found <code>pip.py</code> in <code>C:\Users\UserName\AppData\Local\Programs\Python\Python35-32\Scripts</code>.</p>

<p>I selected the option to ""Add python to environment variables"" during installation, but it doesn't seem to have done anything.</p>

<p>I googled this and got <a href=""http://www.howtogeek.com/197947/how-to-install-python-on-windows/"" rel=""noreferrer"">this guide</a>, which says that earlier versions need to add some path names. I don't have a <code>C:\Python...</code> directory so I tried adding the <code>Scripts</code> folder from above, same result.</p>

<p>How do I install python so that it actually works (ie. I can run pip, install modules, etc.)?</p>
","1613983","","1613983","","2015-10-18 01:28:26","2015-10-18 03:11:16","How do I run pip on python for windows?","<python><python-3.x>","3","8","13","2015-10-18 03:23:53","","CC BY-SA 3.0","0"
"38694800","1","","","2016-08-01 09:02:01","","31","101622","<p>I want to convert the following string into dictionary without using <code>eval()</code> function in Python 3.5.</p>

<pre class=""lang-py prettyprint-override""><code>d=""{'Age': 7, 'Name': 'Manni'}"";
</code></pre>

<p>Can anybody tell me the good way than using the <code>eval()</code> function?</p>

<p>What I <em>really</em> want is a function which can directly convert a dictionary to a string.</p>
","6575955","","3025856","","2020-05-11 00:51:47","2020-05-11 00:51:47","how to convert string into dictionary in python 3.*?","<python-3.x>","1","2","6","2016-08-01 09:05:11","","CC BY-SA 4.0","0"
"58568175","1","","","2019-10-26 04:55:03","","104","101177","<p>Python 3.8.0 is out, but I haven't been able to find any post on how to update to python 3.8 using conda - maybe they will wait for the official release? Any suggestions?</p>
","3587464","","","","","2020-10-19 02:20:48","Upgrade to python 3.8 using conda","<python><python-3.x><anaconda><conda>","4","0","20","","","CC BY-SA 4.0","0"
"42572582","1","","","2017-03-03 06:58:37","","20","101074","<p>I have a Fortran program and want to execute it in python for multiple files. I have 2000 input files but in my Fortran code I am able to run only one file at a time. How should I call the Fortran program in python?</p>

<p><strong>My Script:</strong></p>

<pre><code>import subprocess
import glob
input = glob.glob('C:/Users/Vishnu/Desktop/Fortran_Program_Rum/*.txt')
output = glob.glob('C:/Users/Vishnu/Desktop/Fortran_Program_Rum/Output/')
f = open(""output"", ""w"")
for i in input:
    subprocess.Popen([""FORTRAN ~/C:/Users/Vishnu/Desktop/Fortran_Program_Rum/phase1.f"", ""--domain ""+i])
f.write(i)
</code></pre>

<p><strong>Error:</strong></p>

<pre><code>runfile('C:/Users/Vishnu/Desktop/test_fn/test.py', wdir='C:/Users/Vishnu/Desktop/test_fn')
Traceback (most recent call last):

   File ""&lt;ipython-input-3-f8f378816004&gt;"", line 1, in &lt;module&gt;
runfile('C:/Users/Vishnu/Desktop/test_fn/test.py', wdir='C:/Users/Vishnu/Desktop/test_fn')

  File ""C:\Users\Vishnu\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 866, in runfile
execfile(filename, namespace)

  File ""C:\Users\Vishnu\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/Vishnu/Desktop/test_fn/test.py"", line 30, in &lt;module&gt;
subprocess.Popen([""FORTRAN ~/C:/Users/Vishnu/Desktop/Fortran_Program_Rum/phase1.f"", ""--domain ""+i])

  File ""C:\Users\Vishnu\Anaconda3\lib\subprocess.py"", line 707, in __init__
restore_signals, start_new_session)

  File ""C:\Users\Vishnu\Anaconda3\lib\subprocess.py"", line 990, in _execute_child
startupinfo)

FileNotFoundError: [WinError 2] The system cannot find the file specified
</code></pre>

<p><strong>Edit:</strong></p>

<pre><code>import subprocess
import os
input = os.path.normcase(r'C:/Users/Vishnu/Desktop/Fortran_Program_Rum/*.txt')
output = os.path.normcase(r'~/C:/Users/Vishnu/Desktop/Fortran_Program_Rum/Output/')
f = open(""output"", ""w"")
for i in input:
    exe = os.path.normcase(r'~/C:/Program Files (x86)/Silverfrost/ftn95.exe')
    fortran_script = os.path.normcase(r'~/C:/Users/Vishnu/Desktop/test_fn/test_f2py.f95')
    i = os.path.normcase(i)
    subprocess.Popen([exe, fortran_script, ""--domain"", i])
    f.write(i)
</code></pre>

<p><strong>Error:</strong></p>

<pre><code>FileNotFoundError: [WinError 2] The system cannot find the file specified
</code></pre>

<p><strong>Edit - 2:</strong></p>

<p>I have change my script as below: but error is same</p>

<pre><code>input = os.path.normcase(r'C:/Users/Vishnu/Desktop/Fortran_Program_Rum/*.txt')
output = os.path.normcase(r'C:/Users/Vishnu/Desktop/Fortran_Program_Rum/Output/')
f = open(""output"", ""w"")
for i in input:
    exe = os.path.normcase(r'C:/Program Files (x86)/Silverfrost/ftn95.exe')
    fortran_script = os.path.normcase(r'C:/Users/Vishnu/Desktop/test_fn/test_f2py.f95')
    i = os.path.normcase(i)
    subprocess.Popen([exe, fortran_script, ""--domain"", i])
    f.write(i)
</code></pre>

<p><strong>Error: 2</strong></p>

<pre><code>FileNotFoundError: [WinError 2] The system cannot find the file specified
</code></pre>

<p><strong>Error: 3 - 15-03-2017</strong></p>

<pre><code>import subprocess
import os
input = os.path.normcase(r'C:/Users/Vishnu/Desktop/Fortran_Program_Rum/*.txt')
output = os.path.normcase(r'C:/Users/Vishnu/Desktop/Fortran_Program_Rum/Output/')
f = open('output', 'w+')
for i in input:
    exe = os.path.normcase(r'C:/Program Files (x86)/Silverfrost/ftn95.exe')
    fortran_script = os.path.normcase(r'C:/Users/Vishnu/Desktop/Fortran_Program_Rum/phase1.f')
    i = os.path.normcase(i)
    subprocess.Popen([exe, fortran_script, ""--domain"", i], shell = True)
    f.write(i)
</code></pre>

<p>** Error **</p>

<pre><code>PermissionError: [Errno 13] Permission denied: 'output'
</code></pre>
","6439828","","6439828","","2017-03-15 05:43:06","2018-02-10 01:06:51","WinError 2 The system cannot find the file specified (Python)","<python><python-2.7><python-3.x><bioinformatics><f2py>","3","2","3","","","CC BY-SA 3.0","0"
"37661119","1","37661312","","2016-06-06 15:21:52","","50","100091","<p>After command <code>pip install mpl_toolkits</code> I receive next error:</p>

<blockquote>
  <p>Could not find a version that satisfies the requirement mpl_toolkits (from versions: )</p>
  
  <p>No matching distribution found for mpl_toolkits</p>
</blockquote>

<p>I tried to google, but nothing helps. How can I solve this?</p>
","5405478","","771848","","2016-12-28 02:24:18","2019-06-01 00:57:27","python mpl_toolkits installation issue","<python><python-3.x><pip>","4","0","10","","","CC BY-SA 3.0","0"
"39345995","1","39346109","","2016-09-06 09:54:25","","26","99992","<p>I have written the following code:</p>

<pre><code>class FigureOut:
   def setName(self, name):
      fullname = name.split()
      self.first_name = fullname[0]
      self.last_name = fullname[1]

   def getName(self):
      return self.first_name, self.last_name

f = FigureOut()
f.setName(""Allen Solly"")
name = f.getName()
print (name)
</code></pre>

<p>I get the following <strong>Output:</strong></p>

<pre><code>('Allen', 'Solly')
</code></pre>

<p>Whenever multiple values are returned from a function in python, does it always convert the multiple values to a <strong>list of multiple values</strong> and then returns it from the function?</p>

<p><em>Is the whole process same as converting the multiple values to a <code>list</code> explicitly and then returning the list, for example in JAVA, as one can return only one object from a function in JAVA?</em></p>
","6354622","","6354622","","2017-01-23 09:23:19","2018-06-05 20:48:45","How does Python return multiple values from a function?","<python><function><python-3.x><return>","6","5","6","","","CC BY-SA 3.0","0"
"41691327","1","","","2017-01-17 07:31:31","","53","99130","<p>I am trying to use the betbrain.py from Github (<a href=""https://github.com/gto76/betbrain-scraper"" rel=""noreferrer"">https://github.com/gto76/betbrain-scraper</a>) that has the following code:</p>

<pre><code>#!/usr/bin/python3
#
# Usage: betbrain.py [URL or FILE] [OUTPUT-FILE]
# Scrapes odds from passed betbrain page and writes them to
# stdout, or file if specified.

import os
import sys
import urllib.request


from bs4 import BeautifulSoup
from http.cookiejar import CookieJar

import parser_betbrain
import printer

DEFAULT_URL = 'https://www.betbrain.com/football/england/premier-league/#!/matches/'


# If no arguments are present, it parses the default page.
# Argument can be an URL or a local file.
def main():
  html = getHtml(sys.argv)
  soup = BeautifulSoup(html, ""html.parser"")
  matches = parser_betbrain.getMatches(soup)
  string = printer.matchesToString(matches)
  output(string, sys.argv)

def getHtml(argv):
  if len(argv) &lt;= 1:
    return scrap(DEFAULT_URL)
  elif argv[1].startswith(""http""):
    return scrap(argv[1])
  else:
    return readFile(argv[1])

# Returns html file located at URL.
def scrap(url):
  cj = CookieJar()
  opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))
  try:
    return opener.open(url)
  except ValueError:
    error(""Invalid URL: "" + url)

def readFile(path):
  try:
    return open(path, encoding='utf8')
  except IOError:
    error(""Invalid input filename: "" + path)

def output(string, argv):
  if len(argv) &lt;= 2:
    print(string)
  else:
    writeFile(argv[2], string)

def writeFile(path, string):
  try:  
    fo = open(path, ""w"", encoding='utf8')
    fo.write(string);
    fo.close()
  except IOError:
    error(""Invalid output filename: "" + path)

def error(msg):
  msg = os.path.basename(__file__)+"": ""+msg
  print(msg, file=sys.stderr)
  sys.exit(1)

if __name__ == '__main__':
  main()
</code></pre>

<p><strong>However when run it comes back with this error</strong></p>

<pre><code>Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1318, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1026, in _send_output
    self.send(msg)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 964, in send
    self.connect()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1400, in connect
    server_hostname=server_hostname)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 401, in wrap_socket
    _context=self, _session=session)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 808, in __init__
    self.do_handshake()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 1061, in do_handshake
    self._sslobj.do_handshake()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 683, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/Daniel/Desktop/betbrain-scraper-master 2/betbrain.py"", line 71, in &lt;module&gt;
    main()
  File ""/Users/Daniel/Desktop/betbrain-scraper-master 2/betbrain.py"", line 22, in main
    html = getHtml(sys.argv)
  File ""/Users/Daniel/Desktop/betbrain-scraper-master 2/betbrain.py"", line 30, in getHtml
    return scrap(DEFAULT_URL)
  File ""/Users/Daniel/Desktop/betbrain-scraper-master 2/betbrain.py"", line 41, in scrap
    return opener.open(url)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 526, in open
    response = self._open(req, data)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 544, in _open
    '_open', req)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 504, in _call_chain
    result = func(*args)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1361, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1320, in do_open
    raise URLError(err)
urllib.error.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)&gt;
</code></pre>

<p><strong>How can I fix this problem? I am running Python 3.6.0 on MacOS 10.12.1</strong></p>
","7408231","","5088142","","2017-01-17 08:40:36","2020-07-14 05:36:28","ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:749)","<python><python-3.x><ssl>","4","0","24","","","CC BY-SA 3.0","0"
"33626623","1","33626754","","2015-11-10 09:23:55","","71","98538","<p>I need to remove the first n elements from a list of objects in Python 2.7. Is there an easy way, without using loops?</p>
","2526842","","1222951","","2018-06-18 17:02:31","2018-06-18 17:02:31","The most efficient way to remove first N elements in a list?","<python><performance><list><python-2.7><python-3.x>","4","4","7","","","CC BY-SA 4.0","0"
"38727520","1","38727786","","2016-08-02 18:01:36","","269","97707","<p>If I have a function like this:</p>

<pre><code>def foo(name, opts={}):
  pass
</code></pre>

<p>And I want to add type hints to the parameters, how do I do it? The way I assumed gives me a syntax error:</p>

<pre><code>def foo(name: str, opts={}: dict) -&gt; str:
  pass
</code></pre>

<p>The following doesn't throw a syntax error but it doesn't seem like the intuitive way to handle this case:</p>

<pre><code>def foo(name: str, opts: dict={}) -&gt; str:
  pass
</code></pre>

<p>I can't find anything in the <a href=""https://docs.python.org/3/library/typing.html"" rel=""noreferrer""><code>typing</code> documentation</a> or on a Google search. </p>

<p>Edit: I didn't know how default arguments worked in Python, but for the sake of this question, I will keep the examples above. In general it's much better to do the following:</p>

<pre><code>def foo(name: str, opts: dict=None) -&gt; str:
  if not opts:
    opts={}
  pass
</code></pre>
","1383785","","1383785","","2016-08-02 18:53:37","2019-06-19 14:41:58","Adding default parameter value with type hint in Python","<python><python-3.x><type-hinting>","3","4","32","","","CC BY-SA 3.0","0"
"39473297","1","","","2016-09-13 15:04:48","","30","97640","<p>I have a simple print statement:</p>

<pre><code>print('hello friends')
</code></pre>

<p>I would like the output to be blue in the terminal. How can I accomplish this with Python3?</p>
","6827031","","1902776","","2016-09-13 17:21:11","2020-04-11 14:29:40","How do I print colored output with Python 3?","<python-3.x><printing><colors>","12","4","13","","","CC BY-SA 3.0","0"
"41744368","1","41744403","","2017-01-19 14:33:33","","70","97548","<p>I am still learning and in response to one of my questions: <a href=""https://stackoverflow.com/questions/41737321/same-command-works-once-when-executed-but-throws-an-exception-when-executed-a-se?noredirect=1#comment70669855_41737321"">here</a>, I was told to that it might be due because the element in question is not in view. </p>

<p>I looked through the documentation and SO, here was the most relevant answer: <a href=""https://stackoverflow.com/questions/3401343/scroll-element-into-view-with-selenium"">here</a></p>

<p>You can use the ""org.openqa.selenium.interactions.Actions"" class to move to an element:</p>

<pre><code>WebElement element = driver.findElement(By.id(""my-id""));
Actions actions = new Actions(driver);
actions.moveToElement(element);
## actions.click();
actions.perform();
</code></pre>

<p>When I try to use the above to scroll to the element:
It says WebElement not defined.</p>

<p>I think this is because I have not imported the relevant module. Can someone point out what I am supposed to import?</p>

<p>Edit:
As pointed out by alecxe, this was java code.</p>

<p>But in the meantime right after trying to figure it out for some time. I have found out the import method for WebElement:</p>

<pre><code>from selenium.webdriver.remote.webelement import WebElement
</code></pre>

<p>Might help someone like me.</p>

<p>The how of it is also a good lesson, IMO:</p>

<p>Went to: <a href=""http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement"" rel=""noreferrer"">Documentation</a>
The </p>

<pre><code>class selenium.webdriver.remote.webelement.WebElement(parent, id_, w3c=False)
</code></pre>

<p>Need to be separated into the command form mentioned above.</p>
","6387095","","7429447","","2017-11-11 12:07:13","2020-05-14 11:24:12","Scrolling to element using webdriver?","<python><python-3.x><selenium><selenium-webdriver>","6","0","26","","","CC BY-SA 3.0","0"
"37625208","1","37625243","","2016-06-04 00:30:48","","30","97478","<p>I'm new to python and I'm trying to scan multiple numbers separated by spaces (let's assume '1 2 3' as an example) in a single line and add it to a list of int. I did it by using:</p>
<pre><code>#gets the string 
string = input('Input numbers: ') 
#converts the string into an array of int, excluding the whitespaces
array = [int(s) for s in string.split()] 
</code></pre>
<p>Apparently it works, since when I type in '1 2 3' and do a <code>print(array)</code> the output is:</p>
<blockquote>
<p>[1, 2, 3]</p>
</blockquote>
<p>But I want to print it in a single line without the brackets, and with a space in between the numbers, like this:</p>
<blockquote>
<p>1 2 3</p>
</blockquote>
<p>I've tried doing:</p>
<pre><code>for i in array:
    print(array[i], end=&quot; &quot;)
</code></pre>
<p>But I get an error:</p>
<blockquote>
<p>2 3 Traceback (most recent call last):</p>
<p>print(array[i], end=&quot; &quot;)</p>
<p>IndexError: list index out of range</p>
</blockquote>
<p>How can I print the list of ints (assuming my first two lines of code are right) in a single line, and without the brackets and commas?</p>
","5033494","","-1","","2020-06-20 09:12:55","2020-09-25 10:03:29","Printing an int list in a single line python3","<python><python-3.x>","10","1","8","","","CC BY-SA 3.0","0"
"43124775","1","","","2017-03-30 17:37:00","","127","97192","<p>I just installed Python 3.6.1 for MacOS X</p>

<p>When I attempt to run the Console(or run anything with Python3), this error is thrown:</p>

<pre><code>  AttributeError: module 'enum' has no attribute 'IntFlag'

$ /Library/Frameworks/Python.framework/Versions/3.6/bin/python3  
Failed to import the site module  
Traceback (most recent call last):  
  File ""/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site.py"", line 544, in &lt;module&gt;  
    main()  
  File ""/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site.py"", line 530, in main  
    known_paths = addusersitepackages(known_paths)  
  File ""/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site.py"", line 282, in addusersitepackages  
    user_site = getusersitepackages()  
  File ""/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site.py"", line 258, in getusersitepackages  
    user_base = getuserbase() # this will also set USER_BASE  
  File ""/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site.py"", line 248, in getuserbase  
    USER_BASE = get_config_var('userbase')  
  File ""/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/sysconfig.py"", line 601, in get_config_var  
    return get_config_vars().get(name)  
  File ""/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/sysconfig.py"", line 580, in get_config_vars  
    import _osx_support  
  File ""/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/_osx_support.py"", line 4, in &lt;module&gt;  
    import re  
  File ""/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/re.py"", line 142, in &lt;module&gt;  
    class RegexFlag(enum.IntFlag):  
AttributeError: module 'enum' has no attribute 'IntFlag'  
</code></pre>

<p>The class IntFlag exists within enum.py. So, why is the AttributeError being thrown?</p>
","86731","","6573902","","2020-03-28 10:23:21","2020-10-20 13:29:22","Why Python 3.6.1 throws AttributeError: module 'enum' has no attribute 'IntFlag'?","<python><python-3.x><enums><python-3.6><attributeerror>","17","14","14","","","CC BY-SA 4.0","0"
"37085430","1","37085824","","2016-05-07 06:47:17","","62","96737","<p>I define a tensor like this:</p>

<p><code>x = tf.get_variable(""x"", [100])</code></p>

<p>But when I try to print shape of tensor :</p>

<p><code>print( tf.shape(x) )</code></p>

<p>I get <strong>Tensor(""Shape:0"", shape=(1,), dtype=int32)</strong>, why the result of output should not be shape=(100)</p>
","1303432","","2956066","","2017-12-11 21:43:07","2020-02-14 10:46:11","tf.shape() get wrong shape in tensorflow","<python><python-3.x><tensorflow><tensor>","6","1","22","","","CC BY-SA 3.0","0"
"30446449","1","30446551","","2015-05-25 21:48:33","","54","95971","<p>Hey everyone I'm trying to write a program in Python that acts as a quiz game. I made a dictionary at the beginning of the program that contains the values the user will be quizzed on. Its set up like so:</p>

<pre><code>PIX0 = {""QVGA"":""320x240"", ""VGA"":""640x480"", ""SVGA"":""800x600""}
</code></pre>

<p>So I defined a function that uses a <code>for</code> loop to iterate through the dictionary keys and asks for input from the user, and compares the user input to the value matched with the key.</p>

<pre><code>for key in PIX0:
    NUM = input(""What is the Resolution of %s?""  % key)
    if NUM == PIX0[key]:
        print (""Nice Job!"")
        count = count + 1
    else:
        print(""I'm sorry but thats wrong. The correct answer was: %s."" % PIX0[key] )
</code></pre>

<p>This is working fine output looks like this:</p>

<pre><code>What is the Resolution of Full HD? 1920x1080
Nice Job!
What is the Resolution of VGA? 640x480
Nice Job!
</code></pre>

<p>So what I would like to be able to do is have a separate function that asks the question the other way, providing the user with the resolution numbers and having the user enter the name of the display standard. So I want to make a for loop but I don't really know how to (or if you even can) iterate over the values in the dictionary and ask the user to input the keys.</p>

<p>I'd like to have output that looks something like this:</p>

<pre><code>Which standard has a resolution of 1920x1080? Full HD
Nice Job!
What standard has a resolution of 640x480? VGA
Nice Job!
</code></pre>

<p>I've tried playing with <code>for value in PIX0.values()</code> and thats allowed me to iterate through the dictionary values, but I don't know how to use that to ""check"" the user answers against the dictionary keys. If anyone could help it would be appreciated.</p>

<p><strong>EDIT:</strong> Sorry I'm using Python3.</p>
","4905747","","6862601","","2020-07-23 01:15:27","2020-07-23 01:15:27","Iterate through dictionary values?","<python><python-3.x><loops><dictionary><key-value>","5","2","9","","","CC BY-SA 4.0","0"
"34293875","1","","","2015-12-15 16:05:03","","75","95497","<p>I want to remove all punctuation marks from a text file using .translate() method. It seems to work well under Python 2.x but under Python 3.4 it doesn't seem to do anything. </p>

<p>My code is as follows and the output is the same as input text.</p>

<pre><code>import string
fhand = open(""Hemingway.txt"")
for fline in fhand:
    fline = fline.rstrip()
    print(fline.translate(string.punctuation))
</code></pre>
","3398398","","297696","","2017-08-22 20:28:49","2020-04-26 04:09:07","How to remove punctuation marks from a string in Python 3.x using .translate()?","<python><python-3.x>","5","0","28","","","CC BY-SA 3.0","0"
"44901806","1","44901902","","2017-07-04 09:13:45","","48","94947","<p>I made a simple program but It shows the following error when I run it:</p>

<pre><code>line1 = []
line1.append(""xyz "")
line1.append(""abc"")
line1.append(""mno"")

file = open(""File.txt"",""w"")
for i in range(3):
    file.write(line1[i])
    file.write(""\n"")

for line in file:
    print(line)
file.close()
</code></pre>

<p>It shows this error message:</p>

<blockquote>
  <p>File ""C:/Users/Sachin Patil/fourth,py.py"", line 18, in <br/>
      for line in file:</p>
  
  <p>UnsupportedOperation: not readable</p>
</blockquote>
","8076419","","550094","","2017-07-04 09:50:15","2020-08-26 08:33:17","Python error message io.UnsupportedOperation: not readable","<python-3.x><file>","4","2","10","","","CC BY-SA 3.0","0"
"44119081","1","","","2017-05-22 17:47:19","","18","94907","<p>I know this has been asked lots of times before but how do you get around the ""element not interactable"" exception?</p>

<p>I'm new to Selenium so excuse me if I get something wrong.</p>

<p>Here is my code:</p>

<pre><code>button = driver.find_element_by_class_name(u""infoDismiss"")
type(button)
button.click()
driver.implicitly_wait(10)
</code></pre>

<p>Here is the HTML:</p>

<pre><code>&lt;button class=""dismiss infoDismiss""&gt;
    &lt;string for=""inplay_button_dismiss""&gt;Dismiss&lt;/string&gt;
&lt;/button&gt;
</code></pre>

<p>And here is the error message:</p>

<pre><code>selenium.common.exceptions.ElementNotInteractableException: Message: 
</code></pre>

<p>After is says message there is literally nothing.</p>

<p>I have spent lots of time searching the web, not finding anything that solves my issue. I would really appreciate an answer.</p>

<p>Thanks in advance.</p>

<p><strong>Edit:</strong> Changed ""w"" to driver so it is easier to read</p>

<p><strong>Update:</strong> I have just realized that I've found the HTML of the wrong button! The real button HTML is below:</p>

<pre><code>&lt;button class=""dismiss""&gt;
    &lt;string for=""exit""&gt;Dismiss&lt;/string&gt;
&lt;/button&gt;
</code></pre>

<p>Also, I've used the answers and comments and edited my code to look like this:</p>

<pre><code>button = driver.find_element_by_css_selector(""button.dismiss"")
w.implicitly_wait(10)
ActionChains(w).move_to_element(button).click(button)
</code></pre>

<p>And now I get a new error:</p>

<pre><code>selenium.common.exceptions.WebDriverException: Message: Tried to run command without establishing a connection
</code></pre>

<p>The error happens in line 1: <code>button = driver.find_element_by_css_selector(""button.dismiss"")</code></p>

<p><strong>Note:</strong> I really appreciate the help that has been given, thanks</p>
","7805116","","7805116","","2017-05-23 18:06:49","2020-10-31 02:22:16","How do you fix the ""element not interactable"" exception?","<python><python-3.x><selenium><selenium-webdriver><webdriver>","11","2","2","","","CC BY-SA 3.0","0"
"28200366","1","28204760","","2015-01-28 19:05:36","","73","94481","<p>How do I solve an <code>ImportError: No module named 'cStringIO'</code> under Python 3.x?</p>
","2811779","","202229","","2019-04-05 11:56:16","2020-10-15 10:28:18","python 3.x ImportError: No module named 'cStringIO'","<python-3.x><stringio><cstringio>","2","2","8","","","CC BY-SA 4.0","0"
"44916637","1","44916657","","2017-07-05 03:39:46","","14","93772","<p>I am having trouble with the following piece of code:</p>

<pre><code>    if verb == ""stoke"":

        if items[""furnace""] &gt;= 1:
            print(""going to stoke the furnace"")

            if items[""coal""] &gt;= 1:
                print(""successful!"")
                temperature += 250 
                print(""the furnace is now "" + (temperature) + ""degrees!"")
                           ^this line is where the issue is occuring
            else:
                print(""you can't"")

        else:
            print(""you have nothing to stoke"")
</code></pre>

<p>The resulting error comes up as the following:</p>

<pre><code>    Traceback(most recent call last):
       File ""C:\Users\User\Documents\Python\smelting game 0.3.1 build 
       incomplete.py""
     , line 227, in &lt;module&gt;
         print(""the furnace is now "" + (temperature) + ""degrees!"")
    TypeError: must be str, not int
</code></pre>

<p>I am unsure what the problem is as i have changed the name from temp to temperature and added the brackets around temperature but still the error occurs.</p>
","8243308","","","","","2019-01-15 17:10:44","Python TypeError must be str not int","<python><python-3.x><typeerror>","3","0","1","2017-07-06 00:22:32","","CC BY-SA 3.0","0"
"35261055","1","","","2016-02-08 00:54:44","","14","93680","<p>I'm writing some code to create an unsorted list but whenever I try to insert a list using the insert method I get the 'method' object is not subscriptable error. Not sure how to fix it. Thanks.</p>

<pre><code>class UnsortedList:
    def __init__(self):
        self.theList = list()
    def __getitem__(self, i):
       print(self.theList[i])
    def insert(self, lst):
        for x in lst:
            try:
                self.theList.append(float(x))
            except:
                print(""oops"")


myList = UnsortedList()
myList.insert[1, 2, 3]
</code></pre>
","5896572","","","","","2018-03-26 14:57:18","'method' object is not subscriptable. Don't know what's wrong","<python><python-3.x>","1","0","1","","","CC BY-SA 3.0","0"
"42128830","1","42444003","","2017-02-09 05:21:23","","20","93402","<p>I am trying a simple demo code of tensorflow from <a href=""https://github.com/llSourcell/tensorflow_demo"" rel=""noreferrer"">github link</a>.<br>
I'm currently using python version 3.5.2 <br><br></p>

<pre><code>Z:\downloads\tensorflow_demo-master\tensorflow_demo-master&gt;py Python
3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AMD64)] on win32&lt;br&gt; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
</code></pre>

<p>I ran into this error when I tried board.py in command-line. I have installed all the dependencies that are required for this to run.</p>

<pre><code>def _read32(bytestream):
    dt = numpy.dtype(numpy.uint32).newbyteorder('&gt;')
    return numpy.frombuffer(bytestream.read(4), dtype=dt)

def extract_images(filename):
    """"""Extract the images into a 4D uint8 numpy array [index, y, x, depth].""""""
    print('Extracting', filename)
    with gzip.open(filename) as bytestream:
        magic = _read32(bytestream)
        if magic != 2051:
            raise ValueError(
                'Invalid magic number %d in MNIST image file: %s' %
                (magic, filename))
        num_images = _read32(bytestream)
        rows = _read32(bytestream)
        cols = _read32(bytestream)
        buf = bytestream.read(rows * cols * num_images)
        data = numpy.frombuffer(buf, dtype=numpy.uint8)
        data = data.reshape(num_images, rows, cols, 1)
    return data

Z:\downloads\tensorflow_demo-master\tensorflow_demo-master&gt;py board.py
Extracting  Z:/downloads/MNIST dataset\train-images-idx3-ubyte.gz
Traceback (most recent call last):  
File ""board.py"", line 3, in &lt;module&gt;
    mnist = input_data.read_data_sets(r'Z:/downloads/MNIST dataset', one_hot=True)  
File ""Z:\downloads\tensorflow_demo-master\tensorflow_demo-master\input_data.py"", line 150, in read_data_sets
    train_images = extract_images(local_file) 
File ""Z:\downloads\tensorflow_demo-master\tensorflow_demo-master\input_data.py"", line 40, in extract_images
    buf = bytestream.read(rows * cols * num_images) 
File ""C:\Users\surak\AppData\Local\Programs\Python\Python35\lib\gzip.py"", line 274, in read
    return self._buffer.read(size)
TypeError: only integer scalar arrays can be converted to a scalar index
</code></pre>
","6833276","","3730397","","2017-04-16 18:43:42","2017-04-16 18:43:42","TypeError: only integer scalar arrays can be converted to a scalar index","<python><python-3.x><numpy><tensorflow><mnist>","3","0","5","","","CC BY-SA 3.0","0"
"41596810","1","41597044","","2017-01-11 17:13:47","","54","92213","<p>Right now, I catch the exception in the <code>except Exception:</code> clause, and do <code>print(exception)</code>. The result provides no information since it always prints <code>&lt;class 'Exception'&gt;</code>. I knew this used to work in python 2, but how do I do it in python3?</p>
","6531092","","2399775","","2019-11-19 22:49:55","2020-10-14 18:48:04","How to print an exception in Python 3?","<python><python-3.x><exception><web-scraping>","7","1","10","","","CC BY-SA 3.0","0"
"51919448","1","51919505","","2018-08-19 16:00:49","","21","92168","<p>I haven't been coding for awhile and trying to get back into Python. I'm trying to write a simple program that sums an array by adding each array element value to a sum. This is what I have:</p>

<pre><code>def sumAnArray(ar):
    theSum = 0
    for i in ar:
        theSum = theSum + ar[i]
    print(theSum)
    return theSum
</code></pre>

<p>I get the following error:</p>

<pre><code>line 13, theSum = theSum + ar[i]
IndexError: list index out of range
</code></pre>

<p>I found that what I'm trying to do is apparently as simple as this:</p>

<pre><code>sum(ar)
</code></pre>

<p>But clearly I'm not iterating through the array properly anyway, and I figure it's something I will need to learn properly for other purposes. Thanks!</p>
","6420521","","","","","2019-09-08 11:56:20","Iterating over arrays in Python 3","<python><arrays><python-3.x><sum>","4","4","2","","","CC BY-SA 4.0","0"
"37139786","1","37140173","","2016-05-10 13:19:39","","214","92006","<p>I am using Python 3.5.1. I read the document and the package section here: <a href=""https://docs.python.org/3/tutorial/modules.html#packages"">https://docs.python.org/3/tutorial/modules.html#packages</a></p>

<p>Now, I have the following structure:</p>

<pre><code>/home/wujek/Playground/a/b/module.py
</code></pre>

<p><code>module.py</code>:</p>

<pre><code>class Foo:
    def __init__(self):
        print('initializing Foo')
</code></pre>

<p>Now, while in <code>/home/wujek/Playground</code>:</p>

<pre><code>~/Playground $ python3
&gt;&gt;&gt; import a.b.module
&gt;&gt;&gt; a.b.module.Foo()
initializing Foo
&lt;a.b.module.Foo object at 0x100a8f0b8&gt;
</code></pre>

<p>Similarly, now in home, superfolder of <code>Playground</code>:</p>

<pre><code>~ $ PYTHONPATH=Playground python3
&gt;&gt;&gt; import a.b.module
&gt;&gt;&gt; a.b.module.Foo()
initializing Foo
&lt;a.b.module.Foo object at 0x10a5fee10&gt;
</code></pre>

<p>Actually, I can do all kinds of stuff:</p>

<pre><code>~ $ PYTHONPATH=Playground python3
&gt;&gt;&gt; import a
&gt;&gt;&gt; import a.b
&gt;&gt;&gt; import Playground.a.b
</code></pre>

<p>Why does this work? I though there needed to be <code>__init__.py</code> files (empty ones would work) in both <code>a</code> and <code>b</code> for <code>module.py</code> to be importable when the Python path points to the <code>Playground</code> folder?</p>

<p>This seems to have changed from Python 2.7:</p>

<pre><code>~ $ PYTHONPATH=Playground python
&gt;&gt;&gt; import a
ImportError: No module named a
&gt;&gt;&gt; import a.b
ImportError: No module named a.b
&gt;&gt;&gt; import a.b.module
ImportError: No module named a.b.module
</code></pre>

<p>With <code>__init__.py</code> in both <code>~/Playground/a</code> and <code>~/Playground/a/b</code> it works fine.</p>
","1385578","","202229","","2019-05-08 03:13:52","2020-05-18 09:51:20","Is __init__.py not required for packages in Python 3.3+","<python><python-3.x><package>","4","0","61","","","CC BY-SA 4.0","0"
"42096280","1","42096429","","2017-02-07 17:31:39","","114","91815","<p>I am a beginner and I want to learn computer programming. So, for now, I have started learning Python by myself with some knowledge about programming in C and Fortran.</p>

<p>Now, I have installed Python version 3.6.0 and I have struggled finding a suitable text for learning Python in this version. Even the online lecture series ask for versions 2.7 and 2.5 . </p>

<p>Now that I have got a book which, however, makes codes in version 2 and <em>tries</em> to make it <em>as close as possible</em> in version 3 (according to the author); the author recommends ""downloading Anaconda for Windows"" for installing Python.</p>

<p>So, my <strong>question</strong> is: What is this <strong>'Anaconda'</strong>? I saw that it was some open data science platform. What does it mean? Is it some editor or something like Pycharm, IDLE or something?</p>

<p>Also, I downloaded my Python (the one that I am using right now) for Windows from Python.org and I didn't need to install any ""open data science platform"".
So what is this happening? </p>

<p>Please explain in easy language. I don't have too much knowledge about these.</p>
","7530167","","","","","2019-12-17 19:45:55","How is Anaconda related to Python?","<python><python-3.x><anaconda>","3","5","25","","","CC BY-SA 3.0","0"
"38350482","1","38350533","","2016-07-13 11:30:23","","7","91606","<p>I'm required to covert the variable:</p>

<pre><code>pi_string = ""3.1415926""
</code></pre>

<p>into a float. Here's what I'm dealing with:</p>

<p><img src=""https://i.stack.imgur.com/ykumQ.png"" alt=""screenshot""></p>
","6584092","","3001761","","2016-07-13 11:35:23","2017-08-23 10:36:31","How to convert String into Float?","<python><python-3.x>","2","4","","2016-07-13 11:35:33","","CC BY-SA 3.0","0"
"29104107","1","29104528","","2015-03-17 16:08:33","","33","91577","<p>I'm working with wechat APIs ...
here I've to upload an image to wechat's server using this API
<a href=""http://admin.wechat.com/wiki/index.php?title=Transferring_Multimedia_Files"" rel=""nofollow noreferrer"">http://admin.wechat.com/wiki/index.php?title=Transferring_Multimedia_Files</a></p>
<pre><code>    url = 'http://file.api.wechat.com/cgi-bin/media/upload?access_token=%s&amp;type=image'%access_token
    files = {
        'file': (filename, open(filepath, 'rb')),
        'Content-Type': 'image/jpeg',
        'Content-Length': l
    }
    r = requests.post(url, files=files)
</code></pre>
<p>I'm not able to post data</p>
","4414786","","623735","","2020-10-15 03:30:03","2020-10-15 03:30:03","Upload Image using POST form data in Python-requests","<python><python-3.x><curl><python-requests><wechat>","7","0","14","","","CC BY-SA 4.0","0"
"30483977","1","30484112","","2015-05-27 13:29:57","","47","91485","<p>As an input to an API request I need to get yesterday's date as a string in the format <code>YYYY-MM-DD</code>. I have a working version which is:</p>

<pre><code>yesterday = datetime.date.fromordinal(datetime.date.today().toordinal()-1)
report_date = str(yesterday.year) + \
   ('-' if len(str(yesterday.month)) == 2 else '-0') + str(yesterday.month) + \
   ('-' if len(str(yesterday.day)) == 2 else '-0') + str(yesterday.day)
</code></pre>

<p>There must be a more elegant way to do this, interested for educational purposes as much as anything else!</p>
","1865336","","2867928","","2016-11-18 08:15:17","2020-09-14 21:47:35","Python - Get Yesterday's date as a string in YYYY-MM-DD format","<python><python-2.7><python-3.x><date><datetime>","4","1","11","","","CC BY-SA 3.0","0"
"43675771","1","43675931","","2017-04-28 08:39:38","","17","90069","<p>I have some data objects on which I want to implement a to string and equals functions that go in depth. </p>

<p>I implemented <strong>str</strong> and <strong>eq</strong> and although equality works fine I cannot make <strong>str</strong> behave in the same way:</p>

<pre><code>class Bean(object):

    def __init__(self, attr1, attr2):
        self.attr1 = attr1
        self.attr2 = attr2

    def __str__(self):
        return str(self.__dict__)

    def __eq__(self, other):
        return self.__dict__ == other.__dict__
</code></pre>

<p>When I run:</p>

<pre><code>t1 = Bean(""bean 1"", [Bean(""bean 1.1"", ""same""), Bean(""bean 1.2"", 42)])
t2 = Bean(""bean 1"", [Bean(""bean 1.1"", ""same""), Bean(""bean 1.2"", 42)])
t3 = Bean(""bean 1"", [Bean(""bean 1.1"", ""different""), Bean(""bean 1.2"", 42)])

print(t1)
print(t2)
print(t3)
print(t1 == t2)
print(t1 == t3)
</code></pre>

<p>I get:</p>

<pre><code>{'attr2': [&lt;__main__.Bean object at 0x7fc092030f28&gt;, &lt;__main__.Bean object at 0x7fc092030f60&gt;], 'attr1': 'bean 1'}
{'attr2': [&lt;__main__.Bean object at 0x7fc091faa588&gt;, &lt;__main__.Bean object at 0x7fc092045128&gt;], 'attr1': 'bean 1'}
{'attr2': [&lt;__main__.Bean object at 0x7fc0920355c0&gt;, &lt;__main__.Bean object at 0x7fc092035668&gt;], 'attr1': 'bean 1'}
True
False
</code></pre>

<p>since t1 and t2 contain the same values the equals return true (as expected) while since t3 contains a different value in the list the result is false (also as expected).
What I would like is to have the same behavior for the to string (basically to also go in depth also for the elements in list (or set or dict ...).</p>

<p>For print(t1) I would like to obtain something like:</p>

<pre><code>{'attr2': [""{'attr2': 'same', 'attr1': 'bean 1.1'}"", ""{'attr2': 42, 'attr1': 'bean 1.2'}""], 'attr1': 'bean 1'}
</code></pre>

<p>which is actually obtained if I do:</p>

<pre><code>Bean(""bean 1"", [Bean(""bean 1.1"", ""same"").__str__(), Bean(""bean 1.2"", 42).__str__()]).__str__
</code></pre>

<p>Since I do not know the types of the attributes attr1, attr2 in my Bean objects (they may be lists but also sets, dictionaries etc.) is would be nice to have a simple and elegant solution that would not require type checking ...</p>

<p>Is this possible ?</p>
","1006698","","","","","2017-04-28 11:08:47","object to string in Python","<python><python-3.x><tostring>","2","2","3","","","CC BY-SA 3.0","0"
"30858392","1","30858457","","2015-06-16 03:43:20","","8","89948","<p>Even though initializing variables in python is not necessary, my professor still wants us to do it for practice. I wrote my program and it worked fine, but after I tried to initialize some of the variables I got an error message when I tried to run it. Here is the first part of my program:</p>

<pre><code>def main():

    grade_1, grade_2, grade_3, average = 0.0
    year = 0

    fName, lName, ID, converted_ID = """"
    infile = open(""studentinfo.txt"", ""r"")
    data = infile.read()
    fName, lName, ID, year = data.split("","")
    year = int(year)

    # Prompt the user for three test scores

    grades = eval(input(""Enter the three test scores separated by a comma: ""))

    # Create a username

    uName = (lName[:4] + fName[:2] + str(year)).lower()
    converted_id = ID[:3] + ""-"" + ID[3:5] + ""-"" + ID[5:]
    grade_1, grade_2, grade_3 = grades
</code></pre>

<h1>The error message:</h1>

<pre><code>grade_1, grade_2, grade_3, average = 0.0

TypeError: 'float' object is not iterable
</code></pre>
","2297237","","248296","","2015-06-16 05:31:22","2019-11-25 07:21:59","""Initializing"" variables in python?","<python><python-3.x>","7","6","2","","","CC BY-SA 3.0","0"
"47632891","1","","","2017-12-04 11:59:51","","14","89159","<p>I've just installed python 3.6 which comes with pip</p>

<p>However, in Windows command prompt, when I do: 'pip install bs4' it returns 'SyntaxError: invalid syntax' under the install word.</p>

<p>Typing 'python' returns the version, which means it is installed correctly. What could be the problem?</p>
","9050754","","104349","","2017-12-04 12:11:03","2020-06-29 16:31:07","pip install returning invalid syntax","<python><python-3.x><pip>","14","6","4","","","CC BY-SA 3.0","0"
"36757752","1","36759995","","2016-04-21 00:56:02","","23","89111","<p>I need to port some code that's Python2+PyQt4 to Python3+PyQt5. </p>

<p>I started installing pip3</p>

<pre><code>sudo apt-get install python3-pip
</code></pre>

<p>Works great. Tried </p>

<pre><code>sudo pip3 install PyQt5
Downloading/unpacking PyQt5
  Could not find any downloads that satisfy the requirement PyQt5
Cleaning up...
No distributions at all found for PyQt5
</code></pre>

<p>Online I find the following steps:</p>

<p><a href=""http://pyqt.sourceforge.net/Docs/PyQt5/installation.html"" rel=""noreferrer"">http://pyqt.sourceforge.net/Docs/PyQt5/installation.html</a></p>

<p>But they are too many. What's the easiest way to Install PyQt5 along with Python3 in Ubuntu 14.04 ?</p>
","965638","","","","","2020-04-10 06:39:58","How to install PyQt5 in Python 3 (Ubuntu 14.04)","<python-3.x><ubuntu-14.04><pyqt5>","3","2","12","","","CC BY-SA 3.0","0"
"43258461","1","43258974","","2017-04-06 14:44:00","","55","87605","<p>I am trying to convert png to jpeg using pillow. I've tried several scrips without success. These 2 seemed to work on small png images like this one.</p>

<p><a href=""https://i.stack.imgur.com/m2GGn.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/m2GGn.jpg"" alt=""enter image description here""></a></p>

<p>First code:</p>

<pre><code>from PIL import Image
import os, sys

im = Image.open(""Ba_b_do8mag_c6_big.png"")
bg = Image.new(""RGB"", im.size, (255,255,255))
bg.paste(im,im)
bg.save(""colors.jpg"")
</code></pre>

<p>Second code: </p>

<pre><code>image = Image.open('Ba_b_do8mag_c6_big.png')
bg = Image.new('RGBA',image.size,(255,255,255))
bg.paste(image,(0,0),image)
bg.save(""test.jpg"", quality=95)
</code></pre>

<p>But if I try to convert a bigger image like this one</p>

<p><img src=""https://upload.wikimedia.org/wikipedia/commons/9/92/Ba_b_do8mag_c6_big.png"" alt=""""> </p>

<p>I'm getting</p>

<pre><code>Traceback (most recent call last):
  File ""png_converter.py"", line 14, in &lt;module&gt;
    bg.paste(image,(0,0),image)
  File ""/usr/lib/python2.7/dist-packages/PIL/Image.py"", line 1328, in paste
    self.im.paste(im, box, mask.im) ValueError: bad transparency mask
</code></pre>

<p>What am i doing wrong?</p>
","7161215","","355230","","2019-08-08 14:46:11","2019-08-08 14:46:11","Convert png to jpeg using Pillow","<python><python-3.x><image><python-imaging-library>","4","2","14","","","CC BY-SA 4.0","0"
"35569042","1","","","2016-02-23 04:47:21","","65","87414","<p>I apologize if this is a silly question, but I have been trying to teach myself how to use BeautifulSoup so that I can create a few projects.</p>

<p>I was following this link as a tutorial: <a href=""https://www.youtube.com/watch?v=5GzVNi0oTxQ"" rel=""noreferrer"">https://www.youtube.com/watch?v=5GzVNi0oTxQ</a></p>

<p>After following the exact same code as him, this is the error that I get: </p>

<pre><code>Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/urllib/request.py"", line 1240, in do_open
    h.request(req.get_method(), req.selector, req.data, headers)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 1083, in request
    self._send_request(method, url, body, headers)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 1128, in _send_request
    self.endheaders(body)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 1079, in endheaders
self._send_output(message_body)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 911, in _send_output
    self.send(msg)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 854, in send
    self.connect()
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py"", line 1237, in connect
server_hostname=server_hostname)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/ssl.py"", line 376, in wrap_socket
_context=self)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/ssl.py"", line 747, in __init__
self.do_handshake()
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/ssl.py"", line 983, in do_handshake
    self._sslobj.do_handshake()
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/ssl.py"", line 628, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:645)
</code></pre>

<p>During handling of the above exception, another exception occurred:</p>

<pre><code>Traceback (most recent call last):
  File ""WorldCup.py"", line 3, in &lt;module&gt;
    x = urllib.request.urlopen('https://www.google.com')
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/urllib/request.py"", line 162, in urlopen
    return opener.open(url, data, timeout)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/urllib/request.py"", line 465, in open
    response = self._open(req, data)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/urllib/request.py"", line 483, in _open
'_open', req)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/urllib/request.py"", line 443, in _call_chain
    result = func(*args)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/urllib/request.py"", line 1283, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/urllib/request.py"", line 1242, in do_open
raise URLError(err)
urllib.error.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED]     certificate verify failed (_ssl.c:645)&gt;
</code></pre>

<p>Can someone help me figure out how to fix this?</p>
","4790055","","958624","","2019-01-18 21:57:23","2020-06-10 18:48:10","SSL: CERTIFICATE_VERIFY_FAILED with Python3","<python><python-3.x><macos><urllib>","5","5","22","","","CC BY-SA 3.0","0"
"42767489","1","42768387","","2017-03-13 15:30:29","","28","87150","<p>I am plotting multiple dataframes as point plot using <code>seaborn</code>. Also I am plotting all the dataframes <strong>on the same axis</strong>. </p>

<p><strong>How would I add legend to the plot ?</strong></p>

<p>My code takes each of the dataframe and plots it one after another on the same figure.</p>

<p>Each dataframe has same columns </p>

<pre><code>date        count
2017-01-01  35
2017-01-02  43
2017-01-03  12
2017-01-04  27 
</code></pre>

<p>My code :</p>

<pre><code>f, ax = plt.subplots(1, 1, figsize=figsize)
x_col='date'
y_col = 'count'
sns.pointplot(ax=ax,x=x_col,y=y_col,data=df_1,color='blue')
sns.pointplot(ax=ax,x=x_col,y=y_col,data=df_2,color='green')
sns.pointplot(ax=ax,x=x_col,y=y_col,data=df_3,color='red')
</code></pre>

<p>This plots 3 lines on the same plot. However the legend is missing. <a href=""http://seaborn.pydata.org/generated/seaborn.pointplot.html"" rel=""noreferrer"">The documentation</a> does not accept <code>label</code> argument .</p>

<p>One workaround that worked was creating a new dataframe and using <code>hue argument</code>.</p>

<pre><code>df_1['region'] = 'A'
df_2['region'] = 'B'
df_3['region'] = 'C'
df = pd.concat([df_1,df_2,df_3])
sns.pointplot(ax=ax,x=x_col,y=y_col,data=df,hue='region')
</code></pre>

<p>But I would like to know if there is a way to create a legend for the code that first adds sequentially point plot to the figure and then add a legend.</p>

<p>Sample output :</p>

<p><a href=""https://i.stack.imgur.com/sXN1t.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/sXN1t.jpg"" alt=""Seaborn Image""></a></p>
","7491646","","7491646","","2017-03-13 15:53:06","2020-02-25 06:08:00","Add Legend to Seaborn point plot","<python><python-3.x><matplotlib><seaborn>","3","1","5","","","CC BY-SA 3.0","0"
"44650888","1","44659589","","2017-06-20 10:43:53","","44","86607","<p>I am using python 3 and latest version of openCV. I am trying to resize an image using the resize function provided but after resizing the image is very distorted. Code :</p>

<pre><code>import cv2
file = ""/home/tanmay/Desktop/test_image.png""
img = cv2.imread(file , 0)
print(img.shape)
cv2.imshow('img' , img)
k = cv2.waitKey(0)
if k == 27:
    cv2.destroyWindow('img')
resize_img = cv2.resize(img  , (28 , 28))
cv2.imshow('img' , resize_img)
x = cv2.waitKey(0)
if x == 27:
    cv2.destroyWindow('img')
</code></pre>

<p>The original image is 480 x 640 (RGB therefore i passed the 0 to get it to grayscale)</p>

<p>Is there any way i could resize it and avoid the distortion using OpenCV or any other library perhaps? I intend to make a handwritten digit recogniser and i have trained my neural network using the MNIST data therefore i need the image to be 28x28.</p>
","6916919","","","","","2020-08-06 15:50:52","Resize an image without distortion OpenCV","<python><image><python-3.x><opencv>","9","5","24","","","CC BY-SA 3.0","0"
"36386346","1","36386530","","2016-04-03 14:01:06","","41","86007","<p>I have a problem when I try to assign a value to a variable. The problem shows up when I try to put a date as a tuple or a list in this order: <code>year, month, day.</code></p>

<pre><code>&gt;&gt;&gt; a = (2016,04,03)         # I try to put the date into variable 'a' as a tuple.
SyntaxError: invalid token
&gt;&gt;&gt; a = [2016,04,03]         # I try to put the date into variable 'a' as a list.
SyntaxError: invalid token
</code></pre>

<ol>
<li><p>Why is this happing?</p></li>
<li><p>How do I fix it?</p></li>
<li><p>What does token mean in Python?</p></li>
</ol>
","5104016","","5104016","","2019-06-12 02:49:00","2019-06-12 02:49:00","SyntaxError invalid token","<python><python-3.x><syntax>","4","9","7","","","CC BY-SA 4.0","0"
"48140858","1","","","2018-01-07 19:40:11","","17","85983","<p>I am running the following code-</p>

<pre><code>import json

addrsfile = 
open(""C:\\Users\file.json"", 
""r"")
addrJson = json.loads(addrsfile.read())
addrsfile.close()
if addrJson:
    print(""yes"")
</code></pre>

<p>But giving me following error-</p>

<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):
  File ""C:/Users/Mayur/Documents/WebPython/Python_WebServices/test.py"", line 9, in &lt;module&gt;
    addrJson = json.loads(addrsfile.read())
  File ""C:\Users\Mayur\Anaconda3\lib\json\__init__.py"", line 354, in loads
    return _default_decoder.decode(s)
  File ""C:\Users\Mayur\Anaconda3\lib\json\decoder.py"", line 342, in decode
    raise JSONDecodeError(""Extra data"", s, end)
json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 190)
</code></pre>

<p>Anyone help me please?</p>

<p>JSON file is like-</p>

<pre><code>{""name"": ""XYZ"", ""address"": ""54.7168,94.0215"", ""country_of_residence"": ""PQR"", ""countries"": ""LMN;PQRST"", ""date"": ""28-AUG-2008"", ""type"": null}
{""name"": ""OLMS"", ""address"": null, ""country_of_residence"": null, ""countries"": ""Not identified;No"", ""date"": ""23-FEB-2017"", ""type"": null}
</code></pre>
","6536322","","7053679","","2019-02-22 05:08:47","2020-10-24 15:37:44","json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 190)","<python><json><python-3.x>","3","1","4","2020-10-24 15:47:52","","CC BY-SA 4.0","0"
"28921333","1","28921336","","2015-03-07 23:32:58","","66","85777","<p>I am currently running OS X Yosemite (10.10.2) on my MacBook Pro... By default, Apple ships Python 2.7.6 on Yosemite. </p>

<p>Just downloaded and ran this installer for Python 3: <code>python-3.4.3-macosx10.6.pkg</code></p>

<p>When I opened up my Terminal and typed in <code>python</code>, this is what came up:</p>

<pre><code>Python 2.7.6 (default, Sep  9 2014, 15:04:36) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt;
</code></pre>

<p>Question(s):</p>

<ol>
<li>Does anyone know where the Python 3.4.3 interpreter was installed? </li>
<li>Do I need to uninstall Python 2.7.3 (if so, how do I go about doing this) before setting a global environmental variable such as PYTHON_HOME to the location of the installed Python 3.4.3?</li>
</ol>
","146234","","5183619","","2016-10-16 14:26:27","2020-09-30 16:27:05","Installed Python 3 on Mac OS X but its still Python 2.7","<python><python-2.7><python-3.x><osx-yosemite>","9","0","7","","","CC BY-SA 3.0","0"
"30015983","1","30016474","","2015-05-03 15:59:26","","12","85288","<p>I am trying to start learning Django, but I can't even pass through the installation. I have Python 3.4, Django, setuptools and Apache installed. Is database all that is left to setup? </p>

<p>Also, I wanted to try with MySQL and this is the link from djangoproject: <a href=""https://pypi.python.org/pypi/mysqlclient"" rel=""noreferrer"">https://pypi.python.org/pypi/mysqlclient</a></p>

<p>The problem is that I can't find any information on what .whl file should I download, how to open it and if that is all I have to do. I dont get why there isn't some integrated Django developement kit or something, but that is a question for some other forum :)</p>
","2975357","","","","","2020-10-21 15:45:24","Django mysqlclient install","<mysql><django><database><python-3.x><installation>","7","0","6","","","CC BY-SA 3.0","0"
"31161243","1","31161384","","2015-07-01 12:24:56","","43","85052","<p>Am trying to a run this piece of code, and it keeps giving an error saying ""String argument without an encoding""</p>

<pre><code>ota_packet = ota_packet.encode('utf-8') + bytearray(content[current_pos:(final_pos)]) + '\0'.encode('utf-8')
</code></pre>

<p>Any help?</p>
","5069576","","1245190","","2017-09-02 15:12:58","2019-05-31 15:28:48","Python string argument without an encoding","<python><python-3.x><encoding><python-unicode>","1","1","4","","","CC BY-SA 3.0","0"
"41837247","1","41837653","","2017-01-24 19:23:03","","27","85027","<p>I usually do this to convert string to int:</p>

<pre><code>my_input = int(my_input)
</code></pre>

<p>but I was wondering if there was a less clumsy way, because it feels kind of long.</p>
","7294139","","3064538","","2019-10-07 00:34:05","2019-10-07 00:42:33","Short way to convert string to int","<python><python-3.x><type-conversion>","3","0","7","","","CC BY-SA 4.0","0"
"29780060","1","29781023","","2015-04-21 18:14:31","","61","84507","<p>For some reason I cannot figure out why Django isn't handling my <code>request.body</code> content correctly.</p>

<p>It is being sent in <code>JSON</code> format, and looking at the <code>Network</code> tab in Dev Tools shows this as the request payload:</p>

<pre><code>{creator: ""creatorname"", content: ""postcontent"", date: ""04/21/2015""}
</code></pre>

<p>which is exactly how I want it to be sent to my API.</p>

<p>In Django I have a view that accepts this request as a parameter and just for my testing purposes, should print <code>request.body[""content""]</code> to the console. </p>

<p>Of course, nothing is being printed out, but when I print <code>request.body</code> I get this:</p>

<pre><code>b'{""creator"":""creatorname"",""content"":""postcontent"",""date"":""04/21/2015""}'
</code></pre>

<p>so I know that I <em>do</em> have a body being sent.</p>

<p>I've tried using <code>json = json.loads(request.body)</code> to no avail either. Printing <code>json</code> after setting that variable also returns nothing.</p>
","1767263","","113962","","2015-04-21 20:47:30","2017-11-08 13:31:24","Trying to parse `request.body` from POST in Django","<python><json><django><python-3.x><backbone.js>","1","0","14","2016-10-08 11:10:20","","CC BY-SA 3.0","0"
"28218466","1","28218598","","2015-01-29 15:32:39","","135","84139","<p>I'm wondering if there is a way to load an object that was pickled in Python 2.4, with Python 3.4.</p>

<p>I've been running 2to3 on a large amount of company legacy code to get it up to date.</p>

<p>Having done this, when running the file I get the following error:</p>

<pre><code>  File ""H:\fixers - 3.4\addressfixer - 3.4\trunk\lib\address\address_generic.py""
, line 382, in read_ref_files
    d = pickle.load(open(mshelffile, 'rb'))
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 1: ordinal
not in range(128)
</code></pre>

<p>Looking at the pickled object in contention, it's a <code>dict</code> in a <code>dict</code>, containing keys and values of type <code>str</code>.</p>

<p>So my question is: Is there a way to load an object, originally pickled in python 2.4, with python 3.4?</p>
","2830986","","2830986","","2019-07-01 10:29:24","2020-01-18 10:50:32","Unpickling a python 2 object with python 3","<python><python-3.x><pickle><python-2.4><python-2to3>","2","3","28","","","CC BY-SA 4.0","0"
"31633635","1","31633656","","2015-07-26 04:31:28","","35","83594","<p>I cannot understand this. I have seen this in people's code. But cannot figure out what it does. This is in Python.</p>

<pre><code>str(int(a[::-1]))
</code></pre>
","4404487","","2577654","","2015-07-26 04:51:10","2020-05-11 09:28:48","What is the meaning of ""int(a[::-1])"" in Python?","<python><python-2.7><python-3.x>","2","4","19","2018-05-18 18:11:10","","CC BY-SA 3.0","0"
"50408941","1","50410256","","2018-05-18 10:10:58","","32","83224","<p>I am interrested in knowing the recommended way to install pip3 for python3.6 (as of today, may 2018) on current version of centos7 (7.5.1804) and the accepted answer of <a href=""https://stackoverflow.com/questions/32618686/how-to-install-pip-in-centos-7"">How to install pip in CentOS 7?</a> seems to be outdated because:</p>

<pre><code>yum search -v pip
</code></pre>

<p>outputs (among other things):</p>

<pre><code>python2-pip.noarch : A tool for installing and managing Python 2 packages
Repo        : epel

python34-pip.noarch : A tool for installing and managing Python3 packages
Repo        : epel
</code></pre>

<p>and <code>python34-pip</code> seems to be a (newer?) simpler way than the accepted answer of <a href=""https://stackoverflow.com/questions/32618686/how-to-install-pip-in-centos-7"">How to install pip in CentOS 7?</a> :</p>

<blockquote>
  <p>sudo yum install python34-setuptools</p>
  
  <p>sudo easy_install-3.4 pip</p>
</blockquote>

<p>But since the versions of python installed on my machine are 2.7.5 and 3.6.3 why is it python34-pip and not python36-pip ? Is pip the same for 3.4+ (up to current 3.6.3) ?</p>
","5675881","","","","","2020-02-24 03:44:38","Recommended way to install pip(3) on centos7","<python><python-3.x><pip><python-3.6><centos7>","8","1","10","","","CC BY-SA 4.0","0"
"35642855","1","35648343","","2016-02-26 03:24:30","","26","83222","<p>In Python 3 I imported the pySerial library so I could communicate with my Arduino Uno by serial commands.<br>
It worked very well in Python 2.7 but in Python 3 I keep running into a error it says this </p>

<blockquote>
  <p><strong><em>TypeError: unicode strings are not supported, please encode to bytes: 'allon'</em></strong> </p>
</blockquote>

<p>In Python 2.7 the only thing I did differently is use <code>raw_input</code> but I don't know what is happening in Python 3. Here is my code </p>

<pre><code>import serial, time
import tkinter
import os

def serialcmdw():
    os.system('clear')
    serialcmd = input(""serial command: "")
    ser.write (serialcmd)

serialcmdw()

ser = serial.Serial()
os.system('clear')
ser.port = ""/dev/cu.usbmodem4321""
ser.baudrate = 9600
ser.open()
time.sleep(1)
serialcmdw()
</code></pre>
","5983858","","531679","","2020-05-16 17:55:11","2020-05-16 17:55:11","python3 pySerial TypeError: unicode strings are not supported, please encode to bytes:","<python-3.x><arduino><pyserial>","3","0","4","","","CC BY-SA 4.0","0"
"30279783","1","","","2015-05-16 19:19:06","","92","82512","<p>I built Spark 1.4 from the GH development master, and the build went through fine. But when I do a <code>bin/pyspark</code> I get the Python 2.7.9 version. How can I change this?</p>
","1414455","","","","","2019-03-12 16:28:12","Apache Spark: How to use pyspark with Python 3","<python><python-3.x><apache-spark>","5","1","34","","","CC BY-SA 3.0","0"
"47103712","1","47103744","","2017-11-03 20:07:40","","45","82354","<p>Is it possible to do a try-except catch all that still shows the error without catching every possible exception? I have a case where exceptions will happen once a day every few days in a script running 24/7. I can't let the script die but they also don't matter since it retries regardless as long as I try except everything. So while I track down any last rare exceptions I want to log those to a file for future debugging.</p>

<p>example: </p>

<pre><code>try:
    print(555)
except:
    print(""type error: ""+ str(the_error))
</code></pre>

<p>Any way to replace <code>the_error</code> with a stack trace or something similar?</p>
","725793","","3991470","","2017-11-03 20:16:06","2019-10-25 15:36:55","python 3 try-except all with error","<python><python-3.x><try-except>","2","0","10","2017-11-03 20:14:20","","CC BY-SA 3.0","0"
"43865291","1","43865354","","2017-05-09 08:52:27","","38","81970","<p>I'm building a Flask app with Python 3.5 following a tutorial, based on different import rules. By looking for similar questions, I managed to solve an ImportError based on importing from a nested folder by adding the folder to the path, but I keep failing at importing a function from a script in the same folder (already in the path). The folder structure is this:</p>

<pre><code>DoubleDibz  
├── app
│   ├── __init__.py
│   ├── api 
│   │   ├── __init__.py
│   │   └── helloworld.py
│   ├── app.py
│   ├── common
│   │   ├── __init__.py
│   │   └── constants.py
│   ├── config.py
│   ├── extensions.py
│   ├── static
│   └── templates
└── run.py
</code></pre>

<p>In app.py I import a function from config.py by using this code:</p>

<pre><code>import config as Config
</code></pre>

<p>but I get this error: </p>

<pre><code>ImportError: No module named 'config'
</code></pre>

<p>I don't understand what's the problem, being the two files in the same folder.
Thanks in advance </p>
","4837827","","","","","2020-01-08 15:43:33","import function from a file in the same folder","<python><python-3.x><flask>","4","2","5","","","CC BY-SA 3.0","0"
"53680913","1","53681554","","2018-12-08 08:41:30","","18","81756","<p>I know this question has been asked before but I can't seem to get mine to work. Can someone help me out with this?</p>

<pre><code>import numpy as np


def load_dataset():
    def download(filename, source=""http://yaan.lecun.com/exdb/mnist/""):
        print (""Downloading "",filename)
        import urllib
        urllib.urlretrieve(source+filename,filename)

    import gzip

    def load_mnist_images(filename):
        if not os.path.exists(filename):
            download(filename)
        with gzip.open(filename,""rb"") as f:
            data=np.frombuffer(f.read(), np.uint8, offset=16)

            data = data.reshape(-1,1,28,28)

            return data/np.float32(256)

        def load_mnist_labels(filename):
            if not os.path.exists(filename):
                download(filename)
            with gzip.open(filename,""rb"") as f:
                data = np.frombuffer(f.read(), np.uint8, offset=8)
            return data

        X_train = load_mnist_images(""train-images-idx3-ubyte.gz"")
        y_train = load_mnist_labels(""train-labels-idx1-ubyte.gz"")
        X_test = load_mnist_images(""t10k-images-idx3-ubyte.gz"")
        y_test = load_mnist_labels(""t10k-labels-idx1-ubyte.gz"")

        return X_train, y_train, X_test, y_test


X_train, y_train, X_test, y_test = load_dataset()


import matplotlib
matplotlib.use(""TkAgg"")

import matplotlib.pyplot as plt
plt.show(plt.imshow(X_train[3][0]))
</code></pre>

<p>This is the error I am getting:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\nehad\Desktop\Neha\Non-School\Python\Handwritten Digits 
Recognition.py"", line 38, in &lt;module&gt;
    X_train, y_train, X_test, y_test = load_dataset()
TypeError: cannot unpack non-iterable NoneType object
</code></pre>

<p>I am new to machine learning. Did I just miss something simple? I am trying a Handwritten Digit Recognition project for my school Science Exhibition.</p>

<p>Thanks in advance!</p>
","10763076","","8076979","","2018-12-08 09:40:33","2020-06-09 19:30:16","TypeError: cannot unpack non-iterable NoneType object","<python><python-3.x>","3","1","2","","","CC BY-SA 4.0","0"
"38246412","1","","","2016-07-07 13:11:23","","20","81706","<p>I'm trying to store salt and hashed password before inserting each document into a collection. But on encoding the salt and password, it shows the following error:</p>

<pre><code> line 26, in before_insert
 document['salt'] = bcrypt.gensalt().encode('utf-8')

AttributeError: 'bytes' object has no attribute 'encode'
</code></pre>

<p>This is my code: </p>

<pre><code>def before_insert(documents):
    for document in documents:
        document['salt'] = bcrypt.gensalt().encode('utf-8')
        password = document['password'].encode('utf-8')
        document['password'] = bcrypt.hashpw(password, document['salt'])
</code></pre>

<p>I'm using eve framework in virtualenv with python 3.4</p>
","6168014","","3995261","","2018-08-27 23:05:05","2018-08-27 23:35:58","'bytes' object has no attribute 'encode'","<python><python-3.x><bcrypt>","2","5","2","","","CC BY-SA 4.0","0"
"34247930","1","","","2015-12-13 04:16:48","","17","81010","<p>I have installed python 3.5 on my Windows 7 machine. When I installed it, I marked the check box to install <code>pip</code>. After the installation, I wanted to check whether pip was working, so I typed <code>pip</code> on the command line and hit enter, but did not respond. The cursor blinks but it does not display anything.</p>

<p>Please help.</p>

<p>Regards.   </p>
","4542278","","4542278","","2020-05-06 08:59:42","2020-05-06 08:59:42","pip not working in python 3.5 on Windows 7","<python-3.x><windows-7-x64>","10","1","9","","","CC BY-SA 4.0","0"
"33945261","1","33945518","","2015-11-26 18:45:00","","256","80618","<p>I have a function in python that can either return a <code>bool</code> or a <code>list</code>. Is there a way to specify the return types using type hints.</p>

<p>For example, Is this the correct way to do it?</p>

<pre><code>def foo(id) -&gt; list or bool:
      ...
</code></pre>
","3006737","","4099593","","2015-11-26 20:22:49","2019-12-18 14:34:51","How to specify multiple return types using type-hints","<python><python-3.x><return-type><type-hinting><python-3.5>","3","3","47","","","CC BY-SA 3.0","0"
"40893602","1","","","2016-11-30 16:38:31","","8","80100","<p>I am using Python 3.6b3 for a long running project, developing on Windows.
For this project I also need NumPy.
I've tried Python36 -m pip install numpy, but it seems that pip is not yet in the beta.
What's the best way to install NumPy for Python 3.6b3?</p>

<p>[EDIT: Added installation log, after using ensurepip]</p>

<pre><code>D:\aaa\numpy-1.12.0b1&gt;call C:\Python36\python.exe -m pip install numpy 
Collecting numpy
  Using cached numpy-1.11.2.tar.gz
Installing collected packages: numpy
  Running setup.py install for numpy: started
    Running setup.py install for numpy: finished with status 'error'
    Complete output from command C:\Python36\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\info_000\\AppData\\Local\\Temp\\pip-build-ueljt0po\\numpy\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record C:\Users\info_000\AppData\Local\Temp\pip-nmezr3c7-record\install-record.txt --single-version-externally-managed --compile:
    Running from numpy source directory.

    Note: if you need reliable uninstall behavior, then install
    with pip instead of using `setup.py install`:

      - `pip install .`       (from a git repo or downloaded source
                               release)
      - `pip install numpy`   (last Numpy release on PyPi)


    blas_opt_info:
    blas_mkl_info:
      libraries mkl_rt not found in ['C:\\Python36\\lib', 'C:\\', 'C:\\Python36\\libs']
      NOT AVAILABLE

    openblas_info:
      libraries openblas not found in ['C:\\Python36\\lib', 'C:\\', 'C:\\Python36\\libs']
      NOT AVAILABLE

    atlas_3_10_blas_threads_info:
    Setting PTATLAS=ATLAS
      libraries tatlas not found in ['C:\\Python36\\lib', 'C:\\', 'C:\\Python36\\libs']
      NOT AVAILABLE

    atlas_3_10_blas_info:
      libraries satlas not found in ['C:\\Python36\\lib', 'C:\\', 'C:\\Python36\\libs']
      NOT AVAILABLE

    atlas_blas_threads_info:
    Setting PTATLAS=ATLAS
      libraries ptf77blas,ptcblas,atlas not found in ['C:\\Python36\\lib', 'C:\\', 'C:\\Python36\\libs']
      NOT AVAILABLE

    atlas_blas_info:
      libraries f77blas,cblas,atlas not found in ['C:\\Python36\\lib', 'C:\\', 'C:\\Python36\\libs']
      NOT AVAILABLE

    C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\system_info.py:1630: UserWarning:
        Atlas (http://math-atlas.sourceforge.net/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [atlas]) or by setting
        the ATLAS environment variable.
      warnings.warn(AtlasNotFoundError.__doc__)
    blas_info:
      libraries blas not found in ['C:\\Python36\\lib', 'C:\\', 'C:\\Python36\\libs']
      NOT AVAILABLE

    C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\system_info.py:1639: UserWarning:
        Blas (http://www.netlib.org/blas/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [blas]) or by setting
        the BLAS environment variable.
      warnings.warn(BlasNotFoundError.__doc__)
    blas_src_info:
      NOT AVAILABLE

    C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\system_info.py:1642: UserWarning:
        Blas (http://www.netlib.org/blas/) sources not found.
        Directories to search for the sources can be specified in the
        numpy/distutils/site.cfg file (section [blas_src]) or by setting
        the BLAS_SRC environment variable.
      warnings.warn(BlasSrcNotFoundError.__doc__)
      NOT AVAILABLE

    non-existing path in 'numpy\\distutils': 'site.cfg'
    F2PY Version 2
    lapack_opt_info:
    openblas_lapack_info:
      libraries openblas not found in ['C:\\Python36\\lib', 'C:\\', 'C:\\Python36\\libs']
      NOT AVAILABLE

    lapack_mkl_info:
      libraries mkl_rt not found in ['C:\\Python36\\lib', 'C:\\', 'C:\\Python36\\libs']
      NOT AVAILABLE

    atlas_3_10_threads_info:
    Setting PTATLAS=ATLAS
      libraries tatlas,tatlas not found in C:\Python36\lib
      libraries lapack_atlas not found in C:\Python36\lib
      libraries tatlas,tatlas not found in C:\
      libraries lapack_atlas not found in C:\
      libraries tatlas,tatlas not found in C:\Python36\libs
      libraries lapack_atlas not found in C:\Python36\libs
    &lt;class 'numpy.distutils.system_info.atlas_3_10_threads_info'&gt;
      NOT AVAILABLE

    atlas_3_10_info:
      libraries satlas,satlas not found in C:\Python36\lib
      libraries lapack_atlas not found in C:\Python36\lib
      libraries satlas,satlas not found in C:\
      libraries lapack_atlas not found in C:\
      libraries satlas,satlas not found in C:\Python36\libs
      libraries lapack_atlas not found in C:\Python36\libs
    &lt;class 'numpy.distutils.system_info.atlas_3_10_info'&gt;
      NOT AVAILABLE

    atlas_threads_info:
    Setting PTATLAS=ATLAS
      libraries ptf77blas,ptcblas,atlas not found in C:\Python36\lib
      libraries lapack_atlas not found in C:\Python36\lib
      libraries ptf77blas,ptcblas,atlas not found in C:\
      libraries lapack_atlas not found in C:\
      libraries ptf77blas,ptcblas,atlas not found in C:\Python36\libs
      libraries lapack_atlas not found in C:\Python36\libs
    &lt;class 'numpy.distutils.system_info.atlas_threads_info'&gt;
      NOT AVAILABLE

    atlas_info:
      libraries f77blas,cblas,atlas not found in C:\Python36\lib
      libraries lapack_atlas not found in C:\Python36\lib
      libraries f77blas,cblas,atlas not found in C:\
      libraries lapack_atlas not found in C:\
      libraries f77blas,cblas,atlas not found in C:\Python36\libs
      libraries lapack_atlas not found in C:\Python36\libs
    &lt;class 'numpy.distutils.system_info.atlas_info'&gt;
      NOT AVAILABLE

    C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\system_info.py:1532: UserWarning:
        Atlas (http://math-atlas.sourceforge.net/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [atlas]) or by setting
        the ATLAS environment variable.
      warnings.warn(AtlasNotFoundError.__doc__)
    lapack_info:
      libraries lapack not found in ['C:\\Python36\\lib', 'C:\\', 'C:\\Python36\\libs']
      NOT AVAILABLE

    C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\system_info.py:1543: UserWarning:
        Lapack (http://www.netlib.org/lapack/) libraries not found.
        Directories to search for the libraries can be specified in the
        numpy/distutils/site.cfg file (section [lapack]) or by setting
        the LAPACK environment variable.
      warnings.warn(LapackNotFoundError.__doc__)
    lapack_src_info:
      NOT AVAILABLE

    C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\system_info.py:1546: UserWarning:
        Lapack (http://www.netlib.org/lapack/) sources not found.
        Directories to search for the sources can be specified in the
        numpy/distutils/site.cfg file (section [lapack_src]) or by setting
        the LAPACK_SRC environment variable.
      warnings.warn(LapackSrcNotFoundError.__doc__)
      NOT AVAILABLE

    C:\Python36\lib\distutils\dist.py:261: UserWarning: Unknown distribution option: 'define_macros'
      warnings.warn(msg)
    running install
    running build
    running config_cc
    unifing config_cc, config, build_clib, build_ext, build commands --compiler options
    running config_fc
    unifing config_fc, config, build_clib, build_ext, build commands --fcompiler options
    running build_src
    build_src
    building py_modules sources
    creating build
    creating build\src.win-amd64-3.6
    creating build\src.win-amd64-3.6\numpy
    creating build\src.win-amd64-3.6\numpy\distutils
    building library ""npymath"" sources
    No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils
    customize GnuFCompiler
    Could not locate executable g77
    Could not locate executable f77
    customize IntelVisualFCompiler
    Could not locate executable ifort
    Could not locate executable ifl
    customize AbsoftFCompiler
    Could not locate executable f90
    customize CompaqVisualFCompiler
    Could not locate executable DF
    customize IntelItaniumVisualFCompiler
    Could not locate executable efl
    customize Gnu95FCompiler
    Could not locate executable gfortran
    Could not locate executable f95
    customize G95FCompiler
    Could not locate executable g95
    customize IntelEM64VisualFCompiler
    customize IntelEM64TFCompiler
    Could not locate executable efort
    Could not locate executable efc
    don't know how to compile Fortran code on platform 'nt'
    cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Inumpy\core\src\private -Inumpy\core\src -Inumpy\core -Inumpy\core\src\npymath -Inumpy\core\src\multiarray -Inumpy\core\src\umath -Inumpy\core\src\npysort -IC:\Python36\include -IC:\Python36\include /Tc_configtest.c /Fo_configtest.obj
    Could not locate executable cl.exe
    Executable cl.exe does not exist

    failure.
    removing: _configtest.c _configtest.obj
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\setup.py"", line 386, in &lt;module&gt;
        setup_package()
      File ""C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\setup.py"", line 378, in setup_package
        setup(**metadata)
      File ""C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\core.py"", line 169, in setup
        return old_setup(**new_attr)
      File ""C:\Python36\lib\distutils\core.py"", line 148, in setup
        dist.run_commands()
      File ""C:\Python36\lib\distutils\dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""C:\Python36\lib\distutils\dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\command\install.py"", line 62, in run
        r = self.setuptools_run()
      File ""C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\command\install.py"", line 36, in setuptools_run
        return distutils_install.run(self)
      File ""C:\Python36\lib\distutils\command\install.py"", line 545, in run
        self.run_command('build')
      File ""C:\Python36\lib\distutils\cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""C:\Python36\lib\distutils\dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\command\build.py"", line 47, in run
        old_build.run(self)
      File ""C:\Python36\lib\distutils\command\build.py"", line 135, in run
        self.run_command(cmd_name)
      File ""C:\Python36\lib\distutils\cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""C:\Python36\lib\distutils\dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\command\build_src.py"", line 147, in run
        self.build_sources()
      File ""C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\command\build_src.py"", line 158, in build_sources
        self.build_library_sources(*libname_info)
      File ""C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\command\build_src.py"", line 293, in build_library_sources
        sources = self.generate_sources(sources, (lib_name, build_info))
      File ""C:\Users\info_000\AppData\Local\Temp\pip-build-ueljt0po\numpy\numpy\distutils\command\build_src.py"", line 376, in generate_sources
        source = func(extension, build_dir)
      File ""numpy\core\setup.py"", line 653, in get_mathlib_info
        raise RuntimeError(""Broken toolchain: cannot link a simple C program"")
    RuntimeError: Broken toolchain: cannot link a simple C program

    ----------------------------------------
</code></pre>
","1577341","","1577341","","2016-11-30 17:23:34","2018-07-26 20:17:41","How to install NumPy for Python 3.6","<python><python-3.x><numpy><pip>","6","2","3","","","CC BY-SA 3.0","0"
"29712519","1","29712690","","2015-04-18 03:22:24","","22","79988","<p>My OS is <a href=""https://en.wikipedia.org/wiki/Lubuntu"" rel=""noreferrer"">Lubuntu</a> 14.04 and the default Python version is Python 2.7.6, but in</p>

<pre><code>/usr/bin
</code></pre>

<p>it says I have Python 3.4 installed (when I run <code>python3 -V</code> it says I have Python 3.4.0). Does Python 3.4 come with a pre-installed <a href=""https://en.wikipedia.org/wiki/Pip_%28package_manager%29"" rel=""noreferrer"">pip</a>? Because when I run</p>

<pre><code>pip -V
</code></pre>

<p>in a terminal it says that the program is currently not installed. With that said, assume I want to create a <a href=""http://en.wikipedia.org/wiki/Django_%28web_framework%29"" rel=""noreferrer"">Django</a> project which uses Python 3.4.3: do I first download python3-pip and then virtualenv and then do</p>

<pre><code>pip3 install Django==1.8
</code></pre>

<p>? or is there a pre-installed pip&nbsp;3 which comes with Python 3.4 which I already have installed?</p>
","2719875","","63550","","2016-10-23 10:38:35","2019-06-04 10:25:45","How do I use pip 3 with Python 3.4?","<python-3.x><pip>","3","0","19","","","CC BY-SA 3.0","0"
"35120250","1","35120519","","2016-01-31 22:21:12","","45","79860","<p>How would I parse a json api response with python?
I currently have this:</p>

<pre><code>import urllib.request
import json

url = 'https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty'

def response(url):
    with urllib.request.urlopen(url) as response:
        return response.read()

res = response(url)
print(json.loads(res))
</code></pre>

<p>I'm getting this error:
TypeError: the JSON object must be str, not 'bytes'</p>

<p>What is the pythonic way to deal with json apis?</p>
","4178623","","9210255","","2018-11-07 19:24:37","2019-10-25 06:47:56","Python 3 Get and parse JSON API","<python><json><python-3.x><api>","5","0","10","","","CC BY-SA 3.0","0"
"32382686","1","32383309","","2015-09-03 18:10:42","","53","79407","<p>I keep getting UnicodeEncodeError when trying to print a 'Á' that I get from a website requested using selenium in python 3.4.</p>

<p>I already defined at the top of my .py file</p>

<p><code>#  -*- coding: utf-8 -*-</code></p>

<p>the def is something like this:</p>

<pre><code>from selenium import webdriver

b = webdriver.Firefox()
b.get('http://fisica.uniandes.edu.co/personal/profesores-de-planta')
dataProf = b.find_elements_by_css_selector('td[width=""508""]')
for dato in dataProf:
        print(datos.text)
</code></pre>

<p>and the exception:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/Andres/Desktop/scrap/scrap.py"", line 444, in &lt;module&gt;
    dar_p_fisica()
  File ""C:/Users/Andres/Desktop/scrap/scrap.py"", line 390, in dar_p_fisica
    print(datos.text) #.encode().decode('ascii', 'ignore')
  File ""C:\Python34\lib\encodings\cp1252.py"", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode character '\u2010' in position 173: character maps to &lt;undefined&gt;
</code></pre>

<p>thanks in advance</p>
","2109083","","2109083","","2015-09-03 18:16:40","2016-06-23 14:24:31","UnicodeEncodeError: 'charmap' codec can't encode character '\u2010': character maps to <undefined>","<python-3.x><selenium><encoding><utf-8>","1","1","35","2016-06-04 19:16:56","","CC BY-SA 3.0","0"
"41994485","1","","","2017-02-02 04:59:58","","41","79256","<p>I am getting the error ""could not find or load the Qt platform plugin windows"" while using matplotlib in PyCharm.</p>

<p>How can I solve this?</p>

<p><img src=""https://i.stack.imgur.com/XXw0M.png"" alt=""enter image description here""></p>
","7504037","","128421","","2020-06-11 16:55:41","2020-08-17 04:33:08","How to fix ""could not find or load the Qt platform plugin windows"" while using Matplotlib in PyCharm","<python><python-3.x><pycharm>","21","5","14","","","CC BY-SA 4.0","0"
"42648610","1","42667069","","2017-03-07 12:41:09","","97","78550","<p>When I execute <code>jupyter notebook</code> in my virtual environment in Arch Linux, the following error occurred.</p>
<p><code>Error executing Jupyter command 'notebook': [Errno 2] No such file or directory</code></p>
<p>My Python version is 3.6, and my Jupyter version is 4.3.0</p>
<p>How can I resolve this issue?</p>
","6355435","","6355435","","2020-07-14 02:08:55","2020-07-14 02:08:55","Error when executing `jupyter notebook` (No such file or directory)","<python-3.x><jupyter-notebook>","12","2","18","","","CC BY-SA 4.0","0"
"28590669","1","28590707","","2015-02-18 18:04:29","","29","77562","<p>I'm trying to show a open file dialog using Tkinter in Python.  Every example I find seems very easy to use, but they all start with the line:</p>

<pre><code>import tkFileDialog
</code></pre>

<p>This line throws an error for me, saying</p>

<pre><code>No module named 'tkFileDialog'
</code></pre>

<p>It seems my Python doesn't have <code>tkFileDialog</code>.  So I tried searching for it, but it seems that you don't ""download"" Tkinter, it just comes with Python.  <strong>Why is my Tkinter missing tkFileDialog?  Is there somewhere I can acquire it so that I can use it?</strong></p>

<p>Another thing I thought is that maybe it has changed names since the examples I've read were written.  Is there a different way to import <code>tkFileDialog</code> in Python 3?</p>

<p>I'm running Windows 7 64-bit, Python version</p>

<pre><code>3.4.1 (v3.4.1:c0e311e010fc, May 18 2014, 10:45:13) [MSC v.1600 64 bit (AMD64)]
</code></pre>

<p>Any help would be greatly appreciated!</p>
","1465113","","","","","2015-02-18 18:06:23","Tkinter tkFileDialog doesn't exist","<python><python-3.x><tkinter>","1","1","7","2018-03-30 17:18:39","","CC BY-SA 3.0","0"
"45542690","1","45543116","","2017-08-07 08:39:34","","29","76945","<p>Is there a way to set the Python 3.5.2 as the default Python version on CentOS 7? currently, I have Python 2.7 installed as default and Python 3.5.2 installed separately.</p>

<p>I used the following commands</p>

<pre><code>mv /usr/bin/python /usr/bin/python-old
sudo ln -fs /usr/bin/python3 /usr/bin/python
</code></pre>

<p>but after that <code>yum</code> gives the error.</p>

<pre><code>-bash: /usr/bin/yum: /usr/bin/python: bad interpreter: No such file or directory
</code></pre>

<p>is there something I'm missing here?</p>

<p>NOTE: its the similar but opposite question of <a href=""https://stackoverflow.com/questions/44730002/linux-centos-7-how-to-set-python2-7-as-default-python-version"">Linux CentOS 7, how to set Python2.7 as default Python version?</a> </p>
","2968016","","7120073","","2020-05-23 16:01:14","2020-05-23 16:01:14","How to set Python3.5.2 as default Python version on CentOS?","<python><python-3.x><bash><centos><python-3.5>","4","0","13","","","CC BY-SA 4.0","0"
"49392972","1","","","2018-03-20 19:48:55","","50","76548","<p>I am working on training a VGG16-like model in Keras, on a 3 classes subset from Places205, and encountered the following error: </p>

<pre><code>ValueError: Error when checking target: expected dense_3 to have shape (3,) but got array with shape (1,)
</code></pre>

<p>I read multiple similar issues but none helped me so far. The error is on the last layer, where I've put 3 because this is the number of classes I'm trying right now.</p>

<p>The code is the following:</p>

<pre><code>import keras from keras.datasets
import cifar10 from keras.preprocessing.image 
import ImageDataGenerator from keras.models 
import Sequential 
from keras.layers import Dense, Dropout, Activation, Flatten from keras.layers import Conv2D, MaxPooling2D 
from keras import backend as K import os


# Constants used  
img_width, img_height = 224, 224  
train_data_dir='places\\train'  
validation_data_dir='places\\validation'  
save_filename = 'vgg_trained_model.h5'  
training_samples = 15  
validation_samples = 5  
batch_size = 5  
epochs = 5


if K.image_data_format() == 'channels_first':
    input_shape = (3, img_width, img_height) else:
    input_shape = (img_width, img_height, 3)

model = Sequential([
    # Block 1
    Conv2D(64, (3, 3), activation='relu', input_shape=input_shape, padding='same'),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    # Block 2
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    # Block 3
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    Conv2D(256, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    # Block 4
    Conv2D(512, (3, 3), activation='relu', padding='same'),
    Conv2D(512, (3, 3), activation='relu', padding='same'),
    Conv2D(512, (3, 3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    # Block 5
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    Conv2D(512, (3, 3), activation='relu', padding='same',),
    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),
    # Top
    Flatten(),
    Dense(4096, activation='relu'),
    Dense(4096, activation='relu'),
    Dense(3, activation='softmax') ])

model.summary()

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

# no augmentation config train_datagen = ImageDataGenerator() validation_datagen = ImageDataGenerator()
     train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

validation_generator = validation_datagen.flow_from_directory(
    validation_data_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='binary')

model.fit_generator(
    train_generator,
    steps_per_epoch=training_samples // batch_size,
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=validation_samples // batch_size)

model.save_weights(save_filename)
</code></pre>
","4445080","","4445080","","2018-03-20 20:55:51","2020-06-13 21:31:36","Error when checking target: expected dense_3 to have shape (3,) but got array with shape (1,)","<python><python-3.x><tensorflow><keras>","8","2","14","","","CC BY-SA 3.0","0"
"41405632","1","41406147","","2016-12-31 05:03:48","","42","76195","<p>I want to draw a rectangle and a text in it, here's a part of my code and it's a bit obfuscated:</p>

<pre><code>from PIL import Image
from PIL import ImageFont
from PIL import ImageDraw
from PIL import ImageEnhance

  source_img = Image.open(file_name).convert(""RGB"")

  img1 = Image.new(""RGBA"", img.size, (0,0,0,0))
  draw1 = ImageDraw.Draw(watermark, ""RGBA"")
  draw1.rectangle(((0, 00), (100, 100)), fill=""black"")
  img_rectangle = Image.composite(img1, source_img, img1)

  draw2 = ImageDraw.Draw(img1, ""RGBA"")
  draw2.text((20, 70), ""something123"", font=ImageFont.truetype(""font_path123""))

  Image.composite(img1, source_img, img1).save(out_file, ""JPEG"")
</code></pre>

<p>This draws them both, but they're separate: the text is under the rectangle. Whereas I want a text to be drawn inside the rectangle. 
How can I do that? Should I <strong>necessarily</strong> compose them or what?</p>
","7359679","","","","","2017-08-22 03:18:10","Draw a rectangle and a text in it using PIL","<python><python-3.x><image-processing><python-imaging-library>","1","2","11","","","CC BY-SA 3.0","0"
"36965864","1","36965905","","2016-05-01 10:59:50","","23","76116","<p>i'm trying to open the URL of this API from the sunlight foundation and return the data from the page in json. this is the code Ive produced, minus the parenthesis around myapikey.</p>

<pre><code>import urllib.request.urlopen
import json

urllib.request.urlopen(""https://sunlightlabs.github.io/congress/legislators?api_key='(myapikey)"")
</code></pre>

<p>and im getting this error</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;input&gt;"", line 1, in &lt;module&gt;
ImportError: No module named request.urlopen
</code></pre>

<p>what am i doing wrong? ive researched into <a href=""https://docs.python.org/3/library/urllib.request.html"" rel=""noreferrer"">https://docs.python.org/3/library/urllib.request.html</a> and still no progress</p>
","6277572","","5043793","","2016-05-01 11:02:24","2020-08-20 08:19:57","opening a url with urllib in python 3","<python><python-3.x><url><urllib>","5","0","10","","","CC BY-SA 3.0","0"
"42373104","1","42373582","","2017-02-21 16:41:49","","34","76073","<p>I am trying to import <code>matplotlib.finance</code> module in python so that I can make a Candlestick OCHL graph. My <code>matplotlib.pyplot</code> version is 2.00. I've tried to import it using the following commands:</p>

<pre><code>import matplotlib.finance
from matplotlib.finance import candlestick_ohlc
</code></pre>

<p>I get this error:</p>

<blockquote>
  <p>warnings.warn(message, mplDeprecation, stacklevel=1)
  MatplotlibDeprecationWarning: The finance module has been deprecated in mpl 2.0 and will be removed in mpl 2.2. Please use the module mpl_finance instead.</p>
</blockquote>

<p>Then instead of using the above lines in python I tried using the following line:</p>

<pre><code>import mpl_finance
</code></pre>

<p>I get this error:</p>

<blockquote>
  <p>ImportError: No module named 'mpl_finance'</p>
</blockquote>

<p>What should I do to import candlestick from <code>matplotlib.pyplot</code>?</p>
","7326981","","8309965","","2018-05-29 06:31:31","2020-08-22 14:00:17","Since matplotlib.finance has been deprecated, how can I use the new mpl_finance module?","<python-3.x><matplotlib><candlestick-chart>","9","0","12","","","CC BY-SA 4.0","0"
"48072619","1","","","2018-01-03 06:56:51","","46","75901","<p>I would like to use <code>urlparse</code>. But python3.4.1 is not finding the module. </p>

<p>I do <code>import urlparse</code>, but it gives me this error </p>

<pre><code>importError: no 'module' named ulrparse
</code></pre>
","9076413","","4652706","","2018-01-03 07:13:37","2018-01-03 07:14:02","How can i import urlparse in python-3?","<python><python-3.x><import><importerror><urlparse>","2","0","7","2018-01-03 07:16:19","","CC BY-SA 3.0","0"
"52805115","1","","","2018-10-14 17:12:19","","65","75159","<p>I am trying to get data from the web using python. I imported urllib.request package for it but while executing, I get error:</p>

<pre><code>certificate verify failed: unable to get local issuer certificate (_ssl.c:1045)
</code></pre>

<ul>
<li>I am using Python 3.7 on Mac OS High Sierra.</li>
<li>I am trying to get CSV file from:
<a href=""https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv"" rel=""noreferrer"">https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv</a></li>
</ul>

<p>When I changed the URL to 'http' - I am able to get data. But, I believe, this avoids checking SSL certificate.</p>

<p>So I checked on the internet and found one solution:
Run <code>/Applications/Python\ 3.7/Install\ Certificates.command</code></p>

<p>This solved my problem. But I have no knowledge on SSL and the likes. Can you help me understand what it actually did to solve my issue.</p>

<p>If possible, please recommend me any good resource to learn about the security and certificates. I am new to this.</p>

<p>Thanks!</p>

<p>Note: I did go through the link - <a href=""https://stackoverflow.com/questions/22027418/openssl-python-requests-error-certificate-verify-failed"">openssl, python requests error: &quot;certificate verify failed&quot;</a></p>

<p>My question differs from the one in link because, I want to know what actually happens when I install <code>certifi</code> package or run <code>Install\ Certificates.command</code> to fix the error. I have a poor understanding of securities. </p>
","1943192","","1943192","","2018-10-15 07:41:48","2020-10-10 20:01:37","certificate verify failed: unable to get local issuer certificate","<python><python-3.x><ssl><openssl>","10","4","10","","","CC BY-SA 4.0","0"
"42891603","1","","","2017-03-19 20:05:25","","9","75144","<p>Basically, I have to check whether a particular pattern appear in a line or not. If yes, I have to print that line otherwise not. So here is my code:</p>

<pre><code>p = input()
 while 1:
   line = input()
   a=line.find(p)
   if a!=-1:
     print(line)
   if line=='':
     break
</code></pre>

<p>This code seems to be good and is being accepted as the correct answer. But there's a catch. I'm getting a run time error EOFError: EOF when reading a line which is being overlooked by the code testing website.</p>

<p>I have three questions:
1) Why it is being overlooked?
2) How to remove it?
3) Is there a better way to solve the problem?</p>
","7736560","","","","","2020-02-23 15:29:47","How to remove EOFError: EOF when reading a line?","<python><python-3.x>","2","0","1","","","CC BY-SA 3.0","0"
"35546627","1","36710442","","2016-02-22 05:52:39","","17","74690","<p>In my terminal, I type <code>$ which python3</code>, outputting </p>

<pre><code>/opt/local/bin/python3
</code></pre>

<p>I would like to configure Atom to run Python3 scripts. In my Atom Config, I have </p>

<pre><code>runner:
python: ""/opt/local/bin/python3""
</code></pre>

<p>However, if I run the following script in some script named <code>filename.py</code>,</p>

<pre><code>import sys
print(sys.version)
</code></pre>

<p>I get the following output: </p>

<pre><code>2.7.11 (default, Feb 18 2016, 22:00:44) 
[GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)]
</code></pre>

<p>How exactly does one set up the PATH for Python3.x scripts to run correctly? Is there a different package I could use? </p>
","5269850","","5269850","","2016-02-22 12:39:29","2020-05-09 23:13:51","How to configure Atom to run Python3 scripts?","<python><python-3.x><path><atom-editor>","8","1","7","","","CC BY-SA 3.0","0"
"43047284","1","43165082","","2017-03-27 13:04:48","","28","74125","<p>I am trying to install something in my virtual environment, which uses anaconda python 3.6. I get <code>the gcc failed with exit status 1</code>, hinting on the absence of the right python3-devel package, as described in <a href=""https://stackoverflow.com/questions/11094718/error-command-gcc-failed-with-exit-status-1-while-installing-eventlet"">error: command &#39;gcc&#39; failed with exit status 1 while installing eventlet</a>.</p>

<p>To fix the error, I tried to install the python3-devel package on my server running RHEL 7.3. 
I did <code>yum install python3-devel</code>, but got a <code>'package not found'</code> error. Then I found <a href=""https://serverfault.com/questions/710354/repository-for-python3-devel-on-centos-7"">https://serverfault.com/questions/710354/repository-for-python3-devel-on-centos-7</a>, which hints to the python34-devel package in the EPEL repository. I installed it using yum, but upon trying to install something in my virtual environment, I still get <code>the gcc failed with exit status 1</code> error.</p>

<p>Does someone know how I can fix this? All help would be much apprechiated.</p>
","5347728","","-1","","2017-05-23 12:17:30","2020-09-04 02:18:19","How to install python3-devel on red hat 7","<python-3.x><installation><virtualenv><rhel7>","3","5","12","","","CC BY-SA 3.0","0"
"45446418","1","","","2017-08-01 19:42:56","","67","74053","<p>Currently trying to work in Python3 and use absolute imports to import one module into another but I get the error <code>ModuleNotFoundError: No module named '__main__.moduleB'; '__main__' is not a package</code>. Consider this project structure:</p>

<pre><code>proj
    __init__.py3 (empty)
    moduleA.py3
    moduleB.py3
</code></pre>

<p>moduleA.py3</p>

<pre><code>from .moduleB import ModuleB
ModuleB.hello()
</code></pre>

<p>moduleB.py3</p>

<pre><code>class ModuleB:
    def hello():
        print(""hello world"")
</code></pre>

<p>Then running <code>python3 moduleA.py3</code> gives the error. What needs to be changed here?</p>
","2258219","","1709587","","2019-04-18 13:57:16","2020-06-04 06:38:05","ModuleNotFoundError: No module named '__main__.xxxx'; '__main__' is not a package","<python-3.x><import><module>","5","4","10","","","CC BY-SA 4.0","0"
"33533148","1","33533514","","2015-11-04 22:17:54","","498","73088","<p>I have the following code in python 3:</p>

<pre><code>class Position:

    def __init__(self, x: int, y: int):
        self.x = x
        self.y = y

    def __add__(self, other: Position) -&gt; Position:
        return Position(self.x + other.x, self.y + other.y)
</code></pre>

<p>But my editor (PyCharm) says that the reference Position can not be resolved (in the <code>__add__</code> method). How should I specify that I expect the return type to be of type <code>Position</code>?</p>

<p>Edit: I think this is actually a PyCharm issue. It actually uses the information in its warnings, and code completion</p>

<p><img src=""https://i.imgur.com/yjjCWw3.png"" alt=""""></p>

<p>But correct me if I'm wrong, and need to use some other syntax.</p>
","4665133","","3000206","","2020-09-16 14:15:18","2020-09-26 18:38:58","How do I type hint a method with the type of the enclosing class?","<python><python-3.x><pycharm><typing><python-3.5>","5","0","103","","","CC BY-SA 4.0","0"
"44780357","1","44780467","","2017-06-27 12:30:35","","121","72937","<p>I would like to know how to format this case in a Pythonic way with f-strings:</p>

<pre><code>names = ['Adam', 'Bob', 'Cyril']
text = f""Winners are:\n{'\n'.join(names)}""
print(text)
</code></pre>

<p>The problem is that <code>'\'</code> cannot be used inside the <code>{...}</code> expression portions of an f-string.
Expected output:</p>

<pre><code>Winners are:
Adam
Bob
Cyril
</code></pre>
","3749484","","202229","","2019-06-12 02:47:50","2020-02-03 16:03:02","How to use newline '\n' in f-string to format output in Python 3.6?","<python><python-3.x><newline><python-3.6><f-string>","4","1","16","","","CC BY-SA 4.0","0"
"27733685","1","27733728","","2015-01-01 19:00:16","","17","72846","<p>If i would like to iterate over dictionary values that are stored in a tuple.</p>

<p>i need to return the object that hold the ""CI"" value, i assume that i will need some kind of a for loop :</p>

<pre><code>z = {'x':(123,SE,2,1),'z':(124,CI,1,1)}
for i, k in db.z:
    for k in db.z[i]:
        if k == 'CI':
            return db.z[k]
</code></pre>

<p>i am probably missing something here, a point of reference would be good.</p>

<p>if there is a faster way doing so it would all so help greatly </p>
","4312230","","4312230","","2015-01-01 19:30:23","2019-02-28 14:41:47","Iterating over dict values","<python><python-3.x><dictionary>","5","1","4","","","CC BY-SA 3.0","0"
"43757820","1","43758816","","2017-05-03 10:44:51","","22","72767","<p>I am trying to plot lots of diagrams, and for each diagram, I want to use a variable to label them. How can I add a variable to <code>plt.title?</code> For example:</p>

<pre><code>import numpy as np
import matplotlib.pyplot as plt

plt.figure(1)
plt.ylabel('y')
plt.xlabel('x')

for t in xrange(50, 61):
    plt.title('f model: T=t')

    for i in xrange(4, 10):
        plt.plot(1.0 / i, i ** 2, 'ro')

    plt.legend
    plt.show()
</code></pre>

<p>In the argument of <code>plt.title()</code>, I want <code>t</code> to be variable changing with the loop.</p>
","7946034","","4685471","","2019-01-08 17:31:14","2019-12-09 11:25:11","How to add a variable to Python plt.title?","<python><python-3.x><matplotlib>","3","0","5","","","CC BY-SA 3.0","0"
"44623184","1","46268531","","2017-06-19 05:58:24","","24","72432","<p>i'm new to tensorflow, today i installed tensorflow using:</p>

<pre><code>C:\&gt;pip3 install --upgrade tensorflow
Collecting tensorflow
  Using cached tensorflow-1.2.0-cp35-cp35m-win_amd64.whl
Requirement already up-to-date: bleach==1.5.0 in c:\python35\lib\site-packages (
from tensorflow)
Requirement already up-to-date: werkzeug&gt;=0.11.10 in c:\python35\lib\site-packag
es (from tensorflow)
Requirement already up-to-date: html5lib==0.9999999 in c:\python35\lib\site-pack
ages (from tensorflow)
Requirement already up-to-date: protobuf&gt;=3.2.0 in c:\python35\lib\site-packages
 (from tensorflow)
Requirement already up-to-date: backports.weakref==1.0rc1 in c:\python35\lib\sit
e-packages (from tensorflow)
Requirement already up-to-date: markdown==2.2.0 in c:\python35\lib\site-packages
 (from tensorflow)
Requirement already up-to-date: numpy&gt;=1.11.0 in c:\python35\lib\site-packages (
from tensorflow)
Requirement already up-to-date: six&gt;=1.10.0 in c:\python35\lib\site-packages (fr
om tensorflow)
Requirement already up-to-date: wheel&gt;=0.26 in c:\python35\lib\site-packages (fr
om tensorflow)
Requirement already up-to-date: setuptools in c:\python35\lib\site-packages (fro
m protobuf&gt;=3.2.0-&gt;tensorflow)
Installing collected packages: tensorflow
Successfully installed tensorflow-1.2.0
</code></pre>

<p>when i tried to import tensorflow, it throws:</p>

<pre><code>C:\&gt;python
Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 25 2016, 22:18:55) [MSC v.1900 64 bit (AM
D64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_intern
al.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 666, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 577, in module_from_spec
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 906, in create_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", l
ine 41, in &lt;module&gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_intern
al.py"", line 21, in &lt;module&gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_intern
al.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in &lt;modu
le&gt;
    from tensorflow.python import *
  File ""C:\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 49, i
n &lt;module&gt;
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", l
ine 52, in &lt;module&gt;
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_intern
al.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 666, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 577, in module_from_spec
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 906, in create_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", l
ine 41, in &lt;module&gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_intern
al.py"", line 21, in &lt;module&gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_intern
al.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_probl
ems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
&gt;&gt;&gt;
</code></pre>

<p>i'm using python 3.5.2 64bit, i don't really know why the import process throws error, please help me gurus</p>

<p>thanks, best regards</p>
","2558463","","","","","2020-10-04 08:46:51","error: Failed to load the native TensorFlow runtime","<python><python-3.x><tensorflow>","14","4","9","","","CC BY-SA 3.0","0"
"45465463","1","45465497","","2017-08-02 15:48:42","","4","71654","<p>I'm brand new at python, and didn't understand the other answers for this question. Why when I run my code, does <code>int(weight[0])</code> not convert variable ""weight"" into a integer. Try your best to dumb it down because I'm really new and still don't quite understand most of it. Here is the relevant section of my code</p>

<pre><code>weight = (lb.curselection())
    print (""clicked"")
    int(weight[0])
    print (weight)
    print (type(weight))
</code></pre>

<p>and heres my code for this script</p>

<pre><code>lb = Listbox(win, height=240)
lb.pack()
for i in range(60,300):
    lb.insert(END,(i))
def select(event):
    weight = (lb.curselection())
    print (""clicked"")
    int(weight[0])
    print (weight)
    print (type(weight))
lb.bind(""&lt;Double-Button-1&gt;"", select)
</code></pre>

<p>Thanks</p>

<p>When I run the code, it comes up with <code>TypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'
</code>
and I want it instead to convert the ""weight"" variable into a integer, so I can use it for math operations.</p>

<p>Full Traceback:<code>Traceback (most recent call last):
  File ""C:\Users\Casey\AppData\Local\Programs\Python\Python36-32\lib\tkinter\__init__.py"", line 1699, in __call__
    return self.func(*args)
  File ""C:/Users/Casey/AppData/Local/Programs/Python/Python36-32/s.py"", line 11, in select
    int(weight)
TypeError: int() argument must be a string, a bytes-like object or a number, not 'tuple'
</code></p>
","8406625","","8406625","","2017-08-02 15:58:06","2017-08-02 16:14:55","Convert tuple to int in Python","<python><python-3.x><tkinter>","2","2","1","","","CC BY-SA 3.0","0"
"29643544","1","","","2015-04-15 06:59:06","","55","71339","<p>I'm moving my Twitch bot from Python 2.7 to Python 3.5. I keep getting the error: 
<code>a bytes like object is required not 'str'</code> on the 2nd line of the code below.</p>

<pre><code>twitchdata = irc.recv(1204)
    data = twitchdata.split("":"")[1]
    twitchuser = data.split(""!"")[0]
    twitchmsg = twitchdata.split("":"")[2]
    chat = str(twitchuser) +"": ""+ str(twitchmsg)
    print(chat) #prints chat to console
</code></pre>
","4504099","","1603480","","2017-06-06 06:18:35","2017-06-06 06:18:35","Python - a bytes like object is required, not str","<python><python-3.x><irc><twitch>","1","1","8","","","CC BY-SA 3.0","0"
"39299838","1","39311677","","2016-09-02 19:54:43","","27","70848","<p>I have used jupyter notebook for data analysis for quite sometime. I would like to develop a module in my jupyter notebook directory and be able to import that new module into notebooks. My jupyter notebook file directory can be represented as follows;</p>

<pre><code>Jupyter notebooks\

    notebook1.ipynb

    new_module\
        __init__.py
        newfunction.py

    currentnotebooks\
        notebook2.ipynb
</code></pre>

<p>When use <code>import new_module</code> in notebook1.ipynb it works however when I try the same command in notebook2.ipynb I get the following <code>ImportError: No module named 'new_module'</code>. The two obvious solutions are A) move new_module into the currentnotebooks directory or B) move notebook2.ipynb up to the same level as new_module. I don't want to mess around with the file structure at all. Is this possible?</p>
","3727854","","434217","","2016-09-03 09:31:14","2019-01-22 05:29:57","How do I import module in jupyter notebook directory into notebooks in lower directories?","<python><python-3.x><ipython><python-import><jupyter-notebook>","1","1","10","2017-02-26 16:35:36","","CC BY-SA 3.0","0"
"41384040","1","41384984","","2016-12-29 16:59:15","","32","70785","<p>I have a dataframe like this</p>

<pre><code>import seaborn as sns
import pandas as pd
%pylab inline
df = pd.DataFrame({'a' :['one','one','two','two','one','two','one','one','one','two'], 'b': [1,2,1,2,1,2,1,2,1,1], 'c': [1,2,3,4,6,1,2,3,4,6]})
</code></pre>

<p>A single boxplot is OK</p>

<pre><code>sns.boxplot(  y=""b"", x= ""a"", data=df,  orient='v' )
</code></pre>

<p>But i want to build a subplot for all variables. I do</p>

<pre><code>names = ['b', 'c']
plt.subplots(1,2)
sub = []
for name in names:

    ax = sns.boxplot(  y=name, x= ""a"", data=df,  orient='v' )
    sub.append(ax)
</code></pre>

<p>and i get </p>

<p><a href=""https://i.stack.imgur.com/tEqqC.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/tEqqC.png"" alt=""enter image description here""></a></p>

<p>how to fix it? thanx for your help</p>
","5998425","","","","","2016-12-30 12:18:12","Subplot for seaborn boxplot","<python-3.x><loops><boxplot><seaborn>","2","0","14","","","CC BY-SA 3.0","0"
"44008489","1","","","2017-05-16 17:58:51","","-1","70715","<p>I want to write a program that rolls a dice. Now this is what I have :</p>

<pre><code>import random
print(""You rolled"",random.randint(1,6))
</code></pre>

<p>And I also want to be able to do something like this:</p>

<pre><code>print(""Do you want to roll again? Y/N"")
</code></pre>

<p>and then if I press Y it rolls again and if I press N I quit the app. Thanks in advance!</p>
","7972041","","","","","2019-03-06 09:31:44","Dice rolling simulator in Python","<python-3.x><simulator><dice>","6","2","0","","","CC BY-SA 3.0","0"
"29156187","1","29156393","","2015-03-19 22:31:23","","4","70513","<p>I have an array of 3 numbers per row, 4 columns deep.
I am struggling to figure out how I can write the code to print all numbers from a specified column rather than from a row.</p>

<p>I have searched for tutorials that explain this easily and just cannot find any that have helped.
Can anyone point me in the right direction?</p>
","4239803","","","","","2019-09-11 16:11:58","How to print column in python array?","<python><python-3.x>","2","2","","","","CC BY-SA 3.0","0"
"40713268","1","","","2016-11-21 05:21:53","","32","70334","<p>I have tried the following code to download a video in YouTube and it is working, but I want to save the video at a particular location. Now it is saving the video in <code>C:/Users/Download</code>. If I want to save the video in  the desktop, what changes do I need in the code?</p>

<pre><code>from __future__ import unicode_literals
import youtube_dl
import urllib
import shutil
ydl_opts = {}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    ydl.download(['https://www.youtube.com/watch?v=n06H7OcPd-g'])
</code></pre>
","7144759","","6306190","","2019-01-13 00:26:59","2020-07-23 10:30:56","Download YouTube video using Python to a certain directory","<python><python-3.x><python-2.7>","10","2","17","","","CC BY-SA 4.0","0"
"49704364","1","49711594","","2018-04-07 05:52:43","","66","70253","<p>What I'm trying to do here is to <strong>make python3 as my default python.</strong> Except the python 2.7 which automatically installed on mac, I installed <strong>python3</strong> with <strong>homebrew</strong>. This is the website that I'm following. <a href=""http://docs.python-guide.org/en/latest/starting/install3/osx/#install3-osx"" rel=""noreferrer"">http://docs.python-guide.org/en/latest/starting/install3/osx/#install3-osx</a></p>
<p>I guess I followed every instruction well, got xcode freshly installed, Command line tools, and homebrew. But here's my little confusion occurs.</p>
<blockquote>
<p>The script will explain what changes it will make and prompt you before the installation begins. Once you’ve installed Homebrew, insert the Homebrew directory at the top of your PATH environment variable. You can do this by adding the following line at the bottom of your ~/.profile file</p>
<p><strong>export PATH=/usr/local/bin:/usr/local/sbin:$PATH</strong></p>
</blockquote>
<p>I was really confused what this was, but I concluded that I should just add this following line at the bottom of ~/.profile file. So I opened the <strong>~/.profile</strong> file by <strong>open .profile</strong> in the terminal, and added following line at the bottom. And now it looks like this.</p>
<pre><code>export PATH=/usr/local/bin:/usr/local/sbin:$PATH
# Setting PATH for Python 3.6
# The original version is saved in .profile.pysave
export PATH=/usr/local/bin:/usr/local/sbin:$PATH
</code></pre>
<p>And then I did <strong>brew install python</strong>, and was hoping to see <strong>python3</strong> when I do <strong>python --version.</strong>
But it just shows me <strong>python 2.7.10.</strong> I want my default python to be <strong>python3</strong> not 2.7</p>
<p>And I found a little clue from the website.</p>
<blockquote>
<p>Do I have a Python 3 installed?</p>
</blockquote>
<pre><code>$ python --version
Python 3.6.4
</code></pre>
<blockquote>
<p>If you still see 2.7 ensure in <strong>PATH /usr/local/bin/ takes pecedence over /usr/bin/</strong></p>
</blockquote>
<p>Maybe it has to do something with <strong>PATH?</strong> Could someone explain in simple English what <strong>PATH</strong> exactly is and how I could make my default python to be python3 when I run <strong>python --version</strong> in the terminal?</p>
","8424406","","-1","","2020-06-20 09:12:55","2020-03-02 07:06:34","Make python3 as my default python on Mac","<python><python-3.x><macos><terminal><homebrew>","4","8","33","","","CC BY-SA 3.0","0"
"46408051","1","46408435","","2017-09-25 14:44:39","","20","69218","<p>I have this code:</p>

<pre><code>keys_file = open(""keys.json"")
keys = keys_file.read().encode('utf-8')
keys_json = json.loads(keys)
print(keys_json)
</code></pre>

<p>There are some none-english characters in keys.json.
But as a result I get:</p>

<pre><code>[{'category': 'РјР±С‚', 'keys': ['Р‘Р»РµРЅРґРµСЂ Philips',
'РјСѓР»СЊС‚РёРІР°СЂРєР° Polaris']}, {'category': 'РљР‘Рў', 'keys':
['С…РѕР»РѕРґ РёР»СЊРЅРёРє Р°С‚Р»Р°РЅС‚', 'РїРѕСЃСѓРґРѕРјРѕРµС‡РЅР°СЏ
РјР°С€РёРЅР° Bosch']}]
</code></pre>

<p>what do I do?</p>
","2950593","","2950593","","2017-09-25 14:56:59","2017-09-25 15:03:09","python json load set encoding to utf-8","<python><python-3.x>","1","12","5","","","CC BY-SA 3.0","0"
"51402579","1","51402703","","2018-07-18 12:55:16","","28","68987","<p>I am having trouble switching from ggplot2 into seaborn.  Currently using Anaconda v. 4.5.8  and Python 3.6.3</p>

<p>Any graph I use cannot be found.  For example I can take any code from seaborn's site and run:</p>

<pre><code>import matplotlib as plt
import seaborn as sns
sns.set(style=""ticks"")

dots = sns.load_dataset(""dots"")

# Define a palette to ensure that colors will be
# shared across the facets
palette = dict(zip(dots.coherence.unique(),
                   sns.color_palette(""rocket_r"", 6)))

# Plot the lines on two facets
sns.relplot(x=""time"", y=""firing_rate"",
            hue=""coherence"", size=""choice"", col=""align"",
            size_order=[""T1"", ""T2""], palette=palette,
            height=5, aspect=.75, facet_kws=dict(sharex=False),
            kind=""line"", legend=""full"", data=dots)
sns.plt.show() #this was not on site code but tried it(plt.show() as referenced by other posts)
</code></pre>

<p>Error message:</p>

<pre><code>  File ""&lt;ipython-input-8-893759310442&gt;"", line 13, in &lt;module&gt;
    sns.relplot(x=""time"", y=""firing_rate"",

AttributeError: module 'seaborn' has no attribute 'relplot'
</code></pre>

<p>Looked at these posts( among others) </p>

<p>(1) AtributeError: 'module' object has no attribute 'plt' - Seaborn</p>

<p>(2) Seaborn ImportError: DLL load failed: The specified module could not be found</p>

<p>(3) ImportError after successful pip installation</p>

<p>(4) Error importing Seaborn module in Python</p>

<p>and tried the install/uninstall methods they described ( python -m pip install seaborn,  uninstall seaborn/ reinstall - etc.)  I did this in both conda using conda and cmd using pip.</p>

<p>I haven't spent much time with PATHs but here are screenshots:</p>

<p><a href=""https://i.stack.imgur.com/Kj8ZT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Kj8ZT.png"" alt=""conda ""></a></p>

<p><a href=""https://i.stack.imgur.com/AXmCs.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/AXmCs.png"" alt=""pip""></a></p>

<p>Any ideas?</p>

<p>Many Thanks</p>
","9305523","","","","","2018-07-28 03:30:53","Module Seaborn has no attribute '<any graph>'","<python><python-3.x><seaborn>","3","6","8","","","CC BY-SA 4.0","0"
"53637182","1","53637669","","2018-12-05 16:55:55","","81","67715","<p>I was following the <a href=""https://docs.djangoproject.com/en/2.1/intro/tutorial01/"" rel=""noreferrer"">first app tutorial</a> from the official Django docs and got this error when trying to save some changes made through the admin page. I did some research on it, but the possible solutions I was able to find, such as migrating the db, simply won't work. Just let me know if you want to see some specific part of my code.</p>

<p>Following is error:</p>

<blockquote>
  <p>OperationalError at /admin/polls/question/1/change/ no such table:
  main.auth_user__old Request Method:   POST Request
  URL:  <a href=""http://127.0.0.1:8000/admin/polls/question/1/change/"" rel=""noreferrer"">http://127.0.0.1:8000/admin/polls/question/1/change/</a> Django
  Version:  2.1.4 Exception Type:   OperationalError Exception Value:    no
  such table: main.auth_user__old Exception
  Location: /Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/backends/sqlite3/base.py
  in execute, line 296 Python
  Executable:   /Users/gfioravante/Projects/test_app/ta_env/bin/python3
  Python Version:   3.7.1 Python Path:<br>
  ['/Users/gfioravante/Projects/test_app/test_app', 
  '/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python37.zip',
  '/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7',
  '/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload',
  '/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages']
  Server time:  Wed, 5 Dec 2018 16:45:00 +0000</p>
</blockquote>

<p>and the traceback:</p>

<blockquote>
  <p>Environment:</p>
  
  <p>Request Method: POST Request URL:
  <a href=""http://127.0.0.1:8000/admin/polls/question/1/change/"" rel=""noreferrer"">http://127.0.0.1:8000/admin/polls/question/1/change/</a></p>
  
  <p>Django Version: 2.1.4 Python Version: 3.7.1 Installed Applications:
  ['polls.apps.PollsConfig',  'django.contrib.admin', 
  'django.contrib.auth',  'django.contrib.contenttypes', 
  'django.contrib.sessions',  'django.contrib.messages', 
  'django.contrib.staticfiles'] Installed Middleware:
  ['django.middleware.security.SecurityMiddleware', 
  'django.contrib.sessions.middleware.SessionMiddleware', 
  'django.middleware.common.CommonMiddleware', 
  'django.middleware.csrf.CsrfViewMiddleware', 
  'django.contrib.auth.middleware.AuthenticationMiddleware', 
  'django.contrib.messages.middleware.MessageMiddleware', 
  'django.middleware.clickjacking.XFrameOptionsMiddleware']</p>
  
  <p>Traceback:</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/backends/utils.py""
  in _execute
    85.                 return self.cursor.execute(sql, params)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/backends/sqlite3/base.py""
  in execute
    296.         return Database.Cursor.execute(self, query, params)</p>
  
  <p>The above exception (no such table: main.auth_user__old) was the
  direct cause of the following exception:</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/core/handlers/exception.py""
  in inner
    34.             response = get_response(request)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/core/handlers/base.py""
  in _get_response
    126.                 response = self.process_exception_by_middleware(e, request)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/core/handlers/base.py""
  in _get_response
    124.                 response = wrapped_callback(request, *callback_args, **callback_kwargs)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/contrib/admin/options.py""
  in wrapper
    604.                 return self.admin_site.admin_view(view)(*args, **kwargs)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/utils/decorators.py""
  in _wrapped_view
    142.                     response = view_func(request, *args, **kwargs)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/views/decorators/cache.py""
  in _wrapped_view_func
    44.         response = view_func(request, *args, **kwargs)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/contrib/admin/sites.py"" in inner
    223.             return view(request, *args, **kwargs)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/contrib/admin/options.py""
  in change_view
    1640.         return self.changeform_view(request, object_id, form_url, extra_context)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/utils/decorators.py""
  in _wrapper
    45.         return bound_method(*args, **kwargs)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/utils/decorators.py""
  in _wrapped_view
    142.                     response = view_func(request, *args, **kwargs)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/contrib/admin/options.py""
  in changeform_view
    1525.             return self._changeform_view(request, object_id, form_url, extra_context)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/contrib/admin/options.py""
  in _changeform_view
    1571.                     self.log_change(request, new_object, change_message)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/contrib/admin/options.py""
  in log_change
    826.             change_message=message,</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/contrib/admin/models.py""
  in log_action
    35.             change_message=change_message,</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/models/manager.py""
  in manager_method
    82.                 return getattr(self.get_queryset(), name)(*args, **kwargs)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/models/query.py""
  in create
    413.         obj.save(force_insert=True, using=self.db)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/models/base.py""
  in save
    718.                        force_update=force_update, update_fields=update_fields)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/models/base.py""
  in save_base
    748.             updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/models/base.py""
  in _save_table
    831.             result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/models/base.py""
  in _do_insert
    869.                                using=using, raw=raw)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/models/manager.py""
  in manager_method
    82.                 return getattr(self.get_queryset(), name)(*args, **kwargs)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/models/query.py""
  in _insert
    1136.         return query.get_compiler(using=using).execute_sql(return_id)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/models/sql/compiler.py""
  in execute_sql
    1289.                 cursor.execute(sql, params)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/backends/utils.py""
  in execute
    100.             return super().execute(sql, params)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/backends/utils.py""
  in execute
    68.         return self._execute_with_wrappers(sql, params, many=False, executor=self._execute)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/backends/utils.py""
  in _execute_with_wrappers
    77.         return executor(sql, params, many, context)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/backends/utils.py""
  in _execute
    85.                 return self.cursor.execute(sql, params)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/utils.py""
  in <strong>exit</strong>
    89.                 raise dj_exc_value.with_traceback(traceback) from exc_value</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/backends/utils.py""
  in _execute
    85.                 return self.cursor.execute(sql, params)</p>
  
  <p>File
  ""/Users/gfioravante/Projects/test_app/ta_env/lib/python3.7/site-packages/django/db/backends/sqlite3/base.py""
  in execute
    296.         return Database.Cursor.execute(self, query, params)</p>
  
  <p>Exception Type: OperationalError at /admin/polls/question/1/change/
  Exception Value: no such table: main.auth_user__old</p>
</blockquote>
","10750522","","10750522","","2020-04-17 21:43:33","2020-09-29 16:29:26","Django - No such table: main.auth_user__old","<python><django><python-3.x><django-models>","25","1","20","","","CC BY-SA 4.0","0"
"39574813","1","39575675","","2016-09-19 13:47:27","","13","67712","<p>I have tried a lot to solve this issue but I did not solve it. I have searched a lot on google and stackoverflow, no option is working for me. Please help me. Thanks in advance. I am using django 1.10, python 3.4.
I have tried :</p>

<ol>
<li>pip install mysqldb.</li>
<li>pip install mysql.</li>
<li>pip install mysql-python.</li>
<li>pip install MySQL-python.</li>
<li>easy_install mysql-python.</li>
<li>easy_install MySQL-python.</li>
</ol>

<p>Anything else left ?</p>

<pre><code>      C:\Users\benq\Desktop\dimo-develop\Project&gt;python manage.py runserver
Unhandled exception in thread started by &lt;function check_errors.&lt;locals&gt;.wrapper at 0x0332D348&gt;
Traceback (most recent call last):
  File ""C:\Python34\lib\site-packages\django\db\backends\mysql\base.py"", line 25, in &lt;module&gt;
    import MySQLdb as Database
ImportError: No module named 'MySQLdb'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Python34\lib\site-packages\django\utils\autoreload.py"", line 226, in wrapper
    fn(*args, **kwargs)
  File ""C:\Python34\lib\site-packages\django\core\management\commands\runserver.py"", line 109, in inner_run
    autoreload.raise_last_exception()
  File ""C:\Python34\lib\site-packages\django\utils\autoreload.py"", line 249, in raise_last_exception
    six.reraise(*_exception)
  File ""C:\Python34\lib\site-packages\django\utils\six.py"", line 685, in reraise
    raise value.with_traceback(tb)
  File ""C:\Python34\lib\site-packages\django\utils\autoreload.py"", line 226, in wrapper
    fn(*args, **kwargs)
  File ""C:\Python34\lib\site-packages\django\__init__.py"", line 18, in setup
    apps.populate(settings.INSTALLED_APPS)
  File ""C:\Python34\lib\site-packages\django\apps\registry.py"", line 108, in populate
    app_config.import_models(all_models)
  File ""C:\Python34\lib\site-packages\django\apps\config.py"", line 202, in import_models
    self.models_module = import_module(models_module_name)
  File ""C:\Python34\lib\importlib\__init__.py"", line 109, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 2254, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 2237, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 2226, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 1200, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 1129, in _exec
  File ""&lt;frozen importlib._bootstrap&gt;"", line 1471, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 321, in _call_with_frames_removed
  File ""C:\Python34\lib\site-packages\django\contrib\auth\models.py"", line 4, in &lt;module&gt;
    from django.contrib.auth.base_user import AbstractBaseUser, BaseUserManager
  File ""C:\Python34\lib\site-packages\django\contrib\auth\base_user.py"", line 49, in &lt;module&gt;
    class AbstractBaseUser(models.Model):
  File ""C:\Python34\lib\site-packages\django\db\models\base.py"", line 108, in __new__
    new_class.add_to_class('_meta', Options(meta, app_label))
  File ""C:\Python34\lib\site-packages\django\db\models\base.py"", line 299, in add_to_class
    value.contribute_to_class(cls, name)
  File ""C:\Python34\lib\site-packages\django\db\models\options.py"", line 263, in contribute_to_class
    self.db_table = truncate_name(self.db_table, connection.ops.max_name_length())
  File ""C:\Python34\lib\site-packages\django\db\__init__.py"", line 36, in __getattr__
    return getattr(connections[DEFAULT_DB_ALIAS], item)
  File ""C:\Python34\lib\site-packages\django\db\utils.py"", line 212, in __getitem__
    backend = load_backend(db['ENGINE'])
  File ""C:\Python34\lib\site-packages\django\db\utils.py"", line 116, in load_backend
    return import_module('%s.base' % backend_name)
  File ""C:\Python34\lib\importlib\__init__.py"", line 109, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Python34\lib\site-packages\django\db\backends\mysql\base.py"", line 28, in &lt;module&gt;
    raise ImproperlyConfigured(""Error loading MySQLdb module: %s"" % e)
django.core.exceptions.ImproperlyConfigured: Error loading MySQLdb module: No module named 'MySQLdb'
</code></pre>
","7055286","","442945","","2017-02-06 22:11:22","2019-11-20 08:17:23","Error loading MySQLdb module: No module named 'MySQLdb'","<python><mysql><django><python-3.x>","7","0","10","","","CC BY-SA 3.0","0"
"54598292","1","54613085","","2019-02-08 18:29:12","","7","67592","<p>I'm using Python 3.7.1 on macOS Mojave Version 10.14.1</p>

<p>This is my directory structure:</p>

<pre><code>man/                          
  Mans/                  
          man1.py
  MansTest/
          SoftLib/
                  Soft/
                      SoftWork/
                              manModules.py
          Unittests/
                    man1test.py
</code></pre>

<p><code>man1.py</code> contains the following <em>import</em> statement, <strong>which I do not want to change</strong>:</p>

<pre><code>from Soft.SoftWork.manModules import *
</code></pre>

<p><code>man1test.py</code> contains the following <em>import</em> statements:</p>

<pre><code>from ...MansTest.SoftLib import Soft
from ...Mans import man1
</code></pre>

<p>I need the second <em>import</em> in <code>man1test.py</code> because <code>man1test.py</code> needs access to a function in <code>man1.py</code>.</p>

<p>My rationale behind the first import (<em>Soft</em>) was to facilitate the aforementioned <em>import</em> statement in <code>man1.py</code>.</p>

<p>Contrary to my expectation, however, the <em>import</em> statement in <code>man1.py</code> gives rise to:</p>

<pre><code>ModuleNotFoundError: No module named 'Soft'
</code></pre>

<p>when I run</p>

<pre><code>python3 -m man.MansTest.Unittests.man1test
</code></pre>

<p>from a directory above <em>man/</em>.</p>

<p>Is there any way to resolve this error without changing the <em>import</em> statement in <code>man1.py</code> <strong>and</strong> without adding anything to <em>sys.path</em>?</p>

<p>Edit: <code>python3 -m man.ManTest.Unittests.man1test</code> from the original version of the question changed to <code>python3 -m man.MansTest.Unittests.man1test</code></p>
","9156986","","9156986","","2019-02-08 22:48:15","2020-09-14 02:53:05","Python: 'ModuleNotFoundError' when trying to import module from imported package","<python><python-3.x><python-import>","3","1","3","","","CC BY-SA 4.0","0"
"28130722","1","28130783","","2015-01-24 21:59:45","","21","67295","<p>I want to concatenate the first byte of a bytes string to the end of the string:</p>

<pre><code>a = b'\x14\xf6'
a += a[0]
</code></pre>

<p>I get an error:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: can't concat bytes to int
</code></pre>

<p>When I type <code>bytes(a[0])</code> I get:</p>

<pre><code>b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'
</code></pre>

<p>And <code>bytes({a[0]})</code> gives the correct <code>b'\x14'</code>.</p>

<p>Why do I need <code>{}</code> ?</p>
","4490663","","293594","","2015-01-24 22:07:05","2019-02-14 18:35:17","Python bytes concatenation","<python-3.x>","3","2","4","","","CC BY-SA 3.0","0"
"42988348","1","42988384","","2017-03-23 22:44:32","","13","66916","<p>I have a dataframe (df) that looks like:</p>

<pre><code>date                 A
2001-01-02      1.0022
2001-01-03      1.1033
2001-01-04      1.1496
2001-01-05      1.1033

2015-03-30    126.3700
2015-03-31    124.4300
2015-04-01    124.2500
2015-04-02    124.8900
</code></pre>

<p>For the entire time-series I'm trying to divide today's value by yesterdays and log the result using the following:</p>

<pre><code>df[""B""] = math.log(df[""A""] / df[""A""].shift(1))
</code></pre>

<p>However I get the following error:</p>

<pre><code>TypeError: cannot convert the series to &lt;class 'float'&gt;
</code></pre>

<p>How can I fix this? I've tried to cast as float using:</p>

<pre><code>df[""B""] .astype(float)
</code></pre>

<p>But can't get anything to work. </p>
","4972716","","472495","","2020-05-05 12:32:27","2020-08-15 21:03:43","TypeError: cannot convert the series to <class 'float'>","<python><python-3.x>","5","4","2","","","CC BY-SA 4.0","0"
"35830924","1","35831137","","2016-03-06 18:41:26","","18","66829","<p>I currently am trying to get the code from this website: <a href=""http://netherkingdom.netai.net/pycake.html"" rel=""noreferrer"">http://netherkingdom.netai.net/pycake.html</a>
Then I have a python script parse out all code in html div tags, and finally write the text from between the div tags to a file. The problem is it adds a bunch of \r and \n to the file. How can I either avoid this or remove the \r and \n. Here is my code:</p>

<pre><code>import urllib.request
from html.parser import HTMLParser
import re
page = urllib.request.urlopen('http://netherkingdom.netai.net/pycake.html')
t = page.read()
class MyHTMLParser(HTMLParser):
    def handle_data(self, data):
        print(data)
        f = open('/Users/austinhitt/Desktop/Test.py', 'r')
        t = f.read()
        f = open('/Users/austinhitt/Desktop/Test.py', 'w')
        f.write(t + '\n' + data)
        f.close()
parser = MyHTMLParser()
t = t.decode()
parser.feed(t)
</code></pre>

<p>And here is the resulting file it makes:</p>

<pre><code>b'
import time as t\r\n
from os import path\r\n
import os\r\n
\r\n
\r\n
\r\n
\r\n
\r\n'
</code></pre>

<p>Preferably I would also like to have the beginning b' and last ' removed. I am using Python 3.5.1 on a Mac.</p>
","6015572","","6015572","","2016-03-07 03:15:49","2020-07-08 08:03:08","How to remove \n and \r from a string","<python><html><python-3.x><file-writing>","3","5","4","","","CC BY-SA 3.0","0"
"51427729","1","","","2018-07-19 16:30:02","","13","66384","<p>I receive the attribute error when I try to run the code. </p>

<pre><code>    with ParamExample(URI) as pe:
    with MotionCommander(pe, default_height=0.3)as mc:
</code></pre>

<p>This is where the error occurs. </p>

<pre><code>Traceback (most recent call last):
File ""test44.py"", line 156, in &lt;module&gt;
with ParamExample(URI) as pe:
AttributeError: __enter__
</code></pre>

<p>That is the traceback that I receive in my terminal. 
If you need to see more of my code, please let me know. 
Any help is appreciated, thank you! </p>
","10106662","","7692463","","2018-07-19 16:31:03","2018-07-19 16:34:04","Python Error: AttributeError: __enter__","<python><python-3.x><crazyflie>","1","5","1","2018-07-19 18:20:10","","CC BY-SA 4.0","0"
"40963659","1","40963979","","2016-12-04 21:06:36","","13","66243","<p>I want to calculate root mean square of a function in Python. My function is in a simple form like y = f(x). x and y are arrays.</p>

<p>I tried <a href=""https://docs.scipy.org/doc/"" rel=""noreferrer"">Numpy and Scipy Docs</a> and couldn't find anything.</p>
","","user4179448","","","","2016-12-04 21:38:21","Root mean square of a function in python","<python><arrays><python-3.x><numpy><scipy>","1","2","3","","","CC BY-SA 3.0","0"
"28272322","1","","","2015-02-02 06:38:32","","9","66221","<p>When running the code in IDLE gives the following error:</p>

<pre><code>Traceback (most recent call last):   File ""C:/Python34/inversion3.py"",
line 44, in &lt;module&gt;
    nInversions.inversionMergeSort(m)   File ""C:/Python34/inversion3.py"", line 16, in inversionMergeSort
     left = m[0:half] TypeError: slice indices must be integers or None or have an __index__ method
</code></pre>

<p><strong>CODE:-</strong></p>

<pre><code>from collections import deque

m = []
f = open(""IntegerArray.txt"")
for line in f:
    m.append(int(line))

class InversionCount:

    def __init__(self, n):
        self.n = n
    def inversionMergeSort(self, m):
        if len(m) &lt;= 1:
            return m
        half = len(m)/2
        left = m[0:half]
        right = m[half:]
        left = self.inversionMergeSort(left)
        right = self.inversionMergeSort(right)
        return self.inversionSort(left, right)

    def inversionSort(self, left, right):
        leftQueue = deque(i for i in left)
        rightQueue = deque(j for j in right)
        orderedList = []
        while len(leftQueue) &gt; 0 or len(rightQueue) &gt; 0:
            if len(leftQueue) &gt; 0 and len(rightQueue) &gt; 0:
                if leftQueue[0] &lt;= rightQueue[0]:
                    orderedList.append(leftQueue[0])
                    leftQueue.popleft()
                else:
                    orderedList.append(rightQueue[0])
                    self.n += len(leftQueue)
                    rightQueue.popleft()
            elif len(leftQueue) &gt; 0:
                orderedList.append(leftQueue[0])
                leftQueue.popleft()
            elif len(rightQueue) &gt; 0:
                orderedList.append(rightQueue[0])
                rightQueue.popleft()
        return orderedList

nInversions = InversionCount(0)
nInversions.inversionMergeSort(m)
print (nInversions.n)
</code></pre>
","4518813","","1530508","","2015-02-02 06:50:54","2019-05-29 07:48:54","TypeError: slice indices must be integers or None or have an __index__ method","<python-3.x><typeerror><inversion>","2","1","1","","","CC BY-SA 3.0","0"
"43333207","1","","","2017-04-10 21:31:45","","32","66142","<p>I have created a python virtual environment using <em>virtualenv</em>, after activating it, I can see where is Python installed in my shell as following:</p>

<pre class=""lang-sh prettyprint-override""><code>(virtualenv-test) bash-4.1$ whereis python
python: /usr/bin/python2.6 /usr/bin/python2.6-config /usr/bin/python
/usr/lib/python2.6 /usr/lib64/python2.6 /usr/X11R6/bin/python2.6
/usr/X11R6/bin/python2.6-config /usr/X11R6/bin/python
/usr/bin/X11/python2.6 /usr/bin/X11/python2.6-config
/usr/bin/X11/python /usr/include/python2.6
/usr/share/man/man1/python.1.gz
</code></pre>

<p>Also I can see what python version I'm using: </p>

<pre class=""lang-sh prettyprint-override""><code>(virtualenv-test) bash-4.1$ which python
/data/virtualenv-test/bin/python
</code></pre>

<p>However, after typing python, I got the following error message:</p>

<pre class=""lang-sh prettyprint-override""><code>(virtualenv-test) bash-4.1$ python
python: error while loading shared libraries: libpython3.4m.so.1.0: cannot open shared object file: No such file or directory
</code></pre>

<p>What can be the underlying reason?</p>
","785099","","1048404","","2020-06-11 11:19:13","2020-10-15 09:42:55","python: error while loading shared libraries: libpython3.4m.so.1.0: cannot open shared object file: No such file or directory","<python><python-3.x><virtualenv>","8","0","7","","","CC BY-SA 4.0","0"
"28685931","1","28686090","","2015-02-24 00:01:09","","28","65454","<p>I want to create the tables of one database called ""database1.sqlite"", so I run the command: </p>

<blockquote>
  <p>python manage.py syncdb</p>
</blockquote>

<p>but when I execute the command I receive the following error:</p>

<blockquote>
  <p>Unknown command: 'syncdb'
  Type 'manage.py help' for usage.</p>
</blockquote>

<p>But when I run </p>

<blockquote>
  <p>manage.py help </p>
</blockquote>

<p>I don`t see any command suspicious to substitute </p>

<blockquote>
  <p>python manage.py syncdb</p>
</blockquote>

<p>Version of Python I use: 3.4.2    Version of Django I use:1.9 </p>

<p>I would be very grateful if somebody could help me to solve this issue.</p>

<p>Regards and thanks in advance</p>
","4594621","","1628832","","2015-05-15 16:51:01","2020-05-02 14:34:43","""Unknown command syncdb"" running ""python manage.py syncdb""","<django><sqlite><python-3.x><django-1.9>","10","4","5","","","CC BY-SA 3.0","0"
"48541801","1","","","2018-01-31 12:07:35","","28","65169","<p>I have tried all methods mentioned on the internet but there is no use.
I am trying to install misaka by writing <code>pip install misaka</code> it keeps complaining by showing the same message. I have downloaded and installed MS build tool 2015 and 2017, Restarted my laptop. Whatever I did, couldn't figure out why it complains.
Python version 3.6.4
Windows 10</p>
","5303460","","1249664","","2020-08-08 18:00:40","2020-08-08 18:00:40","Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft Visual C++ Build Tools"": http://landinghub.visualstudio.com/visual-cpp-build-tools","<python><django><python-3.x><msbuild><misaka>","4","3","10","2020-08-18 16:55:37","","CC BY-SA 4.0","0"
"35466429","1","","","2016-02-17 19:53:34","","8","64830","<p>I have searched quite a bit regarding this and I've tried some of these methods myself but I'm unable to work with OpenCV.So can anyone of you help me install OpenCV for python 3.5.1?</p>

<p>I'm using anaconda along with Pycharm in windows </p>

<p>Or is this not possible and i have to use python 2.7?</p>

<p>Thanks in advance</p>
","5864426","","7256341","","2017-06-25 11:22:35","2020-08-03 09:21:31","OpenCV for Python 3.5.1","<python><python-3.x><opencv>","5","2","2","","","CC BY-SA 3.0","0"
"46396827","1","46396854","","2017-09-25 02:34:24","","34","64790","<p>I am trying to prettify the json format but i am getting this error:</p>

<pre><code>import requests as tt
from bs4 import BeautifulSoup
import json

get_url=tt.get(""https://in.pinterest.com/search/pins/?rs=ac&amp;len=2&amp;q=batman%20motivation&amp;eq=batman%20moti&amp;etslf=5839&amp;term_meta[]=batman%7Cautocomplete%7Cundefined&amp;term_meta[]=motivation%7Cautocomplete%7Cundefined"")
soup=BeautifulSoup(get_url.text,""html.parser"")

select_css=soup.select(""script#jsInit1"")[0]
for i in select_css:
    print(json.dump(json.loads(i),indent=4,sort_keys=True))
</code></pre>

<p>Basically i want to extract this type of element :</p>

<pre><code>'orig': {'width': 1080, 'url': '', 'height': 1349},
</code></pre>

<p>I know i can do this with </p>

<pre><code>select_css.get('orig').get('url')
</code></pre>

<p>But i am not sure is this json element is nested element under any element ? That's why i am trying to prettify to get idea.</p>
","5904928","","","","","2020-05-06 14:38:28","dump() missing 1 required positional argument: 'fp' in python json","<python><json><python-2.7><python-3.x>","1","1","5","","","CC BY-SA 3.0","0"
"43550407","1","43550927","","2017-04-21 19:18:06","","8","64512","<p>Ubuntu 16.04 LTS, trying to install cpickle with pip. I've searched a bit, haven't found anything useful yet. </p>

<p>PYTHONPATH isn't set.</p>

<h1>Error message</h1>

<pre><code>user@hostname:~$ sudo -H pip3 install cpickle
Collecting cpickle
  Using cached cpickle-0.5.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""/usr/lib/python3.5/tokenize.py"", line 454, in open
        buffer = _builtin_open(filename, 'rb')
    FileNotFoundError: [Errno 2] No such file or directory: '/tmp/pip-build-wn926hef/cpickle/setup.py'

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-wn926hef/cpickle/
You are using pip version 8.1.1, however version 9.0.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.


    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in /tmp/pip-build-q46tq1l8/cpickle/
You are using pip version 8.1.1, however version 9.0.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
</code></pre>

<h1>troubleshooting steps</h1>

<pre><code># version info
user@hostname:~$ python --version
Python 2.7.12
user@hostname:~$ python3 --version
Python 3.5.2

# I don't think cache is the problem
rm -rf ~/.cache/
sudo -H pip install  cpickle --no-cache-dir # same problem
sudo -H pip3 install  cpickle --no-cache-dir # same problem
</code></pre>
","2620549","","2572645","","2017-07-27 06:37:59","2020-04-10 17:29:21","pip install pickle not working - no such file or directory","<python><linux><bash><python-3.x><pip>","3","5","","","","CC BY-SA 3.0","0"
"54065751","1","54066158","","2019-01-06 20:40:36","","7","64380","<p>Recently I have been working on a code and got stuck for days on this error.
Basically the program calculates how many calories you have to eat everyday. I have to take input from an entry and I don't know how to convert that input(it's a string by default) into a float to start using the numbers.
I am using Python 3 with Tkinter.</p>

<p>Here is the code:</p>

<pre><code>from tkinter import *

root = Tk()
root.geometry(""1000x500"")
root.resizable(FALSE, FALSE)
root.title(""BMI Calculator"")

def calc(args):

    def BMI_temp(args):
        print(str(boyage))
        BMI = IntVar()
        BMI = 66.5 + (13.75 * float(boykg)) + (5.003 * float(boycm)) - (6.755 * float(boyage))
        bmi_temp = Label(root, text=""This is how many calories you have to eat if you have a non-active life: "" + str(float(BMI)))
        bmi_temp.grid(row=3, sticky=W)

    def boy_age_fnct(args):
        boy_age_entry.focus_set()
        boy_cm_entry.delete(0, ""end"")
        boy_age.grid(row=2, sticky=W)
        boy_age_entry.grid(row=2, column=1)
        boy_age_entry.bind(""&lt;Return&gt;"", BMI_temp)

    def boy_cm_fnct(args):
        boy_cm_entry.focus_set()
        boy_kg_entry.delete(0, ""end"")
        boy_cm.grid(row=1, sticky=W)
        boy_cm_entry.grid(row=1, column=1)
        boy_cm_entry.bind(""&lt;Return&gt;"", boy_age_fnct)

    boy_kg_entry.focus_set()
    temp = boygirle.get()
    gender = temp.title()
    welcome.destroy()
    hello_lbl.destroy()
    boygirle.destroy()
    boygirlq.destroy()

    if gender[0] == 'B':
        boy_kg.grid(row=0, sticky=W)
        boy_kg_entry.grid(row=0, column=1)
        boy_kg_entry.bind(""&lt;Return&gt;"", boy_cm_fnct)

    boyage = boy_age_entry.get()
    boycm = boy_cm_entry.get()
    boykg = boy_kg_entry.get()

def hello(args):
    name_user = name_entry.get()
    name2 = name_user.title()
    name_entry.delete(0, ""end"")
    hello = ""Hello "" + name2 + ""!""
    hello_lbl[""text""] = hello
    hello_lbl.grid(row=2, sticky=W)
    btn_cont.grid(row=3, sticky=W)
    name.destroy()
    name_entry.destroy()
    btn_cont.focus_set()

def BMI():
    btn_cont.destroy()
    boygirlq.grid(row=3, sticky=W)
    boygirle.grid(row=3, column=0, ipadx=35)
    boygirle.bind(""&lt;Return&gt;"", calc)
    boygirle.focus_set()

welcome = Label(root, text=""Hello! This is a BMR calculator. It tells you how many calories you have to eat!"", font=""System 14 bold"")
name = Label(root, text=""Please enter your name:"", font=""System 12"")
hello_lbl = Label(root, font=""System 14"")
boygirlq = Label(root, text=""Are you a boy or a girl?"", font=""System 12 bold"")
boy_kg = Label(root, text=""Please enter your weight(in kg):"", font=""System 12 bold"")
boy_cm = Label(root, text=""Please enter your height(in cm):"", font=""System 12"")
boy_age = Label(root, text=""Please enter your age(in years):"", font=""System 12"")

btn_cont = Button(root, text=""Continue"", font=""Helvetica 12"", command=BMI, relief=RAISED)

boy_kg_entry = Entry(root, font=""System 12"", relief=SUNKEN)
boy_cm_entry = Entry(root, font=""System 12"", relief=SUNKEN)
boy_age_entry = Entry(root, font=""System 12"", relief=SUNKEN)
name_entry = Entry(root, font=""System 12"", relief=SUNKEN)
boygirle = Entry(root, font=""System 12"", relief=SUNKEN)
name_entry.bind(""&lt;Return&gt;"", hello)
name_entry.focus_set()

welcome.grid(row=0, columnspan=2, ipadx=200)
name.grid(row=1, sticky=W)
name_entry.grid(row=1, column=0)

root.mainloop()
</code></pre>

<p>I tried all the methods I found on the internet but nothing worked.</p>
","10874248","","3784008","","2019-01-06 20:58:52","2020-10-07 18:42:09","""ValueError: could not convert string to float"" when converting input","<python><python-3.x><tkinter>","1","4","1","","","CC BY-SA 4.0","0"
"42513056","1","42513249","","2017-02-28 15:44:29","","58","64032","<p>Making a path object with <code>pathlib</code> module like:</p>

<pre><code>p = pathlib.Path('file.txt')
</code></pre>

<p>The <code>p</code> object will point to some file in the filesystem, since I can do for example <code>p.read_text()</code>.</p>

<p>How can I get the absolute path of the <code>p</code> object in a string?</p>

<p>Appears that I can use for example <code>os.path.abspath(p)</code> to get the absolute path, but it awkward to use an <code>os.path</code> method, since I assume that <code>pathlib</code> should be a replacement for <code>os.path</code>.</p>
","3989931","","8704798","","2018-08-01 07:40:30","2020-08-02 11:43:19","How to get absolute path of a pathlib.Path object?","<python><python-3.x><pathlib>","4","2","6","","","CC BY-SA 4.0","0"
"40575067","1","40575741","","2016-11-13 14:45:20","","27","64016","<p>How do I increase the space between each bar with matplotlib barcharts, as they keep cramming them self to the centre.<a href=""https://i.stack.imgur.com/oYxW1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/oYxW1.png"" alt=""enter image description here""></a> (this is what it currently looks)</p>

<pre><code>import matplotlib.pyplot as plt
import matplotlib.dates as mdates
def ww(self):#wrongwords text file

    with open(""wrongWords.txt"") as file:
        array1 = []
        array2 = [] 
        for element in file:
            array1.append(element)

        x=array1[0]
    s = x.replace(')(', '),(') #removes the quote marks from csv file
    print(s)
    my_list = ast.literal_eval(s)
    print(my_list)
    my_dict = {}

    for item in my_list:
        my_dict[item[2]] = my_dict.get(item[2], 0) + 1

    plt.bar(range(len(my_dict)), my_dict.values(), align='center')
    plt.xticks(range(len(my_dict)), my_dict.keys())

    plt.show()
</code></pre>
","6475597","","","","","2020-05-28 14:15:38","matplotlib bar chart: space out bars","<python><python-3.x><matplotlib>","3","1","2","","","CC BY-SA 3.0","0"
"33709391","1","33711433","","2015-11-14 14:22:28","","51","63934","<p>I would like to use Python for scientific applications and after some research decided that I will use Anaconda as it comes bundled with loads of packages and add new modules using <code>conda install</code> through the cmd is easy.</p>

<p>I prefer to use the 64 bit version for better RAM use and efficiency but
32bit version is needed as well because some libraries are 32bit. Similarly, I prefer to use Python 3.5 as that is the future and the way things go. But loads of libraries are still 2.7 which means I need both.</p>

<p>I have to install 4 versions of Anaconda (64bit 2.7, 64bit 3.5, 32bit 2.7, 64bit 3.5). Each version is about 380MB. I am aiming to use Jupyter notebook and Spyder as the IDE. I had to switch between versions when required. I had conflicting libraries, path issues and all sorts of weird problems.</p>

<p>So, I am planning to do a clean install from scratch. I would like to know if there is a more sensible way to handle this. I use Windows 7 64 bit for now if that matters.</p>
","1310511","","1310511","","2017-11-27 11:56:12","2020-09-24 17:01:03","Using multiple Python engines (32Bit/64bit and 2.7/3.5)","<python><python-3.x><python-2.7><anaconda><conda>","3","3","45","","","CC BY-SA 3.0","0"
"31064981","1","31067445","","2015-06-26 04:33:15","","67","63904","<p>While porting code from <code>python2</code> to <code>3</code>, I get this error when reading from a URL </p>

<blockquote>
  <p>TypeError: initial_value must be str or None, not bytes.</p>
</blockquote>

<pre><code>import urllib
import json
import gzip
from urllib.parse import urlencode
from urllib.request import Request


service_url = 'https://babelfy.io/v1/disambiguate'
text = 'BabelNet is both a multilingual encyclopedic dictionary and a semantic network'
lang = 'EN'
Key  = 'KEY'

    params = {
        'text' : text,
        'key'  : Key,
        'lang' :'EN'

        }

url = service_url + '?' + urllib.urlencode(params)
request = Request(url)
request.add_header('Accept-encoding', 'gzip')
response = urllib.request.urlopen(request)
if response.info().get('Content-Encoding') == 'gzip':
            buf = StringIO(response.read())
            f = gzip.GzipFile(fileobj=buf)
            data = json.loads(f.read())
</code></pre>

<p>The exception is thrown at this line  </p>

<pre><code>buf = StringIO(response.read())  
</code></pre>

<p>If I use python2, it works fine.</p>
","3974476","","202229","","2020-03-26 20:14:02","2020-03-26 20:14:02","Python3 error: initial_value must be str or None, with StringIO","<python><python-3.x><urllib><urllib2><stringio>","3","4","18","","","CC BY-SA 4.0","0"
"40443331","1","40443345","","2016-11-05 21:20:28","","48","63531","<p>So I have something that I am parsing, however here is an example of what I would like to do:</p>

<pre><code>list = ['A', 'B', 'C']
</code></pre>

<p>And using list slicing have it return to me everything but the first index. So in this case: </p>

<pre><code>['B', 'C']
</code></pre>

<p>I have been messing with stuff like list[:-1], list[::-1], list[0:-1], etc. But I can't seem to be able to find this out.</p>

<p>What I am actual doing is:
* I have a error message that has a error code in the beginning such as:</p>

<pre><code>['226', 'Transfer', 'Complete']
</code></pre>

<p>and I want to just display Transfer Complete on a popup widget. Of course I am casting to a string.</p>

<p>Thank you for all help, and if answer differs via Python 2.7.x and Python 3.x.x Please answer for both versions. </p>

<p>Thanks, looked a lot around stackoverflow and python tutorials couldn't really quite get what I was looking for. Thanks for your help!</p>
","5366130","","5366130","","2016-11-05 21:22:36","2016-11-06 04:40:33","How to get everything from the list except the first element using list slicing","<python><python-2.7><list><python-3.x>","2","8","3","2016-11-05 21:22:41","","CC BY-SA 3.0","0"
"48428415","1","48429585","","2018-01-24 17:30:16","","37","63323","<p>currently I have cuda 8.0 and cuda 9.0 installed in Gpu support system. I ran into this error while importing from keras module. It says like failed to load native tensorflow runtime. The error log which i received was:</p>

<pre><code>Traceback (most recent call last):
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in &lt;module&gt;
from tensorflow.python.pywrap_tensorflow_internal import *
File ""/usr/local/lib/python3.5/dist-
packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
_pywrap_tensorflow_internal = swig_import_helper()
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
return load_dynamic(name, filename, file)
File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
return _load(spec)

ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""Try1.py"", line 11, in &lt;module&gt;
from keras.models import Sequential
File ""/usr/local/lib/python3.5/dist-packages/Keras-2.0.9-py3.5.egg/keras/__init__.py"", line 3, in &lt;module&gt;
File ""/usr/local/lib/python3.5/dist-packages/Keras-2.0.9-py3.5.egg/keras/utils/__init__.py"", line 6, in &lt;module&gt;
File ""/usr/local/lib/python3.5/dist-packages/Keras-2.0.9-py3.5.egg/keras/utils/conv_utils.py"", line 3, in &lt;module&gt;
File ""/usr/local/lib/python3.5/dist-packages/Keras-2.0.9-py3.5.egg/keras/backend/__init__.py"", line 83, in &lt;module&gt;
File ""/usr/local/lib/python3.5/dist-packages/Keras-2.0.9-py3.5.egg/keras/backend/tensorflow_backend.py"", line 1, in &lt;module&gt;
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 24, in &lt;module&gt;
from tensorflow.python import *
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 49, in &lt;module&gt;
from tensorflow.python import pywrap_tensorflow
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 73, in &lt;module&gt;
raise ImportError(msg)
ImportError: Traceback (most recent call last):
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in &lt;module&gt;
from tensorflow.python.pywrap_tensorflow_internal import *
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
_pywrap_tensorflow_internal = swig_import_helper()
File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
return load_dynamic(name, filename, file)
File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.
</code></pre>

<p>When I run nvcc --version, the cuda version returned is,</p>

<pre><code>Cuda compilation tools, release 8.0, V8.0.61
</code></pre>

<p>I read about some similar post but couldn't solve my issue. Mostly I think this is a clash between two cuda versions. Can anyone tell me how to solve this? Thanks in advance.</p>
","3316224","","","","","2019-07-02 00:43:15","ImportError: libcublas.so.9.0: cannot open shared object file","<python-3.x><tensorflow><cuda><keras>","10","5","9","","","CC BY-SA 3.0","0"
"28633555","1","28633573","","2015-02-20 16:21:19","","28","63305","<p>How can I catch an error on python 3? I've googled a lot but none of the answers seem to be working. The file open.txt doesn't exist so it should print e.errno.</p>

<p>This is what I tried now:</p>

<p>This is in my defined function</p>

<pre><code>try:
    with open(file, 'r') as file:
        file = file.read()
        return file.encode('UTF-8')
except OSError as e:
    print(e.errno)
</code></pre>

<p>However I does not print anything when I get this error</p>

<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 'test.txt'
</code></pre>
","4576519","","4576519","","2015-02-20 16:31:18","2020-03-01 13:02:03","How to handle FileNotFoundError when ""try .. except IOError"" does not catch it?","<python><exception><python-3.x><try-catch>","1","3","2","","","CC BY-SA 3.0","0"
"31104416","1","","","2015-06-28 20:27:36","","19","63140","<p>I try to execute following code but can't with mistake: name 'xrange' is not defined</p>

<pre><code>pages = (
    requests.get(
        build_group_request({
            ""offset"": WINDOW_SIZE * i,
            ""count"": WINDOW_SIZE,
            ""fields"": ""sex,interests,bdate""
        })
    ).json()['response']['items']
    for i in xrange(int(float(COUNT) / 100 + 1))
)
</code></pre>
","5058997","","202229","","2015-06-28 21:12:12","2015-06-28 21:12:12","Name 'xrange' is not defined in Python 3","<python><python-3.x><range><xrange>","1","4","6","2015-06-28 21:23:08","","CC BY-SA 3.0","0"
"50868322","1","","","2018-06-15 02:20:25","","22","62922","<p>I have one script in which I am trying to execute </p>

<pre><code>python3 env/common_config/add_imagepullsecret.py
</code></pre>

<p>But, I am getting the following error:</p>

<pre><code> [root@kevin]# python3 env/common_config/add_imagepullsecret.py
 Traceback (most recent call last):
 File ""env/common_config/add_imagepullsecret.py"", line 4, in &lt;module&gt;
 import yaml
 ImportError: No module named 'yaml'
 [root@kevin]# pip3 install pyyaml
 Requirement already satisfied: pyyaml in /usr/lib64/python3.4/site-packages 
 (3.12)
 [root@kevin]#
</code></pre>

<p>PyYAML is already installed in the machine:</p>

<pre><code> [root@bhimsvm31 k8s]# pip3 install pyyaml
 Requirement already satisfied: pyyaml in /usr/lib64/python3.4/site-packages 
 (3.12)
 [root@bhimsvm31 k8s]#
</code></pre>

<p>How can I get this script to import PyYAML?</p>
","8616291","","1307905","","2018-06-15 04:41:34","2020-04-02 03:40:38","ImportError: No module named 'yaml'","<python><python-3.x><pip><pyyaml>","6","9","4","","","CC BY-SA 4.0","0"
"34869889","1","34870210","","2016-01-19 06:28:09","","89","62799","<p>I have code that expects <code>str</code> but will handle the case of being passed <code>bytes</code> in the following way:</p>

<pre><code>if isinstance(data, bytes):
    data = data.decode()
</code></pre>

<p>Unfortunately, this does not work in the case of <code>bytearray</code>.  Is there a more generic way to test whether an object is either <code>bytes</code> or <code>bytearray</code>, or should I just check for both?  Is <code>hasattr('decode')</code> as bad as I feel it would be?</p>
","640296","","","","","2020-07-30 08:21:19","What is the proper way to determine if an object is a bytes-like object in Python?","<python><python-3.x>","7","3","16","","","CC BY-SA 3.0","0"
"45947887","1","45948252","","2017-08-29 20:32:41","","9","62545","<p>so I've seen this done is other questions asked here but I'm still a little confused. I've been learning python3 for the last few days and figured I'd start working on a project to really get my hands dirty. I need to loop through a certain amount of CSV files and make edits to those files. I'm having trouble with going to a specific column and also for loops in python in general. I'm used to the convention (int i = 0; i &lt; expression; i++), but in python it's a little different. Here's my code so far and I'll explain where my issue is.</p>

<pre><code>import os
import csv

pathName = os.getcwd()

numFiles = []
fileNames = os.listdir(pathName)
for fileNames in fileNames:
    if fileNames.endswith("".csv""):
        numFiles.append(fileNames)

for i in numFiles:
    file = open(os.path.join(pathName, i), ""rU"")
    reader = csv.reader(file, delimiter=',')
    for column in reader:
        print(column[4])
</code></pre>

<p>My issue falls on this line:</p>

<pre><code>for column in reader:
        print(column[4])
</code></pre>

<p>So in the Docs it says column is the variable and reader is what I'm looping through. But when I write 4 I get this error:</p>

<pre><code>IndexError: list index out of range
</code></pre>

<p>What does this mean? If I write 0 instead of 4 it prints out all of the values in column 0 cell 0 of each CSV file. I basically need it to go through the first row of each CSV file and find a specific value and then go through that entire column. Thanks in advance!</p>
","5991927","","","","","2017-08-29 21:01:08","Python Looping through CSV files and their columns","<python><python-3.x><csv>","2","5","4","","","CC BY-SA 3.0","0"
"43305577","1","43308104","","2017-04-09 10:18:56","","19","62513","<p>I have two datetime.time objects and I want to calculate the difference in hours between them. For example</p>

<pre><code>a = datetime.time(22,00,00)
b = datetime.time(18,00,00)
</code></pre>

<p>I would like to be able to subtract these so that it gives me the value 4.</p>
","3080600","","","","","2019-09-07 15:25:36","Python - Calculate the difference between two datetime.time objects","<python-3.x><datetime>","3","0","8","","","CC BY-SA 3.0","0"
"32407365","1","32407430","","2015-09-04 22:20:59","","87","62293","<p>This question is not a duplicate. </p>

<p>It pertains not just to <em>renaming</em> a virtual environment, but to actually <em>moving</em> it to a different directory, including, potentially, a different user's directory. </p>

<p>This is not the same as merely renaming a virtual environment, especially to people unfamiliar with virtualenvs.</p>

<p>If I create a virtualenv, and I move it to a different folder, will it still work? </p>

<pre><code>$ virtualenv -p /usr/bin/python3 /home/me/Env/my-python-venv
$ source Env/my-python-venv/bin/activate
(my-python-venv) $ 
</code></pre>

<p>...later that day, the virtual environment MOVED...  </p>

<pre><code>(my-python-venv) $ deactivate
$ mkdir -p /home/me/PeskyPartyPEnvs
$ mv /home/me/Env/my-python-venv /home/me/PeskyPartyPEnvs/
</code></pre>

<p>Question: </p>

<p>Will this work? </p>

<pre><code>$ source /home/me/PeskyPartyPEnvs/my-python-venv/bin/activate
(my-python-venv) $ /home/me/PeskyPartyPEnvs/my-python-venv/bin/pip3 install foaas
</code></pre>

<p>I mean this as less of a question about the wisdom of trying this (unless that wisdom is humorous, of course), and more about whether it's possible. I really want to know whether it's possible to do in Python 3, or whether I just have to <a href=""http://www.urbandictionary.com/define.php?term=suck%20it%20up"" rel=""noreferrer"">suck it up</a> and clone it. </p>

<p>Can I just <code>mv</code> a <code>virtualenv</code> like that without sadness? I do want to avoid sadness.</p>
","2146138","","2146138","","2017-02-06 18:54:58","2020-10-14 18:56:50","Can I move a virtualenv?","<python><python-3.x><virtualenv><virtualenv-commands>","6","0","21","","","CC BY-SA 3.0","0"
"40015439","1","40015733","","2016-10-13 08:01:41","","82","62156","<p>I am interested in understanding the <a href=""https://stackoverflow.com/questions/1303347/getting-a-map-to-return-a-list-in-python-3-x"">new language design of Python 3.x</a>.</p>

<p>I do enjoy, in Python 2.7, the function <code>map</code>:</p>

<pre><code>Python 2.7.12
In[2]: map(lambda x: x+1, [1,2,3])
Out[2]: [2, 3, 4]
</code></pre>

<p>However, in Python 3.x things have changed:</p>

<pre><code>Python 3.5.1
In[2]: map(lambda x: x+1, [1,2,3])
Out[2]: &lt;map at 0x4218390&gt;
</code></pre>

<p>I understand the how, but I could not find a reference to the why. Why did the language designers make this choice, which, in my opinion, introduces a great deal of pain. Was this to arm-wrestle developers in sticking to list comprehensions?</p>

<p>IMO, list can be naturally thought as <a href=""http://learnyouahaskell.com/functors-applicative-functors-and-monoids"" rel=""noreferrer"">Functors</a>; and I have been somehow been thought to think in this way:</p>

<pre><code>fmap :: (a -&gt; b) -&gt; f a -&gt; f b
</code></pre>
","2085376","","-1","","2017-05-23 12:34:36","2017-10-15 11:28:20","Why does map return a map object instead of a list in Python 3?","<python><python-3.x>","4","22","21","","","CC BY-SA 3.0","0"
"29934032","1","35024841","","2015-04-29 03:54:33","","30","62068","<p>I am trying to install virtualenv for Python 3 on Ubuntu 64bit 14.04.</p>

<p>I have installed pip for Python3 using:</p>

<pre><code>pip3 install virtualenv
</code></pre>

<p>and everything works fine. Now though I am trying to use virtualenv command to actually create the environment and getting the error that it is not install (i guess because I haven't installed it for Python 2 and that is what it is trying to use)</p>

<p>How do I use the virtualenv for Python 3?  I have searched the documentation but can't see where it says what to do.</p>
","3724464","","329318","","2015-04-29 05:28:17","2019-09-27 12:28:01","Virtualenv - Python 3 - Ubuntu 14.04 64 bit","<python><python-3.x><pip><virtualenv><ubuntu-14.04>","6","1","13","","","CC BY-SA 3.0","0"
"44210656","1","44210735","","2017-05-26 22:00:02","","21","61924","<p>I would like to install the modules 'mutagen' and 'gTTS' for my code, but I want to have it so it will install the modules on every computer that doesn't have them, but it won't try to install them if they're already installed. I currently have:</p>

<pre><code>def install(package):
    pip.main(['install', package])

install('mutagen')

install('gTTS')

from gtts import gTTS
from mutagen.mp3 import MP3
</code></pre>

<p>However, if you already have the modules, this will just add unnecessary clutter to the start of the program whenever you open it.</p>
","7781906","","7781906","","2017-05-26 22:05:27","2020-07-10 15:05:36","How to check if a module is installed in Python and, if not, install it within the code?","<python><python-3.x><module><python-module>","6","4","9","","","CC BY-SA 3.0","0"
"30109449","1","30109730","","2015-05-07 18:55:31","","39","61721","<p>I am trying to use connect to another party using Python 3 asyncio module and get this error:</p>

<pre><code>     36     sslcontext = ssl.SSLContext(ssl.PROTOCOL_TLSv1)
---&gt; 37     sslcontext.load_cert_chain(cert, keyfile=ca_cert)
     38

SSLError: [SSL] PEM lib (_ssl.c:2532)
</code></pre>

<p>The question is just what the error mean. My certificate is correct, <em>the keyfile (CA certificate) might not</em>.</p>
","2251437","","56541","","2017-11-18 10:02:51","2020-09-22 18:25:54","What does ""SSLError: [SSL] PEM lib (_ssl.c:2532)"" mean using the Python ssl library?","<python><python-3.x><ssl><ssl-certificate><python-asyncio>","5","8","3","","","CC BY-SA 3.0","0"
"29687837","1","53021998","","2015-04-16 23:45:36","","19","61079","<p>I am not sure why I am getting this <code>ImportError</code>. <code>queue.Queue()</code> is in the documentation.</p>

<p><a href=""https://docs.python.org/3/library/queue.html?highlight=queue#queue.Queue"" rel=""noreferrer"">https://docs.python.org/3/library/queue.html?highlight=queue#queue.Queue</a></p>

<p>I am using it in a function like so:</p>

<p><code>node_queue = queue.Queue()</code></p>

<p>error:</p>

<pre><code>Traceback (most recent call last):
  File ""./test_jabba.py"", line 15, in &lt;module&gt;
    from utils import gopher, jsonstream, datagen, event_gen, tree_diff, postal
  File ""/Users/bli1/Development/QE/TrinityTestFramework/poc/utils/tree_diff.py"", line 5, in &lt;module&gt;
    import queue
ImportError: No module named queue
</code></pre>

<p>Line 5 is <code>import queue</code>:</p>

<pre><code>#!/usr/bin/env python3
import sys                      # access to basic things like sys.argv
import os                       # access pathname utilities
import argparse                 # for command-line options parsing
import queue
</code></pre>
","1815710","","","","","2019-12-14 06:27:24","queue ImportError in python 3","<python><python-3.x>","4","13","7","","","CC BY-SA 3.0","0"
"33161448","1","33161467","","2015-10-16 02:09:27","","78","60616","<p>When a Python list is known to always contain a single item, is there way to access it other than:</p>

<pre><code>mylist[0]
</code></pre>

<p>You may ask, 'Why would you want to?'. Curiosity alone. There seems to be an alternative way to do <em>everything</em> in Python.</p>
","1389110","","364696","","2018-11-08 15:12:36","2020-10-14 15:03:09","Getting only element from a single-element list in Python?","<python><python-3.x><python-2.7><list><iterable-unpacking>","2","6","10","","","CC BY-SA 3.0","0"
"34507744","1","34510541","","2015-12-29 08:59:26","","25","60522","<p>I have both Python 2.7 and Python 3.5 installed. When I type <code>pip install beautifulsoup4</code> it tells me that it is already installed in python2.7/site-package directory.</p>

<p>But how do I install it into the python3 dir?</p>
","898042","","1000551","","2019-06-10 09:14:35","2020-08-26 13:39:24","How to install beautifulsoup into python3, when default dir is python2.7?","<python><python-3.x><beautifulsoup><pip>","5","2","5","","","CC BY-SA 4.0","0"
"28107123","1","28111899","","2015-01-23 09:52:12","","54","60229","<p>I am trying to install numpy from whl file. I get the error: </p>

<pre><code>numpy-1.9.1%2Bmkl-cp34-none-win_amd64.whl is not a supported wheel on this platform.
</code></pre>

<p>Details:
Windows 8.1 pro x64, elevated command prompt</p>

<p>Python 3.4.2</p>

<p>package numpy from <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy"" rel=""noreferrer"">Gohlke's site</a></p>

<p>numpy-1.9.1%2Bmkl-cp34-none-win_amd64.whl copied in the pip.exe folder</p>

<p>The log file shows:</p>

<blockquote>
  <hr>
  
  <p>d:\Program Files\WinPython-64bit-3.4.2.4\python-3.4.2.amd64\Scripts\pip run on 01/23/15 11:55:21
  numpy-1.9.1%2Bmkl-cp34-none-win_amd64.whl is not a supported wheel on this platform.
  Exception information:
  Traceback (most recent call last):
    File ""D:\Python34\lib\site-packages\pip\basecommand.py"", line 122, in main
      status = self.run(options, args)
    File ""D:\Python34\lib\site-packages\pip\commands\install.py"", line 257, in run
      InstallRequirement.from_line(name, None))
    File ""D:\Python34\lib\site-packages\pip\req.py"", line 167, in from_line
      raise UnsupportedWheel(""%s is not a supported wheel on this platform."" % wheel.filename)
  pip.exceptions.UnsupportedWheel: numpy-1.9.1%2Bmkl-cp34-none-win_amd64.whl is not a supported wheel on this platform.</p>
</blockquote>

<p>What is wrong?</p>
","11464","","9802","","2015-10-11 23:59:13","2017-08-24 20:00:13","Cannot install numpy from wheel format","<python-3.x><numpy><python-wheel>","10","8","20","","","CC BY-SA 3.0","0"
"32417379","1","41224335","","2015-09-05 20:28:18","","39","59682","<p>I have a running Python 2.7/3.4 installation on my Windows 7 (x64) machine. I would like to test curses on Windows.</p>

<p>Curses is installed but not working:</p>

<pre><code>&gt;&gt;&gt; import curses
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Tools\Python3.4.2\lib\curses\__init__.py"", line 13, in &lt;module&gt;
    from _curses import *
ImportError: No module named '_curses'
</code></pre>

<p>The <a href=""https://docs.python.org/3/howto/curses.html"" rel=""noreferrer"">documentation</a> says:</p>

<blockquote>
  <p>The Windows version of Python doesn’t include the <a href=""https://docs.python.org/3/library/curses.html#module-curses"" rel=""noreferrer"">curses</a> module. A ported version called <a href=""https://pypi.python.org/pypi/UniCurses"" rel=""noreferrer"">UniCurses</a> is available.</p>
</blockquote>

<p>So, the Windows installer of Python 3.4 installed curses with unresolved dependencies. One could name this a bug...</p>

<p>OK, I looked into UniCurses. It's a wrapper for <a href=""http://pdcurses.sourceforge.net/"" rel=""noreferrer"">PDCurses</a>:</p>

<blockquote>
  <p>UniCurses is a wrapper for Python 2.x/3.x that provides a unified set of Curses functions on all platforms (MS Windows, Linux, and Mac OS X) with syntax close to that of the original NCurses. To provide the Curses functionality on Microsoft Windows systems it wraps <a href=""http://pdcurses.sourceforge.net/"" rel=""noreferrer"">PDCurses</a>.</p>
</blockquote>

<p>Installing UniCurses via <code>pip3</code> results in an error:</p>

<pre><code>C:\Users\Paebbels&gt;pip3 install UniCurses
Downloading/unpacking UniCurses
  Could not find any downloads that satisfy the requirement UniCurses
  Some externally hosted files were ignored (use --allow-external UniCurses to allow).
Cleaning up...
No distributions at all found for UniCurses
Storing debug log for failure in C:\Users\Paebbels\pip\pip.log
</code></pre>

<p>The link to SourceForge on Python's UniCurses site is dead. A manual search an SourceForge helped to find <a href=""https://sourceforge.net/projects/pyunicurses/?source=directory"" rel=""noreferrer"">UniCurses for Python</a> again.</p>

<p>But, the UniCurses 1.2 installer can not find any Python installation in my Windows registry. (Python 2.7.9 and Python 3.4.2 are available).</p>

<p>I also looked into Public Domain Curses (PDCurses). PD Cureses 3.4 is from late 2008. So it's 7 years old. I don't believe it will work either with Windows 7 nor Windows 8.1 or Windows 10.</p>

<p><strong>Is there any way to get curses running on Windows with Python.</strong></p>

<p>(The Windows Python, not the CygWin Python!)</p>
","3719459","","","","","2020-01-11 15:13:09","What is needed for curses in Python 3.4 on Windows7?","<windows><python-3.x><curses><pdcurses>","3","4","8","","","CC BY-SA 3.0","0"
"33700626","1","33700770","","2015-11-13 19:43:18","","23","59621","<p>No offence, if the questions is too basic. Let me know if you need more information.</p>

<p>I am looking for an idea to convert square-form tuple of tuples to pandas.DataFrame in a clean/efficient/pythonic way,
i.e. from </p>

<pre><code>s =((1,0,0,0,),(2,3,0,0,),(4,5,6,0,),(7,8,9,10,))
</code></pre>

<p>to <code>pandas.DataFrame</code> like</p>

<pre><code>   1  2  3   4
1  1  0  0   0
2  2  3  0   0
3  4  5  6   0
4  7  8  9  10
</code></pre>

<p>Naturally, this list can grow with more zeros appended in the upper-triangular (if we think of s as a tuple of rows).</p>

<p><code>DataFrame(t)</code> seems to fail.</p>
","3043335","","3043335","","2015-11-13 19:57:08","2019-02-05 17:28:39","How to convert tuple of tuples to pandas.DataFrame in Python?","<python><python-3.x><pandas><data-structures><tuples>","2","0","","","","CC BY-SA 3.0","1"
"33323172","1","35243904","","2015-10-24 20:46:18","","52","59409","<p>I am trying to install numpy in python 3.5 under windows 10 with visual studio 2015 ultimate installed.</p>

<p>Short version: file <code>vcvarsall.bat</code> is missing from vs14 folder <code>C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC</code> folder. Why?</p>

<p>Long version:
Running <code>pip install numpy</code> gives me this error <code>error: Unable to find vcvarsall.bat</code></p>

<p>researching this error got me to several stackoverflow answers that helped me figure out that python needs c++ compiler to compile some of the packages. And it needs the to do it using the same version of compiler that was used to compile python 3.5 (
<a href=""https://stackoverflow.com/questions/2817869/error-unable-to-find-vcvarsall-bat?rq=1"">error: Unable to find vcvarsall.bat</a> ). My python is compiled using [MSC v.1900 64 bit (AMD64)] - which is vs 14 ( visual studio 2015 )</p>

<p>Moving further with my research i learned out from: 
<a href=""https://stackoverflow.com/questions/19830942/pip-install-gives-error-unable-to-find-vcvarsall-bat"">pip install gives error: Unable to find vcvarsall.bat</a> that 
<code>get_build_version()</code> from <code>$python_install_prefix/Lib/distutils/msvc9compiler.py</code> returns the version of vs that shoud be used to find the path of <code>vcvarsall.bat</code>
For me this method returns 14. So everything correct.</p>

<p>Now when I look into the folder <code>C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC</code> there file vcvarsall.bat is missing.</p>

<p>But when I look into the folder <code>C:\Program Files (x86)\Microsoft Visual Studio 12.0\VC</code> there is a vcvarsall.bat.</p>

<p>Why is vcvarsall.bat missing from vs 14.0 ??</p>
","719031","","7429447","","2018-09-12 12:00:57","2018-09-12 12:05:56","vcvarsall.bat needed for python to compile missing from visual studio 2015 ( v 14)","<python-3.x><visual-c++><visual-studio-2015><pip><python-2.x>","6","5","15","","","CC BY-SA 3.0","0"
"32659552","1","","","2015-09-18 19:21:05","","69","59206","<p>I am trying to import the izip module like so: </p>

<pre><code>from itertools import izip
</code></pre>

<p>However after recently changing over from Python 2.7 to 3 - it doesn't seem to work.</p>

<p>I am trying to write to a csv file:</p>

<pre><code>writer.writerows(izip(variable1,2))
</code></pre>

<p>But I have no luck. Still encounter an error.</p>
","5000450","","7740870","","2019-04-24 15:39:28","2019-04-24 15:39:28","importing izip from itertools module gives NameError in Python 3.x","<python><python-3.x><python-2.7><itertools><izip>","3","0","4","","","CC BY-SA 4.0","0"
"51390968","1","51408997","","2018-07-17 22:23:31","","12","58895","<p>I'm using requests to access a RESTful API.  Everything seems to work.  I can authenticate, pull back a session token and even unit test the methods in my class I wrote for the API.  Then I tried to run my code.</p>

<p>First, here is the call I'm making.  The headers are static session-related items that get set in <strong>init</strong>().  The body is build dynamically from data in a file and passed in to this function.  All of the data is valid.</p>

<pre><code>response = requests.post(url, headers=(Requestheader), data=json.dumps((Requestbody)))
</code></pre>

<p>When I run the code, it updates well over 100 records with the metadata I supply.  Somewhere around item 150 I get the following:</p>

<blockquote>
  <p>ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED]
  certificate verify failed: self signed certificate in certificate
  chain (_ssl.c:1045)</p>
</blockquote>

<p>My first step was to call the vendor and find out if ALL of their web servers had properly signed certs figuring they were load balancing me and I found a misconfigured server.  They tell me this is not the case.</p>

<p>Then I Googled the message and found that there is a verify kwarg, so I tried:</p>

<pre><code>response = requests.post(url, headers=Requestheader, data=json.dumps(Requestbody), verify=False)
</code></pre>

<p>I know this isn't ideal long-term, but I wanted to test it to see if The behavior is the same.  It did the same thing.  It ran for a while and threw the ssl error.  I thought the idea of verify=False is that it wouldn't check.</p>

<p>The vendor suggested I check the url I'm using, but it's fine.  I would think if there were a proxy server or real man-in-the-middle attack causing problems I wouldn't see success so many times before a failure.  I thought maybe it is a session timeout, but that should throw a 401 status and my activity level is too high for an inactivity timeout.  </p>

<p>I'm a python noob and not a security professional.  Suggestions appreciated.</p>
","10071946","","","","","2020-07-14 05:32:53","Python SSL certificate verify error","<python-3.x><ssl><openssl><python-requests>","2","1","4","","","CC BY-SA 4.0","0"
"36212905","1","36212932","","2016-03-25 01:56:05","","27","58864","<pre><code>TypeError: b'Pizza is a flatbread generally topped with tomato sauce and cheese and baked in an oven. It is commonly topped with a selection of meats, vegetables and condiments. The term was first recorded in the 10th century, in a Latin manuscript from Gaeta in Central Italy. The modern pizza was invented in Naples, Italy, and the dish and its variants have since become popular in many areas of the world.\nIn 2009, upon Italy\'s request, Neapolitan pizza was safeguarded in the European Union as a Traditional Speciality Guaranteed dish. The Associazione Verace Pizza Napoletana (the True Neapolitan Pizza Association) is a non-profit organization founded in 1984 with headquarters in Naples. It promotes and protects the ""true Neapolitan pizza"".\nPizza is sold fresh, frozen or in portions, and is a common fast food item in North America and the United Kingdom. Various types of ovens are used to cook them and many varieties exist. Several similar dishes are prepared from ingredients commonly used in pizza preparation, such as calzone and stromboli.' is not JSON serializable
</code></pre>

<p>I have a program that adds this into a JSON string, which works fine for most text strings - but not this one apparently. Can you tell why not, or how to fix it?</p>
","2214180","","","","","2019-07-25 04:57:08","Python 3: Is not JSON serializable","<python><python-3.x>","2","1","7","","","CC BY-SA 3.0","0"
"29440482","1","29441115","","2015-04-03 22:24:35","","29","58680","<p>I'm trying to install <code>lmxl</code> on my Windows 8.1 laptop with Python 3.4 and failing miserably.</p>

<p>First off, I tried the simple and obvious solution: <code>pip install lxml</code>. However, this didn't work. Here's what it said:</p>

<pre><code>Downloading/unpacking lxml
  Running setup.py (path:C:\Users\CARTE_~1\AppData\Local\Temp\pip_build_carte_000\lxml\setup.py) egg_info for package lxml
    Building lxml version 3.4.2.
    Building without Cython.
    ERROR: b""'xslt-config' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n""
    ** make sure the development packages of libxml2 and libxslt are installed **

    Using build configuration of libxslt
    C:\Python34\lib\distutils\dist.py:260: UserWarning: Unknown distribution option: 'bugtrack_url'
      warnings.warn(msg)

    warning: no previously-included files found matching '*.py'
Installing collected packages: lxml
  Running setup.py install for lxml
    Building lxml version 3.4.2.
    Building without Cython.
    ERROR: b""'xslt-config' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n""
    ** make sure the development packages of libxml2 and libxslt are installed **

    Using build configuration of libxslt
    building 'lxml.etree' extension
    C:\Python34\lib\distutils\dist.py:260: UserWarning: Unknown distribution option: 'bugtrack_url'
      warnings.warn(msg)
    error: Unable to find vcvarsall.bat
    Complete output from command C:\Python34\python.exe -c ""import setuptools, tokenize;__file__='C:\\Users\\CARTE_~1\\AppData\\Local\\Temp\\pip_build_carte_000\\lxml\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record C:\Users\CARTE_~1\AppData\Local\Temp\pip-l8vvrv9g-record\install-record.txt --single-version-externally-managed --compile:
    Building lxml version 3.4.2.

Building without Cython.

ERROR: b""'xslt-config' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n""

** make sure the development packages of libxml2 and libxslt are installed **



Using build configuration of libxslt

running install

running build

running build_py

creating build

creating build\lib.win32-3.4

creating build\lib.win32-3.4\lxml

copying src\lxml\builder.py -&gt; build\lib.win32-3.4\lxml

copying src\lxml\cssselect.py -&gt; build\lib.win32-3.4\lxml

copying src\lxml\doctestcompare.py -&gt; build\lib.win32-3.4\lxml

copying src\lxml\ElementInclude.py -&gt; build\lib.win32-3.4\lxml

copying src\lxml\pyclasslookup.py -&gt; build\lib.win32-3.4\lxml

copying src\lxml\sax.py -&gt; build\lib.win32-3.4\lxml

copying src\lxml\usedoctest.py -&gt; build\lib.win32-3.4\lxml

copying src\lxml\_elementpath.py -&gt; build\lib.win32-3.4\lxml

copying src\lxml\__init__.py -&gt; build\lib.win32-3.4\lxml

creating build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\__init__.py -&gt; build\lib.win32-3.4\lxml\includes

creating build\lib.win32-3.4\lxml\html

copying src\lxml\html\builder.py -&gt; build\lib.win32-3.4\lxml\html

copying src\lxml\html\clean.py -&gt; build\lib.win32-3.4\lxml\html

copying src\lxml\html\defs.py -&gt; build\lib.win32-3.4\lxml\html

copying src\lxml\html\diff.py -&gt; build\lib.win32-3.4\lxml\html

copying src\lxml\html\ElementSoup.py -&gt; build\lib.win32-3.4\lxml\html

copying src\lxml\html\formfill.py -&gt; build\lib.win32-3.4\lxml\html

copying src\lxml\html\html5parser.py -&gt; build\lib.win32-3.4\lxml\html

copying src\lxml\html\soupparser.py -&gt; build\lib.win32-3.4\lxml\html

copying src\lxml\html\usedoctest.py -&gt; build\lib.win32-3.4\lxml\html

copying src\lxml\html\_diffcommand.py -&gt; build\lib.win32-3.4\lxml\html

copying src\lxml\html\_html5builder.py -&gt; build\lib.win32-3.4\lxml\html

copying src\lxml\html\_setmixin.py -&gt; build\lib.win32-3.4\lxml\html

copying src\lxml\html\__init__.py -&gt; build\lib.win32-3.4\lxml\html

creating build\lib.win32-3.4\lxml\isoschematron

copying src\lxml\isoschematron\__init__.py -&gt; build\lib.win32-3.4\lxml\isoschematron

copying src\lxml\lxml.etree.h -&gt; build\lib.win32-3.4\lxml

copying src\lxml\lxml.etree_api.h -&gt; build\lib.win32-3.4\lxml

copying src\lxml\includes\c14n.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\config.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\dtdvalid.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\etreepublic.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\htmlparser.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\relaxng.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\schematron.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\tree.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\uri.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\xinclude.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\xmlerror.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\xmlparser.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\xmlschema.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\xpath.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\xslt.pxd -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\etree_defs.h -&gt; build\lib.win32-3.4\lxml\includes

copying src\lxml\includes\lxml-version.h -&gt; build\lib.win32-3.4\lxml\includes

creating build\lib.win32-3.4\lxml\isoschematron\resources

creating build\lib.win32-3.4\lxml\isoschematron\resources\rng

copying src\lxml\isoschematron\resources\rng\iso-schematron.rng -&gt; build\lib.win32-3.4\lxml\isoschematron\resources\rng

creating build\lib.win32-3.4\lxml\isoschematron\resources\xsl

copying src\lxml\isoschematron\resources\xsl\RNG2Schtrn.xsl -&gt; build\lib.win32-3.4\lxml\isoschematron\resources\xsl

copying src\lxml\isoschematron\resources\xsl\XSD2Schtrn.xsl -&gt; build\lib.win32-3.4\lxml\isoschematron\resources\xsl

creating build\lib.win32-3.4\lxml\isoschematron\resources\xsl\iso-schematron-xslt1

copying src\lxml\isoschematron\resources\xsl\iso-schematron-xslt1\iso_abstract_expand.xsl -&gt; build\lib.win32-3.4\lxml\isoschematron\resources\xsl\iso-schematron-xslt1

copying src\lxml\isoschematron\resources\xsl\iso-schematron-xslt1\iso_dsdl_include.xsl -&gt; build\lib.win32-3.4\lxml\isoschematron\resources\xsl\iso-schematron-xslt1

copying src\lxml\isoschematron\resources\xsl\iso-schematron-xslt1\iso_schematron_message.xsl -&gt; build\lib.win32-3.4\lxml\isoschematron\resources\xsl\iso-schematron-xslt1

copying src\lxml\isoschematron\resources\xsl\iso-schematron-xslt1\iso_schematron_skeleton_for_xslt1.xsl -&gt; build\lib.win32-3.4\lxml\isoschematron\resources\xsl\iso-schematron-xslt1

copying src\lxml\isoschematron\resources\xsl\iso-schematron-xslt1\iso_svrl_for_xslt1.xsl -&gt; build\lib.win32-3.4\lxml\isoschematron\resources\xsl\iso-schematron-xslt1

copying src\lxml\isoschematron\resources\xsl\iso-schematron-xslt1\readme.txt -&gt; build\lib.win32-3.4\lxml\isoschematron\resources\xsl\iso-schematron-xslt1

running build_ext

building 'lxml.etree' extension

C:\Python34\lib\distutils\dist.py:260: UserWarning: Unknown distribution option: 'bugtrack_url'

  warnings.warn(msg)

error: Unable to find vcvarsall.bat

----------------------------------------
Cleaning up...
Command C:\Python34\python.exe -c ""import setuptools, tokenize;__file__='C:\\Users\\CARTE_~1\\AppData\\Local\\Temp\\pip_build_carte_000\\lxml\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record C:\Users\CARTE_~1\AppData\Local\Temp\pip-l8vvrv9g-record\install-record.txt --single-version-externally-managed --compile failed with error code 1 in C:\Users\CARTE_~1\AppData\Local\Temp\pip_build_carte_000\lxml
Storing debug log for failure in C:\Users\carte_000\pip\pip.log
</code></pre>

<p>So then I looked on this great and helpful thing called <em>The Internet</em> and a lot of people have the same error of needing <code>libxml2</code> and <code>libxlst</code>. They recommend a guy called Christoph Gohlke's page where he provides some sort of binary thingy for a bunch of packages. You can find it <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/"" rel=""noreferrer"">here</a> (<a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml"" rel=""noreferrer"">quicklink to the lxml part</a>).</p>

<p>So after I gave up on trying to find libxml2 and libxslt for pip, I decided to go there, and found <em>an absolute ton</em> of downloads. I know I need a 64-bit one, but I have no idea which ""<code>cp</code>"" I need.</p>

<p>So an answer either giving me a solution on the <code>pip</code> method or the Gohlke index method would be great.</p>
","4172051","","","","","2019-09-19 16:40:43","How to install lxml on Windows","<python><windows><python-3.x><pip><lxml>","7","3","5","","","CC BY-SA 3.0","0"
"59193514","1","59252857","","2019-12-05 10:50:00","","91","58658","<p>Recently, I upgraded the version of Django framework from <strong><code>2.0.6</code></strong> to <strong><code>3.0</code></strong> and suddenly after calling <code>python manage.py shell</code> command, I got this exception:</p>

<blockquote>
  <p>ImportError: cannot import name 'six' from 'django.utils' (/path-to-project/project/venv/lib/python3.7/site-packages/django/utils/<strong>init</strong>.py)</p>
</blockquote>

<p><strong>Full trace:</strong></p>

<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last):
  File ""manage.py"", line 13, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""/path-to-project/project/venv/lib/python3.7/site-packages/django/core/management/__init__.py"", line 401, in execute_from_command_line
    utility.execute()
  File ""/path-to-project/project/venv/lib/python3.7/site-packages/django/core/management/__init__.py"", line 377, in execute
    django.setup()
  File ""/path-to-project/project/venv/lib/python3.7/site-packages/django/__init__.py"", line 24, in setup
    apps.populate(settings.INSTALLED_APPS)
  File ""/path-to-project/project/venv/lib/python3.7/site-packages/django/apps/registry.py"", line 91, in populate
    app_config = AppConfig.create(entry)
  File ""/path-to-project/project/venv/lib/python3.7/site-packages/django/apps/config.py"", line 90, in create
    module = import_module(entry)
  File ""/usr/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 1006, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 983, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 967, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 677, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 728, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 219, in _call_with_frames_removed
  File ""/path-to-project/project/venv/lib/python3.7/site-packages/corsheaders/__init__.py"", line 1, in &lt;module&gt;
    from .checks import check_settings  # noqa: F401
  File ""/path-to-project/project/venv/lib/python3.7/site-packages/corsheaders/checks.py"", line 7, in &lt;module&gt;
    from django.utils import six
</code></pre>

<p><strong>Similar Questions:</strong></p>

<p>I read this <a href=""https://stackoverflow.com/questions/59191180/getting-error-cannot-import-name-six-from-django-utils-when-using-django-3-0"">Question</a> and this <a href=""/questions/tagged/django-3.0"" class=""post-tag"" title=""show questions tagged &#39;django-3.0&#39;"" rel=""tag"">django-3.0</a>, <a href=""https://docs.djangoproject.com/en/3.0/releases/3.0/"" rel=""noreferrer"">release note</a> , but those resources couldn't help me.</p>
","5410779","","5410779","","2020-06-10 17:57:03","2020-09-04 19:56:21","ImportError: cannot import name 'six' from 'django.utils'","<django><python-3.x><upgrade><django-3.0>","17","6","9","","","CC BY-SA 4.0","0"
"38134086","1","46409649","","2016-06-30 22:12:55","","78","58521","<p>I want to configure pylint as an external tool on my entire project directory for a python project that I'm working on. I've tried to use the repository as a module with <code>__init__.py</code> and without, and its not working either way.</p>

<p>I'm having difficulty setting up pylint to run with PyCharm. I know that I should be running it as an external tool, however the settings confuse me. </p>

<p>The authoritative source on their documentation is broken, so I can't check that up either.</p>
","1574632","","","","","2019-04-17 09:24:39","How to run Pylint with PyCharm","<python><python-3.x><pycharm><pylint>","8","0","24","","","CC BY-SA 3.0","0"
"30760728","1","30760915","","2015-06-10 15:25:19","","40","58403","<p>I am trying to convert working Python 2.7 code into Python 3 code and I am receiving a type error from the urllib request module.</p>

<p>I used the inbuilt 2to3 Python tool to convert the below working urllib and urllib2 Python 2.7 code:</p>

<pre><code>import urllib2
import urllib

url = ""https://www.customdomain.com""
d = dict(parameter1=""value1"", parameter2=""value2"")

req = urllib2.Request(url, data=urllib.urlencode(d))
f = urllib2.urlopen(req)
resp = f.read()
</code></pre>

<p>The output from the 2to3 module was the below Python 3 code:</p>

<pre><code>import urllib.request, urllib.error, urllib.parse

url = ""https://www.customdomain.com""
d = dict(parameter1=""value1"", parameter2=""value2"")

req = urllib.request.Request(url, data=urllib.parse.urlencode(d))
f = urllib.request.urlopen(req)
resp = f.read()
</code></pre>

<p>When the Python 3 code is run the following error is produced:</p>

<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-56-206954140899&gt; in &lt;module&gt;()
      5 
      6 req = urllib.request.Request(url, data=urllib.parse.urlencode(d))
----&gt; 7 f = urllib.request.urlopen(req)
      8 resp = f.read()

C:\Users\Admin\Anaconda3\lib\urllib\request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)
    159     else:
    160         opener = _opener
--&gt; 161     return opener.open(url, data, timeout)
    162 
    163 def install_opener(opener):

C:\Users\Admin\Anaconda3\lib\urllib\request.py in open(self, fullurl, data, timeout)
    459         for processor in self.process_request.get(protocol, []):
    460             meth = getattr(processor, meth_name)
--&gt; 461             req = meth(req)
    462 
    463         response = self._open(req, data)

C:\Users\Admin\Anaconda3\lib\urllib\request.py in do_request_(self, request)
   1110                 msg = ""POST data should be bytes or an iterable of bytes. "" \
   1111                       ""It cannot be of type str.""
-&gt; 1112                 raise TypeError(msg)
   1113             if not request.has_header('Content-type'):
   1114                 request.add_unredirected_header(

TypeError: POST data should be bytes or an iterable of bytes. It cannot be of type str.
</code></pre>

<p>I have also read two other tickets (<a href=""https://stackoverflow.com/questions/28850421/post-data-should-be-bytes-or-an-iterable-of-bytes-it-cannot-be-of-type-str"">ticket1</a> and <a href=""https://stackoverflow.com/questions/5440485/typeerror-post-data-should-be-bytes-or-an-iterable-of-bytes-it-cannot-be-str"">ticket2</a>) which mentioned encoding the date. </p>

<p>When I changed the line <code>f = urllib.request.urlopen(req)</code> to <code>f = urllib.request.urlopen(req.encode('utf-8'))</code> I received the following error: <code>AttributeError: 'Request' object has no attribute 'encode'</code></p>

<p>I am stuck as to how to make the Python 3 code work. Could you please help me?</p>
","4605629","","-1","","2017-05-23 12:25:54","2019-08-11 16:24:00","Python 3 urllib produces TypeError: POST data should be bytes or an iterable of bytes. It cannot be of type str","<python><python-2.7><python-3.x><urllib2><urllib>","3","0","3","","","CC BY-SA 3.0","0"
"33703624","1","","","2015-11-14 00:03:32","","151","58135","<p>How does <code>tf.app.run()</code> work in Tensorflow translate demo? </p>

<p>In <code>tensorflow/models/rnn/translate/translate.py</code>, there is a call to <code>tf.app.run()</code>. How is it being handled?</p>

<pre><code>if __name__ == ""__main__"":
    tf.app.run() 
</code></pre>
","2669077","","2956066","","2018-01-18 20:20:13","2020-01-21 09:44:06","How does tf.app.run() work?","<python><python-3.x><tensorflow>","6","0","40","","","CC BY-SA 3.0","0"
"27850113","1","","","2015-01-08 21:49:33","","7","58133","<p>I am having great difficulty getting python 3.4 to recognize a path or text file on a windows 8 system. I have tried a variety of different approaches but get the similar errors (which likely implies something simple regarding the syntax).</p>

<p>The file itself is located in the same folder as the script file trying to open it:
C:\Users\User\Desktop\Python stuff\Data.txt</p>

<p>for simplicity, the simplest means to access the file (at least that I know of) is
<code>f=open</code></p>

<p>These lines were coded as:</p>

<pre><code>f = open(""Data.txt"", ""r"")
</code></pre>

<p>and </p>

<pre><code>f = open(""C:/Users/User/Desktop/Python stuff/Data.txt"", ""r"") 
</code></pre>

<p>but return the error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\User\Desktop\Python stuff\Testscript.py"", line 3, in &lt;module&gt;
    f = open(""C:/Users/User/Desktop/Python stuff/Data.txt"", ""r"")
FileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/User/Desktop/Python stuff/Data.txt'
</code></pre>
","4434674","","1033581","","2017-12-27 10:15:48","2020-04-24 15:32:30","Python can't find file","<python><python-3.x>","6","3","","","","CC BY-SA 3.0","0"
"54175042","1","54240362","","2019-01-14 02:15:38","","52","58066","<p>I created anaconda environment with Python=3.7 and have trouble with the error of _ssl and DLL. When I tried to get back to my base environment, I have trouble getting the background processes to complete as shown in the figure below, this goes on forever.</p>

<p><a href=""https://i.stack.imgur.com/pkjyT.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/pkjyT.png"" alt=""pycharm_0""></a></p>

<p>The error:</p>

<pre><code>C:\Users\abhil\AppData\Local\Continuum\anaconda3\envs\HeisenbergPy37\python.exe ""C:\Program Files\JetBrains\PyCharm Community Edition 2018.1.1\helpers\pydev\pydevconsole.py"" --mode=client --port=63950
Traceback (most recent call last):
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2018.1.1\helpers\pydev\pydevconsole.py"", line 5, in &lt;module&gt;
    from _pydev_comm.rpc import make_rpc_client, start_rpc_server, start_rpc_server_and_make_client
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2018.1.1\helpers\pydev\_pydev_comm\rpc.py"", line 4, in &lt;module&gt;
    from _pydev_comm.server import TSingleThreadedServer
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2018.1.1\helpers\pydev\_pydev_comm\server.py"", line 4, in &lt;module&gt;
    from _shaded_thriftpy.server import TServer
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2018.1.1\helpers\third_party\thriftpy\_shaded_thriftpy\server.py"", line 9, in &lt;module&gt;
    from _shaded_thriftpy.transport import (
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2018.1.1\helpers\third_party\thriftpy\_shaded_thriftpy\transport\__init__.py"", line 57, in &lt;module&gt;
    from .sslsocket import TSSLSocket, TSSLServerSocket  # noqa
  File ""C:\Program Files\JetBrains\PyCharm Community Edition 2018.1.1\helpers\third_party\thriftpy\_shaded_thriftpy\transport\sslsocket.py"", line 7, in &lt;module&gt;
    import ssl
  File ""C:\Users\abhil\AppData\Local\Continuum\anaconda3\envs\HeisenbergPy37\lib\ssl.py"", line 98, in &lt;module&gt;
    import _ssl             # if we can't import it, let the error propagate
ImportError: DLL load failed: The specified module could not be found.
Process finished with exit code 1
</code></pre>

<p>All packages installed with </p>

<blockquote>
  <p>conda create -n  anaconda python=3.7</p>
</blockquote>

<pre><code>C:\WINDOWS\system32&gt;conda list
# packages in environment at C:\Users\abhil\AppData\Local\Continuum\anaconda3\envs\HeisenbergPy37:
#
# Name                    Version                   Build  Channel
alabaster                 0.7.12                   py37_0
anaconda                  2018.12                  py37_0
anaconda-client           1.7.2                    py37_0
anaconda-project          0.8.2                    py37_0
asn1crypto                0.24.0                   py37_0
astroid                   2.1.0                    py37_0
astropy                   3.1              py37he774522_0
atomicwrites              1.2.1                    py37_0
attrs                     18.2.0           py37h28b3542_0
babel                     2.6.0                    py37_0
backcall                  0.1.0                    py37_0
backports                 1.0                      py37_1
backports.os              0.1.1                    py37_0
backports.shutil_get_terminal_size 1.0.0                    py37_2
beautifulsoup4            4.6.3                    py37_0
bitarray                  0.8.3            py37hfa6e2cd_0
bkcharts                  0.2                      py37_0
blas                      1.0                         mkl
blaze                     0.11.3                   py37_0
bleach                    3.0.2                    py37_0
blosc                     1.14.4               he51fdeb_0
bokeh                     1.0.2                    py37_0
boto                      2.49.0                   py37_0
bottleneck                1.2.1            py37h452e1ab_1
bzip2                     1.0.6                hfa6e2cd_5
ca-certificates           2018.03.07                    0
certifi                   2018.11.29               py37_0
cffi                      1.11.5           py37h74b6da3_1
chardet                   3.0.4                    py37_1
click                     7.0                      py37_0
cloudpickle               0.6.1                    py37_0
clyent                    1.2.2                    py37_1
colorama                  0.4.1                    py37_0
comtypes                  1.1.7                    py37_0
console_shortcut          0.1.1                         3
contextlib2               0.5.5                    py37_0
cryptography              2.4.2            py37h7a1dbc1_0
curl                      7.63.0            h2a8f88b_1000
cycler                    0.10.0                   py37_0
cython                    0.29.2           py37ha925a31_0
cytoolz                   0.9.0.1          py37hfa6e2cd_1
dask                      1.0.0                    py37_0
dask-core                 1.0.0                    py37_0
datashape                 0.5.4                    py37_1
decorator                 4.3.0                    py37_0
defusedxml                0.5.0                    py37_1
distributed               1.25.1                   py37_0
docutils                  0.14                     py37_0
entrypoints               0.2.3                    py37_2
et_xmlfile                1.0.1                    py37_0
fastcache                 1.0.2            py37hfa6e2cd_2
filelock                  3.0.10                   py37_0
flask                     1.0.2                    py37_1
flask-cors                3.0.7                    py37_0
freetype                  2.9.1                ha9979f8_1
get_terminal_size         1.0.0                h38e98db_0
gevent                    1.3.7            py37he774522_1
glob2                     0.6                      py37_1
greenlet                  0.4.15           py37hfa6e2cd_0
h5py                      2.8.0            py37h3bdd7fb_2
hdf5                      1.10.2               hac2f561_1
heapdict                  1.0.0                    py37_2
html5lib                  1.0.1                    py37_0
icc_rt                    2019.0.0             h0cc432a_1
icu                       58.2                 ha66f8fd_1
idna                      2.8                      py37_0
imageio                   2.4.1                    py37_0
imagesize                 1.1.0                    py37_0
importlib_metadata        0.6                      py37_0
intel-openmp              2019.1                      144
ipykernel                 5.1.0            py37h39e3cac_0
ipython                   7.2.0            py37h39e3cac_0
ipython_genutils          0.2.0                    py37_0
ipywidgets                7.4.2                    py37_0
isort                     4.3.4                    py37_0
itsdangerous              1.1.0                    py37_0
jdcal                     1.4                      py37_0
jedi                      0.13.2                   py37_0
jinja2                    2.10                     py37_0
jpeg                      9b                   hb83a4c4_2
jsonschema                2.6.0                    py37_0
jupyter                   1.0.0                    py37_7
jupyter_client            5.2.4                    py37_0
jupyter_console           6.0.0                    py37_0
jupyter_core              4.4.0                    py37_0
jupyterlab                0.35.3                   py37_0
jupyterlab_server         0.2.0                    py37_0
keyring                   17.0.0                   py37_0
kiwisolver                1.0.1            py37h6538335_0
krb5                      1.16.1               hc04afaa_7
lazy-object-proxy         1.3.1            py37hfa6e2cd_2
libarchive                3.3.3                h0643e63_5
libcurl                   7.63.0            h2a8f88b_1000
libiconv                  1.15                 h1df5818_7
libpng                    1.6.35               h2a8f88b_0
libsodium                 1.0.16               h9d3ae62_0
libssh2                   1.8.0                h7a1dbc1_4
libtiff                   4.0.9                h36446d0_2
libxml2                   2.9.8                hadb2253_1
libxslt                   1.1.32               hf6f1972_0
llvmlite                  0.26.0           py37ha925a31_0
locket                    0.2.0                    py37_1
lxml                      4.2.5            py37hef2cd61_0
lz4-c                     1.8.1.2              h2fa13f4_0
lzo                       2.10                 h6df0209_2
m2w64-gcc-libgfortran     5.3.0                         6
m2w64-gcc-libs            5.3.0                         7
m2w64-gcc-libs-core       5.3.0                         7
m2w64-gmp                 6.1.0                         2
m2w64-libwinpthread-git   5.0.0.4634.697f757               2
markupsafe                1.1.0            py37he774522_0
matplotlib                3.0.2            py37hc8f65d3_0
mccabe                    0.6.1                    py37_1
menuinst                  1.4.14           py37hfa6e2cd_0
mistune                   0.8.4            py37he774522_0
mkl                       2019.1                      144
mkl-service               1.1.2            py37hb782905_5
mkl_fft                   1.0.6            py37h6288b17_0
mkl_random                1.0.2            py37h343c172_0
more-itertools            4.3.0                    py37_0
mpmath                    1.1.0                    py37_0
msgpack-python            0.5.6            py37he980bc4_1
msys2-conda-epoch         20160418                      1
multipledispatch          0.6.0                    py37_0
nbconvert                 5.4.0                    py37_1
nbformat                  4.4.0                    py37_0
networkx                  2.2                      py37_1
nltk                      3.4                      py37_1
nose                      1.3.7                    py37_2
notebook                  5.7.4                    py37_0
numba                     0.41.0           py37hf9181ef_0
numexpr                   2.6.8            py37hdce8814_0
numpy                     1.15.4           py37h19fb1c0_0
numpy-base                1.15.4           py37hc3f5095_0
numpydoc                  0.8.0                    py37_0
odo                       0.5.1                    py37_0
olefile                   0.46                     py37_0
openpyxl                  2.5.12                   py37_0
openssl                   1.1.1a               he774522_0
packaging                 18.0                     py37_0
pandas                    0.23.4           py37h830ac7b_0
pandoc                    1.19.2.1             hb2460c7_1
pandocfilters             1.4.2                    py37_1
parso                     0.3.1                    py37_0
partd                     0.3.9                    py37_0
path.py                   11.5.0                   py37_0
pathlib2                  2.3.3                    py37_0
patsy                     0.5.1                    py37_0
pep8                      1.7.1                    py37_0
pickleshare               0.7.5                    py37_0
pillow                    5.3.0            py37hdc69c19_0
pip                       18.1                     py37_0
pkginfo                   1.4.2                    py37_1
pluggy                    0.8.0                    py37_0
ply                       3.11                     py37_0
prometheus_client         0.5.0                    py37_0
prompt_toolkit            2.0.7                    py37_0
psutil                    5.4.8            py37he774522_0
py                        1.7.0                    py37_0
pycodestyle               2.4.0                    py37_0
pycosat                   0.6.3            py37hfa6e2cd_0
pycparser                 2.19                     py37_0
pycrypto                  2.6.1            py37hfa6e2cd_9
pycurl                    7.43.0.2         py37h7a1dbc1_0
pyflakes                  2.0.0                    py37_0
pygments                  2.3.1                    py37_0
pylint                    2.2.2                    py37_0
pyodbc                    4.0.25           py37ha925a31_0
pyopenssl                 18.0.0                   py37_0
pyparsing                 2.3.0                    py37_0
pyqt                      5.9.2            py37h6538335_2
pysocks                   1.6.8                    py37_0
pytables                  3.4.4            py37he6f6034_0
pytest                    4.0.2                    py37_0
pytest-arraydiff          0.3              py37h39e3cac_0
pytest-astropy            0.5.0                    py37_0
pytest-doctestplus        0.2.0                    py37_0
pytest-openfiles          0.3.1                    py37_0
pytest-remotedata         0.3.1                    py37_0
python                    3.7.1                h8c8aaf0_6
python-dateutil           2.7.5                    py37_0
python-libarchive-c       2.8                      py37_6
pytz                      2018.7                   py37_0
pywavelets                1.0.1            py37h8c2d366_0
pywin32                   223              py37hfa6e2cd_1
pywinpty                  0.5.5                 py37_1000
pyyaml                    3.13             py37hfa6e2cd_0
pyzmq                     17.1.2           py37hfa6e2cd_0
qt                        5.9.7            vc14h73c81de_0
qtawesome                 0.5.3                    py37_0
qtconsole                 4.4.3                    py37_0
qtpy                      1.5.2                    py37_0
requests                  2.21.0                   py37_0
rope                      0.11.0                   py37_0
ruamel_yaml               0.15.46          py37hfa6e2cd_0
scikit-image              0.14.1           py37ha925a31_0
scikit-learn              0.20.1           py37h343c172_0
scipy                     1.1.0            py37h29ff71c_2
seaborn                   0.9.0                    py37_0
send2trash                1.5.0                    py37_0
setuptools                40.6.3                   py37_0
simplegeneric             0.8.1                    py37_2
singledispatch            3.4.0.3                  py37_0
sip                       4.19.8           py37h6538335_0
six                       1.12.0                   py37_0
snappy                    1.1.7                h777316e_3
snowballstemmer           1.2.1                    py37_0
sortedcollections         1.0.1                    py37_0
sortedcontainers          2.1.0                    py37_0
sphinx                    1.8.2                    py37_0
sphinxcontrib             1.0                      py37_1
sphinxcontrib-websupport  1.1.0                    py37_1
spyder                    3.3.2                    py37_0
spyder-kernels            0.3.0                    py37_0
sqlalchemy                1.2.15           py37he774522_0
sqlite                    3.26.0               he774522_0
statsmodels               0.9.0            py37h452e1ab_0
sympy                     1.3                      py37_0
tblib                     1.3.2                    py37_0
terminado                 0.8.1                    py37_1
testpath                  0.4.2                    py37_0
tk                        8.6.8                hfa6e2cd_0
toolz                     0.9.0                    py37_0
tornado                   5.1.1            py37hfa6e2cd_0
tqdm                      4.28.1           py37h28b3542_0
traitlets                 4.3.2                    py37_0
unicodecsv                0.14.1                   py37_0
urllib3                   1.24.1                   py37_0
vc                        14.1                 h0510ff6_4
vs2015_runtime            14.15.26706          h3a45250_0
wcwidth                   0.1.7                    py37_0
webencodings              0.5.1                    py37_1
werkzeug                  0.14.1                   py37_0
wheel                     0.32.3                   py37_0
widgetsnbextension        3.4.2                    py37_0
win_inet_pton             1.0.1                    py37_1
win_unicode_console       0.5                      py37_0
wincertstore              0.2                      py37_0
winpty                    0.4.3                         4
wrapt                     1.10.11          py37hfa6e2cd_2
xlrd                      1.2.0                    py37_0
xlsxwriter                1.1.2                    py37_0
xlwings                   0.15.1                   py37_0
xlwt                      1.3.0                    py37_0
xz                        5.2.4                h2fa13f4_4
yaml                      0.1.7                hc54c509_2
zeromq                    4.2.5                he025d50_1
zict                      0.1.3                    py37_0
zlib                      1.2.11               h62dcd97_3
zstd                      1.3.7                h508b16e_0
</code></pre>
","2252819","","","","","2020-10-18 01:36:04","Python 3.7 anaconda environment - import _ssl DLL load fail error","<python><python-3.x><pycharm><anaconda>","9","4","23","","","CC BY-SA 4.0","0"
"47324756","1","47324932","","2017-11-16 08:22:14","","11","58027","<p>I am using python 3.6 and a learner.  Below is a simple code of a sin wave.</p>

<pre><code>import matplotlib.pyplot as plt 
import numpy as np 

x = np.linspace(-10 , 10, 100)
y = np.sin(x) 
plt.plot(x, y, marker=""x"")
</code></pre>

<p>I am receiving the error ""AttributeError: module 'matplotlib' has no attribute 'plot'""  Any help would be appreciated.</p>
","257705","","","","","2020-04-25 09:10:28","AttributeError: module 'matplotlib' has no attribute 'plot'","<python><python-3.x>","1","1","","","","CC BY-SA 3.0","0"
"34803467","1","34803630","","2016-01-15 02:33:51","","57","57826","<p>I'm trying to execute ansible2 commnads...</p>

<p>When I do:</p>

<pre><code>ansible-playbook -vvv -i my/inventory my/playbook.yml
</code></pre>

<p>I get:</p>

<blockquote>
  <p>Unexpected Exception: name 'basestring' is not defined
  the full traceback was:</p>

<pre><code>Traceback (most recent call last):
  File ""/usr/local/bin/ansible-playbook"", line 85, in &lt;module&gt;
    sys.exit(cli.run())
  File ""/usr/local/lib/python3.4/site-packages/ansible/cli/playbook.py"", line 150, in run
    results = pbex.run()
  File ""/usr/local/lib/python3.4/site-packages/ansible/executor/playbook_executor.py"", line 87, in run
    self._tqm.load_callbacks()
  File ""/usr/local/lib/python3.4/site-packages/ansible/executor/task_queue_manager.py"", line 149, in load_callbacks
    elif isinstance(self._stdout_callback, basestring):
NameError: name 'basestring' is not defined
</code></pre>
</blockquote>

<p>Here is <code>ansible --version</code>:</p>

<pre><code>ansible 2.0.0.2
  config file = /etc/ansible/ansible.cfg
  configured module search path = Default w/o overrides
</code></pre>

<p>And here is <code>python --version</code></p>

<pre><code>Python 3.4.3
</code></pre>
","977083","","4870915","","2018-07-26 00:10:45","2020-02-13 13:56:57","Unexpected Exception: name 'basestring' is not defined when invoking ansible2","<python><python-3.x><ansible><ansible-2.x>","5","0","7","","","CC BY-SA 3.0","0"
"39801718","1","39801780","","2016-10-01 00:13:59","","61","57757","<p>this is my Python3 project hiearchy:</p>

<pre><code>projet
  \
  script.py
  web
    \
    index.html
</code></pre>

<p>From <code>script.py</code>, I would like to run a http server which serve the content of the <code>web</code> folder.</p>

<p><a href=""https://docs.python.org/3/library/http.server.html"" rel=""noreferrer"">Here</a> is suggested this code to run a simple http server:</p>

<pre><code>import http.server
import socketserver

PORT = 8000
Handler = http.server.SimpleHTTPRequestHandler
httpd = socketserver.TCPServer(("""", PORT), Handler)
print(""serving at port"", PORT)
httpd.serve_forever()
</code></pre>

<p>but this actually serve <code>project</code>, not <code>web</code>. How can I specify the path of the folder I want to serve?</p>
","2914540","","362754","","2019-10-27 18:33:16","2020-06-21 15:56:05","How to run a http server which serves a specific path?","<python><python-3.x><simplehttpserver>","6","2","16","","","CC BY-SA 4.0","0"
"44564414","1","44564729","","2017-06-15 09:54:10","","12","57725","<p>So I am working on a chat-bot for discord, and right now on a feature that would work as a todo-list. I have a command to add tasks to the list, where they are stored in a dict. However my problem is returning the list in a  more readable format (see <a href=""https://imgur.com/a/JGGjF"" rel=""nofollow noreferrer"">pictures</a>).</p>
<pre><code>def show_todo():
    for key, value in cal.items():
        print(value[0], key)
</code></pre>
<p>The tasks are stored in a <code>dict</code> called <code>cal</code>. But in order for the bot to actually send the message I need to use a <code>return</code> statement, otherwise it'll just print it to the console and not to the actual chat (see <a href=""https://imgur.com/a/JGGjF"" rel=""nofollow noreferrer"">pictures</a>).</p>
<pre><code>def show_todo():
    for key, value in cal.items():
        return(value[0], key)
</code></pre>
<p>Here is how I tried to fix it, but since I used <code>return</code> the for-loop does not work properly.</p>
<p>So how do I fix this? How can I use a <code>return</code> statement so that it would print into the chat instead of the console?</p>
","7157625","","4518341","","2020-07-04 04:12:10","2020-07-04 04:29:56","How to use a return statement in a for loop?","<python><python-3.x><function><return>","3","1","10","","","CC BY-SA 4.0","0"
"29384696","1","29384769","","2015-04-01 06:58:11","","33","57540","<p>Please suggest me on the following.
How to find whether a particular day is weekday or weekend in Python?</p>
","4340147","","","","","2019-08-16 20:37:00","how to find current day is weekday or weekends in Python?","<python><python-2.7><python-3.x>","2","0","7","2015-04-01 07:08:17","","CC BY-SA 3.0","0"
"38181710","1","38181986","","2016-07-04 09:52:46","","32","57518","<pre><code>import TkMessageBox
</code></pre>

<p>When I import TkMessageBox it displays the messsge <em>'ImportError: No module named 'TkMessageBox'</em>.</p>

<p>As far as I know im using python 3.3.2 and Tk 8.5.</p>

<p>Am I using the wrong version of python or importing it wrong ?</p>

<p>Any answers would be extremely useful. Alternatively is there something similar in the version i am using?</p>
","5913517","","3329664","","2017-03-28 05:53:10","2020-10-24 20:18:25","TkMessageBox - No Module","<python><python-3.x><tkinter><tk>","5","2","3","","","CC BY-SA 3.0","0"
"38872341","1","38872499","","2016-08-10 11:35:50","","9","57394","<p>I have a list of lists: </p>

<pre><code>a = [[1, 3, 4], [2, 5, 7]]
</code></pre>

<p>I want the output in the following format:</p>

<pre><code>1 3 4
2 5 7
</code></pre>

<p>I have tried it the following way , but the outputs are not in the desired way:</p>

<pre><code>for i in a:
    for j in i:
        print(j, sep=' ')
</code></pre>

<p>Outputs: </p>

<pre><code>1
3
4
2
5
7
</code></pre>

<p>While changing the print call to use <code>end</code> instead:</p>

<pre><code>for i in a:
    for j in i:
        print(j, end = ' ')
</code></pre>

<p>Outputs:</p>

<pre><code>1 3 4 2 5 7
</code></pre>

<p>Any ideas?</p>
","6557700","","4952130","","2017-12-09 13:31:44","2020-07-10 21:35:57","Print list of lists in separate lines","<python><list><python-3.x><printing><nested-lists>","6","0","7","","","CC BY-SA 3.0","0"
"46727787","1","46742583","","2017-10-13 10:27:22","","41","57386","<p>I have a async function and need to run in with apscheduller every N minutes.
There is a python code below</p>

<pre><code>URL_LIST = ['&lt;url1&gt;',
            '&lt;url2&gt;',
            '&lt;url2&gt;',
            ]

def demo_async(urls):
    """"""Fetch list of web pages asynchronously.""""""
    loop = asyncio.get_event_loop() # event loop
    future = asyncio.ensure_future(fetch_all(urls)) # tasks to do
    loop.run_until_complete(future) # loop until done

async def fetch_all(urls):
    tasks = [] # dictionary of start times for each url
    async with ClientSession() as session:
        for url in urls:
            task = asyncio.ensure_future(fetch(url, session))
            tasks.append(task) # create list of tasks
        _ = await asyncio.gather(*tasks) # gather task responses

async def fetch(url, session):
    """"""Fetch a url, using specified ClientSession.""""""
    async with session.get(url) as response:
        resp = await response.read()
        print(resp)

if __name__ == '__main__':
    scheduler = AsyncIOScheduler()
    scheduler.add_job(demo_async, args=[URL_LIST], trigger='interval', seconds=15)
    scheduler.start()
    print('Press Ctrl+{0} to exit'.format('Break' if os.name == 'nt' else 'C'))

    # Execution will block here until Ctrl+C (Ctrl+Break on Windows) is pressed.
    try:
        asyncio.get_event_loop().run_forever()
    except (KeyboardInterrupt, SystemExit):
        pass
</code></pre>

<p>But when i tried to run it i have the next error info</p>

<pre><code>Job ""demo_async (trigger: interval[0:00:15], next run at: 2017-10-12 18:21:12 +04)"" raised an exception.....
..........\lib\asyncio\events.py"", line 584, in get_event_loop
    % threading.current_thread().name)
RuntimeError: There is no current event loop in thread '&lt;concurrent.futures.thread.ThreadPoolExecutor object at 0x0356B150&gt;_0'.
</code></pre>

<p>Could you please help me with this? 
Python 3.6, APScheduler 3.3.1, </p>
","8193990","","","","","2019-07-04 10:19:20","RuntimeError: There is no current event loop in thread in async + apscheduler","<python><python-3.x><python-asyncio><aiohttp><apscheduler>","5","0","15","","","CC BY-SA 3.0","0"
"37042152","1","","","2016-05-05 04:05:51","","31","57153","<p>I have tried </p>

<pre><code>import urllib.request
</code></pre>

<p>or </p>

<pre><code>import urllib
</code></pre>

<p>The path for my urllib is 
<code>/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/urllib/__init__.py</code></p>

<p>I am wondering where is urlopen, or is my python module pointing to the wrong file?</p>
","1999806","","748858","","2016-05-05 04:07:51","2020-08-20 09:36:55","Python 3.5.1 urllib has no attribute request","<python><python-3.x><urllib><urlopen>","6","7","1","","","CC BY-SA 3.0","0"
"44158676","1","44158820","","2017-05-24 12:39:11","","26","56814","<p>How can I delete a virtual environement created with</p>

<pre><code>python3 -m venv &lt;name&gt;
</code></pre>

<p>Can I just remove the directory?    </p>

<p>This seems like a question googling should easily answer, but I only found answers for deleting environments created with <code>virtualenv</code> or <code>pyvenv</code>.</p>
","3881984","","","","","2020-04-20 07:59:25","remove virtual environment created with venv in python3","<python><python-3.x><python-venv>","4","2","2","","","CC BY-SA 3.0","0"
"43743509","1","43743654","","2017-05-02 17:03:13","","35","56778","<p>I just downloaded Python 3.6.1, but when I type <code>python3 -V</code> in the terminal it's still <code>Python 3.5.3</code>. How can I make <code>python3</code> point to <code>Python 3.6</code>? All versions are in the <code>/usr/bin/</code> directory.</p>
","3331268","","67579","","2017-05-02 17:05:56","2020-07-13 19:33:33","How to make python3 command run Python 3.6 instead of 3.5?","<python><linux><python-3.x><ubuntu><environment-variables>","3","2","13","","","CC BY-SA 3.0","0"
"31269437","1","","","2015-07-07 13:08:29","","34","56762","<p>In python 2 I can create a module like this:</p>

<pre><code>parent
-&gt;module
  -&gt;__init__.py (init calls 'from file import ClassName')
    file.py
    -&gt;class ClassName(obj)
</code></pre>

<p>And this works. In python 3 I can do the same thing from the command interpreter and it works (edit: This worked because I was in the same directory running the interpreter). However if I create __ init __.py and do the same thing like this:</p>

<pre><code>""""""__init__.py""""""
from file import ClassName

""""""file.py""""""
class ClassName(object): ...etc etc
</code></pre>

<p>I get ImportError: cannot import name 'ClassName', it doesn't see 'file' at all. It will do this as soon as I import the module even though I can import everything by referencing it directly (which I don't want to do as it's completely inconsistent with the rest of our codebase). What gives?</p>
","","user5076297","1555990","","2015-07-07 13:45:09","2015-07-07 13:45:09","How do I import from a file in the current directory in Python 3?","<python><python-3.x><python-3.4>","2","0","5","","","CC BY-SA 3.0","0"
"45293436","1","45293556","","2017-07-25 03:31:12","","39","56656","<p>My Python virtual environments use <code>python3.6</code> when I create them using <code>virtualenv</code></p>
<blockquote>
<p><code>~ $ virtualenv my_env</code></p>
</blockquote>
<p>but I need to use <code>python3.5</code> as 3.6 is <a href=""https://github.com/menpo/conda-opencv3/issues/21"" rel=""noreferrer"">not currently supported by <code>Opencv3</code>.</a></p>
<p>I've tried using the <code>--python=&lt;py_version&gt;</code> flag when creating a virtual environment but this doesn't work.</p>
<h1>How do I specify the python (3.x) version to install using <code>virtualenv</code> for Mac and/or Linux?</h1>
","4486146","","-1","","2020-06-20 09:12:55","2019-10-22 23:20:10","How to specify python version used to create Virtual Environment?","<python><python-3.x><virtualenv>","4","5","8","","","CC BY-SA 3.0","0"
"51373063","1","51373253","","2018-07-17 04:02:12","","41","56627","<p>I am trying to install dependencies using <code>pip3</code> command</p>

<p>current scenario:</p>

<pre><code>Dev$ which python
/Users/Dev/anaconda/bin/python

Dev$ which python3
/usr/local/bin/python3


Dev$ pip --version
pip 10.0.1 from /usr/local/lib/python2.7/site-packages/pip (python 2.7)

Dev$ pip3 --version
-bash: /usr/local/bin/pip3: /usr/local/opt/python3/bin/python3.6: bad 
interpreter: No such file or directory
</code></pre>

<p>I have no idea why my <code>pip3</code> command is not working. </p>

<p>I have tried things like this:</p>

<pre><code>brew link --overwrite python 
</code></pre>
","9161607","","3805131","","2019-02-01 15:51:57","2019-12-13 02:41:39","pip3: bad interpreter: No such file or directory","<python><python-3.x><macos><pip>","4","0","24","","","CC BY-SA 4.0","0"
"27786868","1","","","2015-01-05 19:55:42","","42","56328","<p>I am trying to append data to a file using numpy's savetxt function. Below is the minimum working example</p>

<pre><code>#!/usr/bin/env python3
import numpy as np
f=open('asd.dat','a')
for iind in range(4):
    a=np.random.rand(10,10)
    np.savetxt(f,a)
f.close()
</code></pre>

<p>The error that I got is something about the type of the error</p>

<blockquote>
  <p>File ""/usr/lib/python3/dist-packages/numpy/lib/npyio.py"", line 1073,
  in savetxt
      fh.write(asbytes(format % tuple(row) + newline)) TypeError: must be str, not bytes</p>
</blockquote>

<p>This error doesn't occur in python2 so I am wondering what the issue could be. Can anyone help me out?</p>
","4421404","","","","","2020-09-21 19:31:09","python3-numpy: Appending to a file using numpy savetxt","<python-3.x><numpy><save><append>","1","3","5","","","CC BY-SA 3.0","0"
"34900042","1","34900138","","2016-01-20 12:25:58","","32","56213","<p>I am beginning to look at python, so when I found a tutorial it said that the first thing to do would be to download python from www.python.org/downloads/</p>

<p>Now when I downloaded python 3, I then started the installation and got to</p>

<p><a href=""https://i.stack.imgur.com/rkjt6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/rkjt6.png"" alt=""enter image description here""></a> </p>

<p>Why would I want to ""Add Python 3.5 to PATH""? What is PATH? Why is it not ticked by default?</p>
","5257342","","2067976","","2016-01-20 14:22:43","2016-01-20 14:22:43","Why would I add python to PATH","<python><python-3.x><path><installation>","2","10","13","","","CC BY-SA 3.0","0"
"44461551","1","44461577","","2017-06-09 15:26:07","","27","56155","<p>This is my code</p>

<pre><code>def fahrenheit(T):
    return ((float(9)/5)*T + 32)

temp = [0, 22.5, 40,100]
F_temps = map(fahrenheit, temp)
</code></pre>

<p>This is mapobject so I tried something like this</p>

<pre><code>for i in F_temps:
    print(F_temps)

&lt;map object at 0x7f9aa050ff28&gt;
&lt;map object at 0x7f9aa050ff28&gt;
&lt;map object at 0x7f9aa050ff28&gt;
&lt;map object at 0x7f9aa050ff28&gt;
</code></pre>

<p>I am not sure but I think that my solution was possible with Python 2.7,how to change this with 3.5?</p>
","8006605","","7879421","","2017-06-09 15:30:38","2019-08-20 02:34:42","How to print map object with Python 3?","<python><python-3.x>","1","3","6","2017-06-09 15:30:21","","CC BY-SA 3.0","0"
"45643650","1","","","2017-08-11 20:59:00","","16","56002","<p>I have such a problem</p>

<pre><code>(face_det) user@pc:~$ python3
Python 3.5.3 (default, Apr 22 2017, 00:00:00) 
[GCC 4.8.4] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import cv2
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named 'cv2
</code></pre>

<p>I don't have it on python2:</p>

<pre><code>(face_det) user@pc:~$ python2
Python 2.7.13 |Anaconda custom (64-bit)| (default, Dec 20 2016, 23:09:15) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org
&gt;&gt;&gt; import cv2
&gt;&gt;&gt; 
</code></pre>

<p>In spite of  the fact, that I have opencv (I've also tryed to remove it and install then):</p>

<pre><code>(face_det) user@pc:~$ pip3 install opencv
Requirement already satisfied: opencv in ./.virtualenvs/face_det/lib/python3.5/site-packages
(face_det) user@pc:~$ conda install opencv
Fetching package metadata .........
Solving package specifications: .

# All requested packages already installed.
# packages in environment at /home/pc/anaconda3:
#
opencv                    3.2.0               np112py27_0    conda-forge
</code></pre>
","8452795","","8452795","","2017-08-16 11:58:26","2020-05-20 16:59:59","ImportError: No module named 'cv2' Python3","<python><python-3.x><opencv><python-import><cv2>","6","5","4","","","CC BY-SA 3.0","0"
"33406313","1","33406382","","2015-10-29 05:06:51","","22","55894","<p>Lets say I have a list of strings,</p>

<pre><code>string_lst = ['fun', 'dum', 'sun', 'gum']
</code></pre>

<p>I want to make a regular expression, where at a point in it, I can match any of the strings i have in that list, within a group, such as this:</p>

<pre><code>import re
template = re.compile(r"".*(elem for elem in string_lst).*"")
template.match(""I love to have fun."")
</code></pre>

<p>What would be the correct way to do this? Or would one have to make multiple regular expressions and match them all separately to the string? </p>
","5129504","","","","","2020-04-22 13:22:32","How to match any string from a list of strings in regular expressions in python?","<python><regex><string><python-3.x>","5","5","7","","","CC BY-SA 3.0","0"
"38938205","1","","","2016-08-14 00:42:32","","45","55874","<p>I am using OSX and I have pip installed for both Python3.5 and Python2.7. I know I can run the command <code>pip2</code> to use Python2 and when I use the command <code>pip3</code> Python3.x will be used. 
The problem is that the default of <code>pip</code> is set to Python2.7 and I want it to be Python3.x.</p>

<p>How can I change that? </p>

<p>edit:
No, I am not running a virtual environment yet. If it was a virtual environment I could just run Python3.x and forget all about Python2.7, unfortunately since OSX requires Python2.7 for it's use I can't do that. Hence why I'm asking this.</p>

<p>Thanks for the answer. I however don't want to change what running <code>python</code> does. Instead I would like to change the path that running <code>pip</code> takes. At the moment <code>pip -V</code> shows me <code>pip 8.1.2 from /Library/Python/2.7/site-packages (python 2.7)</code>, but I am looking for <code>pip 8.1.2 from /Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages (python 3.5)</code> I am sure there has to be a way to do this. Any ideas?</p>
","6713759","","6713759","","2016-08-14 01:04:39","2020-08-22 12:53:06","How to override the pip command to Python3.x instead of Python2.7?","<python><python-3.x><pip>","7","6","10","","","CC BY-SA 3.0","0"
"36924703","1","36925518","","2016-04-28 20:18:32","","18","55811","<p>I want to install kivy (<a href=""https://kivy.org/docs/installation/installation-windows.html#install-win-dist"" rel=""noreferrer"">link here</a>) to use for a project, however, when trying to use pip to install the packages it is dependent upon, I get the following error:</p>

<pre><code>Could not find a version that satisfies the requirement kivy.deps.sdl2 (from versions: )
No matching distribution found for kivy.deps.sdl2
</code></pre>

<p>Further reading on kivy's website revealed that these libraries do not support Python 3.5 on windows 10 because of some kind of graphical bug. The website says you need python 3.4 in order to be able to install it.</p>

<p>Which brings the question: How do I install python 3.4 when I already have python 3.5?</p>
","6249151","","","","","2016-08-31 13:44:05","How to downgrade to Python 3.4 from 3.5","<python><python-3.x><kivy><downgrade>","3","3","2","","","CC BY-SA 3.0","0"
"28121499","1","","","2015-01-24 01:58:31","","1","55702","<p>I got an assignment in my Python 1 class consisting of this:</p>

<p>////////////////////////////////////////////////////////////////////////////////////////////////</p>

<p>You are a score keeper for 20 ice hockey players. Implement the code to keep track of their scores using a list. Program accepts integers as input and it is called YI_ScoreKeeper.py. Simulate a game by entering a good number of scores.</p>

<p>Here are two pics. she gave(I don't have enough rep. to post them as images)</p>

<p><img src=""https://i.stack.imgur.com/TpYtS.png"" alt=""https://e.edim.co/29892634/screen_shot_2015_01_09_at_9_30_52_am_t.png?Expires=1422066625&amp;Signature=Ql0P778epTnFNUD~4AiZtwr5Gip~JTgohs8ShfVD5Yzvghot0hTATBAbktvD6whm~WG8a9gSJ98fihD81NLyPf7E615hMKaYOBIxLJZp4M1l1EBAYHGZZeDY6xblYlb-PelzwDo8USbcCuq8OAIioaiMrWeQ2WV5X4YUmqwZHbgqPGYvXP~nhupH7qoTdLUagdleySQ8S8BhG6at0YeHwd5pgwMh-Lq3hJ97lfmsrhYeWhG~yr6t3WpzZmgWPVg1WRo1lbPNC5Y91952iDub-20aZuK2sDngcTSf7BnBfn6laIeN~Ib3uhX~KJe9tcWs0EY~CwiDl~-rXIB73f5uIQ__&amp;Key-Pair-Id=APKAIJNVNRBLLSTGN23Q""></p>

<p><img src=""https://i.stack.imgur.com/m5TFi.png"" alt=""https://e.edim.co/29892634/screen_shot_2015_01_09_at_9_31_36_am_t.png?Expires=1422066625&amp;Signature=Bgf-JQNNMnkHT3Taocc-rxqo4F2BJLAGdcl-qZpJHcFWBov0rvTvktQxBklJGIXk~Y7or1KFJQIvLWw2Fsr002XtB0N7qqQZLl3FRc8nmEvE~sIt0atsZmj4V8Fq9OSkO6UjMgHroIBtl2NlhRJ2DoSWoDyoMT13ODN7AUuyClwwFB5PlJW2TtUeF5mOUyN0eOXzYV-jk4oEyptnK2RYwbSo-b-mY677mkK66iBiPCkcSQPciJs3VOxfopNhshWYb01CbYcDI0inJ2FqD3t8WLQfzmzY8HRy8A2aFgrJIM1OsAh9xKcb49zRSsfUP9W0lxmh007wxC8dLbu06Y1XVw__&amp;Key-Pair-Id=APKAIJNVNRBLLSTGN23Q""></p>

<p>/////////////////////////////////////////////////////////////////////////////////////////////////////////////</p>

<p>So far, my code is this:   </p>

<pre><code>def scorekeeper():
    Scorekeeper = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    return Scorekeeper

def addscore(Scorekeeper):
    Addscore = int(input(""what player scored a goal?""))
    Addscore = Addscore - 1 
    (Scorekeeper[Addscore]) = ((Scorekeeper[Addscore]) + 1) 
    return Scorekeeper
def histogram(Scorekeeper):
    print(""\nCreating a histogram from values: "")
    print(""%s %10s %10s"" %(""Element"", ""Ranking"", ""Histogram""))
    for i in range(len(Scorekeeper)):
        print(""%7d%5d %-s"" % (i +1, Scorekeeper[i], ""*"" * Scorekeeper[i])) 
def main():
    Scorekeeper = scorekeeper()
    endgame = 'n'
    while endgame == 'n':
        Addscore = addscore(scorekeeper)
        endgame = input(""Has the game ended? y/n"")

    histogram(scorekeeper)

main()
</code></pre>

<p>/////////////////////////////////////////////////////////////////////////////////////////
I keep on getting this error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Python34/scorekeeper.py"", line 27, in &lt;module&gt;
    main()
  File ""C:/Python34/scorekeeper.py"", line 22, in main
    Addscore = addscore(scorekeeper)
  File ""C:/Python34/scorekeeper.py"", line 11, in addscore
    (Scorekeeper[Addscore]) = ((Scorekeeper[Addscore]) + 1)
TypeError: 'function' object is not subscriptable
</code></pre>

<p>////////////////////////////////////////////////////////////////////////</p>

<p>Help? I'm not sure what I'm doing wrong.</p>
","4321901","","2141635","","2015-01-24 02:12:35","2015-01-24 08:11:01","TypeError: 'function' object is not subscriptable Python","<python><python-3.x><typeerror>","2","0","0","","","CC BY-SA 3.0","0"
"38066836","1","38066869","","2016-06-28 04:02:02","","16","55695","<p>I'm removing an char from string like this:</p>

<pre><code>S = ""abcd""
Index=1 #index of string to remove
ListS = list(S)
ListS.pop(Index)
S = """".join(ListS)
print S
#""acd""
</code></pre>

<p>I'm sure that this is <strong>not</strong> the best way to do it. </p>

<p><strong>EDIT</strong>
I didn't mentioned that I need to manipulate a string size with length ~ 10^7. 
So it's important to care about efficiency. </p>

<blockquote>
  <p>Can someone help me.  Which pythonic way to do it?</p>
</blockquote>
","4803173","","4803173","","2016-06-28 04:57:18","2020-06-10 20:19:22","Python best way to remove char from string by index","<python><string><python-2.7><python-3.x><substring>","5","5","4","","","CC BY-SA 3.0","0"
"32071536","1","","","2015-08-18 11:46:13","","35","55662","<pre><code>for line in fo:
    line = "" "".join(line.split())
    line = line.strip()
</code></pre>

<p>I am getting an error  </p>

<pre><code>line = ''.join(line.split())
TypeError: sequence item 0: expected str instance, bytes found
</code></pre>

<p>its working fine in python 2.x, but not working on 3.4
kindly suggest a proper solution for that</p>
","5107969","","2867928","","2017-05-08 05:37:07","2019-12-03 19:08:36","TypeError: sequence item 0: expected str instance, bytes found","<python><string><python-3.x><byte><typeerror>","3","2","5","","","CC BY-SA 3.0","0"
"36342899","1","36415477","","2016-03-31 20:15:20","","100","55445","<p>I've seen several basic Python 3.5 tutorials on asyncio doing the same operation in various flavours.
In this code:</p>

<pre><code>import asyncio  

async def doit(i):
    print(""Start %d"" % i)
    await asyncio.sleep(3)
    print(""End %d"" % i)
    return i

if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    #futures = [asyncio.ensure_future(doit(i), loop=loop) for i in range(10)]
    #futures = [loop.create_task(doit(i)) for i in range(10)]
    futures = [doit(i) for i in range(10)]
    result = loop.run_until_complete(asyncio.gather(*futures))
    print(result)
</code></pre>

<p>All the three variants above that define the <code>futures</code> variable achieve the same result; the only difference I can see is  that with the third variant the execution is out of order (which should not matter in most cases). Is there any other difference? Are there cases where I can't just use the simplest variant (plain list of coroutines)?</p>
","2660810","","","","","2019-01-02 05:59:45","asyncio.ensure_future vs. BaseEventLoop.create_task vs. simple coroutine?","<python><python-3.x><python-3.5><coroutine><python-asyncio>","4","0","52","","","CC BY-SA 3.0","0"
"43019951","1","","","2017-03-25 17:59:23","","41","55374","<p>I have first installed openCV from source using this <a href=""http://milq.github.io/install-opencv-ubuntu-debian"" rel=""noreferrer"">script</a>.
When I tested it was working well.</p>

<p>After I installed <a href=""http://wiki.ros.org/kinetic/Installation/Ubuntu"" rel=""noreferrer"">ROS kinetic</a>, and open <code>python3</code> and run <code>import cv2</code>, got the following error:</p>

<pre><code>Python 3.5.2 (default, Nov 17 2016, 17:05:23) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import cv2
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: /opt/ros/kinetic/lib/python2.7/dist-packages/cv2.so: undefined symbol: PyCObject_Type
</code></pre>
","1527457","","9315332","","2018-03-30 07:31:44","2020-01-05 10:59:53","After install ROS Kinetic, cannot import OpenCV","<python-3.x><opencv><ros>","11","1","15","","","CC BY-SA 3.0","0"
"28173520","1","28173596","","2015-01-27 15:02:06","","8","55338","<p>I have done through the other questions online here, and I feel that mine is different enough to warrant a new question.</p>

<p>So I have a <code>Centos 6 box</code>, which is running a small website for me, acts as an office git server and I am trying to configure <code>Python3</code> on it.</p>

<p>So I followed the following <a href=""https://www.digitalocean.com/community/tutorials/how-to-set-up-python-2-7-6-and-3-3-3-on-centos-6-4"" rel=""noreferrer"">these steps</a> to set up <code>python3</code> on the server. However it seems that I cannot import paramiko into my script.</p>

<p>I downloaded the paramiko rpm however I get this message:</p>

<p>When I try to import paramiko I get:</p>

<pre><code>[root@GIT Python-3.4.2]# rpm -ivh /usr/lib/Python-3.4.2/Modules/python-paramiko-1.7.5-2.1.el6.noarch.rpm
Preparing...                ########################################### [100%]
package python-paramiko-1.7.5-2.1.el6.noarch is already installed
</code></pre>

<p>When I run python3 directly:</p>

<pre><code>[root@GIT inserv_health_check]# python3
Python 3.4.2 (default, Jan 21 2015, 06:28:04)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-11)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import paramiko
Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named 'paramiko'
&gt;&gt;&gt;
</code></pre>

<p>I am sure there is a simple solution to this problem perhaps the path is wrong, or I should have put a symbolic link in somewhere.  Any help would be appreciated :)</p>

<p>Before anyone asks, which python output:</p>

<pre><code>[root@GIT Python-3.4.2]# which python
/usr/bin/python
[root@GIT Python-3.4.2]# which pytho~n3
/usr/bin/which: no pytho~n3 in (/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin)
[root@GIT Python-3.4.2]# which python3
/usr/local/bin/python3
</code></pre>

<p>Thanks</p>
","4442013","","5131937","","2016-07-26 13:09:47","2020-07-08 04:00:07","ImportError: No module named 'paramiko'","<python><python-3.x><paramiko>","2","0","1","","","CC BY-SA 3.0","0"
"28908142","1","","","2015-03-06 21:40:24","","23","55232","<p>I was doing a fresh installation for <code>Python 2.7.9</code> and <code>3.4.3</code> on <code>Win7 X64</code> today, and I found that there is no <code>Script</code> folder in <code>Python27</code> and <code>Python34</code> folder as first child level folder, but there is one in <code>Tools</code>. However, I couldn't find <code>pip</code> within that Script folder, although <code>pip</code> should be installed with <code>Python</code> by default. The other I was doing the same installation for my other PC and laptop, there was <code>Script</code> folder (as first level child folder in <code>Python27</code> and <code>Python34</code>) containing <code>pip</code>. So what is going on? how to install <code>pip</code> and maybe other useful scripts this way?</p>

<p>[EDIT] I tried <code>python -m ensurepip</code> in <code>Python34</code>. I got the following errors:</p>

<pre><code>Ignoring indexes: https://pypi.python.org/simple
Collecting setuptools
 Exception:
 Traceback (most recent call last):
File ""C:\Users\daiyue\AppData\Local\Temp\tmppvmc8dv1\pip-6.0.8-py2.py3-none-
any.whl\pip\basecommand.py"", line 232, in main
  status = self.run(options, args)
File ""C:\Users\daiyue\AppData\Local\Temp\tmppvmc8dv1\pip-6.0.8-py2.py3-none-
any.whl\pip\commands\install.py"", line 339, in run
  requirement_set.prepare_files(finder)
File ""C:\Users\daiyue\AppData\Local\Temp\tmppvmc8dv1\pip-6.0.8-py2.py3-none-
any.whl\pip\req\req_set.py"", line 333, in prepare_files
  upgrade=self.upgrade,
File ""C:\Users\daiyue\AppData\Local\Temp\tmppvmc8dv1\pip-6.0.8-py2.py3-none-
any.whl\pip\index.py"", line 326, in find_requirement
  file_locations, url_locations = self._sort_locations(locations)
File ""C:\Users\daiyue\AppData\Local\Temp\tmppvmc8dv1\pip-6.0.8-py2.py3-none-
any.whl\pip\index.py"", line 158, in _sort_locations
  sort_path(os.path.join(path, item))
File ""C:\Users\daiyue\AppData\Local\Temp\tmppvmc8dv1\pip-6.0.8-py2.py3-none-
any.whl\pip\index.py"", line 139, in sort_path
  if mimetypes.guess_type(url, strict=False)[0] == 'text/html':
File ""C:\Python34\lib\mimetypes.py"", line 287, in guess_type
  init()
File ""C:\Python34\lib\mimetypes.py"", line 348, in init
  db.read_windows_registry()
File ""C:\Python34\lib\mimetypes.py"", line 255, in read_windows_registry
  with _winreg.OpenKey(hkcr, subkeyname) as subkey:
TypeError: OpenKey() argument 2 must be str without null characters or None,  not str
</code></pre>

<p>[EDIT] The problem is due to multiple null Registry keys in <code>HKEY_LOCAL_MACHINE</code> that make <code>read_windows_registry()</code> in <code>mimetypes.py</code> searches failed. Here is the post that leads to the solution:</p>

<p><a href=""http://www.swarley.me.uk/blog/2014/04/23/python-pip-and-windows-registry-corruption/"" rel=""noreferrer"">Python ‘pip’ and Windows registry corruption</a> </p>

<p>thanks </p>
","766708","","766708","","2015-03-09 11:43:09","2020-09-09 03:22:36","Python 3.4 and 2.7 installation no Script folder and no pip installed","<python><python-2.7><python-3.x><pip>","17","9","5","","","CC BY-SA 3.0","0"
"39577984","1","40167445","","2016-09-19 16:36:39","","170","54253","<p>When I run <code>pip freeze</code> I see (among other expected packages) <code>pkg-resources==0.0.0</code>. I have seen a few posts mentioning this package (including <a href=""https://stackoverflow.com/questions/38992194/why-does-pip-freeze-list-pkg-resources-0-0-0"">this one</a>), but none explaining what it is, or why it is included in the output of <code>pip freeze</code>. The main reason I am wondering is out of curiosity, but also, it seems to break things in some cases when trying to install packages with a <code>requirements.txt</code> file generated with <code>pip freeze</code> that includes the <code>pkg-resources==0.0.0</code> line (for example when <a href=""https://travis-ci.org/"" rel=""noreferrer"">Travis CI</a> tries to install dependencies through <code>pip</code> and finds this line).</p>
<p><strong>What is <code>pkg-resources</code>, and is it OK to remove this line from <code>requirements.txt</code>?</strong></p>
<h1>Update:</h1>
<p>I have found that this line only seems to exist in the output of <code>pip freeze</code> when I am in a <code>virtualenv</code>. I am still not sure what it is or what it does, but I will investigate further knowing that it is likely related to <code>virtualenv</code>.</p>
","3642398","","-1","","2020-06-20 09:12:55","2020-05-14 01:12:52","What is ""pkg-resources==0.0.0"" in output of pip freeze command","<python><python-3.x><pip><ubuntu-16.04>","3","10","25","","","CC BY-SA 3.0","0"
"33656966","1","33657052","","2015-11-11 18:06:24","","13","54097","<p>So I'm trying to program a good ol' game of war and I'm getting an error when I try to move an object (a card) from one list (hand) to another. There have been some other posts about this but I wasn't able to piece together what to do...
Here's the code:</p>

<pre><code>import random
cardvalues = {""ace"" : 13 , ""2"" : 2 , ""3"" : 3 , ""4"" : 4 , ""5"" : 5 , ""6"" : 6, ""7"" : 7 , ""8"" : 8 , ""9"" : 9, ""10"" : 10 , ""jack"" : 11 , ""queen"" : 12 , ""king"" : 13}
suits = {""clubs"" : 1, ""diamonds"" : 2 , ""spades"" : 3 , ""hearts"" : 4}
deck = []
currentDeck = []

class card:
    def __init__ (self, value, suit):
        if value not in cardvalues:
            raise RuntimeError(""must input valid card value"")...
</code></pre>

<p>### [code here was erased because it's not causing the problem]</p>

<pre><code>def battle():            
    if hand1[0] &gt;= hand2[0]:
        hand1.append[hand2.pop(0)] #The error happens in this function in the .append
        print(""player 1 won the battle"")
    elif hand1[0] &lt; hand2[0]:
        hand2.append[hand1.pop(0)]
        print(""player 2 won the battle"")
    else:
        raise RuntimeError(""something wrong"")


while len(hand1) &gt; 0:
    while len(hand2) &gt; 0:
        battle()
if len(hand1) == 0:
    print(""PLAYER 2 WON THE WAR"")
elif len(hand2) == 0:
    print(""PLAYER 1 WON THE WAR"")
else:
    print(""no one won?"")
</code></pre>

<p>Thanks!</p>

<p>Not only do I need help finding my error, but what does it mean? </p>
","5474086","","2826421","","2018-05-04 05:55:04","2018-05-04 05:55:04","What does 'builtin_function_or_method' object is not subscriptable error' mean?","<python-3.x>","1","1","2","2015-11-11 18:11:46","","CC BY-SA 4.0","0"
"37791744","1","37792143","","2016-06-13 14:01:56","","5","53971","<p>I would like to run a command line tool to run in a separate function and passed to the button click the additional command for this program but each time I get this as a response.</p>

<p><strong>takes 1 positional argument but 2 were given</strong></p>



<pre><code>from tkinter import *
import subprocess


class StdoutRedirector(object):
    def __init__(self,text_widget):
        self.text_space = text_widget

    def write(self,string):
        self.text_space.insert('end', string)
        self.text_space.see('end')

class CoreGUI(object):
    def __init__(self,parent):
        self.parent = parent
        self.InitUI()

        button = Button(self.parent, text=""Check Device"", command= self.adb(""devices""))
        button.grid(column=0, row=0, columnspan=1)

    def InitUI(self):
        self.text_box = Text(self.parent, wrap='word', height = 6, width=50)
        self.text_box.grid(column=0, row=10, columnspan = 2, sticky='NSWE', padx=5, pady=5)
        sys.stdout = StdoutRedirector(self.text_box)

    def adb(self, **args):
        process = subprocess.Popen(['adb.exe', args], stdout=subprocess.PIPE, shell=True)
        print(process.communicate())
        #return x.communicate(stdout)


root = Tk()
gui = CoreGUI(root)
root.mainloop()
</code></pre>

<p>
the error
</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/Maik/PycharmProjects/Lernen/subprocessExtra.py"", line 33, in &lt;module&gt;
    gui = CoreGUI(root)
  File ""C:/Users/Maik/PycharmProjects/Lernen/subprocessExtra.py"", line 18, in __init__
    button = Button(self.parent, text=""Check Device"", command= self.adb(""devices""))
TypeError: adb() takes 1 positional argument but 2 were given
Exception ignored in: &lt;__main__.StdoutRedirector object at 0x013531B0&gt;
AttributeError: 'StdoutRedirector' object has no attribute 'flush'

Process finished with exit code 1
</code></pre>



<p>can some body help me </p>

<p>there is something wrong with **args</p>
","6460034","","2986905","","2017-09-27 14:37:35","2017-09-27 14:37:35","takes 1 positional argument but 2 were given","<python><python-3.x><tkinter><subprocess><popen>","2","5","1","","","CC BY-SA 3.0","0"
"34517540","1","34517664","","2015-12-29 19:14:13","","24","53864","<p>I have a list of numbers, e.g.</p>

<pre><code>numbers = [1, 2, 3, 7, 7, 9, 10]
</code></pre>

<p>As you can see, numbers may appear more than once in this list.</p>

<p>I need to get all combinations of these numbers that have a given sum, e.g. <code>10</code>.  </p>

<p>The items in the combinations may not be repeated, but each item in <code>numbers</code> has to be treated uniquely, that means e.g. the two <code>7</code> in the list represent different items with the same value.</p>

<p>The order is unimportant, so that <code>[1, 9]</code> and <code>[9, 1]</code> are the same combination.</p>

<p>There are no length restrictions for the combinations, <code>[10]</code> is as valid as <code>[1, 2, 7]</code>.</p>

<p><strong>How can I create a list of all combinations meeting the criteria above?</strong></p>

<p>In this example, it would be <code>[[1,2,7], [1,2,7], [1,9], [3,7], [3,7], [10]]</code> </p>
","4464570","","2997179","","2015-12-29 19:52:43","2020-06-01 13:08:43","Find all combinations of a list of numbers with a given sum","<python><algorithm><python-3.x><combinations><subset-sum>","5","0","13","","","CC BY-SA 3.0","0"
"36300158","1","36300197","","2016-03-30 05:08:14","","33","53816","<p>I need to split text before the second occurrence of the '-' character.  What I have now is producing inconsistent results.  I've tried various combinations of <code>rsplit</code> and read through and tried other solutions on SO, with no results.</p>

<p>Sample file name to split: <code>'some-sample-filename-to-split'</code> returned in <code>data.filename</code>. In this case, I would only like to have <code>'some-sample'</code> returned.  </p>

<pre><code>fname, extname = os.path.splitext(data.filename)
file_label = fname.rsplit('/',1)[-1]
file_label2 = file_label.rsplit('-',maxsplit=3)
print(file_label2,'\n','---------------','\n')
</code></pre>
","3672870","","7851470","","2019-12-11 09:17:55","2019-12-11 09:17:55","Split text after the second occurrence of character","<python><python-3.x><string><split>","4","0","9","","","CC BY-SA 4.0","0"
"58451650","1","58538514","","2019-10-18 13:13:03","","75","53686","<p>After a pip update, pip has stopped working completely.</p>

<pre><code>Z:\&gt;pip install matplotlib
Traceback (most recent call last):
  File ""c:\program files\python37\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\program files\python37\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Program Files\Python37\Scripts\pip.exe\__main__.py"", line 9, in &lt;module&gt;
TypeError: 'module' object is not callable
</code></pre>

<p>Any help please?</p>

<p>Edit: I am working on windows 10</p>
","9988108","","9988108","","2019-10-18 13:16:18","2020-01-18 22:45:31","pip no longer working after update error 'module' object is not callable","<python><python-3.x><pip><python-3.7>","9","9","23","","","CC BY-SA 4.0","0"
"40705480","1","40705575","","2016-11-20 14:55:00","","25","53440","<p>I have data frames which contain e.g.:</p>
<pre><code>&quot;vendor a::ProductA&quot;
&quot;vendor b::ProductA&quot;
&quot;vendor a::Productb&quot;
</code></pre>
<p>I need to remove everything (and including) the two :: so that I end up with:</p>
<pre><code>&quot;vendor a&quot;
&quot;vendor b&quot;
&quot;vendor a&quot;
</code></pre>
<p>I tried str.trim (which seems to not exist) and str.split without success.
what would be the easiest way to accomplish this?</p>
","4370943","","7579547","","2020-09-22 02:21:24","2020-09-22 02:21:24","Python pandas: remove everything after a delimiter in a string","<python><python-3.x><pandas>","4","1","14","","","CC BY-SA 4.0","1"
"41946222","1","","","2017-01-30 22:04:45","","8","53377","<p>I have a problem creating a popup window for a program.</p>

<p>Code:</p>

<pre><code>from tkinter import *
from tkinter import ttk
import tkinter as tk

def popupBonus():
    popupBonusWindow = tk.Tk()
    popupBonusWindow.wm_title(""Window"")
    labelBonus = Label(popupBonusWindow, text=""Input"")
    labelBonus.grid(row=0, column=0)
    B1 = ttk.Button(popupBonusWindow, text=""Okay"", command=popupBonusWindow.destroy())
    B1.pack()

class Application(ttk.Frame):
    def __init__(self, master):
        ttk.Frame.__init__(self, master)
        mainwindow = ttk.Frame(self)

        self.buttonBonus = ttk.Button(self, text=""Bonuses"", command=popupBonus)
        self.buttonBonus.pack()
</code></pre>

<p>The code generates a window with a button and when you press the button, it's supposed to generate a popup window with title ""Window"", text ""Input"", and have a button saying ""Okay"" to exit popup window and return to main window. However, I am getting this error.</p>

<pre><code> Traceback (most recent call last):
  File ""D:\Softwares\Python 3.6.0\lib\tkinter\__init__.py"", line 1699, in __call__
return self.func(*args)
  File ""C:\Users\J---- M--\Desktop\Python\GUI-Messagebox 5.py"", line 12, in popupBonus
B1 = ttk.Button(popupBonusWindow, text=""Okay"", command=popupBonusWindow.destroy())
  File ""D:\Softwares\Python 3.6.0\lib\tkinter\ttk.py"", line 614, in __init__
Widget.__init__(self, master, ""ttk::button"", kw)
  File ""D:\Softwares\Python 3.6.0\lib\tkinter\ttk.py"", line 559, in __init__
tkinter.Widget.__init__(self, master, widgetname, kw=kw)
  File ""D:\Softwares\Python 3.6.0\lib\tkinter\__init__.py"", line 2293, in __init__
(widgetName, self._w) + extra + self._options(cnf))
_tkinter.TclError: NULL main window
</code></pre>

<p>I have no idea what the problem is. I have trying to find answer for 4 hours and basically gave up.</p>

<p>Also, I don't want to use tkinter's messagebox feature because I don't want the exclamation mark image and I want to include multiple checkboxs inside the popup window later on.</p>
","5622697","","5622697","","2017-01-30 22:13:05","2020-03-05 11:47:38","How do I create a popup window in tkinter?","<python-3.x><tkinter><popup><popupwindow>","1","2","3","","","CC BY-SA 3.0","0"
"51194303","1","56095408","","2018-07-05 14:49:34","","22","53081","<pre><code>%%javascript
IPython.OutputArea.prototype._should_scroll = function(lines) {
    return false;
}

%run rl_base.py
</code></pre>

<p>I run this giving error saying rl_base.py file not found. I have uploaded the same to gdrive in colab and from the same folder I am running my .ipynb file, containing the above code</p>
","2458922","","2458922","","2019-11-13 14:03:20","2020-09-14 14:04:08","How to run a Python script in a '.py' file from a Google Colab notebook?","<python><python-3.x><jupyter-notebook><google-colaboratory>","6","1","8","","","CC BY-SA 4.0","0"
"42899389","1","42899473","","2017-03-20 09:01:23","","13","53071","<p>Sorry guys, I couldn't find the satisfying answer to print part of json response. Can someone help me here please:</p>

<pre><code>import json
import requests
import pprint 

response = requests.get('&lt;api endpoing&gt;')
json_data = response.json()
print(json.dumps(json_data, indent=4, sort_keys=True))
</code></pre>

<p>Json response would be </p>

<pre><code>{
    ""Value1"": ""SomeValue"",
    ""data"": {
        ""subval1"": false,
        ""subval2"": ""0a4"",
        ""subval3"": """",
        ""subval4"": ""Click h!"",
        ""subval5"": ""1002"",
        ""subval6"": ""932"",
        ""subval7"": ""i2"",
        ""subval8"": 250,
        ""subval9"": 0,
        ""subval10"": 1,
        ""subval11"": 3,
        ""subval12"": 1,
        ""subval13"": ""&lt;!&gt;"",
        ""subval14"": """",
        ""subval15"": ""Click !!"",
        ""subval16"": """",
        ""subval17"": 300
    },
    ""error"": true,
    ""message"": ""Success"",
    ""status"": 200
}
</code></pre>

<p>Now, I would like to traverse and print only the ""data"": values. I will do the following </p>

<pre><code>data = json.loads(json_data)
data_set = (data['data'])
print(data_set)
</code></pre>

<p>But the error Im getting: TypeError: the JSON object must be str, not 'dict'</p>
","6517451","","","","","2017-03-20 09:13:43","TypeError: the JSON object must be str, not 'dict'","<python><json><python-3.x>","2","1","","","","CC BY-SA 3.0","0"
"50533812","1","","","2018-05-25 16:56:36","","15","53031","<p>I am writing a program in python which contains many constant variables. I would like to create a file which will hold all these variables like .h file in C that contains many #define. I tried to use <a href=""https://docs.python.org/3/library/configparser.html"" rel=""noreferrer"">configparser</a> however I didn't find it easy and fun to use. </p>

<p>Do you know a better way? </p>
","5881955","","223424","","2018-05-25 17:34:43","2020-02-17 14:47:31","what is the best way to define constant variables python 3","<python><python-3.x><constants><configuration-files>","5","2","1","","","CC BY-SA 4.0","0"
"32667398","1","32669303","","2015-09-19 11:00:49","","43","52947","<p>I am using Python 3.4 and need to extract all the text from a PDF and then use it for text processing.</p>

<p>All the answers I have seen suggest options for Python 2.7.</p>

<p>I need something in Python 3.4.</p>

<p>Bonson</p>
","1181744","","","","","2020-06-24 09:10:46","Best tool for text extraction from PDF in Python 3.4","<python-3.x><pdf>","5","3","16","2020-06-24 14:07:05","","CC BY-SA 3.0","0"
"47761758","1","47777052","","2017-12-11 21:32:48","","12","52937","<p>hello everyone I am working on PySpark Python and I have mentioned the code and getting some issue, I am wondering if someone knows about the following issue?</p>

<pre><code>windowSpec = Window.partitionBy(df_Broadcast['id']).orderBy(df_Broadcast['id'])
windowSpec

IdShift = lag(df_Broadcast[""id""]).over(windowSpec).alias('IdShift')

df_Broadcast = df_Broadcast.withColumn('CheckId', df_Broadcast[idI'] != IdShift)

df_Broadcast.show()
</code></pre>

<p>This is my piece of Code and it will return the bool values true false, when first time I was running this code it was working fine, but after restarting the kernal, this is what I am getting an error.</p>

<pre><code>---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
&lt;ipython-input-11-2d28913c9e2c&gt; in &lt;module&gt;()
----&gt; 1 df_Broadcast.show()

~/anaconda3/lib/python3.6/site-packages/pyspark/sql/dataframe.py in show(self, n, truncate)
    334         """"""
    335         if isinstance(truncate, bool) and truncate:
--&gt; 336             print(self._jdf.showString(n, 20))
    337         else:
    338             print(self._jdf.showString(n, int(truncate)))

~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py in __call__(self, *args)
   1131         answer = self.gateway_client.send_command(command)
   1132         return_value = get_return_value(
-&gt; 1133             answer, self.gateway_client, self.target_id, self.name)
   1134 
   1135         for temp_arg in temp_args:

~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py in deco(*a, **kw)
     61     def deco(*a, **kw):
     62         try:
---&gt; 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:
     65             s = e.java_exception.toString()

~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    317                 raise Py4JJavaError(
    318                     ""An error occurred while calling {0}{1}{2}.\n"".
--&gt; 319                     format(target_id, ""."", name), value)
    320             else:
    321                 raise Py4JError(

Py4JJavaError: An error occurred while calling o48.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 5.0 failed 1 times, most recent failure: Lost task 18.0 in stage 5.0 (TID 116, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space

Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)
    at scala.Option.foreach(Option.scala:257)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:336)
    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
    at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:2853)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)
    at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2153)
    at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:2837)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2836)
    at org.apache.spark.sql.Dataset.head(Dataset.scala:2153)
    at org.apache.spark.sql.Dataset.take(Dataset.scala:2366)
    at org.apache.spark.sql.Dataset.showString(Dataset.scala:245)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at py4j.Gateway.invoke(Gateway.java:280)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:214)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.OutOfMemoryError: Java heap space
</code></pre>
","2298012","","","","","2019-06-07 20:01:13","PySpark python issue: Py4JJavaError: An error occurred while calling o48.showString","<python-3.x><pyspark>","1","0","2","","","CC BY-SA 3.0","0"
"51624449","1","","","2018-08-01 02:08:26","","51","52767","<p>I recently installed python 3.7 and at the end of the download there is the option to Disable Path Length Limit. I don't know whether or not I should do this. What are the pros and cons of doing it. Just from the sound of it you should always disable it.</p>
","9933130","","","","","2020-04-18 23:41:21","Python Setup Disabling Path Length Limit Pros and Cons?","<python><python-3.x>","1","0","11","","","CC BY-SA 4.0","0"
"29217399","1","29218210","","2015-03-23 18:10:42","","14","52725","<p>What is the fastest way of converting a list of elements of type numpy.float64 to type float?  I am currently using the straightforward <code>for loop</code> iteration in conjunction with <code>float()</code>.</p>

<p>I came across this post: <a href=""https://stackoverflow.com/questions/9452775/converting-numpy-dtypes-to-native-python-types"">Converting numpy dtypes to native python types</a>, however my question isn't one of how to convert types in python but rather more specifically how to best convert an entire list of one type to another in the quickest manner possible in python (i.e. in this specific case numpy.float64 to float).  I was hoping for some secret python machinery that I hadn't come across that could do it all at once :)</p>
","3431836","","-1","","2017-05-23 12:34:54","2015-03-23 21:08:41","Convert list of numpy.float64 to float in Python quickly","<python-3.x><numpy><type-conversion>","3","3","5","","","CC BY-SA 3.0","0"
"47970844","1","47970971","","2017-12-25 17:49:57","","8","52664","<p>I am beginner in Python. I tried to develop a simple currency program but I have a problem. The program should calculate money when the calculate button is clicked(like an exchange). But I can't do it. PyCharm writes &quot;Process finished with exit code 1&quot;</p>
","9138583","","9138583","","2020-08-13 19:15:14","2020-08-13 19:15:14","What does ""Process finished with exit code 1"" mean?","<python-3.x>","2","6","","","","CC BY-SA 4.0","0"
"48235169","1","48364708","","2018-01-12 22:58:41","","14","52451","<p>I have updated numpy to 1.14.0. I use Windows 10. I tried to run my code and I got this error:</p>

<blockquote>
  <p>AttributeError: module 'numpy' has no attribute 'square'</p>
</blockquote>

<p>Here are my imports:</p>

<pre><code>%matplotlib inline
import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
from sklearn.metrics import confusion_matrix
import math
</code></pre>
","7561641","","5660517","","2018-01-12 23:16:07","2018-01-21 07:46:26","How to fix AttributeError: module 'numpy' has no attribute 'square'","<python><python-3.x><numpy><keras><attributeerror>","1","4","1","2018-01-21 07:52:52","","CC BY-SA 3.0","0"
"36982858","1","36982926","","2016-05-02 12:48:01","","51","52386","<p>I have a problem with Python 3. I got Python 2.7 code and at the moment I am trying to update it. I get the error:</p>

<blockquote>
  <p>TypeError: object of type 'map' has no len()</p>
</blockquote>

<p>at this part:</p>

<pre><code>str(len(seed_candidates))
</code></pre>

<p>Before I initialized it like this:</p>

<pre><code>seed_candidates = map(modify_word, wordlist)
</code></pre>

<p>So, can someone explain me what I have to do?</p>

<p>(EDIT: Previously this code example was wrong because it used <code>set</code> instead of <code>map</code>. It has been updated now.)</p>
","5986558","","7098259","","2018-08-08 14:50:56","2020-05-14 12:57:30","Object of type 'map' has no len() in Python 3","<python><python-3.x><variable-length>","2","2","3","","","CC BY-SA 4.0","0"
"42219874","1","42220023","","2017-02-14 07:06:14","","1","52353","<p>I have written the code for quick sort in python, but this code is throwing an error.</p>

<pre><code>----------


    k=0
    def partition(arr,low_index,high_index):
        key = arr[low_index]
        i = low_index + 1;
        j = high_index

        while True:
            while (i&lt;high_index and key&gt;=arr[i]):
                i+=1
            while (key&lt;arr[j]):
                j-=1
            if i&lt;j:
                arr[i,j] = arr[j,i]
            else:
                arr[low_index,j]=arr[j,low_index]
                return j

    def quicksort(arr,low_index,high_index):
         if low_index &lt; high_index:
            j = partition(low_index,high_index,arr)
            print(""Pivot element with index ""+str(j)+"" has thread ""+str(k))
            if left&lt;j:
                k=k+1
                quicksort(arr,low_index, j - 1)
            if i&lt;right:
                k=k+1
                quicksort(arr,j+1,high_index)
         return arr

    n = input(""Enter the value n "")
    arr=input(""Enter the ""+str(n)+"" no. of elements "")
    brr=quicksort(arr,0,n-1)
    print(""Elements after sorting are ""+str(brr))

----------
</code></pre>

<p>The error it is throwing is </p>

<blockquote>
  <p>Enter the value n 4</p>
  
  <blockquote>
    <p>Enter the 4 no. of elements [5,6,2,7]
        Traceback (most recent call last):
        File ""C:\Users\devendrabhat\Documents\dev\dev\quick.py"", line 38, in 
            brr=quicksort(arr,0,n-1)
        TypeError: unsupported operand type(s) for -: 'str' and 'int'</p>
  </blockquote>
</blockquote>
","4647076","","","","","2017-02-14 07:38:28","TypeError: unsupported operand type(s) for -: 'str' and 'int' (Python)","<python><python-2.7><python-3.x><ipython><quicksort>","3","3","3","","","CC BY-SA 3.0","0"
"37521162","1","","","2016-05-30 08:38:03","","15","52181","<p>I have installed <code>anaconda4</code> on my ubuntu and I have these modules on my Python:</p>

<pre><code>dbus-python (1.2.4)

gi (1.2)

pydbus (0.2)

QtAwesome (0.3.2)
qtconsole (4.2.0)
QtPy (1.0)

sip (4.18)
</code></pre>

<p>I tried installing <code>dbus-python (1.2.4)</code> and <code>pydbus (0.2)</code>, however, neither of them works! </p>

<p>After testing a simple program in python 3.5.1, an error appeared:</p>

<pre><code>import dbus
system_bus = dbus.SystemBus()

ImportError: No module named 'dbus'
</code></pre>

<p>When I use <code>pydbus</code> in Python 2.7.11 and 3.5.1:</p>

<pre><code>from pydbus import SystemBus

bus = SystemBus()
systemd = bus.get("".systemd1"")

for unit in systemd.ListUnits():
    print(unit)
</code></pre>

<p>I get this error:</p>

<pre><code>ImportError: No module named repository
</code></pre>

<p>The only thing that works is <a href=""https://github.com/Werkov/PyQt4/tree/master/examples/dbus/pingpong"" rel=""noreferrer"">this example</a> with <code>PyQT4</code> which I don't have any tutorial for.</p>

<p>What is the problem? Is it my installation or something else?</p>
","6170340","","4428725","","2016-05-30 09:38:52","2020-06-04 06:58:27","How solve ImportError: No module named 'dbus'?","<python><python-2.7><python-3.x><dbus><qtdbus>","8","1","5","","","CC BY-SA 3.0","0"
"45271344","1","","","2017-07-24 01:11:57","","10","52179","<p>So basically, I am fairly new to programming and using python. I am trying to build an ANN model for which I have to use Tensor flow, Theano and Keras library. I have Anaconda 4.4.1 with Python 3.5.2 on Windows 10 x64 and I have installed these libraries by following method.</p>

<ol>
<li>Create a new environment with Anaconda and Python 3.5:
conda create -n tensorflow python=3.5 anaconda</li>
<li>Activate the environment: 
activate tensorflow</li>
<li>After this you can install Theano, TensorFlow and Keras:
conda install theano,
conda install mingw libpython,
pip install tensorflow,
pip install keras,</li>
<li>Update the packages:
conda update --all</li>
</ol>

<p>All these packages are installed correctly and I have check them with conda list.
However, when I am trying to import any of these 3 libraries (i.e. Tensor flow, Theano and Keras), it is giving me the following error:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;ipython-input-3-c74e2bd4ca71&gt;"", line 1, in &lt;module&gt;
import keras
ImportError: No module named 'keras'
</code></pre>
","8354975","","2449192","","2017-07-25 18:35:32","2020-04-23 08:14:27","ImportError: No module named 'keras'","<python-3.x><tensorflow><anaconda><keras>","7","5","0","","","CC BY-SA 3.0","0"
"45579525","1","45579565","","2017-08-08 23:36:41","","11","52171","<p>I am trying to create and return a data frame from a python function </p>

<pre><code>def create_df():
    data = {'state': ['Ohio','Ohio','Ohio','Nevada','Nevada'],
           'year': [2000,2001,2002,2001,2002],
           'pop': [1.5,1.7,3.6,2.4,2.9]}
    df = pd.DataFrame(data)
    return df
create_df()
df
</code></pre>

<p>I get an error that df is not defined. If I replace 'return' by 'print' I get print of the data frame correctly. Is there a way to do this? thanks</p>
","6900680","","","","","2020-07-25 17:41:49","Returning a dataframe in python function","<python-3.x><function><dataframe><return>","4","1","5","","","CC BY-SA 3.0","0"
"36377777","1","","","2016-04-02 19:24:50","","19","52077","<p>I have some problems with python 3 module installing.
I'm trying to install mysql using pip :</p>

<pre><code>python -m pip install mysql
</code></pre>

<p>for python 3.4.4 and here is the error I get:</p>

<p><code>error: command 'C:\\Program Files (x86\\Microsoft Visual Studio 10.0\\VC\\BIN\\cl.exe' failed with exit status 2</code></p>

<p>here is a screenshot : <a href=""http://i.stack.imgur.com/qamNK.png"" rel=""noreferrer"">screenshot of the error message</a></p>

<p>I had other problems with module installation before, and I solved those by installing Visual Studio C++ 2010</p>

<p>Btw, the installation of other modules are working just fine.</p>

<p>My specs :
Windows 10 x64</p>

<p>Python 3.4.4 x86</p>
","5930384","","5930384","","2016-04-02 19:45:08","2020-05-20 09:58:07","Python error: command '...\Microsoft Visual Studio 10.0\\VC\\BIN\\cl.exe' failed with exit status 2","<visual-studio-2010><python-3.x><pip><mysql-python>","4","10","4","","","CC BY-SA 3.0","0"
"34619790","1","34634301","","2016-01-05 19:31:32","","182","52070","<p>For the following code:</p>

<pre><code>logger.debug('message: {}'.format('test'))
</code></pre>

<p><code>pylint</code> produces the following warning:</p>

<blockquote>
  <p><strong>logging-format-interpolation (W1202):</strong></p>
  
  <p>Use % formatting in logging functions and pass the % parameters as
  arguments Used when a logging statement has a call form of
  “logging.(format_string.format(format_args...))”. Such
  calls should use % formatting instead, but leave interpolation to the
  logging function by passing the parameters as arguments.</p>
</blockquote>

<p>I know I can turn off this warning, but I'd like to understand it. I assumed using <code>format()</code> is the preferred way to print out statements in Python 3. Why is this not true for logger statements?</p>
","1945981","","10388629","","2019-03-07 01:51:41","2020-09-24 12:47:56","PyLint message: logging-format-interpolation","<python><python-3.x><pylint>","3","0","25","","","CC BY-SA 4.0","0"
"48894060","1","48894161","","2018-02-20 20:55:49","","4","51734","<pre><code>fruit = {
    ""banana"": 1.00,
    ""apple"": 1.53,
    ""kiwi"": 2.00,
    ""avocado"": 3.23,
    ""mango"": 2.33,
    ""pineapple"": 1.44,
    ""strawberries"": 1.95,
    ""melon"": 2.34,
    ""grapes"": 0.98
}

for key,value in fruit.items():
     print(value)
</code></pre>

<p>I want to print the kiwi key, how?   </p>

<pre><code>print(value[2]) 
</code></pre>

<p>This is not working.</p>
","9198426","","3987854","","2020-01-16 12:51:21","2020-09-03 12:50:49","How to print Specific key value from a dictionary?","<python><python-3.x>","8","5","3","","","CC BY-SA 4.0","0"
"41488279","1","41716648","","2017-01-05 15:06:55","","41","51495","<p>I'm trying to implement a neural network that classifies images into one of the two discrete categories. The problem is, however, that it currently always predicts 0 for any input and I'm not really sure why.</p>

<p>Here's my feature extraction method:</p>

<pre><code>def extract(file):
    # Resize and subtract mean pixel
    img = cv2.resize(cv2.imread(file), (224, 224)).astype(np.float32)
    img[:, :, 0] -= 103.939
    img[:, :, 1] -= 116.779
    img[:, :, 2] -= 123.68
    # Normalize features
    img = (img.flatten() - np.mean(img)) / np.std(img)

    return np.array([img])
</code></pre>

<p>Here's my gradient descent routine:</p>

<pre><code>def fit(x, y, t1, t2):
    """"""Training routine""""""
    ils = x.shape[1] if len(x.shape) &gt; 1 else 1
    labels = len(set(y))

    if t1 is None or t2 is None:
        t1 = randweights(ils, 10)
        t2 = randweights(10, labels)

    params = np.concatenate([t1.reshape(-1), t2.reshape(-1)])
    res = grad(params, ils, 10, labels, x, y)
    params -= 0.1 * res

    return unpack(params, ils, 10, labels)
</code></pre>

<p>Here are my forward and back(gradient) propagations:</p>

<pre><code>def forward(x, theta1, theta2):
    """"""Forward propagation""""""

    m = x.shape[0]

    # Forward prop
    a1 = np.vstack((np.ones([1, m]), x.T))
    z2 = np.dot(theta1, a1)

    a2 = np.vstack((np.ones([1, m]), sigmoid(z2)))
    a3 = sigmoid(np.dot(theta2, a2))

    return (a1, a2, a3, z2, m)

def grad(params, ils, hls, labels, x, Y, lmbda=0.01):
    """"""Compute gradient for hypothesis Theta""""""

    theta1, theta2 = unpack(params, ils, hls, labels)

    a1, a2, a3, z2, m = forward(x, theta1, theta2)
    d3 = a3 - Y.T
    print('Current error: {}'.format(np.mean(np.abs(d3))))

    d2 = np.dot(theta2.T, d3) * (np.vstack([np.ones([1, m]), sigmoid_prime(z2)]))
    d3 = d3.T
    d2 = d2[1:, :].T

    t1_grad = np.dot(d2.T, a1.T)
    t2_grad = np.dot(d3.T, a2.T)

    theta1[0] = np.zeros([1, theta1.shape[1]])
    theta2[0] = np.zeros([1, theta2.shape[1]])

    t1_grad = t1_grad + (lmbda / m) * theta1
    t2_grad = t2_grad + (lmbda / m) * theta2

    return np.concatenate([t1_grad.reshape(-1), t2_grad.reshape(-1)])
</code></pre>

<p>And here's my prediction function:</p>

<pre><code>def predict(theta1, theta2, x):
    """"""Predict output using learned weights""""""
    m = x.shape[0]

    h1 = sigmoid(np.hstack((np.ones([m, 1]), x)).dot(theta1.T))
    h2 = sigmoid(np.hstack((np.ones([m, 1]), h1)).dot(theta2.T))

    return h2.argmax(axis=1)
</code></pre>

<p>I can see that the error rate is gradually decreasing with each iteration, generally converging somewhere around 1.26e-05.</p>

<p>What I've tried so far:</p>

<ol>
<li>PCA</li>
<li>Different datasets (Iris from sklearn and handwritten numbers from Coursera ML course, achieving about 95% accuracy on both). However, both of those were processed in a batch, so I can assume that my general implementation is correct, but there is something wrong with either how I extract features, or how I train the classifier.</li>
<li>Tried sklearn's SGDClassifier and it didn't perform much better, giving me a ~50% accuracy. So something wrong with the features, then?</li>
</ol>

<p><strong>Edit</strong>:
An average output of h2 looks like the following:</p>

<pre><code>[0.5004899   0.45264441]
[0.50048522  0.47439413]
[0.50049019  0.46557124]
[0.50049261  0.45297816]
</code></pre>

<p>So, very similar sigmoid outputs for all validation examples.</p>
","7022561","","7022561","","2017-01-06 14:23:20","2020-04-09 03:42:47","Neural network always predicts the same class","<python-3.x><numpy><neural-network><deep-learning><gradient-descent>","7","7","32","","","CC BY-SA 3.0","0"
"52308749","1","55579762","","2018-09-13 07:35:39","","29","51423","<p>I am trying out Seaborn to make my plot visually better than matplotlib. I have a dataset which has a column 'Year' which I want to plot on the X-axis and 4 Columns say A,B,C,D on the Y-axis using different coloured lines. I was trying to do this using the sns.lineplot method but it allows for only one variable on the X-axis and one on the Y-axis. I tried doing this</p>

<pre><code>sns.lineplot(data_preproc['Year'],data_preproc['A'], err_style=None)
sns.lineplot(data_preproc['Year'],data_preproc['B'], err_style=None)
sns.lineplot(data_preproc['Year'],data_preproc['C'], err_style=None)
sns.lineplot(data_preproc['Year'],data_preproc['D'], err_style=None)
</code></pre>

<p>But this way I don't get a legend in the plot to show which coloured line corresponds to what. I tried checking the documentation but couldn't find a proper way to do this.</p>
","10357133","","","","","2019-04-08 18:35:39","How do I create a multiline plot using seaborn?","<python><python-3.x><dataframe><plot><seaborn>","3","0","10","","","CC BY-SA 4.0","0"
"37434227","1","","","2016-05-25 10:21:09","","13","51420","<p>I have an xlsx file with 1 sheet.
I am trying to open it using python 3 (xlrd lib), but I get an empty file!</p>

<p>I use this code:</p>

<pre><code>file_errors_location = ""C:\\Users\\atheelm\\Documents\\python excel mission\\errors1.xlsx""
workbook_errors = xlrd.open_workbook(file_errors_location)
</code></pre>

<p>and I have no errors, but when I type:</p>

<pre><code>workbook_errors.nsheets
</code></pre>

<p>I get ""0"", even the file has some sheets... when I type:</p>

<pre><code>workbook_errors 
</code></pre>

<p>I get:</p>

<pre><code>xlrd.book.Book object at 0x2..
</code></pre>

<p>any help? thanks</p>
","4393849","","5900093","","2019-07-22 21:35:08","2020-10-07 19:44:38","how to open xlsx file with python 3","<python-3.x><xlsx><xlrd>","3","1","3","","","CC BY-SA 4.0","0"
"37446710","1","37447917","","2016-05-25 20:09:29","","40","51385","<p>I need a Python script that uses the Tkinter module to create a static (not resizable) window.</p>

<p>I have a pretty simple Tkinter script but I don't want it to be resizable. How do I prevent a Tkinter window from being resizable? I honestly don't know what to do.</p>

<p>This is my script:</p>

<pre><code>from tkinter import *
import ctypes, os

def callback():
    active.set(False)
    quitButton.destroy()
    JustGo = Button(root, text="" Keep Going!"", command= lambda: KeepGoing())
    JustGo.pack()   
    JustGo.place(x=150, y=110)
    #root.destroy()         # Uncomment this to close the window

def sleep():
    if not active.get(): return
    root.after(1000, sleep)
    timeLeft.set(timeLeft.get()-1)
    timeOutLabel['text'] = ""Time Left: "" + str(timeLeft.get())  #Update the label
    if timeLeft.get() == 0:                                     #sleep if timeLeft = 0
        os.system(""Powercfg -H OFF"")
        os.system(""rundll32.exe powrprof.dll,SetSuspendState 0,1,0"")

def KeepGoing():
    active.set(True)   
    sleep()
    quitButton1 = Button(root, text=""do not sleep!"", command=callback)
    quitButton1.pack()   
    quitButton1.place(x=150, y=110)

root = Tk()
root.geometry(""400x268"")
root.title(""Alert"")
root.configure(background='light blue')

timeLeft = IntVar()
timeLeft.set(10)            # Time in seconds until shutdown

active = BooleanVar()
active.set(True)            # Something to show us that countdown is still going.

label = Label(root, text=""ALERT this device will go to sleep soon!"",   fg=""red"")
label.config(font=(""Courier"", 12))
label.configure(background='light blue')
label.pack()
timeOutLabel = Label(root, text = 'Time left: ' + str(timeLeft.get()),     background='light blue') # Label to show how much time we have left.
timeOutLabel.pack()
quitButton = Button(root, text=""do not sleep!"", command=callback)
quitButton.pack()   
quitButton.place(x=150, y=110)



root.after(0, sleep)
root.mainloop()  
</code></pre>
","6286922","","404469","","2018-10-10 13:47:29","2020-06-11 14:12:16","How to make a Tkinter window not resizable?","<python-3.x><tkinter><static>","2","2","8","","","CC BY-SA 4.0","0"
"41171791","1","41172862","","2016-12-15 19:21:10","","54","51367","<p>From the examples in docs on <a href=""https://docs.python.org/3.5/library/subprocess.html#subprocess.run"" rel=""noreferrer""><code>subprocess.run()</code></a> it seems like there shouldn't be any output from</p>
<pre class=""lang-py prettyprint-override""><code>subprocess.run([&quot;ls&quot;, &quot;-l&quot;])  # doesn't capture output
</code></pre>
<p>However, when I try it in a python shell the listing gets printed. I wonder if this is the default behaviour and how to suppress the output of <code>run()</code>.</p>
","275088","","12708583","","2020-07-10 22:53:46","2020-07-10 22:53:46","How to suppress or capture the output of subprocess.run()?","<python><python-3.x><subprocess>","1","7","12","","","CC BY-SA 4.0","0"
"32144173","1","32144218","","2015-08-21 15:29:44","","14","51090","<p>I've already looked at this post about iterable python errors:</p>

<p><a href=""https://stackoverflow.com/questions/19821026/can-only-iterable-python-error"">&quot;Can only iterable&quot; Python error</a></p>

<p>But that was about the error ""cannot assign an iterable"". My question is why is python telling me:</p>

<pre><code> ""list.py"", line 6, in &lt;module&gt;
    reversedlist = ' '.join(toberlist1)
TypeError: can only join an iterable
</code></pre>

<p>I don't know what I am doing wrong! I was following this thread:</p>

<p><a href=""https://stackoverflow.com/questions/10745593/reverse-word-order-of-a-string-with-no-str-split-allowed"">Reverse word order of a string with no str.split() allowed</a></p>

<p>and specifically this answer:</p>

<pre><code>&gt;&gt;&gt; s = 'This is a string to try'
&gt;&gt;&gt; r = s.split(' ')
['This', 'is', 'a', 'string', 'to', 'try']
&gt;&gt;&gt; r.reverse()
&gt;&gt;&gt; r
['try', 'to', 'string', 'a', 'is', 'This']
&gt;&gt;&gt; result = ' '.join(r)
&gt;&gt;&gt; result
'try to string a is This'
</code></pre>

<p>and adapter the code to make it have an input. But when I ran it, it said the error above. I am a complete novice so could you please tell me what the error message means and how to fix it.</p>

<p>Code Below:</p>

<pre><code>import re
list1 = input (""please enter the list you want to print"")
print (""Your List: "", list1)
splitlist1 = list1.split(' ')
tobereversedlist1 = splitlist1.reverse()
reversedlist = ' '.join(tobereversedlist1)
yesno = input (""Press 1 for original list or 2 for reversed list"")
yesnoraw = int(yesno)
if yesnoraw == 1:
    print (list1)
else:
    print (reversedlist)
</code></pre>

<p>The program should take an input like apples and pears and then produce an output pears and apples.</p>

<p>Help would be appreciated!</p>
","5232451","","-1","","2017-05-23 12:26:19","2018-04-10 15:04:38","""Can only join an iterable"" python error","<python><python-3.x><iterable>","2","0","3","","","CC BY-SA 3.0","0"
"37512182","1","37512537","","2016-05-29 16:17:38","","67","50964","<p>I'm migrating from <code>tornado</code> to <code>asyncio</code>, and I can't find the <code>asyncio</code> equivalent of <code>tornado</code>'s <code>PeriodicCallback</code>. (A <code>PeriodicCallback</code> takes two arguments: the function to run and the number of milliseconds between calls.)</p>

<ul>
<li>Is there such an equivalent in <code>asyncio</code>?</li>
<li>If not, what would be the cleanest way to implement this without running the risk of getting a <code>RecursionError</code> after a while?</li>
</ul>
","6119465","","","","","2019-04-03 22:18:26","How can I periodically execute a function with asyncio?","<python><python-3.x><tornado><python-3.5><python-asyncio>","6","4","27","","","CC BY-SA 3.0","0"
"33019698","1","33019948","","2015-10-08 15:11:43","","62","50877","<p>I am facing a strange behavior of the <code>round()</code> function:</p>

<pre><code>for i in range(1, 15, 2):
    n = i / 2
    print(n, ""=&gt;"", round(n))
</code></pre>

<p>This code prints:</p>

<pre><code>0.5 =&gt; 0
1.5 =&gt; 2
2.5 =&gt; 2
3.5 =&gt; 4
4.5 =&gt; 4
5.5 =&gt; 6
6.5 =&gt; 6
</code></pre>

<p>I expected the floating values to be always rounded up, but instead, it is rounded to the nearest even number.</p>

<p>Why such behavior, and what is the best way to get the correct result?</p>

<p>I tried to use the <a href=""https://docs.python.org/3.5/library/fractions.html"" rel=""noreferrer""><code>fractions</code></a> but the result is the same.</p>
","2291710","","","","","2020-03-26 11:06:57","How to properly round up half float numbers in Python?","<python><python-3.x><floating-point><rounding><precision>","16","4","13","","","CC BY-SA 3.0","0"
"55084977","1","56430236","","2019-03-10 06:11:27","","48","50868","<pre><code>from ..box_utils import decode, nms
</code></pre>

<p>This line is giving <strong>error</strong></p>

<blockquote>
  <p>ImportError: <em>attempted relative import with no known parent package</em></p>
</blockquote>

<p>What is this error and how to resolve this error?</p>
","9581273","","6573902","","2020-03-16 07:08:28","2020-06-22 07:07:21","Attempted relative import with no known parent package","<python-3.x><python-import><importerror>","3","0","2","","","CC BY-SA 4.0","0"
"45156080","1","45167575","","2017-07-18 01:36:45","","19","50809","<p>When I want to install modules to Anaconda, I run <code>conda install</code>. However, now I have a <code>.tar.gz</code> file and want to install this. How to do?</p>
","","user4550164","2666289","","2019-08-01 07:18:21","2019-08-01 07:18:21","Installing modules to Anaconda from .tar.gz","<python-3.x><anaconda>","4","1","12","","","CC BY-SA 4.0","0"
"33496350","1","42169354","","2015-11-03 10:09:29","","68","50750","<p>I would like to execute a long running Python script from within a Jupyter notebook so that I can hack on the data structures generated mid-run.</p>

<p>The script has many dependencies and command line arguments and is executed with a specific virtualenv. Is it possible to interactively run a Python script inside a notebook from a specified virtualenv (different to that of the Jupyter installation)?</p>

<p>Thanks!</p>
","709445","","3130926","","2019-10-11 13:34:36","2019-10-11 13:34:36","Execute Python script within Jupyter notebook using a specific virtualenv","<python><python-3.x><jupyter-notebook><virtualenv><jupyter>","6","2","38","","","CC BY-SA 3.0","0"
"49031798","1","49082781","","2018-02-28 14:23:51","","20","50432","<p>I use Google Colaboratory then I want to save output images in my Google Drive or SSD, HHD but its directory is ""/content""  </p>

<pre><code>import os     
print(os.getcwd())
# ""/content""
</code></pre>

<p>so is it possible to change path (HDD, SSD, googledrive)?</p>
","8448203","","5781745","","2018-02-28 23:22:21","2018-10-11 14:36:39","When I use Google Colaboratory, how to save image, weights in my Google Drive?","<python-3.x><google-colaboratory>","3","1","11","","","CC BY-SA 3.0","0"
"50504500","1","50504635","","2018-05-24 08:27:43","","112","50398","<p>I've met a problem with <code>re</code> module in Python 3.6.5.
I have this pattern in my regular expression:</p>

<pre><code>'\\nRevision: (\d+)\\n'
</code></pre>

<p>But when I run it, I'm getting a <code>DeprecationWarning</code>.</p>

<p>I searched for <a href=""https://stackoverflow.com/search?q=DeprecationWarning%3A%20invalid%20escape%20sequence%20%5Cd"">the problem on SO</a>, and haven't found the answer, actually - what should I use instead of <code>\d+</code>? Just <code>[0-9]+</code> or maybe something else?</p>
","3079726","","","","","2019-12-09 23:00:19","DeprecationWarning: invalid escape sequence - what to use instead of \d?","<python><regex><python-3.x>","1","2","6","","","CC BY-SA 4.0","0"
"43120112","1","","","2017-03-30 13:59:29","","18","50309","<p>I have the following package (and working directory):</p>

<pre><code>WorkingDirectory--
                 |--MyPackage--
                 |            |--__init__.py
                 |            |--module1.py
                 |            |--module2.py
                 |
                 |--notebook.ipynb
</code></pre>

<p>In <code>__init__.py</code> I have:</p>

<pre><code>import module1
import module2
</code></pre>

<p>If I try to import MyPackage into my notebook:</p>

<pre><code>import MyPackage as mp 
</code></pre>

<p>I will get <code>ModuleNotFoundError: No module named 'module1'</code>. But import works fine if I execute the script outside a notebook: if I create <code>test.py</code> in the same directory and do the same as in the notebook the import would work properly. It will work inside the notebook if I use fully qualified name in <code>__init__.py</code> (<code>import MyPackage.module1</code>).</p>

<p>What's the reason for different import behavior?</p>

<p>I have confirmed the working directory of the notebook is <code>WorkingDirectory</code>.</p>

<p>---Update---------</p>

<p>Exact error is:</p>

<pre><code>C:\Users\Me\Documents\Working Directory\MyPackage\__init__.py in &lt;module&gt;()
---&gt; 17 import module1

ModuleNotFoundError: No module named 'module1'
</code></pre>

<p>My problem differs from the possible duplicate:</p>

<ol>
<li><p>The notebook was able to find the package, but only unable to load the module. This was inferred from substituting <code>module1</code> with <code>MyPackage.module1</code> worked well and suggests it may not be a problem related with <code>PATH</code>.</p></li>
<li><p>I cded into <code>WorkingDirectory</code> and started the server there. The working directory should be the folder containing my package.</p></li>
</ol>
","6305831","","6305831","","2017-03-30 16:09:43","2020-07-06 01:49:29","Module Not found during import in Jupyter Notebook","<python><python-3.x><jupyter-notebook><python-import><python-module>","3","3","3","","","CC BY-SA 3.0","0"
"41913043","1","41913227","","2017-01-28 18:05:20","","8","50271","<p>I have a method that build huffman tree which is as follows:</p>

<pre><code>def buildTree(tuples) :
    while len(tuples) &gt; 1 :
        leastTwo = tuple(tuples[0:2])                  # get the 2 to combine
        theRest  = tuples[2:]                          # all the others
        combFreq = leastTwo[0][0] + leastTwo[1][0]     #enter code here the branch points freq
        tuples   = theRest + [(combFreq,leastTwo)]     # add branch point to the end
        tuples.sort()                                  # sort it into place
    return tuples[0]            # Return the single tree inside the list
</code></pre>

<p>but while I feed the function with following parameter:</p>

<pre><code>[(1, 'b'), (1, 'd'), (1, 'g'), (2, 'c'), (2, 'f'), (3, 'a'), (5, 'e')]
</code></pre>

<p>I get the error as </p>

<pre><code>  File ""&lt;stdin&gt;"", line 7, in buildTree
    tuples.sort()
TypeError: '&lt;' not supported between instances of 'tuple' and 'str'
</code></pre>

<p>While debugging I found the error was in <code>tuples.sort()</code>.</p>
","2786475","","3329664","","2017-01-28 18:12:02","2017-01-28 18:23:45","TypeError: '<' not supported between instances of 'tuple' and 'str'","<python><python-3.x><sorting>","1","4","1","","","CC BY-SA 3.0","0"
"32456881","1","32464486","","2015-09-08 11:41:29","","55","49920","<p>I was trying the following code:</p>

<pre><code>import asyncio

@asyncio.coroutine
def func_normal():
        print(""A"")
        yield from asyncio.sleep(5)
        print(""B"")
        return 'saad'

@asyncio.coroutine
def func_infinite():
    i = 0
    while i&lt;10:
        print(""--""+str(i))
        i = i+1
    return('saad2')

loop = asyncio.get_event_loop()

tasks = [
    asyncio.async(func_normal()),
    asyncio.async(func_infinite())]

loop.run_until_complete(asyncio.wait(tasks))
loop.close()
</code></pre>

<p>I can't figure out how to get values in variables from these functions. I can't do this:</p>

<pre><code>asyncio.async(a = func_infinite())
</code></pre>

<p>as this would make this a keyword argument. How do I go about accomplishing this?</p>
","2748768","","2073595","","2015-09-08 15:31:30","2017-05-15 21:02:59","Getting values from functions that run as asyncio tasks","<python><python-3.x><python-3.4><python-asyncio>","3","0","4","","","CC BY-SA 3.0","0"
"32811713","1","32823074","","2015-09-27 19:02:33","","16","49846","<p>Supposedly Python 2.7 is included native to OSX 10.8 and above (if I remember correctly), but I recently installed Python 3.5 to use for projects while I work through UDacity. Lo and behold, the UDacity courses seem to use 2.7 - wups! So instead of trying to uninstall 3.5 (this procedure seemed to scary for neophytes such as myself), I simply installed 2.7 in addition to the recently installed 3.5 and just run the 2.7 IDLE and Shell. Is this ok, or will I run into problems down the road?</p>
","5378083","","1000551","","2019-04-04 08:25:21","2019-04-04 08:25:21","Is it ok to install both Python 2.7 and 3.5?","<python><python-3.x><python-2.7><osx-yosemite>","7","2","2","","","CC BY-SA 3.0","0"
"57743230","1","","","2019-09-01 04:38:53","","51","49714","<p>After Installing Google Cloud Bigquery Module, if I import the module into python code. I see this warning message. Happening to me in python 3.7.3 Virtualenv.</p>

<p>Tried to reinstall GCP bigquery module
Expectation-in python code if we write"" from google.cloud import bigquery "".Should not result in any error or messege.</p>

<pre class=""lang-py prettyprint-override""><code>import os
import sys
import logging
from datetime import datetime
from google.cloud import bigquery
</code></pre>

<pre><code>/home/informatica/.local/lib/python3.7/site-packages/pandas/compat/__init__.py:84: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.
  warnings.warn(msg)
 exit()
</code></pre>
","9623730","","4092569","","2019-10-02 07:24:47","2020-06-16 11:11:26","UserWarning: Could not import the lzma module. Your installed Python is incomplete","<python-3.x>","5","5","11","","","CC BY-SA 4.0","0"
"42026036","1","42026886","","2017-02-03 14:02:21","","21","49710","<p>I have a string that contains a path
</p>

<pre><code>str = ""/example/path/with/different/trailing/delimiter\""
</code></pre>

<p>and I want to trim the leading and trailing <code>/</code> and <code>\</code>. What is the best practice in Python 3?</p>

<p>Currently I'm using
</p>

<pre><code>trimmedPath = str.strip(""/\\"")
# trimmedPath is ""example/path/with/different/trailing/delimiter"" as desired
</code></pre>

<p>Two questions:</p>

<ol>
<li>Is this the best trim function for trimming specific characters in Python 3?</li>
<li>Are there specific path functions for such operations in Python 3 so I don't have to set the delimiters manually?</li>
</ol>
","4566599","","775954","","2019-03-25 19:04:28","2019-03-25 19:04:28","Trim specific leading and trailing characters from a string","<python><python-3.x>","2","1","4","","","CC BY-SA 4.0","0"
"42183479","1","42183505","","2017-02-12 02:06:41","","17","49556","<p>So I am making a program that takes a text file, breaks it into words, then writes the list to a new text file.</p>

<p>The issue I am having is I need the strings in the list to be with double quotes not single quotes.</p>

<p>For example</p>

<p>I get this <code>['dog','cat','fish']</code> when I want this <code>[""dog"",""cat"",""fish""]</code></p>

<p>Here is my code</p>

<pre><code>with open('input.txt') as f:
    file = f.readlines()
nonewline = []
for x in file:
    nonewline.append(x[:-1])
words = []
for x in nonewline:
    words = words + x.split()
textfile = open('output.txt','w')
textfile.write(str(words))
</code></pre>

<p>I am new to python and haven't found anything about this.
Anyone know how to solve this?</p>

<p>[Edit: I forgot to mention that i was using the output in an arduino project that required the list to have double quotes.]</p>
","7552029","","7552029","","2017-02-12 02:44:53","2017-02-12 02:44:53","I want to replace single quotes with double quotes in a list","<python><string><list><python-3.x>","3","3","7","","","CC BY-SA 3.0","0"
"49264194","1","49264448","","2018-03-13 19:14:04","","23","49454","<p>My question is related to <a href=""https://stackoverflow.com/questions/4383571/importing-files-from-different-folder"">this</a>. I am using Python 3.6 in Jupyter Notebook. My project directory is <code>/user/project</code>. In this directory I'm building a number of models and each has it's own folder. However, there is a common <code>functions.py</code> file with functions that I want to use across all models. So I want to keep the <code>functions.py</code> file in <code>/user/project</code> but be able to call it from an <code>.ipynb</code> file in <code>/user/project/model1</code>, <code>/user/project/model2</code>, etc... How can I do this?</p>
","4913108","","1909378","","2018-03-14 01:01:11","2020-10-30 07:22:29","Import py file in another directory in Jupyter notebook","<python-3.x><python-import>","5","2","9","","","CC BY-SA 3.0","0"
"32280091","1","","","2015-08-28 21:35:09","","18","49435","<pre><code>for i in range (0, 81):
    output = send command
    while True:
        last_byte = last_byte - offset
    if last_byte &gt; offset:
       output = send command
       i+
    else:
        output = send command
        i+
        break
</code></pre>

<p>I want to increase the iterator every time the send command is executed. Right now it only increases by one when the for loop is executed. Please advise </p>

<pre><code>for i in range(0,10):
    print(i)
    i +=2
    print(""increased i"", i)
</code></pre>

<p>I ran this code and it produced out from 0 to 9. I was expecting it would increase the iterator by 2.</p>
","873393","","2867928","","2017-10-01 09:51:43","2020-10-16 11:47:14","how to increment the iterator from inside for loop in python 3?","<python><python-3.x><for-loop><iterator>","6","1","3","","","CC BY-SA 3.0","0"
"39977808","1","40016776","","2016-10-11 12:58:24","","16","49378","<p>I have Anaconda (version: conda 4.2.9, python3) installed and am trying to do <code>import cv2</code> when I get the following error:</p>
<pre><code>ImportError: No module named 'cv2'
</code></pre>
<p>With <code>conda search cv2</code> I get this:</p>
<pre><code>  opencv                     2.4.2                np15py26_0  defaults        
                             2.4.2                np15py27_0  defaults        
                             2.4.2                np16py26_0  defaults        
                             2.4.2                np16py27_0  defaults        
                             2.4.2                np17py26_0  defaults        
                             2.4.2                np17py27_0  defaults        
                             2.4.2                np15py26_1  defaults        
                             2.4.2                np15py27_1  defaults        
                             2.4.2                np16py26_1  defaults        
                             2.4.2                np16py27_1  defaults        
                             2.4.2                np17py26_1  defaults        
                             2.4.2                np17py27_1  defaults        
                             2.4.6                np16py26_0  defaults        
                             2.4.6                np16py27_0  defaults        
                             2.4.6                np17py26_0  defaults        
                             2.4.6                np17py27_0  defaults        
                             2.4.6                np18py26_0  defaults        
                             2.4.6                np18py27_0  defaults        
                             2.4.9                np18py27_0  defaults        
                             2.4.10               np19py26_0  defaults        
                             2.4.10               np19py27_0  defaults        
                             2.4.10              np110py27_1  defaults        
                             2.4.10               np19py26_1  defaults        
                             2.4.10               np19py27_1  defaults        
</code></pre>
<p>What do I need to do to be able to import the cv2 module?</p>
<p>I am using Ubuntu 16.04.</p>
","4795786","","128421","","2020-07-05 17:22:21","2020-07-05 17:22:21","Anaconda: cannot import cv2 even though opencv is installed (how to install opencv3 for python3)","<python><python-3.x><opencv><anaconda><opencv3.0>","5","1","4","","","CC BY-SA 4.0","0"
"35443278","1","35443495","","2016-02-16 21:24:27","","9","49318","<pre><code>import os

def rename(directory):
    for name in os.listdir(directory):
        print(name)
        os.rename(name,""0""+name)


path = input(""Enter the file path"")
rename(path)
</code></pre>

<p>I want to rename every file in a certain directory so that it adds a 0 to the beginning of the file name, however when I try to run the code it comes up with this error: </p>

<p>(FileNotFoundError: [WinError 2] The system cannot find the file specified: '0.jpg' -> '00.jpg')</p>

<p>I'm sure that there is a file in there named 0.jpg and i'm not sure what the problem is. </p>

<p>Sorry if this is a stupid question i'm new to coding.</p>
","5571172","","42346","","2016-07-08 22:34:29","2020-04-18 13:29:02","FileNotFoundError: [WinError 2] The system cannot find the file specified:","<python><python-3.x>","3","0","3","","","CC BY-SA 3.0","0"
"39808908","1","39808935","","2016-10-01 16:25:36","","24","49099","<p>I am using Python 3, and trying to detect if an item is the last in a list, but sometimes there will repeats. This is my code:</p>

<pre><code>a = ['hello', 9, 3.14, 9]
for item in a:
    print(item, end='')
    if item != a[-1]:
        print(', ')
</code></pre>

<p>And I would like this output:</p>

<pre><code>hello,
9,
3.14,
9
</code></pre>

<p>But I get this output:</p>

<pre><code>hello, 
93.14, 
9
</code></pre>

<p>I understand why I am getting the output I do not want.
I would prefer if I could still use the loop, but I can work around them. (I would like to use this with more complicated code)</p>
","6312416","","4099593","","2016-10-01 16:44:17","2019-11-09 06:07:29","Detect If Item is the Last in a List","<python><list><python-3.x>","6","4","3","2020-07-01 23:48:05","","CC BY-SA 3.0","0"
"50100221","1","50103791","","2018-04-30 12:07:46","","26","48912","<p>I am trying to download a file from Amazon S3 bucket to my local using the below code but I get an error saying ""Unable to locate credentials""</p>

<p>Given below is the code I have written:</p>

<pre><code>from boto3.session import Session
import boto3

ACCESS_KEY = 'ABC'
SECRET_KEY = 'XYZ'

session = Session(aws_access_key_id=ACCESS_KEY,
              aws_secret_access_key=SECRET_KEY)
s3 = session.resource('s3')
your_bucket = s3.Bucket('bucket_name')

for s3_file in your_bucket.objects.all():
    print(s3_file.key) # prints the contents of bucket

s3 = boto3.client ('s3')

s3.download_file('your_bucket','k.png','/Users/username/Desktop/k.png')
</code></pre>

<p>Could anyone help me on this. Thanks.</p>
","9282755","","9614249","","2019-11-08 15:31:49","2019-12-18 15:38:13","Download file from AWS S3 using Python","<python-3.x><amazon-s3><download><boto3>","3","0","11","","","CC BY-SA 4.0","0"
"48047079","1","","","2018-01-01 04:07:34","","6","48813","<p>I want to write a function that returns a string, not bytes.<br>
  the function:</p>

<pre><code>def read_image(path):
    with open(path, ""rb"") as f:
        data = f.read()
    return data
image_data = read_image(""/home/user/icon.jpg"")
</code></pre>

<p>How to convert the value <code>image_data</code> to type str.
If convert to string successfully, how to reconvert the string to bytes.</p>
","3410960","","2875563","","2018-01-01 04:30:33","2018-01-01 06:13:23","how to convert bytes to string in Python 3","<python><python-3.x>","1","6","2","2018-01-01 10:56:17","","CC BY-SA 3.0","0"
"30361824","1","30364143","","2015-05-20 23:04:46","","44","48774","<p>I've the following code using <code>asyncio</code> and <code>aiohttp</code> to make asynchronous HTTP requests.</p>

<pre><code>import sys
import asyncio
import aiohttp

@asyncio.coroutine
def get(url):
    try:
        print('GET %s' % url)
        resp = yield from aiohttp.request('GET', url)
    except Exception as e:
        raise Exception(""%s has error '%s'"" % (url, e))
    else:
        if resp.status &gt;= 400:
            raise Exception(""%s has error '%s: %s'"" % (url, resp.status, resp.reason))

    return (yield from resp.text())

@asyncio.coroutine
def fill_data(run):
    url = 'http://www.google.com/%s' % run['name']
    run['data'] = yield from get(url)

def get_runs():
    runs = [ {'name': 'one'}, {'name': 'two'} ]
    loop = asyncio.get_event_loop()
    task = asyncio.wait([fill_data(r) for r in runs])
    loop.run_until_complete(task)   
    return runs

try:
    get_runs()
except Exception as e:
    print(repr(e))
    sys.exit(1)
</code></pre>

<p>For some reason, exceptions raised inside the <code>get</code> function are not caught:</p>

<pre><code>Future/Task exception was never retrieved
Traceback (most recent call last):
  File ""site-packages/asyncio/tasks.py"", line 236, in _step
    result = coro.send(value)
  File ""mwe.py"", line 25, in fill_data
    run['data'] = yield from get(url)
  File ""mwe.py"", line 17, in get
    raise Exception(""%s has error '%s: %s'"" % (url, resp.status, resp.reason))
Exception: http://www.google.com/two has error '404: Not Found'
</code></pre>

<p>So, what is correct way to handle exceptions raised by couroutines?</p>
","2900735","","2900735","","2015-05-21 00:22:26","2018-10-26 16:41:27","Asynchronous exception handling in Python","<python><python-3.x><python-asyncio>","2","0","16","","","CC BY-SA 3.0","0"
"42126794","1","","","2017-02-09 01:38:22","","46","48697","<p>I have recently been learning python 3 and I cannot get any examples involving string interpolation (formatting) to work. </p>

<pre><code>In [1]: state = ""Washington""

In [2]: state
Out[2]: 'Washington'

In [3]: my_message = f""I live in {state}""
File ""&lt;ipython-input-3-d004dd9e0255&gt;"", line 1
my_message = f""I live in {state}""
                                ^
SyntaxError: invalid syntax
</code></pre>

<p>I figured my machine was defaulting to python 2, but a quick check reveals:</p>

<pre><code>Python 3.5.2 (default, Nov 17 2016, 17:05:23) 
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 5.2.2 -- An enhanced Interactive Python.
</code></pre>

<p>I am on Ubuntu 16.04:</p>

<pre><code>python3 --version
Python 3.5.2
</code></pre>

<p>Am I just overlooking basic syntax? I have run the same commands on a few computers from fellow students and it seems to execute just fine.</p>
","7537622","","364696","","2017-02-09 02:58:28","2020-09-06 04:53:46","Python 3 returns ""invalid syntax"" when trying to perform string interpolation","<python><python-3.x><ubuntu-16.04><string-interpolation>","3","3","4","","","CC BY-SA 3.0","0"
"51562067","1","","","2018-07-27 16:29:24","","19","48637","<p>For some reason I get the following error <strong><em>only</em></strong> when I open up a nested <code>webdriver</code> instance. No idea what is happening here.</p>

<p>I am using <em>Windows 10,</em> <em>geckodriver 0.21.0,</em> and <em>Python 3.7.</em></p>

<p><strong><em>ConnectionAbortedError: [WinError 10053]</em></strong></p>

<pre><code>An established connection was aborted by the software in your host machine
</code></pre>

<p><strong><em>Part of Script That Is Working Fine</em></strong></p>

<pre><code>tab_backers = ff.find_element_by_xpath('//a[@gogo-test=""backers_tab""]')

try:
    funding_backers_count = int(''.join(filter(str.isdigit, str(tab_backers.text))))
except ValueError:
    funding_backers_count = 0

if funding_backers_count &gt; 0:
    tab_backers.click()

    see_more_backers = WebDriverWait(ff, 10).until(
        EC.element_to_be_clickable((By.XPATH, '//ui-view//a[text()=""See More Backers""]'))
    )
    clicks = 0
    while clicks &lt; 0:
        clicks += 1
        ff.WebDriverWait(ff, 5).until(
            see_more_backers.click()
        )

    for container in ff.find_elements_by_xpath('//ui-view//div[@class=""campaignBackers-pledge ng-scope""]'):
        backers_profile = container.find_elements_by_xpath('./*/div[@class=""campaignBackers-pledge-backer-details""]/a')
        if len(backers_profile) &gt; 0:
            backers_profile = backers_profile[0].get_attribute('href') 
        else:
            backers_profile = 'Unknown'
        backers_name = safe_encode(container.find_element_by_xpath('(./*/div[@class=""campaignBackers-pledge-backer-details""]/*)[1]').text)
        backers_timestamp = container.find_element_by_xpath('./*/div[@class=""campaignBackers-pledge-backer-details""]/div[contains(@class, ""campaignBackers-pledge-backer-details-note"")]').text
        backers_contribution = container.find_element_by_xpath('./*//*[contains(@class, ""campaignBackers-pledge-amount-bold"")]').text
        if backers_contribution != 'Private':
            backers_contribution = int(''.join(filter(str.isdigit, str(backers_contribution))))
        if backers_profile != 'Unknown':
</code></pre>

<p><strong><em>Part of Script Causing System to Abort Connection</em></strong></p>

<pre><code>            _ff = create_webdriver_instance()
            _ff.get(backers_profile)
            _ff.quit()
</code></pre>

<p><strong><em>Traceback</em></strong></p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\Anthony\Desktop\test.py"", line 271, in &lt;module&gt;
    backers_profile = container.find_elements_by_xpath('./*/div[@class=""campaignBackers-pledge-backer-details""]/a')
  File ""C:\Users\Anthony\AppData\Local\Programs\Python\Python37\lib\site-packages\selenium\webdriver\remote\webelement.py"", line 381, in find_elements_by_xpath
    return self.find_elements(by=By.XPATH, value=xpath)
  File ""C:\Users\Anthony\AppData\Local\Programs\Python\Python37\lib\site-packages\selenium\webdriver\remote\webelement.py"", line 680, in find_elements
    {""using"": by, ""value"": value})['value']
  File ""C:\Users\Anthony\AppData\Local\Programs\Python\Python37\lib\site-packages\selenium\webdriver\remote\webelement.py"", line 628, in _execute
    return self._parent.execute(command, params)
  File ""C:\Users\Anthony\AppData\Local\Programs\Python\Python37\lib\site-packages\selenium\webdriver\remote\webdriver.py"", line 318, in execute
    response = self.command_executor.execute(driver_command, params)
  File ""C:\Users\Anthony\AppData\Local\Programs\Python\Python37\lib\site-packages\selenium\webdriver\remote\remote_connection.py"", line 472, in execute
    return self._request(command_info[0], url, body=data)
  File ""C:\Users\Anthony\AppData\Local\Programs\Python\Python37\lib\site-packages\selenium\webdriver\remote\remote_connection.py"", line 495, in _request
    self._conn.request(method, parsed_url.path, body, headers)
  File ""C:\Users\Anthony\AppData\Local\Programs\Python\Python37\lib\http\client.py"", line 1229, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""C:\Users\Anthony\AppData\Local\Programs\Python\Python37\lib\http\client.py"", line 1275, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""C:\Users\Anthony\AppData\Local\Programs\Python\Python37\lib\http\client.py"", line 1224, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""C:\Users\Anthony\AppData\Local\Programs\Python\Python37\lib\http\client.py"", line 1055, in _send_output
    self.send(chunk)
  File ""C:\Users\Anthony\AppData\Local\Programs\Python\Python37\lib\http\client.py"", line 977, in send
    self.sock.sendall(data)
ConnectionAbortedError: [WinError 10053] An established connection was aborted by the software in your host machine
</code></pre>

<p><strong>geckodriver.log</strong></p>

<p>Here it is in a <a href=""https://codepen.io/tOkyO1/pen/mjxxRN?editors=0010"" rel=""noreferrer"">codepen,</a> since it is wayyy too long!</p>

<p><strong><em>create_webdriver_instance Function</em></strong></p>

<pre><code>def create_webdriver_instance():
    options = Options()
    options.add_argument('-headless')
    try:
        ua_string = random.choice(ua_strings)
        profile = webdriver.FirefoxProfile()
        profile.set_preference('general.useragent.override', ua_string)
        return webdriver.Firefox(profile) # profile, firefox_options=options
    except IndexError as error:
        print('\nSection: Function to Create Instances of WebDriver\nCulprit: random.choice(ua_strings)\nIndexError: {}\n'.format(error))
        return webdriver.Firefox() # firefox_options=options
</code></pre>

<p><br>
<strong>Does anybody have any idea whatsoever what might be causing the connection to abort?</strong>
<br><br><br></p>
","7543162","","7429447","","2018-08-09 14:41:12","2020-09-05 12:51:51","ConnectionAbortedError: [WinError 10053] An established connection was aborted by the software in your host machine","<python><python-3.x><selenium><selenium-webdriver><geckodriver>","6","7","4","","","CC BY-SA 4.0","0"
"39971929","1","39972031","","2016-10-11 07:00:15","","82","48629","<p>Python 3.6 is about to be released. <a href=""https://www.python.org/dev/peps/pep-0494/"" rel=""noreferrer"">PEP 494 -- Python 3.6 Release Schedule</a> mentions the end of December, so I went through <a href=""https://docs.python.org/3.6/whatsnew/3.6.html"" rel=""noreferrer"">What's New in Python 3.6</a> to see they mention the <em>variable annotations</em>:</p>

<blockquote>
  <p><a href=""https://www.python.org/dev/peps/pep-0484"" rel=""noreferrer"">PEP 484</a> introduced standard for type annotations of function parameters, a.k.a. type hints. This PEP adds syntax to Python for annotating the types of variables including class variables and instance variables:</p>

<pre><code>primes: List[int] = []

captain: str  # Note: no initial value!

class Starship:
     stats: Dict[str, int] = {}
</code></pre>
  
  <p>Just as for function annotations, the Python interpreter does not attach any particular meaning to variable annotations and only stores them in a special attribute <code>__annotations__</code> of a class or module. In contrast to variable declarations in statically typed languages, the goal of annotation syntax is to provide an easy way to specify structured type metadata for third party tools and libraries via the abstract syntax tree and the <code>__annotations__</code> attribute.</p>
</blockquote>

<p>So from what I read they are part of the type hints coming from Python 3.5, described in <a href=""https://stackoverflow.com/q/32557920/1983854"">What are Type hints in Python 3.5</a>.</p>

<p>I follow the <code>captain: str</code> and <code>class Starship</code> example, but not sure about the last one: How does <code>primes: List[int] = []</code> explain? Is it defining an empty list that will just allow integers?</p>
","1983854","","1265393","","2019-11-18 18:32:06","2019-11-18 18:32:06","What are variable annotations in Python 3.6?","<python><python-3.x><annotations><type-hinting><python-3.6>","2","3","44","","","CC BY-SA 4.0","0"
"49998463","1","49998511","","2018-04-24 09:48:01","","3","48567","<p>How can I transfer </p>

<pre><code>A = [0.12075357905088335, -0.192198145631724, 0.9455373400335009, -0.6811922263715244, 0.7683786941009969, 0.033112227984689206, -0.3812622359989405] 
</code></pre>

<p>to </p>

<pre><code>A = [[0.12075357905088335], [-0.192198145631724], [0.9455373400335009], [-0.6811922263715244], [0.7683786941009969], [0.033112227984689206], [-0.3812622359989405]]
</code></pre>

<p>I tried to the code below but an error occurred:</p>

<pre><code>new = []
for i in A:
    new.append.list(i)
</code></pre>

<blockquote>
  <p><code>TypeError: 'float' object is not iterable</code></p>
</blockquote>

<p>Could anyone help me?</p>
","9511689","","1717069","","2018-04-24 10:00:10","2020-04-14 13:37:16","how to solve TypeError: 'float' object is not iterable","<python><python-3.x>","4","1","","","","CC BY-SA 3.0","0"
"36120426","1","36131160","","2016-03-20 22:28:44","","10","48303","<p>This is my sample code. I want the items typed in the entry to be inserted to the treeview when the enter button is pressed. Im new to python and tkinter and there is not much tuts about treeview.</p>

<pre><code>class PurchaseEntry(tk.Frame):
    def __init__(self, parent, controller):
       tk.Frame.__init__(self, parent)
       self.controller = controller
       PurchaseEntry.configure(self, bg='white')

       label = ttk.Label(self, text='Purchase Entry', font=LARGE_FONT2)
       label.grid(row=0, columnspan=3, sticky='w')

       purchase_entry = ttk.Label(self, text='Purchase Entry:')
       purchase_entry.grid(row=1, column=0)

       self.entry_val = tk.StringVar()
       self.entry_1 = ttk.Entry(self, width=100, textvariable=self.entry_val)
       self.entry_1.grid(row=1, column=2, columnspan=2, sticky='w')
       self.entry_1.focus()

       self.entry_btn = ttk.Button(self,text='Enter', command=self.insert_value)
       self.entry_btn.grid(row=1, column=4, columnspan=2, sticky='w')

       self.chat1 = ttk.Treeview(self)

       chat1 = ttk.Treeview( self, height=28, columns=('dose', 'date   modified'), selectmode=""extended"")
       chat1.heading('#0', text='item', anchor=tk.CENTER)
       chat1.heading('#1', text='dose', anchor=tk.CENTER)
       chat1.heading('#2', text='date modified', anchor=tk.CENTER)
       chat1.column('#1', stretch=tk.YES, minwidth=50, width=100)
       chat1.column('#2', stretch=tk.YES, minwidth=50, width=120)
       chat1.column('#0', stretch=tk.YES, minwidth=50, width=400)
       chat1.grid(row=2, column=2, columnspan=4, sticky='nsew')

    def insert_value(self):
       value = self.entry_val.get()
       # Inserts data written in the entry box to the treeview widget when Enter button is pressed.
       # Clears the Entry box, ready for another data entry.
       self.entry_1.delete(0, 'end')
       self.chat1.insert('WHAT SHOULD I GIVE AS AN ARGUMENT?')
</code></pre>

<p>What should I pass as an argument?
Or is treeview the right widget for this or can someone suggest a widget suitable for this issue? thanks</p>
","6082012","","3329664","","2017-02-24 13:32:22","2019-05-25 18:35:44","tkinter Treeview widget inserting data","<python><python-3.x><tkinter><treeview>","1","2","9","","","CC BY-SA 3.0","0"
"43252542","1","43260678","","2017-04-06 10:29:36","","43","48163","<p>I create <code>requests</code> POST-requests like this, where I specify timeout threshold:</p>

<p><code>response = requests.post(url, data=post_fields, timeout=timeout)</code></p>

<p>However, to determine a ""good"" threshold value, I would like to benchmark the server response time in advance.</p>

<p>How do I compute the minimum and maximum response times for the server?</p>
","1020139","","376535","","2018-10-29 13:48:05","2019-03-31 10:08:24","How to measure server response time for Python requests POST-request?","<python><python-3.x><server><network-programming><python-requests>","2","1","6","","","CC BY-SA 3.0","0"
"34568774","1","34568920","","2016-01-02 17:42:16","","19","48120","<p>I am learning how to read CSV files using Python 3, and have been playing around with my code and have managed to read either the whole document or certain columns, however I am trying to now read only certain records that contain a certain value.</p>

<p>For example I want to read all records where the car is blue, how would I make it read only those records? I can't figure this out and would be grateful for any help or guidance! </p>

<pre><code>import csv

with open('cars.csv') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        print(row['ID'], row['Make'], row['Colour'])
</code></pre>
","5508371","","5007059","","2018-06-13 00:53:59","2018-06-13 00:53:59","Reading a CSV file using Python 3","<python><python-3.x><csv>","3","1","7","","","CC BY-SA 4.0","0"
"34220959","1","34221270","","2015-12-11 10:11:31","","11","48048","<p>I'm a newbie in Django and just started looking at it before a day by installing Django 1.10 on my local.</p>

<p>I've followed all the instructions of this link <a href=""https://docs.djangoproject.com/en/dev/intro/tutorial01/"" rel=""noreferrer"">https://docs.djangoproject.com/en/dev/intro/tutorial01/</a>. However I'm continuously getting this error:</p>

<pre><code>Page not found (404) Request Method:    GET Request URL:    http://127.0.0.1:8000/polls/

Using the URLconf defined in mysite.urls, Django tried these URL patterns, in this order:

    ^admin/

The current URL, polls/, didn't match any of these.
</code></pre>

<p>I've created polls app to getting started.</p>

<p>To make a start, I went with view.py, here is the code:</p>

<blockquote>
  <p>polls/views.py</p>
</blockquote>

<pre><code>from django.shortcuts import render

# Create your views here.

from django.http import HttpResponse


def index(request):
    return HttpResponse(""Hello World!"")
</code></pre>

<p>Here is my code for urls.py:</p>

<blockquote>
  <p>polls/urls.py</p>
</blockquote>

<pre><code>from django.conf.urls import patterns, url

from . import views

urlpatterns = patterns('',
    url(r'^$', views.index, name='index'),
)
</code></pre>

<p>And here is the urls.py in root:</p>

<blockquote>
  <p>mysite/urls.py</p>
</blockquote>

<pre><code>from django.conf.urls import patterns,include, url
from django.contrib import admin

urlpatterns = patterns('',
    url(r'^polls/', include('polls.urls')),
    url(r'^admin/', include(admin.site.urls)),
)
</code></pre>

<p>I've spent good piece of time to find out the solution, found that this question has been asked for many times,so tried with those solutions as well but none of them worked for me.</p>

<p>I can't find out what I'm missing here so please draw my attention to the gap.</p>
","1436588","","5493302","","2015-12-14 19:58:00","2020-02-01 10:56:50","The current URL, app/, didn't match any of these","<python><django><python-3.x><django-urls><django-1.10>","8","5","2","","","CC BY-SA 3.0","0"
"28936140","1","","","2015-03-09 06:14:28","","22","47607","<p>I been working on this for hours and I cant get it right, any help would be appreciated! My question is how do I use the function <code>.readline()</code> to read until the end of a text file? I know that <code>.readlines()</code> work as well but I'm trying to process one line at a time.</p>

<p>Here's what I have for my code so far:</p>

<pre><code>    a = open(""SampleTxt.txt"",""r"")

    While True:

        a.readline()
</code></pre>

<p>My problem is that I get an infinite loop when I run this, shouldn't it have stopped once it couldn't read a line any more?</p>
","4643201","","1252759","","2015-03-09 06:22:10","2015-03-09 06:22:10","Use readline to read txt file python3","<python><python-3.x><readline>","1","0","1","","","CC BY-SA 3.0","0"
"34803040","1","34803087","","2016-01-15 01:42:49","","23","47543","<p>I'm now currently using Python on ubuntu 15.10</p>

<p>But in my OS, I have many different python version installed:</p>

<ul>
<li>Python (2.7.9) </li>
<li>Python3 (3.4.3)</li>
<li>Python3.5</li>
<li>PyPy</li>
</ul>

<p>So, I got mess about the version of their package environment, for example, if I run:</p>

<pre><code>pip3 install django
</code></pre>

<p>In fact I cannot import django inside <code>python3.5</code>.</p>

<p>Is there any efficiently way to call the relating version of <code>pip</code>?</p>

<p><em>PS: Don't suggest that I use virtualenv, I know about it and am seeking another solution.</em></p>
","2544762","","964789","","2017-03-06 09:10:26","2017-03-06 09:10:26","How to run pip of different version of python using python command?","<python><python-3.x><pip><pypi><pypy>","4","7","12","","","CC BY-SA 3.0","0"
"33174804","1","","","2015-10-16 16:00:44","","29","47459","<pre><code>def download_torrent(url):
    fname = os.getcwd() + '/' + url.split('title=')[-1] + '.torrent'
    try:
        schema = ('http:')
        r = requests.get(schema + url, stream=True)
        with open(fname, 'wb') as f:
            for chunk in r.iter_content(chunk_size=1024):
                if chunk:
                    f.write(chunk)
                    f.flush()
    except requests.exceptions.RequestException as e:
        print('\n' + OutColors.LR + str(e))
        sys.exit(1)

    return fname
</code></pre>

<p>In that block of code I am getting an error when I run the full script. When I go to actually download the torrent, I get:</p>

<pre><code>('Connection aborted.', BadStatusLine(""''"",))
</code></pre>

<p>I only posted the block of code that I think is relevant above. The entire script is below. It's from  pantuts, but I  don't think it's maintained any longer, and I  am  trying to get it running  with python3. From my research, the error might mean I'm using http instead of https, but I have tried both.</p>

<p><a href=""https://github.com/pantuts/asskick/blob/master/asskick.py"" rel=""noreferrer"">Original script</a></p>
","5454533","","4907496","","2016-11-06 17:14:05","2020-07-17 20:34:36","Python Requests getting ('Connection aborted.', BadStatusLine(""''"",)) error","<python><python-3.x><python-requests>","3","5","4","","","CC BY-SA 3.0","0"
"43561622","1","43561741","","2017-04-22 16:25:00","","9","47322","<p>I am trying to merge two arrays with the same number of arguments.</p>

<p>Input:</p>

<pre><code>first = [[650001.88, 300442.2,   18.73,  0.575,  650002.094, 300441.668, 18.775],
         [650001.96, 300443.4,   18.7,   0.65,   650002.571, 300443.182, 18.745],
         [650002.95, 300442.54,  18.82,  0.473,  650003.056, 300442.085, 18.745]]

second = [[1],
          [2],
          [3]]
</code></pre>

<p>My expected output:</p>

<pre><code>final = [[650001.88, 300442.2,   18.73,  0.575,  650002.094, 300441.668, 18.775, 1],
             [650001.96, 300443.4,   18.7,   0.65,   650002.571, 300443.182, 18.745, 2],
             [650002.95, 300442.54,  18.82,  0.473,  650003.056, 300442.085, 18.745, 3]]
</code></pre>

<p>To do that i create simple loop:</p>

<pre><code>for i in first:
        for j in second:
            final += np.append(j, i)
</code></pre>

<p>I got i filling that i missing something. First of all my loop i extremely slow. Secondly my data is quite have i got more than 2 mlns rows to loop. So I tried to find faster way for example with this code:</p>

<pre><code>final = [np.append(i, second[0]) for i in first] 
</code></pre>

<p>It working far more faster than previous loop but its appending only first value of second array. 
Can you help me?</p>
","5935388","","3927314","","2017-10-25 19:15:42","2020-06-09 19:26:47","Merge two numpy arrays","<arrays><python-3.x><numpy><merge>","3","0","","","","CC BY-SA 3.0","0"
"31582750","1","31583528","","2015-07-23 09:01:44","","52","47165","<p>Unit testing conn() using mock:</p>
<p>app.py</p>
<pre class=""lang-py prettyprint-override""><code>import mysql.connector
import os, urlparse


def conn():
    if &quot;DATABASE_URL&quot; in os.environ:
        url = urlparse(os.environ[&quot;DATABASE_URL&quot;])
        g.db = mysql.connector.connect(
            user=url.username,
            password=url.password,
            host=url.hostname,
            database=url.path[1:],
        )
    else:
        return &quot;Error&quot;

</code></pre>
<p>test.py</p>
<pre class=""lang-py prettyprint-override""><code>def test_conn(self):
    with patch(app.mysql.connector) as mock_mysql:
        with patch(app.os.environ) as mock_environ:
            con()
            mock_mysql.connect.assert_callled_with(&quot;credentials&quot;)
</code></pre>
<p>Error: <em>Assertion</em> <code>mock_mysql.connect.assert_called_with</code> is not called.</p>
<p>which i believe it is because 'Database_url' is not in my patched os.environ and because of that  test call is not made to mysql_mock.connect.</p>
<p>Questions:</p>
<p>1 what changes i need to make to make this test code work?</p>
<p>2.Do i also have to patch 'urlparse'?</p>
","4963334","","562769","","2020-06-30 15:14:02","2020-09-17 13:46:06","Python mock Patch os.environ and return value","<python><unit-testing><python-3.x><dependency-injection><mocking>","4","0","5","","","CC BY-SA 4.0","0"
"54334304","1","54409674","","2019-01-23 19:24:21","","31","47157","<p>what is difference between <code>spacy.load('en_core_web_sm')</code> and <code>spacy.load('en')</code>? <a href=""https://stackoverflow.com/questions/50487495/what-is-difference-between-en-core-web-sm-en-core-web-mdand-en-core-web-lg-mod"">This link</a> explains different model sizes. But i am still not clear how <code>spacy.load('en_core_web_sm')</code> and <code>spacy.load('en')</code> differ</p>

<p><code>spacy.load('en')</code> runs fine for me. But the <code>spacy.load('en_core_web_sm')</code> throws error</p>

<p>i have installed <code>spacy</code>as below. when i go to jupyter notebook and run command <code>nlp = spacy.load('en_core_web_sm')</code> I get the below error </p>

<pre><code>---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
&lt;ipython-input-4-b472bef03043&gt; in &lt;module&gt;()
      1 # Import spaCy and load the language library
      2 import spacy
----&gt; 3 nlp = spacy.load('en_core_web_sm')
      4 
      5 # Create a Doc object

C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder\lib\site-packages\spacy\__init__.py in load(name, **overrides)
     13     if depr_path not in (True, False, None):
     14         deprecation_warning(Warnings.W001.format(path=depr_path))
---&gt; 15     return util.load_model(name, **overrides)
     16 
     17 

C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder\lib\site-packages\spacy\util.py in load_model(name, **overrides)
    117     elif hasattr(name, 'exists'):  # Path or Path-like to model data
    118         return load_model_from_path(name, **overrides)
--&gt; 119     raise IOError(Errors.E050.format(name=name))
    120 
    121 

OSError: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.
</code></pre>

<p>how I installed Spacy ---</p>

<pre><code>(C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder) C:\Users\nikhizzz&gt;conda install -c conda-forge spacy
Fetching package metadata .............
Solving package specifications: .

Package plan for installation in environment C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder:

The following NEW packages will be INSTALLED:

    blas:           1.0-mkl
    cymem:          1.31.2-py35h6538335_0    conda-forge
    dill:           0.2.8.2-py35_0           conda-forge
    msgpack-numpy:  0.4.4.2-py_0             conda-forge
    murmurhash:     0.28.0-py35h6538335_1000 conda-forge
    plac:           0.9.6-py_1               conda-forge
    preshed:        1.0.0-py35h6538335_0     conda-forge
    pyreadline:     2.1-py35_1000            conda-forge
    regex:          2017.11.09-py35_0        conda-forge
    spacy:          2.0.12-py35h830ac7b_0    conda-forge
    termcolor:      1.1.0-py_2               conda-forge
    thinc:          6.10.3-py35h830ac7b_2    conda-forge
    tqdm:           4.29.1-py_0              conda-forge
    ujson:          1.35-py35hfa6e2cd_1001   conda-forge

The following packages will be UPDATED:

    msgpack-python: 0.4.8-py35_0                         --&gt; 0.5.6-py35he980bc4_3 conda-forge

The following packages will be DOWNGRADED:

    freetype:       2.7-vc14_2               conda-forge --&gt; 2.5.5-vc14_2

Proceed ([y]/n)? y

blas-1.0-mkl.t 100% |###############################| Time: 0:00:00   0.00  B/s
cymem-1.31.2-p 100% |###############################| Time: 0:00:00   1.65 MB/s
msgpack-python 100% |###############################| Time: 0:00:00   5.37 MB/s
murmurhash-0.2 100% |###############################| Time: 0:00:00   1.49 MB/s
plac-0.9.6-py_ 100% |###############################| Time: 0:00:00   0.00  B/s
pyreadline-2.1 100% |###############################| Time: 0:00:00   4.62 MB/s
regex-2017.11. 100% |###############################| Time: 0:00:00   3.31 MB/s
termcolor-1.1. 100% |###############################| Time: 0:00:00 187.81 kB/s
tqdm-4.29.1-py 100% |###############################| Time: 0:00:00   2.51 MB/s
ujson-1.35-py3 100% |###############################| Time: 0:00:00   1.66 MB/s
dill-0.2.8.2-p 100% |###############################| Time: 0:00:00   4.34 MB/s
msgpack-numpy- 100% |###############################| Time: 0:00:00   0.00  B/s
preshed-1.0.0- 100% |###############################| Time: 0:00:00   0.00  B/s
thinc-6.10.3-p 100% |###############################| Time: 0:00:00   5.49 MB/s
spacy-2.0.12-p 100% |###############################| Time: 0:00:10   7.42 MB/s

(C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder) C:\Users\nikhizzz&gt;python -V
Python 3.5.3 :: Anaconda custom (64-bit)

(C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder) C:\Users\nikhizzz&gt;python -m spacy download en
Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0
  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)
    100% |################################| 37.4MB ...
Installing collected packages: en-core-web-sm
  Running setup.py install for en-core-web-sm ... done
Successfully installed en-core-web-sm-2.0.0

    Linking successful
    C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder\lib\site-packages\en_core_web_sm
    --&gt;
    C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder\lib\site-packages\spacy\data\en

    You can now load the model via spacy.load('en')


(C:\Users\nikhizzz\AppData\Local\conda\conda\envs\tensorflowspyder) C:\Users\nikhizzz&gt;
</code></pre>
","2543622","","","","","2020-10-30 12:54:07","spacy Can't find model 'en_core_web_sm' on windows 10 and Python 3.5.3 :: Anaconda custom (64-bit)","<python-3.x><spacy>","13","2","8","","","CC BY-SA 4.0","0"
"33446708","1","33446760","","2015-10-31 00:21:50","","8","47021","<p>I have an array of bools and now I want to swap those entries for numbers.</p>

<pre><code>False =&gt; 0
True =&gt; 1
</code></pre>

<p>I have written two different pieces of code and I would like to know, which one is better and why. This is not so much about actually solving the problem, as about learning.</p>

<pre><code>arr = [[True,False],[False,True],[True,True]]

for i,row in enumerate(arr):
    for j,entry in enumerate(row):
        if entry:
            arr[i][j] = 1
        else:
            arr[i][j] = 0
print(arr)
</code></pre>

<p>And the second approach:</p>

<pre><code>arr = [[True,False],[False,True],[True,True]]

for i in range(len(arr)):
    for j in range(len(arr[i])):
        if arr[i][j]:
            arr[i][j] = 1
        else:
            arr[i][j] = 0    
print(arr)
</code></pre>

<p>I read that there are ways to do this with importing <code>itertools</code> or similar. I am really not a fan of importing things if it can be done with “on-board tools”, but should I rather be using them for this problem?</p>
","5231990","","","user2647463","2015-10-31 00:49:38","2015-10-31 02:05:46","Iterating through array","<python><arrays><python-3.x><swap>","2","2","1","","","CC BY-SA 3.0","0"
"47369737","1","","","2017-11-18 18:44:03","","14","46954","<p>I have added <code>python36/Scripts</code> in the environment variable's path file and python36 as well is added. But it still shows the following error</p>

<p>Command = <code>C:\Users\Sonalika\dev\trydjango1-11&gt;virtualenv -p python3</code></p>

<p>Error I receive:</p>

<blockquote>
  <p>The path python3 (from --python=python3) does not exist</p>
</blockquote>
","8582876","","1149398","","2018-11-29 06:38:35","2019-02-25 06:42:24","""The path python3 (from --python=python3) does not exist"" error","<django><python-3.x><virtualenv>","6","3","7","","","CC BY-SA 4.0","0"
"28977477","1","28977760","","2015-03-11 02:18:07","","9","46932","<p>Getting the error as the title says.
Here is the traceback. I know lst[x] is causing this problem but not too sure how to solve this one. I've searched google + stackoverflow already but did not get the solution I am looking for.</p>

<pre><code>Traceback (most recent call last):
File ""C:/Users/honte_000/PycharmProjects/Comp Sci/2015/2015/storelocation.py"", line 30, in &lt;module&gt;
main()
File ""C:/Users/honte_000/PycharmProjects/Comp Sci/2015/2015/storelocation.py"", line 28, in main
print(medianStrat(lst))
File ""C:/Users/honte_000/PycharmProjects/Comp Sci/2015/2015/storelocation.py"", line 24, in medianStrat
return lst[x]
TypeError: '_io.TextIOWrapper' object is not subscriptable
</code></pre>

<p>Here is the actual code</p>

<pre><code>def medianStrat(lst):
    count = 0
    test = []
    for line in lst:
        test += line.split()
        for i in lst:
            count = count +1
            if count % 2 == 0:
                x = count//2
                y = lst[x]
                z = lst[x-1]
                median = (y + z)/2
                return median
            if count %2 == 1:
                x = (count-1)//2
                return lst[x]     # Where the problem persists

def main():
    lst = open(input(""Input file name: ""), ""r"")
    print(medianStrat(lst))
</code></pre>

<p>So what could be the solution to this problem or what could be done instead to make the code work? ( The main function that the code should do is to open a file and get the median )</p>
","4087219","","","","","2015-03-11 02:50:33","TypeError: '_io.TextIOWrapper' object is not subscriptable","<python><python-2.7><python-3.x><typeerror>","1","0","1","","","CC BY-SA 3.0","0"
"36517137","1","36524004","","2016-04-09 13:03:05","","51","46890","<p>I've just started using <a href=""http://coverage.readthedocs.org/en/latest/index.html"" rel=""noreferrer"">Coverage.py</a> module and so decided to make a simple test to check how it works.</p>

<p><strong>Sample.py</strong></p>

<pre><code>def sum(num1, num2):
    return num1 + num2


def sum_only_positive(num1, num2):
    if num1 &gt; 0 and num2 &gt; 0:
        return num1 + num2
    else:
        return None
</code></pre>

<p><strong>test.py</strong>  </p>

<pre><code>from sample import sum, sum_only_positive

def test_sum():
    assert sum(5, 5) == 10

def test_sum_positive_ok():
    assert sum_only_positive(2, 2) == 4

def test_sum_positive_fail():
    assert sum_only_positive(-1, 2) is None
</code></pre>

<p>As you see, all my code is covered with tests and py.test says all of them pass. I expect Coverage.py to show 100% coverage. Well, no.</p>

<p><a href=""https://i.stack.imgur.com/bA2fO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bA2fO.png"" alt=""Coverage.py results""></a></p>

<p>Well, Coverage.py may not see test.py file, so I copied test functions to <code>sample.py</code> file and ran Coverage again:<br>
<a href=""https://i.stack.imgur.com/ZM4dG.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ZM4dG.png"" alt=""enter image description here""></a></p>

<p>Then I added this block of code:</p>

<pre><code>if __name__ == ""__main__"":
    print(sum(2, 4))
    print(sum_only_positive(2, 4))
    print(sum_only_positive(-1, 3))
</code></pre>

<p>and removed all test functions. After that, Coverage.py shows 100%:</p>

<p><a href=""https://i.stack.imgur.com/MmMqf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MmMqf.png"" alt=""enter image description here""></a></p>

<p>Why is it so? Shouldn't Coverage.py show code test coverage, not just execution coverage? I've read an official <a href=""http://coverage.readthedocs.org/en/latest/faq.html"" rel=""noreferrer"">F.A.Q.</a> for Coverage.py, but can't find the solution.<br>
Since many SO users are familiar with code testing and code coverage, I hope you can tell me, where am I mistaken.</p>

<p><em>I have just one thought here: Coverage.py may simply watch which lines of code aren't executed so I should write tests for those lines. But there're lines which are executed already but aren't covered with tests so Coverage.py will fail here.</em></p>
","2032848","","2032848","","2017-09-17 08:35:41","2018-06-16 13:39:55","How to properly use coverage.py in Python?","<python><unit-testing><python-3.x><coverage.py>","2","3","12","","","CC BY-SA 3.0","0"
"30687244","1","30687293","","2015-06-06 20:25:36","","1","46838","<p>I have the following dictionary:</p>

<pre><code>StudentGrades = {
    'Ivan': [4.32, 3, 2],
    'Martin': [3.45, 5, 6],
    'Stoyan': [2, 5.67, 4],
    'Vladimir': [5.63, 4.67, 6]
}
</code></pre>

<p>I want to make a function that prints the average of the grades of the students, i.e. the average of the values, but I have no idea how. Can you help me please?</p>
","4925905","","1318181","","2015-06-06 20:29:00","2020-04-04 18:13:58","Python 3.4 - How to get the average of dictionary values?","<python><python-3.x><dictionary><average>","3","4","3","2015-06-06 21:09:33","","CC BY-SA 3.0","0"
"43991057","1","","","2017-05-16 01:16:48","","9","46831","<p>I've just been doing some random stuff in Python 3.5.
And with 15 minutes of spare time, I came up with this:</p>

<pre><code>a = {""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", ""h"", ""i"", ""j"", ""k"", ""l"", ""m"", ""n"", ""o"", ""p"", ""q"", ""r"", ""s"", ""t"", ""u"", ""v"", ""w"",
 ""x"", ""y"", ""z""}
len_a = len(a)
list = list(range(0, len_a))
message = """"
wordlist = [ch for ch in message]
len_wl = len(wordlist)
for x in list:
    print (a[x])
</code></pre>

<p>But that satisfying feel of random success did not run over me.
Instead, the feeling of failure did:</p>

<pre><code>Traceback (most recent call last):
File ""/Users/spathen/PycharmProjects/soapy/soup.py"", line 9, in  &lt;module&gt;
print (a[x])
TypeError: 'set' object does not support indexing
</code></pre>

<p>Please help</p>
","7337919","","364696","","2017-05-16 01:24:01","2018-02-07 16:46:04","TypeError: 'set' object does not support indexing","<python><python-3.x><set>","4","2","1","","","CC BY-SA 3.0","0"
"39031796","1","39037466","","2016-08-19 05:39:00","","25","46771","<p>I am very new to Python and I am used to R studio so I choose Spyder. On the Spyder layout I saw a button 'run current line (ctrl +f10)'. But it doesn't work by pressing the button or c+10. Am I missing something? I can only select the script and 'ctrl+enter ' to run current line which is not convenient at all. I am using ubuntu with Anaconda distribution.</p>
","6652264","","","","","2019-07-25 13:35:12","How to run current line in Spyder 3.5( ctrl +f10 not working)","<python-3.x><spyder>","5","0","1","","","CC BY-SA 3.0","0"
"43873663","1","","","2017-05-09 15:15:13","","47","46758","<p>I am trying to install awscli using pip3 on Linux Mint 17.2 Rafaela.</p>

<p>I am getting the error:</p>

<pre><code>Traceback (most recent call last):
  File ""/home/jonathan/.local/bin/aws"", line 19, in &lt;module&gt;
    import awscli.clidriver
ImportError: No module named 'awscli'
</code></pre>

<p>These are the steps I am taking, following the aws installation guide:</p>

<pre><code>sudo pip install awscli --upgrade --user
</code></pre>

<p>everything seems to install fine.</p>

<p>adding to my .bashrc</p>

<pre><code>export PATH=~/.local/bin:$PATH
</code></pre>

<p>then</p>

<pre><code>source ~/.bashrc
</code></pre>

<p>then i try the command</p>

<pre><code>aws --version
</code></pre>

<p>and i get</p>

<pre><code>Traceback (most recent call last):
  File ""/home/jonathan/.local/bin/aws"", line 19, in &lt;module&gt;
    import awscli.clidriver
ImportError: No module named 'awscli'
</code></pre>

<p>Can anyone help with this?</p>

<p>EDIT: For anyone visiting this question. There is no way I can test any of these answers because I have since removed this OS and installed Ubuntu. Also I have no need for awscli anymore.</p>
","3662277","","3662277","","2019-01-30 12:48:12","2020-06-28 17:24:21","awscli fails to work: No module named 'awscli'","<python-3.x><pip><aws-cli>","15","2","2","","","CC BY-SA 4.0","0"
"36077266","1","36077407","","2016-03-18 06:06:00","","54","46684","<p>I use a third-party library that's fine but does not handle inexistant files the way I would like. When giving it a non-existant file, instead of raising the good old </p>

<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 'nothing.txt'
</code></pre>

<p>it raises some obscure message:</p>

<pre><code>OSError: Syntax error in file None (line 1)
</code></pre>

<p>I don't want to handle the missing file, don't want to catch nor handle the exception, don't want to raise a custom exception, neither want I to <code>open</code> the file, nor to create it if it does not exist.</p>

<p>I only want to check it exists (<code>os.path.isfile(filename)</code> will do the trick) and if not, then just raise a proper FileNotFoundError.</p>

<p>I tried this:</p>

<pre><code>#!/usr/bin/env python3

import os

if not os.path.isfile(""nothing.txt""):
    raise FileNotFoundError
</code></pre>

<p>what only outputs:</p>

<pre><code>Traceback (most recent call last):
  File ""./test_script.py"", line 6, in &lt;module&gt;
    raise FileNotFoundError
FileNotFoundError
</code></pre>

<p>This is better than a ""Syntax error in file None"", but how is it possible to raise the ""real"" python exception with the proper message, without having to reimplement it?</p>
","3926735","","","","","2016-03-18 06:34:43","How do I raise a FileNotFoundError properly?","<python><python-3.x><file-not-found>","1","0","7","","","CC BY-SA 3.0","0"
"50056356","1","","","2018-04-27 06:15:02","","35","46632","<p>I got this error when I tried to modify the learning rate parameter of SGD optimizer in Keras. Did I miss something in my codes or my Keras was not installed properly? </p>

<p>Here is my code:</p>

<pre><code>from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Activation
import keras
from keras.optimizers import SGD

model = Sequential()
model.add(Dense(64, kernel_initializer='uniform', input_shape=(10,)))
model.add(Activation('softmax'))
model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics= ['accuracy'])*
</code></pre>

<p>and here is the error message:</p>

<blockquote>
  <p>Traceback (most recent call last):   File
  ""C:\TensorFlow\Keras\ResNet-50\test_sgd.py"", line 10, in 
      model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01), metrics=['accuracy'])   File
  ""C:\Users\nsugiant\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\keras_impl\keras\models.py"",
  line 787, in compile
      **kwargs)   File ""C:\Users\nsugiant\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\keras_impl\keras\engine\training.py"",
  line 632, in compile
      self.optimizer = optimizers.get(optimizer)   File ""C:\Users\nsugiant\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\keras_impl\keras\optimizers.py"",
  line 788, in get
      raise ValueError('Could not interpret optimizer identifier:', identifier) ValueError: ('Could not interpret optimizer identifier:',
  )</p>
</blockquote>
","9708599","","","","","2020-09-06 19:22:03","""Could not interpret optimizer identifier"" error in Keras","<python><python-3.x><tensorflow><keras>","14","0","6","","","CC BY-SA 3.0","0"
"50011733","1","50011910","","2018-04-24 22:47:34","","3","46517","<p>I am trying to open the file from folder and read it but it's not locating it. I am using Python3 </p>

<p>Here is my code: </p>

<pre><code>import os
import glob

prefix_path = ""C:/Users/mpotd/Documents/GitHub/Python-Sample-                
codes/Mayur_Python_code/Question/wx_data/""
target_path = open('MissingPrcpData.txt', 'w')
file_array = [os.path.abspath(f) for f in os.listdir(prefix_path) if 
f.endswith('.txt')]
file_array.sort() # file is sorted list

for f_obj in range(len(file_array)):
     file = os.path.abspath(file_array[f_obj])
     join_file = os.path.join(prefix_path, file) #whole file path

for filename in file_array:
     log = open(filename, 'r')#&lt;---- Error is here
</code></pre>

<p><code>Error: FileNotFoundError: [Errno 2] No such file or directory: 'USC00110072.txt'</code></p>
","9058847","","2146491","","2018-04-24 23:40:02","2018-09-12 09:32:45","Python error: FileNotFoundError: [Errno 2] No such file or directory","<python><python-3.x><file>","2","0","5","","","CC BY-SA 3.0","0"
"42798967","1","42799036","","2017-03-15 00:19:24","","15","46372","<p>Basically, if I have a string <code>'AJ'</code> and another string <code>'AJYF'</code>, I would like to be able to write <code>'AJYF'-'AJ'</code> and get <code>'YF'</code>.</p>

<p>I tried this but got a syntax error.</p>

<p>Just on a side note the subtractor will always will be shorter than the string it is subtracted from. Also, the subtractor will always be like the string it is subtracted from. For instance, if I have 'GTYF' and I want to subtract a string of length 3 from it, that string has to be 'GTY'.</p>

<p>If it is possible, the full function I am trying to do is convert a string to a list based on how long each item in the list is supposed to be. Is there any way of doing that?</p>
","7454274","","7454274","","2019-09-29 02:18:17","2019-09-29 02:34:00","How to subtract strings in python","<python><python-3.x><string><bioinformatics>","4","13","1","","","CC BY-SA 3.0","0"
"40324356","1","40324928","","2016-10-29 22:35:17","","41","46276","<p>I'm trying to use argparse in a Python 3 application where there's an explicit list of choices, but a default if none are specified.</p>

<p>The code  I have is:</p>

<pre><code>parser.add_argument('--list', default='all', choices=['servers', 'storage', 'all'], help='list servers, storage, or both (default: %(default)s)') 
args = parser.parse_args()
print(vars(args))
</code></pre>

<p>However, when I run this I get the following with an option:</p>

<pre><code>$ python3 ./myapp.py --list all
{'list': 'all'}
</code></pre>

<p>Or without an option:</p>

<pre><code>$ python3 ./myapp.py --list
usage: myapp.py [-h] [--list {servers,storage,all}]
myapp.py: error: argument --list: expected one argument
</code></pre>

<p>Am I missing something here? Or can I not have a default with choices specified?</p>
","2930436","","-1","","2017-11-13 16:03:48","2017-11-13 16:03:48","python argparse choices with a default choice","<python><python-3.x><argparse>","2","3","3","","","CC BY-SA 3.0","0"
"45623403","1","45623466","","2017-08-10 21:00:52","","4","46252","<p>A very basic question.</p>

<p>I am trying to install tensorflow library in Anaconda python(spyder).</p>

<pre><code>import tf.contrib.keras.preprocessing
</code></pre>

<p>Its is giving me error as <code>""No module found"".</code> I also tried import tensorflow.contrib.keras.preprocessing</p>

<p>I also tried
<code>from tf.contrib.keras.preprocessing.text import Tokenizer</code>. </p>

<p>This also doesnt work</p>

<p>However I verified this in the tensorflow website, and it is present. 
The link to the library is <a href=""https://www.tensorflow.org/api_docs/python/tf/contrib/keras/preprocessing"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/contrib/keras/preprocessing</a>.</p>

<p>I tried to pip and conda install. But that also throwing out error.</p>

<p>From anaconda prompt i typed this: </p>

<pre><code>activate tensorflow 
pip install tf.contrib.keras.preprocessing
conda install tf.contrib.keras.preprocessing
</code></pre>

<p>Is there anything i miss out, Please correct me. </p>
","8241998","","8241998","","2017-08-10 21:12:34","2017-08-11 04:18:51","How to import the Tensorflow libraries in python?","<python><python-3.x><tensorflow><anaconda>","2","9","1","","","CC BY-SA 3.0","0"
"51960857","1","","","2018-08-22 06:21:11","","8","46182","<pre><code>a1=[1,2,3,4,5,6]  
b1=[[1,2,3], [4,5,6]]
</code></pre>

<p>If using <code>np.shape</code> list <code>a1</code> will return <code>(6,)</code> and <code>b1</code> will return <code>(2, 3)</code>.</p>

<p>If Numpy is forbidden, how can I get the shape of list <code>a1</code>?</p>

<p>I am mainly confused about how can I let the python program know <code>a1</code> is only one dimension. Is there any good method?</p>
","10258015","","2988730","","2018-08-22 07:41:29","2020-08-29 17:52:22","How can I get a list shape without using numpy?","<python><python-3.x>","4","7","4","","","CC BY-SA 4.0","0"
"35533803","1","35706103","","2016-02-21 08:12:12","","47","46069","<p>When I use <code>cx_Freeze</code> I get a keyerror <code>KeyError: 'TCL_Library'</code>while building my pygame program. Why do I get this and how do I fix it?</p>

<p>My setup.py is below:</p>

<pre><code>from cx_Freeze import setup, Executable

setup(
    name = ""Snakes and Ladders"",
    version = ""0.9"",
    author = ""Adam"",
    author_email = ""Omitted"",
    options = {""build_exe"": {""packages"":[""pygame""],
                         ""include_files"": [""main.py"", ""squares.py"",
                         ""pictures/Base Dice.png"", ""pictures/Dice 1.png"",
                         ""pictures/Dice 2.png"", ""pictures/Dice 3.png"",
                         ""pictures/Dice 4.png"", ""pictures/Dice 5.png"",
                         ""pictures/Dice 6.png""]}},
    executables = [Executable(""run.py"")],
    )
</code></pre>
","5759134","","1000551","","2017-10-16 09:38:38","2019-08-17 09:30:19","KeyError: 'TCL_Library' when I use cx_Freeze","<python><python-3.x><cx-freeze>","7","0","15","","","CC BY-SA 3.0","0"
"47357090","1","47357363","","2017-11-17 18:31:36","","14","46020","<p>I'm trying to put a jpg image to a tkinter canvas. tkinter gives me this error:</p>

<blockquote>
  <p>couldn't recognize data in image file</p>
</blockquote>

<p>I use the code from the documentation:</p>

<pre><code>canv = Canvas(root, width=80, height=80, bg='white')
canv.grid(row=2, column=3)

img = PhotoImage(file=""bll.jpg"")
canv.create_image(20,20, anchor=NW, image=img)
</code></pre>

<p>Same thing with png images. Even tried to put an image into a label widget, but got the same error. What's wrong?</p>

<p>I am using Python 3 on Mac. Python file and image are in the same folder.</p>
","8739002","","5276734","","2017-11-17 18:50:24","2020-09-16 21:30:24","Tkinter error: Couldn't recognize data in image file","<python><macos><python-3.x><canvas><tkinter>","5","1","2","","","CC BY-SA 3.0","0"
"40955903","1","","","2016-12-04 06:00:24","","5","45935","<p>This might be a simple question. However, I wanted to get some clarifications of how the following code works.</p>

<pre><code>a = np.arange(8)
a
array([1,2,3,4,5,6,7])
Example Function = a[0:-1]+a[1:]/2.0
</code></pre>

<p>In the Example Function, I want to draw your attention to the plus sign between the array <code>a[0:-1]+a[1:]</code>. How does that work? What does that look like?</p>

<p>For instance, is the plus sign (addition) adding the first index of each array? (e.g <code>1+2</code>) or add everything together? (e.g <code>1+2+2+3+3+4+4+5+5+6+6+7</code>)</p>

<p>Then, I assume <code>/2.0</code> is just dividing it by <code>2</code>...</p>
","6646128","","4298200","","2019-11-11 02:07:22","2019-11-11 02:07:22","Simple adding two arrays using numpy in python?","<python-3.x><numpy>","2","0","","","","CC BY-SA 4.0","0"
"52874240","1","52874319","","2018-10-18 12:38:13","","32","45856","<p>I am getting an error on the following line in my Jupyter Notebook</p>

<pre><code>% matplotlib inline
</code></pre>

<p>I am using Python version 3.7, ipython version 7.0.1</p>
","2731627","","4873295","","2019-06-20 04:17:15","2019-06-20 04:17:15","UsageError: Line magic function `%` not found. Jupyter Notebook","<python-3.x><jupyter-notebook><ipython>","2","0","4","","","CC BY-SA 4.0","0"
"46975929","1","47016862","","2017-10-27 13:14:02","","13","45766","<p>I have two lists with usernames and I want to calculate the Jaccard similarity. Is it possible? </p>

<p><a href=""https://stackoverflow.com/questions/11911252/python-jaccard-distance-using-word-intersection-but-not-character-intersection"">This</a> thread shows how to calculate the Jaccard Similarity between two strings, however I want to apply this to two lists, where each element is one word (e.g., a username). </p>
","873309","","","","","2020-10-27 13:01:26","How can I calculate the Jaccard Similarity of two lists containing strings in Python?","<python><python-3.x><similarity>","7","0","4","","","CC BY-SA 3.0","0"
"53348959","1","53349002","","2018-11-17 06:52:21","","36","45751","<p>This seems trivial, but I cannot find a built-in or simple way to determine if two dictionaries are equal.</p>

<p>What I want is:</p>

<pre><code>a = {'foo': 1, 'bar': 2}
b = {'foo': 1, 'bar': 2}
c = {'bar': 2, 'foo': 1}
d = {'foo': 2, 'bar': 1}
e = {'foo': 1, 'bar': 2, 'baz':3}
f = {'foo': 1}

equal(a, b)   # True 
equal(a, c)   # True  - order does not matter
equal(a, d)   # False - values do not match
equal(a, e)   # False - e has additional elements
equal(a, f)   # False - a has additional elements
</code></pre>

<p>I could make a short looping script, but I cannot imagine that mine is such a unique use case.</p>
","5464355","","4002633","","2020-03-17 12:20:10","2020-03-17 12:20:10","Python3 Determine if two dictionaries are equal","<python><python-3.x><dictionary><equality>","2","0","2","2018-11-17 11:09:28","","CC BY-SA 4.0","0"
"42863505","1","43045244","","2017-03-17 17:14:23","","22","45356","<p>I have installed PyQt5 on windows platform and and getting an importError: DLL load failed. </p>

<p>I have installed pyqt5 using the command  </p>

<pre><code>pip3 install pyqt5
Successfully installed pyqt5-5.8.1
</code></pre>

<p>My Python version is as follows: </p>

<pre><code>Python 3.5.2 |Anaconda custom (64-bit)| (default, Jul  5 2016, 11:41:13) [MSC v.1900 64 bit (AMD64)] on win32
</code></pre>

<p>The import Error is as follows:</p>

<pre><code>from PyQt5.QtWidgets import QApplication
Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: DLL load failed: The specified module could not be found.
</code></pre>

<p>Thanks &amp; Regards</p>
","7728359","","","","","2020-10-25 15:58:57","DLL load failed when importing PyQt5","<python><python-3.x><dllimport><pyqt5><importerror>","13","7","11","","","CC BY-SA 3.0","0"
"34852104","1","","","2016-01-18 10:11:45","","24","45221","<p>In a slack team, can we send a message to a user using python?
I have seen various APIs, they offer message to channel, but not to a particular user. Can we do that?</p>
","5729519","","4720629","","2016-01-18 10:18:32","2019-12-05 14:13:25","Can we send message to user in slack using python script?","<python><python-3.x><slack-api><slack>","6","0","2","","","CC BY-SA 3.0","0"
"45964751","1","","","2017-08-30 15:47:08","","2","45208","<p>I am trying to using <code>opencv-3.3.0</code>, <code>cv2</code> &amp; <code>python3.5</code>.</p>

<p>But, I can't seem to show image I have captured. </p>

<ul>
<li>I read all the documentation.  </li>
<li>I tried all possible answers. </li>
<li>But, I still unsuccessful.</li>
</ul>

<p>What am I missing?</p>

<h1>Code:</h1>

<pre><code>import numpy as np
import cv2

img=cv2.imread(""F:/Train/sreen.png"")
cv2.imshow('image',img)
cv2.waitKey(0)
cv2.destroyAllWindows()
</code></pre>

<h1>The Error:</h1>

<pre><code>OpenCV Error: Assertion failed (size.width&gt;0 &amp;&amp; size.height&gt;0) in cv::imshow, file D:\Build\OpenCV\opencv-3.3.0\moules\highgui\src\window.cpp, line 333 
Traceback (most recent call last):
File ""F:\IQ_option\OpenCV\run.py"", line 5, in &lt;module&gt;
    cv2.imshow('image',img)
cv2.error: D:\Build\OpenCV\opencv-3.3.0\modules\highgui\src\window.cpp:333: error: (-215) size.width&gt;0 &amp;&amp; size.height&gt;0 in function cv::imshow
</code></pre>

<h1>The image:</h1>

<p><a href=""https://i.stack.imgur.com/W5H3X.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W5H3X.jpg"" alt=""My Code.""></a></p>
","8538945","","1896134","","2018-09-04 06:04:32","2020-05-07 07:32:51","Python3 - OpenCV & cv2.error: (-215) Unable to show captured image","<python-3.x><opencv3.0><cv2>","6","2","3","","","CC BY-SA 4.0","0"
"38534154","1","38536620","","2016-07-22 19:40:51","","19","45086","<p>I am trying to connect to an Microsoft Azure SQL server database. </p>

<p>This is how i am trying to connect:</p>

<pre><code> conn = pyodbc.connect('DRIVER={SQL Server};SERVER=%s' % (self.config.get(""Sql"", ""DataSource"")),
                        user= self.config.get(""Sql"", ""UserId""),
                        password=self.config.get(""Sql"", ""Password""),
                        database=self.config.get(""Sql"", ""Catalog""))
</code></pre>

<p>I am getting an error while excuting this line. The error:</p>

<pre><code>pyodbc.Error: ('01000', ""[01000] [unixODBC][Driver Manager]Can't open lib 'SQL Server' : file not found (0) (SQLDriverConnect)"")
</code></pre>

<p>Can't figure why this is happening, Any idea?</p>
","4869599","","2094563","","2016-07-23 00:45:34","2019-12-01 11:07:05","Linux python3 - Can't open lib 'SQL Server'","<sql-server><linux><python-3.x><azure><azure-sql-database>","4","1","6","","","CC BY-SA 3.0","0"
"43942185","1","43943029","","2017-05-12 16:10:56","","9","44976","<p>I'm trying to install Tensorflow in my PC, i installed Python 3.5.2 64-bit, cuda_8.0.61 for windows 10 and cudnn-8.0-windows10-x64-v6.0</p>

<p>I used ""native"" pip to install GPU version of Tensorflow, then i open IDLE and testing with ""import tensorflow as tf"" and i got the error bellow</p>

<h2>How can i fix this ? Thanks you in advance &lt;3</h2>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 666, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 577, in module_from_spec
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 906, in create_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in &lt;module&gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in &lt;module&gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""&lt;pyshell#0&gt;"", line 1, in &lt;module&gt;
    import tensorflow as tf
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in &lt;module&gt;
    from tensorflow.python import *
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 51, in &lt;module&gt;
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 52, in &lt;module&gt;
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 18, in swig_import_helper
    return importlib.import_module(mname)
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 666, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 577, in module_from_spec
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 906, in create_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 41, in &lt;module&gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 21, in &lt;module&gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 20, in swig_import_helper
    return importlib.import_module('_pywrap_tensorflow_internal')
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python35\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ImportError: No module named '_pywrap_tensorflow_internal'


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
</code></pre>
","8003861","","","","","2020-10-10 07:01:02","Failed to load the native TensorFlow runtime. Python 3.5.2","<python><python-3.x><tensorflow><pip>","8","1","8","","","CC BY-SA 3.0","0"
"46499808","1","","","2017-09-30 05:19:06","","96","44904","<p>I am using the latest version of Anaconda3. I just installed it and I am trying to download some packages. I am using the Anaconda Prompt. While trying to use pip to do anything (including upgrading existing packages) I get the following traceback.</p>

<pre><code>    Exception:
Traceback (most recent call last):
  File ""C:\Users\csprock\Anaconda3\lib\site-packages\pip\basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""C:\Users\csprock\Anaconda3\lib\site-packages\pip\commands\install.py"", line 335, in run
    wb.build(autobuilding=True)
  File ""C:\Users\csprock\Anaconda3\lib\site-packages\pip\wheel.py"", line 749, in build
    self.requirement_set.prepare_files(self.finder)
  File ""C:\Users\csprock\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""C:\Users\csprock\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 487, in _prepare_file
    req_to_install, finder)
  File ""C:\Users\csprock\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 428, in _check_skip_installed
    req_to_install, upgrade_allowed)
  File ""C:\Users\csprock\Anaconda3\lib\site-packages\pip\index.py"", line 465, in find_requirement
    all_candidates = self.find_all_candidates(req.name)
  File ""C:\Users\csprock\Anaconda3\lib\site-packages\pip\index.py"", line 423, in find_all_candidates
    for page in self._get_pages(url_locations, project_name):
  File ""C:\Users\csprock\Anaconda3\lib\site-packages\pip\index.py"", line 568, in _get_pages
    page = self._get_page(location)
  File ""C:\Users\csprock\Anaconda3\lib\site-packages\pip\index.py"", line 683, in _get_page
    return HTMLPage.get_page(link, session=self.session)
  File ""C:\Users\csprock\Anaconda3\lib\site-packages\pip\index.py"", line 811, in get_page
    inst = cls(resp.content, resp.url, resp.headers)
  File ""C:\Users\csprock\Anaconda3\lib\site-packages\pip\index.py"", line 731, in __init__
    namespaceHTMLElements=False,
TypeError: parse() got an unexpected keyword argument 'transport_encoding'
</code></pre>

<p>Any ideas? (this problem only started after I installed tensorflow) Thanks. </p>
","7971702","","364696","","2017-10-03 01:39:26","2019-10-16 04:34:11","pip throws TypeError: parse() got an unexpected keyword argument 'transport_encoding' when trying to install new packages","<python><python-3.x><pip><anaconda>","7","6","13","","","CC BY-SA 3.0","0"
"43403992","1","43404043","","2017-04-14 01:26:46","","3","44726","<p>Hi I'm stuck with the above error.
The problem come out with the last function ""get_student_average"".
If ""results"" store the ""get_average(student)"" value, why it can't give me back the result of ""get_letter_grade(results)""??</p>

<pre><code>lloyd = {
    ""name"": ""Lloyd"",
    ""homework"": [90.0, 97.0, 75.0, 92.0],
    ""quizzes"": [88.0, 40.0, 94.0],
    ""tests"": [75.0, 90.0]
}
alice = {
    ""name"": ""Alice"",
    ""homework"": [100.0, 97.0, 98.0, 100.0],
    ""quizzes"": [98.0, 99.0, 99.0],
    ""tests"": [100.0, 100.0]
}
tyler = {
    ""name"": ""Tyler"",
    ""homework"": [0.0, 35.0, 45.0, 22.0],
    ""quizzes"": [0.0, 60.0, 58.0],
    ""tests"": [65.0, 58.0]
}
students = [lloyd,alice,tyler]
def average(numbers):
    total= sum(numbers)
    total = float(total)
    return total / len(numbers) 

def get_average(student):
    homework= average(student[""homework""])
    quizzes= average(student[""quizzes""])
    tests= average(student[""tests""])
    return 0.1 * homework + 0.3 * quizzes + 0.6 * tests

def get_letter_grade(score):
    if score &gt;= 90:
        return ""A""
    elif score &gt;= 80:
        return ""B""
    elif score &gt;= 70:
        return ""C""
    elif score &gt;= 60:
        return ""D""
    else:
        return ""F""

def get_student_average(gruppo):
    for student in gruppo:
        results= []
        results.append(get_average(student))
        print (student[""name""])
        print (results)
        print (get_letter_grade(results))

get_student_average(students)
</code></pre>
","7697476","","","","","2017-04-14 01:43:01","TypeError: '>=' not supported between instances of 'list' and 'int'","<python><python-3.x>","2","2","0","","","CC BY-SA 3.0","0"
"40557335","1","40559005","","2016-11-11 22:41:09","","11","44696","<p>I have searched many times online and I have not been able to find a way to convert my binary string variable, <strong><em>X</em></strong></p>

<pre><code>X = ""1000100100010110001101000001101010110011001010100""
</code></pre>

<p>into a UTF-8 string value.</p>

<p>I have found that some people are using methods such as</p>

<pre><code>b'message'.decode('utf-8')
</code></pre>

<p>however, this method has not worked for me, as 'b' is said to be nonexistent, and I am not sure how to replace the 'message' with a variable. Not only, but I have not been able to comprehend how this method works. Is there a better alternative?</p>

<p>So how could I convert a binary string into a text string?</p>

<p>EDIT: I also do not mind ASCII decoding</p>

<p>CLARIFICATION: Here is specifically what I would like to happen.</p>

<pre><code>def binaryToText(z):
    # Some code to convert binary to text
    return (something here);
X=""0110100001101001""
print binaryToText(X)
</code></pre>

<p>This would then yield the string...</p>

<pre><code>hi
</code></pre>
","4535352","","5446749","","2019-05-24 12:04:06","2019-05-24 12:04:06","Binary to String/Text in Python","<python><python-2.7><python-3.x><text><binary>","4","7","5","","","CC BY-SA 4.0","0"
"40705614","1","40707115","","2016-11-20 15:08:27","","25","44538","<p>I can clear the text of the xlabel in a Pandas plot with:</p>

<pre><code>plt.xlabel("""")
</code></pre>

<p>Instead, is it possible to hide the label?</p>

<p>May be something like <code>.xaxis.label.set_visible(False)</code>.</p>
","5082463","","","","","2020-04-15 15:21:53","Hide axis label only, not entire axis, in Pandas plot","<python><python-2.7><python-3.x><pandas>","1","1","8","","","CC BY-SA 3.0","1"
"46045956","1","46049195","","2017-09-05 01:52:52","","65","44361","<p>Whats the difference between <code>ThreadPool</code> and <code>Pool</code> in <code>multiprocessing</code> module.  When I try my code out, this is the main difference I see:</p>

<pre><code>from multiprocessing import Pool
import os, time

print(""hi outside of main()"")

def hello(x):
    print(""inside hello()"")
    print(""Proccess id: "", os.getpid())
    time.sleep(3)
    return x*x

if __name__ == ""__main__"":
    p = Pool(5)
    pool_output = p.map(hello, range(3))

    print(pool_output)
</code></pre>

<p>I see the following output:</p>

<pre><code>hi outside of main()
hi outside of main()
hi outside of main()
hi outside of main()
hi outside of main()
hi outside of main()
inside hello()
Proccess id:  13268
inside hello()
Proccess id:  11104
inside hello()
Proccess id:  13064
[0, 1, 4]
</code></pre>

<p>With ""ThreadPool"":</p>

<pre><code>from multiprocessing.pool import ThreadPool
import os, time

print(""hi outside of main()"")

def hello(x):
    print(""inside hello()"")
    print(""Proccess id: "", os.getpid())
    time.sleep(3)
    return x*x

if __name__ == ""__main__"":
    p = ThreadPool(5)
    pool_output = p.map(hello, range(3))

    print(pool_output)
</code></pre>

<p>I see the following output:</p>

<pre><code>hi outside of main()
inside hello()
inside hello()
Proccess id:  15204
Proccess id:  15204
inside hello()
Proccess id:  15204
[0, 1, 4]
</code></pre>

<p>My questions are:</p>

<ul>
<li><p>why is the “outside __main__()” run each time in the <code>Pool</code>?</p></li>
<li><p><code>multiprocessing.pool.ThreadPool</code> doesn't spawn new processes?  It just creates new threads?  </p></li>
<li><p>If so whats the difference between using <code>multiprocessing.pool.ThreadPool</code> as opposed to just <code>threading</code> module?  </p></li>
</ul>

<p>I don't see any official documentation for <code>ThreadPool</code> anywhere, can someone help me out where I can find it?</p>
","1515997","","355230","","2020-03-08 17:32:13","2020-03-08 17:32:13","What's the difference between ThreadPool vs Pool in the multiprocessing module?","<python><python-3.x><multiprocessing><threadpool><python-multiprocessing>","1","4","27","","","CC BY-SA 4.0","0"
"42602059","1","","","2017-03-04 22:20:16","","11","44308","<p>I have tried using <code>pip -m install win32api</code>, but I still get the error ""<strong><em>can't open file 'pip': [Errno 2] No such file or directory""</em></strong></p>

<p>Can anyone help me on this?</p>

<p><strong>Note</strong>: I have renamed the python.exe file as python2 and python3, since I have both versions installed on my pc.</p>
","5718950","","3154274","","2017-09-11 00:25:42","2019-08-30 05:13:23","pip install gives me this error ""can't open file 'pip': [Errno 2] No such file or directory""","<python><python-2.7><python-3.x><remote-access>","3","2","1","","","CC BY-SA 3.0","0"
"51710037","1","51710151","","2018-08-06 14:33:53","","83","44270","<p>I'm trying to understand how to use the <code>Optional</code> type hint. From <a href=""https://www.python.org/dev/peps/pep-0484/"" rel=""noreferrer"">PEP-484</a>, I know I can use <code>Optional</code> for <code>def test(a: int = None)</code> either as <code>def test(a: Union[int, None])</code> or <code>def test(a: Optional[int])</code>. </p>

<p>But how about following examples?</p>

<pre><code>def test(a : dict = None):
    #print(a) ==&gt; {'a': 1234}
    #or
    #print(a) ==&gt; None

def test(a : list = None):
    #print(a) ==&gt; [1,2,3,4, 'a', 'b']
    #or
    #print(a) ==&gt; None
</code></pre>

<p>If <code>Optional[type]</code> seems to mean the same thing as <code>Union[type, None]</code>, why should I use <code>Optional[]</code> at all?</p>
","6373357","","6862601","","2020-04-05 09:07:20","2020-08-16 22:07:13","How should I use the Optional type hint?","<python><python-3.x><type-hinting>","2","0","12","","","CC BY-SA 4.0","0"
"43069780","1","43070301","","2017-03-28 12:36:59","","34","44222","<p>I am using <strong>python 2.7</strong> + <strong>virtualenv version 1.10.1</strong> for running myproject projects. Due to some other projects requirement I have to work with other version of python(<strong>Python 3.5</strong>) and <strong>Django 1.9</strong>. For this I have installed python in my user directory. Also I have dowloaded and installed virtualenv( <strong>version - 15.1.0</strong>) into my user directory. 
But whenever I am trying to create virtual env I am getting the below error</p>

<pre><code>python virtualenv/virtualenv.py myproject
</code></pre>

<hr>

<pre><code>Using base prefix '/home/myuser/python3'
New python executable in /home/mount/myuser/project_python3/myproject/bin/python
ERROR: The executable /home/mount/myuser/project_python3/myproject/bin/python is not functioning
ERROR: It thinks sys.prefix is '/home/myuser/python3' (should be '/home/mount/myuser/project_python3/myproject')
ERROR: virtualenv is not compatible with this system or executable
</code></pre>

<p>Can anybody tell what I am doing wrong with this </p>
","699010","","","","","2020-08-16 23:34:36","How to create virtual env with python3","<python><django><python-3.x><virtualenv>","8","10","8","","","CC BY-SA 3.0","0"
"38022658","1","38023345","","2016-06-24 21:56:48","","11","44111","<p>I am writing automation test in Selenium using Python. One element may or may not be present. I am trying to handle it with below code, it works when element is present. But script fails when element is not present, I want to continue to next statement if element is not present.</p>

<pre><code>try:
       elem = driver.find_element_by_xpath("".//*[@id='SORM_TB_ACTION0']"")
       elem.click()
except nosuchelementexception:
       pass
</code></pre>

<p>Error - </p>

<pre><code>selenium.common.exceptions.NoSuchElementException: Message: Unable to locate element:{""method"":""xpath"",""selector"":"".//*[@id='SORM_TB_ACTION0']""}
</code></pre>
","6510843","","5409601","","2016-07-01 02:23:15","2019-10-25 13:20:25","Selenium Python - Handling No such element exception","<python><python-3.x><selenium><selenium-webdriver>","5","0","2","","","CC BY-SA 3.0","0"
"27849412","1","27849723","","2015-01-08 21:00:39","","30","43931","<p>This is the error when I try to get anything with pip3
I'm not sure what to do</p>

<pre><code>Exception:
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/pip/basecommand.py"", line 122, in main
    status = self.run(options, args)
  File ""/usr/lib/python3/dist-packages/pip/commands/install.py"", line 283, in run
    requirement_set.install(install_options, global_options, root=options.root_path)
  File ""/usr/lib/python3/dist-packages/pip/req.py"", line 1435, in install
    requirement.install(install_options, global_options, *args, **kwargs)
  File ""/usr/lib/python3/dist-packages/pip/req.py"", line 671, in install
    self.move_wheel_files(self.source_dir, root=root)
  File ""/usr/lib/python3/dist-packages/pip/req.py"", line 901, in move_wheel_files
    pycompile=self.pycompile,
  File ""/usr/lib/python3/dist-packages/pip/wheel.py"", line 206, in move_wheel_files
    clobber(source, lib_dir, True)
  File ""/usr/lib/python3/dist-packages/pip/wheel.py"", line 193, in clobber
    os.makedirs(destsubdir)
  File ""/usr/lib/python3.4/os.py"", line 237, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '/usr/local/lib/python3.4/dist-                        packages/Django-1.7.2.dist-info'

Storing debug log for failure in /home/omega/.pip/pip.log
</code></pre>
","4358680","","113962","","2015-01-08 21:30:39","2019-03-04 08:43:12","PermissionError with pip3","<python><django><ubuntu><python-3.x><pip>","5","0","8","","","CC BY-SA 3.0","0"
"39233973","1","39234024","","2016-08-30 17:47:55","","15","43915","<p>I have the below code which currently just prints the values of the initial dictionary. However I would like to iterate through every key of the nested dictionary to initially just print the names. Please see my code below:</p>

<pre><code>Liverpool = {
    'Keepers':{'Loris Karius':1,'Simon Mignolet':2,'Alex Manninger':3},
    'Defenders':{'Nathaniel Clyne':3,'Dejan Lovren':4,'Joel Matip':5,'Alberto Moreno':6,'Ragnar Klavan':7,'Joe Gomez':8,'Mamadou Sakho':9}
}

for k,v in Liverpool.items():
    if k =='Defenders':
       print(v)
</code></pre>
","4859229","","7579547","","2018-08-14 07:13:29","2018-08-14 07:14:24","Get all keys of a nested dictionary","<python><python-3.x><dictionary><nested>","3","3","7","2016-08-30 17:59:36","","CC BY-SA 4.0","0"
"32615440","1","32616433","","2015-09-16 17:52:57","","3","43773","<p>Im trying to make it so that when the user clicks a button, it becomes ""X"" or ""0"" (Depending on their team). How can I make it so that the text on the button is updated? My best idea so far has been to delete the buttons then print them again, but that only deletes one button. Here's what I have so far:</p>

<pre><code>from tkinter import *

BoardValue = [""-"",""-"",""-"",""-"",""-"",""-"",""-"",""-"",""-""]

window = Tk()
window.title(""Noughts And Crosses"")
window.geometry(""10x200"")

v = StringVar()
Label(window, textvariable=v,pady=10).pack()
v.set(""Noughts And Crosses"")

def DrawBoard():
    for i, b in enumerate(BoardValue):
        global btn
        if i%3 == 0:
            row_frame = Frame(window)
            row_frame.pack(side=""top"")
        btn = Button(row_frame, text=b, relief=GROOVE, width=2, command = lambda: PlayMove())
        btn.pack(side=""left"")

def PlayMove():
    BoardValue[0] = ""X""
    btn.destroy()
    DrawBoard()

DrawBoard()
window.mainloop()
</code></pre>
","5339944","","3650983","","2018-11-06 09:46:20","2020-04-03 17:47:42","Python 3, Tkinter, How to update button text","<python><python-3.x><tkinter>","7","0","3","","","CC BY-SA 3.0","0"
"39323050","1","","","2016-09-05 02:02:25","","5","43763","<p>I'm using python 3.5.1. When I was trying this</p>

<pre><code>print(r'\t\\\')
</code></pre>

<p>I got the error: SyntaxError: EOL while scanning string literal.
But this one worked well</p>

<pre><code>print(r'\t\\')
</code></pre>

<p>Can anyone please explain this?</p>
","6794608","","2588818","","2016-09-05 03:15:52","2020-08-25 03:17:37","In python SyntaxError: EOL while scanning string literal","<python-3.x>","2","1","0","","","CC BY-SA 3.0","0"
"43134753","1","43135194","","2017-03-31 07:18:23","","36","43763","<p>I am running TensorFlow for the first time and using some example code. I got this error when running my code. Does anybody know why this happened, and how to fix it? Thanks!    </p>

<pre><code>2017-03-31 02:12:59.346109: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346968: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346975: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow libbrary wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346979: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346983: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346987: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346991: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-03-31 02:12:59.346995: W c:\tf_jenkins\home\workspace\release-win\device\cpu\os\windows\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
</code></pre>
","7777877","","3574081","","2017-04-17 23:37:38","2018-04-03 05:16:30","TensorFlow wasn't compiled to use SSE (etc.) instructions, but these are available","<python><python-3.x><tensorflow>","3","7","10","","","CC BY-SA 3.0","0"
"44239822","1","44239906","","2017-05-29 10:07:37","","21","43614","<p>I've been playing with beautiful soup and parsing web pages for a few days. I have been using a line of code which has been my saviour in all the scripts that I write. The line of code is : </p>

<pre><code>r = requests.get('some_url', auth=('my_username', 'my_password')).
</code></pre>

<p>BUT ...</p>

<p>I want to do the same thing with (OPEN A URL WITH AUTHENTICATION):</p>

<pre><code>(1) sauce = urllib.request.urlopen(url).read() (1)
(2) soup = bs.BeautifulSoup(sauce,""html.parser"") (2)
</code></pre>

<p>I'm not able to open a url and read, the webpage which needs authentication. 
How do I achieve something like this :</p>

<pre><code>  (3) sauce = urllib.request.urlopen(url, auth=(username, password)).read() (3) 
instead of (1)
</code></pre>
","","user7800892","","","","2020-07-08 15:25:26","urllib.request.urlopen(url) with Authentication","<python><python-3.x><url><beautifulsoup><request>","3","0","6","","","CC BY-SA 3.0","0"
"37835179","1","39624147","","2016-06-15 12:14:51","","157","43546","<p>I want to use type hints in my current Python 3.5 project. My function should receive a function as parameter. </p>

<p><strong>How can I specify the type function in my type hints?</strong></p>

<pre><code>import typing

def my_function(name:typing.AnyStr, func: typing.Function) -&gt; None:
    # However, typing.Function does not exist.
    # How can I specify the type function for the parameter `func`?

    # do some processing
    pass
</code></pre>

<p>I checked <a href=""https://www.python.org/dev/peps/pep-0483/"" rel=""noreferrer"">PEP 483</a>, but could not find a function type hint there.</p>
","1458283","","4952130","","2016-09-21 18:40:57","2017-10-07 22:24:51","How can I specify the function type in my type hints?","<python><python-3.x><python-3.5><type-hinting><mypy>","2","2","19","","","CC BY-SA 3.0","0"
"39429526","1","39429578","","2016-09-10 18:56:49","","191","43543","<p>Suppose I have a function:</p>

<pre><code>def get_some_date(some_argument: int=None) -&gt; %datetime_or_None%:
    if some_argument is not None and some_argument == 1:
        return datetime.utcnow()
    else:
        return None
</code></pre>

<p>How do I specify the return type for something that can be <code>None</code>?</p>
","447967","","4952130","","2016-09-10 20:05:46","2018-09-14 08:03:37","How to specify ""nullable"" return type with type hints","<python><python-3.x><python-3.5><type-hinting>","1","0","19","","","CC BY-SA 3.0","0"
"28378257","1","","","2015-02-07 03:33:48","","17","43515","<pre><code>print (""Hello World"")
print (""{} World"").format(Hello)
</code></pre>

<p>I'm working on my first ""Hello World"" program and I can get it to work by using the print function and just a simple string text but when I try to use <code>.format</code> it gives me the error: </p>

<pre><code>AttributeError: 'NoneType' object has no attribute 'format' 
</code></pre>

<p>Is this saying that I need to initialize a variable for <code>.format</code> or am I missing something? </p>
","4539580","","4099593","","2016-04-06 07:33:23","2019-11-13 13:24:56","AttributeError: 'NoneType' object has no attribute 'format'","<python><string><python-3.x>","3","0","6","","","CC BY-SA 3.0","0"
"45655699","1","","","2017-08-12 23:10:56","","6","43421","<p>So i'm doing a little personal project but i keep getting this error when I try to create the recognizer. i have opencv-contrib and everything. Does anyone know whats going on? code posted below</p>

<pre><code>import cv2, os
import numpy as np
from PIL import Image

cascadePath = ""haarcascade_frontalface_default.xml""
faceCascade = cv2.CascadeClassifier(cascadePath)

recognizer = cv2.face.createLBPHFaceRecognizer()
</code></pre>

<p>it gets caught on that last line. I've tried reinstalling all modules already. Not really sure what else to do. The weird thing is it works on my laptop but not my desktop. They both have the same modules, same python release and running the exact same code. </p>
","8456422","","3002139","","2017-12-27 18:53:26","2020-05-20 15:44:04","attributeerror: module 'cv2.face' has no attribute 'createlbphfacerecognizer'","<python><python-3.x><opencv3.0><opencv-contrib>","16","2","3","","","CC BY-SA 3.0","0"
"51117503","1","","","2018-06-30 18:27:37","","18","43408","<p>I am new to python and I am trying django framework that involves some MySql and ran into this error when try to do <code>pip install mysqlclient</code> and down the lines of cmd messages I got this.</p>

<pre><code>   Failed building wheel for mysqlclient
  Running setup.py clean for mysqlclient
Failed to build mysqlclient
Installing collected packages: mysqlclient
  Running setup.py install for mysqlclient ... error
    Complete output from command c:\users\ronanl~1\envs\py1\scripts\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\RONANL~1\\AppData\\Local\\Temp\\pip-install-pkbqy3t3\\mysqlclient\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\RONANL~1\AppData\Local\Temp\pip-record-moxwf7lu\install-record.txt --single-version-externally-managed --compile --install-headers c:\users\ronanl~1\envs\py1\include\site\python3.7\mysqlclient:
    running install
    running build
    running build_py
    creating build
    creating build\lib.win32-3.7
    copying _mysql_exceptions.py -&gt; build\lib.win32-3.7
    creating build\lib.win32-3.7\MySQLdb
    copying MySQLdb\__init__.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\compat.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\connections.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\converters.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\cursors.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\release.py -&gt; build\lib.win32-3.7\MySQLdb
    copying MySQLdb\times.py -&gt; build\lib.win32-3.7\MySQLdb
    creating build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\__init__.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\CLIENT.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\CR.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\ER.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\FIELD_TYPE.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\FLAG.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    copying MySQLdb\constants\REFRESH.py -&gt; build\lib.win32-3.7\MySQLdb\constants
    running build_ext
    building '_mysql' extension
    creating build\temp.win32-3.7
    creating build\temp.win32-3.7\Release
    C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\bin\HostX86\x86\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -Dversion_info=(1,3,13,'final',0) -D__version__=1.3.13 ""-IC:\Program Files (x86)\MySQL\MySQL Connector C 6.1\include"" ""-Ic:\users\ronan lina\appdata\local\programs\python\python37-32\include"" ""-Ic:\users\ronan lina\appdata\local\programs\python\python37-32\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\ATLMFC\include"" ""-IC:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.14.26428\include"" ""-IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\ucrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\shared"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\um"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\winrt"" ""-IC:\Program Files (x86)\Windows Kits\10\include\10.0.17134.0\cppwinrt"" /Tc_mysql.c /Fobuild\temp.win32-3.7\Release\_mysql.obj /Zl
    _mysql.c
    _mysql.c(29): fatal error C1083: Cannot open include file: 'mysql.h': No such file or directory
    error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\VC\\Tools\\MSVC\\14.14.26428\\bin\\HostX86\\x86\\cl.exe' failed with exit status 2
&gt; 
&gt; 
&gt; Command ""c:\users\ronanl~1\envs\py1\scripts\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\RONANL~1\\AppData\\Local\\Temp\\pip-install-pkbqy3t3\\mysqlclient\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\RONANL~1\AppData\Local\Temp\pip-record-moxwf7lu\install-record.txt --single-version-externally-managed --compile --install-headers c:\users\ronanl~1\envs\py1\include\site\python3.7\mysqlclient"" failed with error code 1 in C:\Users\RONANL~1\AppData\Local\Temp\pip-install-pkbqy3t3\mysqlclient\
</code></pre>

<p>anyone knows how to fix this ?</p>
","9605387","","747744","","2018-06-30 20:11:14","2020-05-27 11:37:17","Python 3.7, Failed building wheel for MySql-Python","<python><mysql><django><python-3.x><mysql-python>","11","6","7","","","CC BY-SA 4.0","0"
"54828713","1","54876839","","2019-02-22 13:59:14","","25","43300","<p>I am getting a bit confused here, the latest Anaconda Distribution, 2018.12 at time of writing comes with an option to install Microsoft Visual Studio Code, which is great.</p>

<p>When launching VSC and after <code>Python: Select Interpreter</code> and with a fresh install of Anaconda, I can see <code>~Anaconda3\python.exe</code> which I assume is the Anaconda Python Environment, however, when I try to run some commands, I am getting:</p>

<pre><code>PS ~\Documents\Python Scripts\vs&gt; ~/Anaconda3/Scripts/activate
PS ~\Documents\Python Scripts\vs&gt; conda activate base
</code></pre>

<blockquote>
  <p>conda : The term 'conda' is not recognized as the name of a cmdlet,
  function, script file, or operable program. Check the spelling of the
  name, or if a path was included, verify that the path is correct and
  try again. At line:1 char:1</p>
</blockquote>

<p>Now I know that it might be related to the environment variables but I find it highly odd as during the Anaconda installation, there are specific mentions that it is not required to add the Anaconda path to the environment variables. However after the error, the integrated terminal manages to launch Python and I am able to run code.</p>

<p>Next in line is that I am unable to view any variables in the debugger after running a simple script, as shown in the tutorial <a href=""https://code.visualstudio.com/docs/python/python-tutorial"" rel=""noreferrer"">here</a>:</p>

<pre><code>msg = ""Hello World""
print(msg)
</code></pre>

<p>I do expect to see similar results as shown in the link such as the dunder variables, I have also updated my <code>launch.json</code> with <code>stopOnEntry = True</code> following the steps.</p>

<p>I would like to know if it is possible to use Visual Studio Code with Anaconda as a interpreter without registering variables from the original distribution and if I am missing out anything required.</p>

<p>I expected the experience to be more straight forward but also I might be missing something, I am running on <em>Windows 10.</em></p>
","3288092","","236574","","2019-02-26 01:41:13","2020-07-08 22:48:30","Working with Anaconda in Visual Studio Code","<python><python-3.x><visual-studio-code><anaconda><conda>","8","0","12","","","CC BY-SA 4.0","0"
"37182528","1","37182622","","2016-05-12 09:20:06","","5","43273","<p>I have different Python list variables(data1, data2, data3 ect) containing data which I want to put into an already existing excel sheet. Presently My loop goes like this.</p>

<pre><code>for row, entry in enumerate(data1,start=1):
  st.cell(row=row, column=1, value=entry)
  work.save('sample.xlsx')
for row, entry in enumerate(data2,start=1):
  st.cell(row=row, column=2, value=entry)
  work.save('sample.xlsx')
for row, entry in enumerate(data3,start=1):
  st.cell(row=row, column=3, value=entry)
  work.save('sample.xlsx')
for row, entry in enumerate(data4,start=1):
  st.cell(row=row, column=4, value=entry)
  work.save('sample.xlsx')
for row, entry in enumerate(data5,start=1):
  st.cell(row=row, column=5, value=entry)
  work.save('sample.xlsx')
for row, entry in enumerate(data6,start=1):
  st.cell(row=row, column=6, value=entry)
  work.save('sample.xlsx')
for row, entry in enumerate(data7,start=1):
  st.cell(row=row, column=7, value=entry)
  work.save('sample.xlsx')      
</code></pre>

<p>Once my Python script runs, It will store the data from the  1st row. <strong>If I am again running the script I want the new data to come below the available data</strong></p>

<p>How to do so? </p>
","","user6319690","","","","2019-08-06 11:27:55","how to append data using openpyxl python to excel file from a specified row?","<python><python-3.x><openpyxl>","4","0","7","","","CC BY-SA 3.0","0"
"43677564","1","43678106","","2017-04-28 10:06:18","","4","43247","<p>I need to make a webpage for an assignment, it doesn't have to be uploaded to the web, I am just using a local .html file.
I did some reading up and came up with the following html and python:</p>

<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;
            CV - Rogier
        &lt;/title&gt;
    &lt;/head
    &lt;body&gt;
        &lt;h3&gt;
            Study
        &lt;/h3&gt;
        &lt;p&gt;
            At my study we learn Python.&lt;br&gt;
            This is a sall example:&lt;br&gt;
            &lt;form action=""/cgi-bin/cvpython.py"" method=""get""&gt;
                First Name: &lt;input type=""text"" name=""first_name""&gt;  &lt;br /&gt;
                Last Name: &lt;input type=""text"" name=""last_name"" /&gt;
                &lt;input type=""submit"" value=""Submit"" /&gt;
            &lt;/form&gt;
        &lt;/p&gt;
    &lt;/body&gt;
&lt;/html&gt;
</code></pre>

<p>Python:</p>

<pre><code>import cgi
import cgitb #found this but isn't used?

form = cgi.FieldStorage()

first_name = form.getvalue('first_name').capitalize()
last_name  = form.getvalue('last_name').capitalize()

print (""Content-type:text/html\r\n\r\n"")
print (""&lt;html&gt;"")
print (""&lt;head&gt;"")
print (""&lt;title&gt;Hello - Second CGI Program&lt;/title&gt;"")
print (""&lt;/head&gt;"")
print (""&lt;body&gt;"")
print (""&lt;h2&gt;Your name is {}. {} {}&lt;/h2&gt;"".format(last_name, first_name, last_name))
print (""&lt;/body&gt;"")
print (""&lt;/html&gt;"")
</code></pre>

<p>However this just gives the prints as text and not as a proper html file with the 1 line that I want.</p>
","7194213","","4244112","","2017-04-30 01:06:13","2019-11-30 06:25:26","Passing input from html to python and back","<python><html><python-3.x><flask><cgi>","4","5","3","","","CC BY-SA 3.0","0"
"39129450","1","39129467","","2016-08-24 17:20:10","","18","43192","<p>I develop for both <code>Python 2</code> and <code>3.</code><br>
Thus, I have to use both <code>pip2</code> and <code>pip3.</code></p>

<p>When using <code>pip3 -</code> I receive this upgrade request (last two lines):</p>

<pre><code>$ pip3 install arrow
Requirement already satisfied (use --upgrade to upgrade): arrow in c:\program files (x86)\python3.5.1\lib\site-packages
Requirement already satisfied (use --upgrade to upgrade): python-dateutil in c:\program files (x86)\python3.5.1\lib\site-packages (from arrow)
Requirement already satisfied (use --upgrade to upgrade): six&gt;=1.5 in c:\program files (x86)\python3.5.1\lib\site-packages (from python-dateutil-&gt;arrow)
You are using pip version 7.1.2, however version 8.1.2 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' command.
</code></pre>

<p>My default <code>pip</code> is for <code>Python 2,</code> namely:</p>

<pre><code>$  python -m pip install --upgrade pip
Requirement already up-to-date: pip in /usr/lib/python2.7/site-packages
</code></pre>

<p>However, none of the following <em>explicit</em> commands succeed in upgrading the <code>Python 3 pip:</code></p>

<pre><code>$  python -m pip3 install --upgrade pip3
/bin/python: No module named pip3

$  python -m pip install --upgrade pip3
Collecting pip3
  Could not find a version that satisfies the requirement pip3 (from versions: )
No matching distribution found for pip3

$  python -m pip install --upgrade pip3.4
Collecting pip3.4
  Could not find a version that satisfies the requirement pip3.4 (from versions: )
No matching distribution found for pip3.4
</code></pre>

<h2>What is the correct command to upgrade pip3 when it is not the default pip?</h2>

<p>Environment:</p>

<pre><code>$ python3 -V
Python 3.4.3
$ uname -a
CYGWIN_NT-6.1-WOW 2.5.2(0.297/5/3) 2016-06-23 14:27 i686 Cygwin
</code></pre>
","1656850","","","","","2017-10-16 10:13:34","What is the correct format to upgrade pip3 when the default pip is pip2?","<python><python-3.x><cygwin><pip><python-3.4>","2","1","3","","","CC BY-SA 3.0","0"
"39146039","1","","","2016-08-25 12:59:03","","32","43092","<p>I keep on getting this error when I run the following code in python 3:</p>

<pre><code>fname1 = ""auth_cache_%s"" % username
fname=fname1.encode(encoding='utf_8')
#fname=fname1.encode()
if os.path.isfile(fname,) and cached:
    response = pickle.load(open(fname))
else:
    response = self.heartbeat()
    f = open(fname,""w"")
    pickle.dump(response, f)
</code></pre>

<p>Here is the error I get:</p>

<pre><code>File ""C:\Users\Dorien Xia\Desktop\Pokemon-Go-Bot-Working-Hack-API-master\pgoapi\pgoapi.py"", line 345, in login
    response = pickle.load(open(fname))
TypeError: a bytes-like object is required, not 'str'
</code></pre>

<p>I tried converting the fname1 to bytes via the encode function, but It still isn't fixing the problem. Can someone tell me what's wrong? </p>
","6172979","","6451573","","2018-05-03 08:21:08","2018-05-03 08:21:08","Pickle: TypeError: a bytes-like object is required, not 'str'","<python><python-3.x><bots>","4","1","2","2018-05-03 08:30:03","","CC BY-SA 4.0","0"
"34611394","1","34611480","","2016-01-05 12:10:40","","17","42898","<p>I followed these steps to set up virtualenv + virtualenvwrapper:</p>

<pre><code>$ sudo apt-get install python3-pip

$ sudo pip3 install virtualenv
$ sudo pip3 install virtualenvwrapper

$ mkdir ~/.virtualenvs

$ export WORKON_HOME=~/.virtualenvs

$ VIRTUALENVWRAPPER_PYTHON='/usr/bin/python3'

$ source /usr/local/bin/virtualenvwrapper.sh

$ mkvirtualenv venv
$ virtualenv venv
</code></pre>

<p>So far it was working fine but I restarted the shell and then I tried <code>workon venv</code> and now it says: <code>command not found</code></p>
","3590662","","4760801","","2016-01-05 13:58:42","2017-10-21 13:05:05","Virtualenv - workon command not found","<python-3.x><virtualenv><virtualenvwrapper>","2","0","7","","","CC BY-SA 3.0","0"
"36893206","1","36894176","","2016-04-27 14:43:14","","9","42645","<p>So, what I am trying to do is convert a float to a bytearray but I keep on receiving both no input, and EXTREME slowing/freezing of my computer. 
My code is</p>

<pre><code>import struct

def float_to_hex(f):
    return hex(struct.unpack('&lt;I', struct.pack('&lt;f', f))[0])
value = 5.1 #example value
...
value = bytearray(int(float_to_hex(float(value)), 16)
</code></pre>

<p>I found on another article a function to convert floats to hex which is</p>

<pre><code>def float_to_hex(f):
    return hex(struct.unpack('&lt;I', struct.pack('&lt;f', f))[0])
</code></pre>

<p>and then I converted it from hex to an int.
What is the problem with this? How could I better convert it from a float to bin or bytearray?</p>
","3709341","","3923281","","2016-04-27 14:47:18","2020-03-23 06:01:49","Converting a float to bytearray","<python><python-3.x>","2","7","3","","","CC BY-SA 3.0","0"
"28702267","1","28702286","","2015-02-24 17:24:10","","2","42485","<p>I have downloaded python from python.org website .  Then I am trying to run the following program . </p>

<pre><code>print ""Hello World !""
</code></pre>

<p>But there is an error like ""missing parenthesis in call to 'print'"" . Why am I getting this error ? </p>

<p><img src=""https://i.stack.imgur.com/9q0TX.png"" alt=""enter image description here""></p>
","1282443","","","","","2015-02-24 17:25:56","python script doesn't work showing ""missing parenthesis in call to 'print'""","<python-3.x>","2","1","1","2015-02-24 17:44:59","","CC BY-SA 3.0","0"
"40181344","1","40181387","","2016-10-21 16:21:36","","105","42380","<p>How do I use type hints to annotate a function that returns an <code>Iterable</code> that always yields two values: a <code>bool</code> and a <code>str</code>?  The hint <code>Tuple[bool, str]</code> is close, except that it limits the return value type to a tuple, not a generator or other type of iterable.</p>

<p>I'm mostly curious because I would like to annotate a function <code>foo()</code> that is used to return multiple values like this:</p>

<pre><code>always_a_bool, always_a_str = foo()
</code></pre>

<p>Usually functions like <code>foo()</code> do something like <code>return a, b</code> (which returns a tuple), but I would like the type hint to be flexible enough to replace the returned tuple with a generator or list or something else.</p>
","712605","","712605","","2016-10-21 16:45:12","2016-10-21 16:57:37","How to annotate types of multiple return values?","<python><python-3.x><python-3.5><type-hinting>","1","2","14","","","CC BY-SA 3.0","0"
"30832999","1","","","2015-06-14 18:27:17","","4","42168","<p>I keep getting </p>

<pre><code>&lt;__main__.Camera object at 0x02C08790&gt;
</code></pre>

<p>and I don't know why.</p>

<p>I would like the code to go from Calc_Speed to Counter and then back to Calc_Speed basically in a loop.</p>

<pre><code>class Camera():
    distance = 2
    speed_limit = 20
    number_of_cars = 0

    def Calc_Speed(self):
        registration = input(""Registration Plate: "")
        Speeding_List=[]
        start = float(input(""Start time: ""))
        end = float(input(""End Time: ""))
        speed = self.distance/(end-start)
        print((""Average Speed: "") + str(round(speed, 2)) + ("" mph""))
        if speed &gt; self.speed_limit:
            list3= [str(self.registration)]
            Speeding_List.append(list3)
            print(""Vehicles Caught Speeding: "" + str(Speeding_List))
            return(program.Counter())
        else:
            print(""Vehicle Not Speeding"")
            return(program.Counter())

    def Counter():
        self.number_of_cars = self.number_of_cars + 1
        print(""Number Of Cars Recorded: "" + str(self.number_of_cars))                                 
        return(program.Calc_Speed())



program = Camera()
print(program)
</code></pre>
","5008940","","3001761","","2020-02-19 22:06:04","2020-10-14 16:44:10","<__main__. object at 0x02C08790>","<python><python-3.x>","5","6","4","","","CC BY-SA 4.0","0"
"37225035","1","37239382","","2016-05-14 09:35:16","","41","42164","<p>I'm writing a script to automate data generation for a demo and I need to serialize in a JSON some data. Part of this data is an image, so I encoded it in base64, but when I try to run my script I get:</p>

<pre><code>Traceback (most recent call last):
  File ""lazyAutomationScript.py"", line 113, in &lt;module&gt;
    json.dump(out_dict, outfile)
  File ""/usr/lib/python3.4/json/__init__.py"", line 178, in dump
    for chunk in iterable:
  File ""/usr/lib/python3.4/json/encoder.py"", line 422, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File ""/usr/lib/python3.4/json/encoder.py"", line 396, in _iterencode_dict
    yield from chunks
  File ""/usr/lib/python3.4/json/encoder.py"", line 396, in _iterencode_dict
    yield from chunks
  File ""/usr/lib/python3.4/json/encoder.py"", line 429, in _iterencode
    o = _default(o)
  File ""/usr/lib/python3.4/json/encoder.py"", line 173, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
  TypeError: b'iVBORw0KGgoAAAANSUhEUgAADWcAABRACAYAAABf7ZytAAAABGdB...
     ...
   BF2jhLaJNmRwAAAAAElFTkSuQmCC' is not JSON serializable
</code></pre>

<p>As far as I know, a base64-encoded-whatever (a PNG image, in this case) is just a string, so it should pose to problem to serializating. What am I missing?</p>
","3110448","","","","","2019-12-13 16:39:52","Serialize in JSON a base64 encoded data","<json><python-3.x><serialization><base64>","3","0","19","","","CC BY-SA 3.0","0"
"55507519","1","55507956","","2019-04-04 03:46:33","","23","42160","<p>I am hoping to run a simple shell script to ease the management around some conda environments.  Activating conda environments via <code>conda activate</code> in a <code>linux</code> os works fine in the shell but is problematic within a shell script.  Could someone point me into the right direction as to why this is happening?</p>

<p>Example to repeat the issue:</p>

<pre><code># default conda env
$ conda info|egrep ""conda version|active environment""
     active environment : base
          conda version : 4.6.9

# activate new env to prove that it works
$ conda activate scratch
$ conda info|egrep ""conda version|active environment""
     active environment : scratch
          conda version : 4.6.9

# revert back to my original conda env
$ conda activate base 

$ cat shell_script.sh
#!/bin/bash
conda activate scratch

# run shell script - this will produce an error even though it succeeded above
$ ./shell_script.sh

CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init &lt;SHELL_NAME&gt;

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.
</code></pre>
","9074332","","11301900","","2020-03-10 22:38:56","2020-10-23 08:05:51","Python - Activate conda env through shell script","<python><python-3.x><anaconda><conda>","6","2","10","","","CC BY-SA 4.0","0"
"37329668","1","37329957","","2016-05-19 16:58:15","","12","42114","<p>array = some kind of list with 3 columns and unlimited amount of rows with data inside of it.</p>

<pre><code>Volume = array[0][2] 
counter = 0
for i in array: 
    if Volume == array[i][2]: #&lt;------ why is this line a problem? 
        counter += 1
</code></pre>
","6357345","","3704831","","2016-05-19 17:03:33","2020-06-29 18:56:06","TypeError: list indices must be integers or slices, not list","<list><python-3.x><indices><equivalent>","2","0","1","","","CC BY-SA 3.0","0"
"39450065","1","39451012","","2016-09-12 12:13:44","","45","42051","<p>For Python3, I followed <a href=""https://stackoverflow.com/questions/20449625/python-compressing-a-series-of-json-objects-while-maintaining-serial-reading"">@Martijn Pieters's code</a> with this:</p>

<pre><code>import gzip
import json

# writing
with gzip.GzipFile(jsonfilename, 'w') as fout:
    for i in range(N):
        uid = ""whatever%i"" % i
        dv = [1, 2, 3]
        data = json.dumps({
            'what': uid,
            'where': dv})

        fout.write(data + '\n')
</code></pre>

<p>but this results in an error:</p>

<pre><code>Traceback (most recent call last):
    ...
  File ""C:\Users\Think\my_json.py"", line 118, in write_json
    fout.write(data + '\n')
  File ""C:\Users\Think\Anaconda3\lib\gzip.py"", line 258, in write
    data = memoryview(data)
TypeError: memoryview: a bytes-like object is required, not 'str'
</code></pre>

<p>Any thoughts about what is going on?</p>
","1736294","","-1","","2017-05-23 12:18:01","2020-03-24 16:16:38","Python 3, read/write compressed json objects from/to gzip file","<json><python-3.x><gzip>","2","2","10","","","CC BY-SA 3.0","0"
"45228395","1","","","2017-07-21 03:16:59","","20","41966","<p>I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/aaaa/Desktop/ttttttt.py"", line 5, in &lt;module&gt;
   import reload
  File ""C:\Users\aaa\AppData\Local\Programs\Python\Python36\lib\site-
packages\reload.py"", line 3, in &lt;module&gt;
    import sys, time, re, os, signal, fcntl
ModuleNotFoundError: No module named 'fcntl'
</code></pre>

<p>So I did a pip install, which also gets an error.</p>

<pre><code>    C:\Users\aaaa&gt;pip install fcntl
    Collecting fcntl
      Could not find a version that satisfies the requirement fcntl (from versions: )
No matching distribution found for fcntl
</code></pre>

<p>Search results cPython, hacking, routing and many other words are coming out.</p>

<p>It's a tough answer for beginners, so I want to get a more detailed solution. </p>

<p>How should I solve it?</p>

<pre><code>#py3
import time
from selenium import webdriver
import codecs
import sys
import reload
import re
import fcntl
import os
import signal
</code></pre>
","8313366","","8313366","","2017-07-21 03:25:14","2019-12-05 14:24:31","Error: No module named 'fcntl'","<python><windows><python-3.x><module><compiler-errors>","2","0","","","","CC BY-SA 3.0","0"
"56561734","1","","","2019-06-12 11:58:43","","26","41942","<p>I have upgraded with tf_upgrade_v2 TF1 code to TF2. I'm a noob with both. I got the next error:</p>

<pre><code>RuntimeError: tf.placeholder() is not compatible with eager execution.
</code></pre>

<p>I have some <code>tf.compat.v1.placeholder()</code>.</p>

<pre><code>self.temperature = tf.compat.v1.placeholder_with_default(1., shape=())
self.edges_labels = tf.compat.v1.placeholder(dtype=tf.int64, shape=(None, vertexes, vertexes))
self.nodes_labels = tf.compat.v1.placeholder(dtype=tf.int64, shape=(None, vertexes))
self.embeddings = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, embedding_dim))
</code></pre>

<p>Could you give me any advice about how to proceed? Any ""fast"" solutions? or should I to recode this?</p>
","10311144","","","","","2020-10-07 09:04:15","RuntimeError: tf.placeholder() is not compatible with eager execution","<python><python-3.x><tensorflow><tensorflow2.0>","4","0","5","","","CC BY-SA 4.0","0"
"37071230","1","37071502","","2016-05-06 11:27:11","","4","41863","<p>Hi I'm trying to make a tic tac toe game in python and I've run into a problem. 
<img src=""https://i.stack.imgur.com/hvIIA.png"" alt=""enter image description here""></p>

<p>As you can see on the picture it rewrites the playing board after your input, what I want it to do is to clear the output and then rewrite the board. So instead of just printing new boards all the time it only clears the current board and rewrites it. I've searched on ""clear output"" etc, but all I find is these kind of codes:</p>

<pre><code>import os
clear = lambda : os.system('cls')
</code></pre>

<p>or</p>

<pre><code>import os
def clear():
os.system( 'cls' )
</code></pre>

<p>Using this clear function above don't work for me. 
It only returns this symbol:<a href=""https://i.stack.imgur.com/2iFLF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2iFLF.png"" alt=""enter image description here""></a></p>

<p>I am currently writing my code in Pycharm and just to make it clear, I wanna keep it in pycharm.</p>
","6299980","","6299980","","2016-05-06 15:01:19","2020-10-11 16:40:25","Clear output and rewrite it in python","<python><python-3.x>","4","6","2","","","CC BY-SA 3.0","0"
"53481088","1","","","2018-11-26 12:25:29","","20","41793","<p>I'm trying to use pdf2image and it seems I need something called <code>propeller</code> :</p>

<pre><code>(sum_env) C:\Users\antoi\Documents\Programming\projects\summarizer&gt;python ocr.py -i fr13_idf.pdf
Traceback (most recent call last):
  File ""c:\Users\antoi\Documents\Programming\projects\summarizer\sum_env\lib\site-packages\pdf2image\pdf2image.py"", line 165, in __page_count
    proc = Popen([""pdfinfo"", pdf_path], stdout=PIPE, stderr=PIPE)
  File ""C:\Python37\lib\subprocess.py"", line 769, in __init__
    restore_signals, start_new_session)
  File ""C:\Python37\lib\subprocess.py"", line 1172, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] The system cannot find the file specified

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""ocr.py"", line 53, in &lt;module&gt;
    pdfspliterimager(image_path)
  File ""ocr.py"", line 32, in pdfspliterimager
    pages = convert_from_path(""document-page%s.pdf"" % i, 500)
  File ""c:\Users\antoi\Documents\Programming\projects\summarizer\sum_env\lib\site-packages\pdf2image\pdf2image.py"", line 30, in convert_from_path
    page_count = __page_count(pdf_path, userpw)
  File ""c:\Users\antoi\Documents\Programming\projects\summarizer\sum_env\lib\site-packages\pdf2image\pdf2image.py"", line 169, in __page_count
    raise Exception('Unable to get page count. Is poppler installed and in PATH?')
Exception: Unable to get page count. Is poppler installed and in PATH?
</code></pre>

<p>I tried <a href=""https://github.com/QISKit/qiskit-terra/issues/586"" rel=""noreferrer"">this link</a> but it the thing to download didn't solved my problem.</p>
","4764604","","3149277","","2019-11-26 17:59:20","2020-06-12 06:37:46","Poppler in path for pdf2image","<python-3.x><path>","3","1","1","","","CC BY-SA 4.0","0"
"43477958","1","43478214","","2017-04-18 16:52:37","","17","41682","<p>I am solving a problem with genetic algorithm in python 3. I have not completed the full code yet. I test a part of the code whenever I complete it.</p>

<p>At present, I am stuck with an error saying: </p>

<blockquote>
  <p>TypeError: '&lt;' not supported between instances of 'part' and 'part'</p>
</blockquote>

<p>The interesting thing is, this error does not always show. Sometimes the code runs smoothly and show the desired output, but sometimes it shows this error.</p>

<p>What is the reason for this?  </p>

<p>I am attaching the code and the error message.<br>
I am using PyCharm.</p>

<pre><code>import random


class part():
    def __init__(self, number):
        self.number = number
        self.machine_sequence = []

    def add_volume(self, volume):
        self.volume = volume

    def add_machine(self, machine_numbers):
        self.machine_sequence.append(machine_numbers)


def create_initial_population():
    part_family = []

    for i in range(8):
        part_family.append(part(i))

    part_population = []

    for i in range(6):
        part_population.append(random.sample(part_family, len(part_family)))

    for i in part_population:
        for j in i:
            j.add_volume(random.randrange(100, 200))

    return part_population


def fitness(part_family):
    sum_of_boundary = []
    for i in range(0, 8, 2):
        sum_of_boundary.append(sum(j.volume for j in part_family[i:i + 2]))

    fitness_value = 0

    for i in range(len(sum_of_boundary) - 1):
        for j in range(i + 1, len(sum_of_boundary)):
            fitness_value = fitness_value + abs(sum_of_boundary[i] - sum_of_boundary[j])

    return fitness_value


def sort_population_by_fitness(population):
    pre_sorted = [[fitness(x),x] for x in population]
    sort = [x[1] for x in sorted(pre_sorted)]
    for i in sort:
        for j in i:
            print(j.volume, end = ' ')
        print()

    return sort


def evolve(population):
    population = sort_population_by_fitness(population)
    return population


population = create_initial_population()
population = evolve(population)
</code></pre>

<p>the error message:
<img src=""https://i.stack.imgur.com/ocrgL.jpg"" alt=""enter image description here""></p>

<p>The Output is (which is randomized every time):
<img src=""https://i.stack.imgur.com/bqpQs.jpg"" alt=""enter image description here""></p>
","7881849","","5411817","","2020-01-10 18:29:41","2020-04-22 14:54:00","TypeError: '<' not supported between instances Python","<python><python-3.x><class><typeerror>","2","1","4","","","CC BY-SA 4.0","0"
"31011631","1","31012663","","2015-06-23 19:19:32","","45","41679","<p>I'm following a tutorial on neural nets<a href=""http://neuralnetworksanddeeplearning.com/chap1.html"" rel=""noreferrer"">1</a></p>
<p>It's in Python 2.7. I'm using 3.4. This is the line that troubles me:</p>
<p><code>if test_data: n_test = len(test_data)</code></p>
<p>I get: <code>TypeError: object of type 'zip' has no len()</code>.</p>
<p>Is there a way to rewrite it so that it works in 3.4?</p>
","4540977","","-1","","2020-06-20 09:12:55","2018-11-09 00:06:19","Python 2 --> 3: object of type 'zip' has no len()","<python-2.7><python-3.x>","5","6","9","","","CC BY-SA 3.0","0"
"42755006","1","42755166","","2017-03-13 00:42:56","","2","41580","<p>I am getting a 'NoneType' object is not iterable TypeError in the code below.  The code below is ment to use pyautogui to scroll through 10 images in the digits folder (named 0 through 9, named with the # in the image) and when ever it finds one, report the value of x along with the number it found.  The dictionary is then sorted by x values to read the number found in the image.</p>

<p><strong>Question:</strong> I am still learning Python and this TypeError has me stomped, how can I correct this?</p>

<pre><code>#! python3
import sys
import pyautogui

# locate Power
found = dict()
for digit in range(10):
    positions = pyautogui.locateOnScreen('digits/{}.png'.format(digit),
                                         region=(888, 920, 150, 40), grayscale=True)
    for x, _, _, _ in positions:
        found[x] = str(digit)
cols = sorted(found)
value = ''.join(found[col] for col in cols)
print(value)
</code></pre>

<p><strong>Traceback of the error:</strong></p>

<pre class=""lang-none prettyprint-override""><code>Traceback (most recent call last):
  File ""C:\Users\test\python3.6\HC\power.py"", line 10, in &lt;module&gt;
    for x, _, _, _ in positions:
TypeError: 'NoneType' object is not iterable
</code></pre>
","7033012","","355230","","2017-03-13 02:24:27","2018-12-26 01:01:50","NoneType object is not iterable","<python><python-3.x>","1","6","1","","","CC BY-SA 3.0","0"
"44699682","1","44699728","","2017-06-22 12:39:43","","13","41579","<p>Currently, I am using this to download a file but it is placing them in the same folder where it is being run from, but how would I save the downloaded file to another directory of my choice.</p>

<pre><code>r = requests.get(url)  
with open('file_name.pdf', 'wb') as f:
    f.write(r.content)
</code></pre>
","7384392","","1448678","","2017-06-22 15:59:51","2017-06-22 15:59:51","How to save a file downloaded from requests to another directory?","<python><python-3.x><python-requests>","3","1","1","","","CC BY-SA 3.0","0"
"30664263","1","30664497","","2015-06-05 10:27:58","","10","41571","<p>I have two files: script1.py and script2.py. I need to invoke script2.py from script1.py and return the value from script2.py back to script1.py. But the catch is script1.py actually runs script2.py through os.</p>

<p>script1.py:</p>

<pre><code>import os
print(os.system(""script2.py 34""))
</code></pre>

<p>script2.py</p>

<pre><code>import sys
def main():
    x=""Hello World""+str(sys.argv[1])
    return x

if __name__ == ""__main__"":
    x= main()
</code></pre>

<p>As you can see, I am able to get the value into script2, but not back to script1. How can I do that? NOTE: script2.py HAS to be called as if its a commandline execution. Thats why I am using os.</p>
","1358676","","","","","2015-06-05 12:02:26","return value from one python script to another","<python><python-3.x><os.system>","2","2","8","","","CC BY-SA 3.0","0"
"35328953","1","35328958","","2016-02-11 00:56:01","","4","41342","<p>I'm trying to compare the first character of two different strings (and so on) to form a new string based on those results. This is what I've tried using, however its comparing every element of each list to each other. </p>

<pre><code>def compare(a,b):
    s = """"
    for x in a:
        for y in b:
            if x == y:
                s+=str(x)
            else:
                s+=str(y)
</code></pre>

<p>It seems like such a simple question but I'm stuck.</p>
","5343132","","841339","","2016-02-11 01:56:19","2020-09-27 08:26:57","How to compare individual characters in two strings in Python 3","<python><string><python-3.x>","5","2","1","","","CC BY-SA 3.0","0"
"51540391","1","51541184","","2018-07-26 13:44:59","","18","41267","<p>i have a python file with the following content saved on my machine:</p>

<pre><code>types_of_people = 10
x = f""There are {types_of_people} types of people""

binary = ""binary""
do_not = ""don't""
y = f""Those who know {binary} and those who {do_not}.""

print(x)
print(y)

print(f""i said: {x}"")
print(f""I also said: '{y}'"")

hilarious = False
joke_evaluation = ""Isn't that joke so funny?! {}""

print(joke_evaluation.format(hilarious))
w = ""This is the left side of ...""
e = ""a string with a right side.""

print(w + e)
</code></pre>

<p>When i open this file with Python 3.7 from within Visual Studio Code i get the following error:</p>

<pre><code>/usr/local/opt/python/bin/python3.7 /Users/andree/Desktop/test.py
  File ""&lt;stdin&gt;"", line 1
    /usr/local/opt/python/bin/python3.7 /Users/andree/Desktop/test.py
    ^
SyntaxError: invalid syntax
</code></pre>

<p>In the following screenshot you can see the command i use to run the file and also which python extension i use.</p>

<p><a href=""https://i.stack.imgur.com/zc4DO.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zc4DO.jpg"" alt=""run python file from within Visual Studio Code""></a></p>

<p>But running the file from within my terminal with <code>python3 test.py</code> works just fine.</p>

<p>Does anyone know what the problem is when running it from within VS Code?</p>
","1248224","","","","","2020-04-29 03:24:34","Invalid Syntax error when running python from inside Visual Studio Code","<python><python-3.x><visual-studio-code><syntax-error>","8","3","5","","","CC BY-SA 4.0","0"
"38028384","1","38033910","","2016-06-25 12:09:28","","39","41029","<p>When you use <strong><em><a href=""https://www.crummy.com/software/BeautifulSoup/bs4/doc/"" rel=""noreferrer"">BeautifulSoup</a></em></strong> to scrape a certain part of a website, you can use </p>

<ul>
<li><code>soup.find()</code> and <code>soup.findAll()</code> or</li>
<li><code>soup.select()</code>.</li>
</ul>

<p>Is there a difference between the <code>.find()</code> and the <code>.select()</code> methods?
(e.g. In performance or flexibility, etc.) Or are they the same?</p>
","1275087","","12470196","","2020-06-11 15:36:40","2020-06-11 15:36:40","Beautifulsoup : Difference between .find() and .select()","<python><python-3.x><beautifulsoup>","1","11","25","","","CC BY-SA 4.0","0"
"52068277","1","","","2018-08-29 01:28:52","","12","40743","<p>I want to reduce the number of frames acquired per second in a webcam, this is the code that I'm using</p>

<pre><code>#!/usr/bin/env python

import cv2

cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FPS, 10)
fps = int(cap.get(5))
print(""fps:"", fps)

while(cap.isOpened()):

    ret,frame = cap.read()
    if not ret:
        break

    cv2.imshow('frame', frame)

    k = cv2.waitKey(1)
    if k == 27:
        break
</code></pre>

<p>But it doesn't take effect, I still have 30 fps by default instead of 10 set up by <code>cap.set(cv2.CAP_PROP_FPS, 10)</code> . I want to reduce the frame rate because I have a hand detector which take quite a lot of time to process each frame, I can not store frames in buffer since it would detect the hand in previous positions. I could run the detector using a timer or something else but I thought changing the fps was an easier way, but it didn't work and I don't know why.</p>

<p>Im using Opencv 3.4.2 with Python 3.6.3 in Windows 8.1</p>
","9176054","","","","","2020-08-26 13:46:16","change frame rate in opencv 3.4.2","<python-3.x><opencv><frame-rate>","4","3","4","","","CC BY-SA 4.0","0"
"55209661","1","","","2019-03-17 17:06:34","","5","40700","<p>I'm new and studying machine learning. I stumble upon a tutorial I found online and I'd like to make the program work so I'll get a better understanding. However, I'm getting problems about loading the CSV File into the Jupyter Notebook. </p>

<p>I get this error:</p>

<pre><code>File ""&lt;ipython-input-2-70e07fb5b537&gt;"", line 2
    student_data = pd.read_csv(""C:\Users\xxxx\Desktop\student-intervention- 
system\student-data.csv"")
                          ^
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in 
position 2-3: truncated \UXXXXXXXX escape
</code></pre>

<p>and here is the code: 
<a href=""https://i.stack.imgur.com/XMz4T.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/XMz4T.png"" alt=""enter image description here""></a></p>

<p>I followed tutorials online regarding this error but none worked. Does anyone know how to fix it? </p>

<p>3rd attempt with r""path""
<a href=""https://i.stack.imgur.com/p2VC1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/p2VC1.png"" alt=""enter image description here""></a></p>

<p>I've tried also ""\"" and utf-8 but none worked. </p>

<p>I'm using the latest version of Anaconda
Windows 7
Python 3.7</p>
","10920729","","10920729","","2019-03-17 17:32:28","2020-09-23 13:05:29","How to load CSV file in Jupyter Notebook?","<python><python-3.x><jupyter-notebook>","7","2","0","","","CC BY-SA 4.0","0"
"35905264","1","35911254","","2016-03-10 00:17:14","","7","40648","<p>I am trying to import the module <code>keras.utils.data_utils</code> but its not working. However, I can find this module <a href=""https://github.com/fluency03/keras/blob/master/keras/utils/data_utils.py"" rel=""nofollow noreferrer"">here</a>. It is indeed existing. Why I cannot import it while I can import some other modules like<code>keras.models</code> and <code>keras.layers.core</code>?</p>

<pre><code>cliu@cliu-ubuntu:bin$ python
Python 2.7.9 (default, Apr  2 2015, 15:33:21) 
[GCC 4.9.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; from __future__ import print_function
&gt;&gt;&gt; from keras.models import Sequential 
&gt;&gt;&gt; from keras.layers.core import Dense, Activation, Dropout 
&gt;&gt;&gt; from keras.layers.recurrent import LSTM
&gt;&gt;&gt; from keras.utils.data_utils import get_file
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named data_utils
</code></pre>

<p><strong>Edit:</strong> </p>

<p>See <a href=""https://stackoverflow.com/a/35911254/4802797"">here</a> for the answer. </p>
","4802797","","-1","","2017-05-23 12:34:11","2017-08-07 10:16:24","Keras: ImportError: No module named data_utils","<python><python-2.7><python-3.x><import><keras>","4","8","2","","","CC BY-SA 3.0","0"
"50777849","1","","","2018-06-09 19:35:57","","50","40641","<p>Hi I usually use conda to manage my environments, but now I am on a project that needs a little more horsepower than my laptop. So I am trying to use my university's workstations which have new Intel Xeons. But I don't have admin rights and the workstation does not have conda so I am forced to work with virtualenv and pip3. </p>

<p>How do I generate a <code>requirements.txt</code> from conda that will work with <code>pip3</code> and <code>venv</code>?</p>

<pre><code>conda list -e &gt; requirements.txt
</code></pre>

<p>does not generate a compatible file:</p>

<pre><code>= is not a valid operator. Did you mean == ?
</code></pre>

<p>The <code>conda</code> output is:</p>

<pre><code># This file may be used to create an environment using:
# $ conda create --name &lt;env&gt; --file &lt;this file&gt;
# platform: osx-64
certifi=2016.2.28=py36_0
cycler=0.10.0=py36_0
freetype=2.5.5=2
icu=54.1=0
libpng=1.6.30=1
matplotlib=2.0.2=np113py36_0
mkl=2017.0.3=0
numpy=1.13.1=py36_0
openssl=1.0.2l=0
pip=9.0.1=py36_1
pyparsing=2.2.0=py36_0
pyqt=5.6.0=py36_2
python=3.6.2=0
python-dateutil=2.6.1=py36_0
pytz=2017.2=py36_0
qt=5.6.2=2
readline=6.2=2
scikit-learn=0.19.0=np113py36_0
scipy=0.19.1=np113py36_0
setuptools=36.4.0=py36_1
sip=4.18=py36_0
six=1.10.0=py36_0
sqlite=3.13.0=0
tk=8.5.18=0
wheel=0.29.0=py36_0
xz=5.2.3=0
zlib=1.2.11=0
</code></pre>

<p>I thought I would just manually change all <code>=</code> to <code>==</code> but the there are two <code>=</code> in the conda output. Which one to change? Surely there is an easier way?</p>

<p>EDIT: <code>pip freeze &gt; requirements.txt</code> gives:</p>

<pre><code>certifi==2016.2.28
cycler==0.10.0
matplotlib==2.0.2
matplotlib-venn==0.11.5
numpy==1.13.1
pyparsing==2.2.0
python-dateutil==2.6.1
pytz==2017.2
scikit-learn==0.19.0
scipy==0.19.1
six==1.10.0
</code></pre>
","6395452","","6395452","","2018-06-09 19:50:03","2020-08-10 03:47:37","From conda create requirements.txt for pip3","<python-3.x><pip><conda>","4","16","12","","","CC BY-SA 4.0","0"
"46026987","1","46027522","","2017-09-03 19:11:55","","20","40533","<p>Trying to get <code>selenium</code> to work with Python 3 for web scraping purposes:</p>

<pre><code>from selenium import webdriver
chrome_path = r""/Library/Frameworks/Python.framework/Versions/3.6/bin/chromedriver""
driver = webdriver.Chrome(chrome_path)
</code></pre>

<p>I get the following error message: </p>

<blockquote>
  <p>selenium.common.exceptions.WebDriverException: Message: unknown error: cannot find Chrome binary</p>
</blockquote>

<p>A similar question was addressed <a href=""https://stackoverflow.com/questions/43287203/selenium-gives-unknown-error-cannot-find-chrome-binary-when-running-chrome-dr"">here</a>, but what is baffling to me is that Chrome is already installed on my system. The other asker apparently didn't have it on their computer. I'm running latest version of Mac OS.</p>
","8555779","","4518341","","2020-05-10 17:43:57","2020-05-10 17:47:13","Selenium gives ""selenium.common.exceptions.WebDriverException: Message: unknown error: cannot find Chrome binary"" on Mac","<python><python-3.x><macos><selenium>","6","4","3","","","CC BY-SA 3.0","0"
"44664040","1","44664064","","2017-06-20 22:32:31","","112","40476","<p>Couldn't seem to find a definitive answer. I want to do a type hint for a function and the type being some custom class that I have defined, called it <code>CustomClass()</code>.</p>

<p>And then let's say in some function, call it <code>FuncA(arg)</code>, I have one argument named <code>arg</code>. Would the correct way to type hint <code>FuncA</code> be:</p>

<p><code>def FuncA(arg: CustomClass):</code></p>

<p>Or would it be:</p>

<p><code>def FuncA(Arg:Type[CustomClass]):</code>?</p>
","7994318","","355230","","2017-06-20 22:35:09","2017-09-06 20:10:10","Type hints with user defined classes","<python><python-3.x><user-defined-types><type-hinting>","1","0","12","","","CC BY-SA 3.0","0"
"33240374","1","33240579","","2015-10-20 15:14:03","","4","40324","<p>I would like to know how to draw a line using the x and y coordinates of two 2-dimensional points. I tried the turtle graphics, but it works using degrees.</p>
","4873694","","3924118","","2018-05-02 21:08:24","2018-08-02 23:33:53","How can draw a line using the x and y coordinates of two points?","<python><python-3.x>","6","1","","","","CC BY-SA 4.0","0"
"40299126","1","40299127","","2016-10-27 23:45:18","","3","40320","<p>I can't figure out why I can't print to the terminal using the following code. </p>

<pre><code>#!/usr/bin/env python3
import sys
def main():
    sys.stdout.write(""Hello"")
</code></pre>

<p>I'm running the program from the terminal by moving into the directory in which the python file is found, making the file executable and running </p>

<pre><code>./filename
</code></pre>

<p>The terminal prints nothing, just goes to newline. How do I print to the terminal if not with sys.stdout.write(""string"")? </p>
","4228168","Levente Makai","","","","2016-10-28 06:16:26","Why can't I print to terminal with my python script?","<ubuntu><scripting><python-3.x>","1","4","0","","","CC BY-SA 3.0","0"
"35166821","1","","","2016-02-03 01:16:39","","43","40292","<p>I was playing the the Python's import system in order to understand better how it works, and I encountered another problem. I have the following structure </p>

<pre><code>pkg/
    __init__.py
    c.py
    d.py

    subpkg/
        __init__.py
        a.py
        b.py
</code></pre>

<p>Inside <code>a.py</code> I have the following code:</p>

<pre><code>from . import b
from .. import d
</code></pre>

<p>And inside <code>c.py</code> I have the following:</p>

<pre><code>import subpkg.a
</code></pre>

<p>Now I receive the following error:</p>

<blockquote>
  <p>ValueError: attempted relative import beyond top-level package</p>
</blockquote>

<p>But <strong>why</strong>? How can I solve it? I am running <code>c.py</code> from the IDLE, and <code>pkg</code> should be considered a package, since it has the <code>__init__.py</code> file.</p>

<p>The first import works fine, but it's the following that doesn't work:</p>

<pre><code>from .. import d
</code></pre>

<p>Because I am attempting to import something from a parent package, but apparently I cannot, for some weird reason.</p>
","3924118","","","","","2019-09-25 21:58:16","ValueError: attempted relative import beyond top-level package","<python-3.x><python-import>","3","0","8","","","CC BY-SA 3.0","0"
"28737292","1","","","2015-02-26 08:10:22","","27","40292","<p>I wrote a script to read text file in python.</p>

<p>Here is the code.    </p>

<pre><code>parser = argparse.ArgumentParser(description='script')    
parser.add_argument('-in', required=True, help='input file',
type=argparse.FileType('r'))
parser.add_argument('-out', required=True, help='outputfile',
type=argparse.FileType('w'))     
args = parser.parse_args()    

try:
    reader = csv.reader(args.in)
    for row in reader:
        print ""good""
except csv.Error as e:
    sys.exit('file %s, line %d: %s' % (args.in, reader.line_num, e))

for ln in args.in:
    a, b = ln.rstrip().split(':')
</code></pre>

<p>I would like to check if the file exists and is not empty file but this code gives me an error.</p>

<p>I would also like to check if program can write to output file. </p>

<p><strong>Command:</strong>   </p>

<pre><code>python script.py -in file1.txt -out file2.txt 
</code></pre>

<p><strong>ERROR:</strong></p>

<pre><code>good
Traceback (most recent call last):
  File ""scritp.py"", line 80, in &lt;module&gt;
    first_cluster = clusters[0]
IndexError: list index out of range
</code></pre>
","3573959","","2063361","","2017-01-29 22:25:40","2020-07-31 13:59:31","How to check text file exists and is not empty in python","<python><python-3.x><filepath>","2","4","6","","","CC BY-SA 3.0","0"
"50639973","1","50643145","","2018-06-01 09:14:00","","11","40162","<p>I think I have some issues with either Python and/or pip on my Mac. I have Python 2.7 installed globally and then I normally setup virtualenvs and install Python3.6.4 but in the last day or so Ive been getting problems with packages such as Fabric and SSH2 where I have either not been able to install them with various errors or with Fabric it throws when I try to import the package.</p>

<p>Im now trying to remove Fabric and install Fabric3 and its throwing errors like this:</p>

<pre><code>Could not install packages due to an EnvironmentError: [Errno 13] Permission denied: '/Users/david/Documents/projects/uptimeapp/env/lib/python3.6/site-packages/Fabric3-1.14.post1.dist-info'
Consider using the `--user` option or check the permissions.

(env) Davids-MacBook-Air:uptimeapp david$ pip install fabric3 --user
Can not perform a '--user' install. User site-packages are not visible in this virtualenv.
</code></pre>

<p>If I do <code>sudo pip install fabric</code>  then it installs but with this warning:</p>

<pre><code>The directory '/Users/david/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
The directory '/Users/david/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.
</code></pre>

<p>But I thought it was not advised to pip install with sudo?</p>

<p>These are the errors I get when I try to <code>pip install ssh2-python</code></p>

<pre><code>ssh2/agent.c:569:10: fatal error: 'libssh2.h' file not found
    #include ""libssh2.h""
             ^~~~~~~~~~~
    1 error generated.
    error: command 'clang' failed with exit status 1

    ----------------------------------------
Command ""/Users/david/Documents/projects/uptimeapp/env/bin/python3.6 -u  -c ""import setuptools,   tokenize;__file__='/private/var/folders/bl/97vt48j97zd2sj05zmt4xst00000gn/T  /pip-install-mpyq41q4/ssh2-python/setup.py';f=getattr(tokenize, 'open',   open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record   /private/var/folders/bl/97vt48j97zd2sj05zmt4xst00000gn/T/pip-record-qul_k3kq/install-record.txt --single-version-externally-managed --compile -  -install-headers /Users/david/Documents/projects/uptimeapp/env/bin/../include/site/python3.6  /ssh2-python"" failed with error code 1 in /private/var/folders/bl/97vt48j97zd2sj05zmt4xst00000gn/T/pip-install-mpyq41q4/ssh2-python/
</code></pre>

<p>I have managed to remove Fabric and install Fabric3 with the sudo command but I would rather not do that.</p>

<p>I should add that Ive not had any other problems with installing other packages either globally in Python2.7 or in envs.</p>
","7266376","","2650249","","2019-07-30 16:06:12","2019-07-30 16:06:12","Pip problems - Could not install packages due to an EnvironmentError","<python><python-3.x><macos><pip>","2","0","5","","","CC BY-SA 4.0","0"
"38847690","1","38847691","","2016-08-09 10:01:59","","67","40161","<p>I want to print some floating point numbers so that they're always written in decimal form (e.g. <code>12345000000000000000000.0</code> or <code>0.000000000000012345</code>, not in <a href=""https://en.wikipedia.org/wiki/Scientific_notation"" rel=""noreferrer"">scientific notation</a>, yet I'd want to the result to have the up to ~15.7 <a href=""https://en.wikipedia.org/wiki/Significant_figures"" rel=""noreferrer"">significant figures</a> of a IEEE 754 double, and no more. </p>

<p>What I want is <strong><em>ideally</em> so that the result is the <em>shortest</em> string in positional decimal format that still results in the same value when converted to a <code>float</code></strong>.</p>

<p>It is well-known that the <code>repr</code> of a <code>float</code> is written in scientific notation if the exponent is greater than 15, or less than -4:</p>

<pre><code>&gt;&gt;&gt; n = 0.000000054321654321
&gt;&gt;&gt; n
5.4321654321e-08  # scientific notation
</code></pre>

<p>If <code>str</code> is used, the resulting string again is in scientific notation:</p>

<pre><code>&gt;&gt;&gt; str(n)
'5.4321654321e-08'
</code></pre>

<hr>

<p>It has been suggested that I can use <code>format</code> with <code>f</code> flag and sufficient precision to get rid of the scientific notation:</p>

<pre><code>&gt;&gt;&gt; format(0.00000005, '.20f')
'0.00000005000000000000'
</code></pre>

<p>It works for that number, though it has some extra trailing zeroes. But then the same format fails for <code>.1</code>, which gives decimal digits beyond the actual machine precision of float:</p>

<pre><code>&gt;&gt;&gt; format(0.1, '.20f')
'0.10000000000000000555'
</code></pre>

<p>And if my number is <code>4.5678e-20</code>, using <code>.20f</code> would still lose relative precision:</p>

<pre><code>&gt;&gt;&gt; format(4.5678e-20, '.20f')
'0.00000000000000000005'
</code></pre>

<p>Thus <strong>these approaches do not match my requirements</strong>.</p>

<hr>

<p>This leads to the question: what is the easiest and also well-performing way to print arbitrary floating point number in decimal format, having the same digits as in <a href=""https://stackoverflow.com/a/28493269/918959""><code>repr(n)</code> (or <code>str(n)</code> on Python 3)</a>, but always using the decimal format, not the scientific notation.</p>

<p>That is, a function or operation that for example converts the float value <code>0.00000005</code> to string <code>'0.00000005'</code>; <code>0.1</code> to <code>'0.1'</code>; <code>420000000000000000.0</code> to <code>'420000000000000000.0'</code> or <code>420000000000000000</code> and formats the float value <code>-4.5678e-5</code> as <code>'-0.000045678'</code>.</p>

<hr>

<p>After the bounty period: It seems that there are at least 2 viable approaches, as Karin demonstrated that using string manipulation one can achieve significant speed boost compared to my initial algorithm on Python 2.</p>

<p>Thus,</p>

<ul>
<li>If performance is important and Python 2 compatibility is required; or if the <code>decimal</code> module cannot be used for some reason, then <a href=""https://stackoverflow.com/a/38983595/918959"">Karin's approach using string manipulation</a> is the way to do it.</li>
<li>On Python 3, <a href=""https://stackoverflow.com/a/38847691/918959"">my somewhat shorter code will also be faster</a>.</li>
</ul>

<p>Since I am primarily developing on Python 3, I will accept my own answer, and shall award Karin the bounty.</p>
","918959","","918959","","2019-09-09 07:17:02","2019-09-09 08:49:54","Convert float to string in positional format (without scientific notation and false precision)","<python><python-3.x><floating-point><number-formatting><python-2.x>","6","10","19","","","CC BY-SA 4.0","0"
"41189928","1","41190352","","2016-12-16 17:56:06","","6","40151","<pre><code>import pygame, sys
pygame.init()
screen = pygame.display.set_mode([800,600])
white = [255, 255, 255]
red = [255, 0, 0]
screen.fill(white)
pygame.display.set_caption(""My program"")
pygame.display.flip()



background = input(""What color would you like?: "")
if background == ""red"":
    screen.fill(red)

running = True
while running:
    for i in pygame.event.get():
        if i.type == pygame.QUIT:
        running = False
        pygame.quit()
</code></pre>

<p>I'm trying to ask the user what background color he would like to have. If the user writes red, the color doesn't change and still stays white.</p>
","7307944","","1718174","","2016-12-16 18:31:59","2016-12-18 06:56:53","Pygame: how to change background color","<python><python-3.x><pygame>","2","4","0","","","CC BY-SA 3.0","0"
"37891188","1","40396403","","2016-06-17 22:22:43","","28","40016","<p>I have a large ML project in python 2 code and I just started using PyCharm as an IDE. I'm currently using WinPython 3.4 and I'd preferably like to do everything in python 3 instead of continue using legacy 2. When I cloned the project from git a popup in pycharm came up that was something along the lines of converting the code to 3 from 2 but I didn`t really think about it and exited it. How do I convert it? </p>
","4305243","","2801026","","2016-06-20 21:45:02","2019-11-10 23:23:56","Convert python 2 code to 3 in PyCharm","<python><git><python-2.7><python-3.x><pycharm>","5","1","11","","","CC BY-SA 3.0","0"
"39123766","1","39123873","","2016-08-24 12:46:54","","13","40002","<p>I am trying to run a quadratic equation in python. However, it keeps on giving me a warning</p>

<pre><code>RuntimeWarning: invalid value encountered in sqrt
</code></pre>

<p>Here's my code:</p>

<pre><code>import numpy as np


a = 0.75 + (1.25 - 0.75)*np.random.randn(10000)
print(a)
b = 8 + (12 - 8)*np.random.randn(10000)
print(b)
c = -12 + 2*np.random.randn(10000)
print(c)
x0 = (-b - np.sqrt(b**2 - (4*a*c)))/(2 * a)
print(x0)
</code></pre>
","6717559","","10239789","","2019-07-24 11:30:14","2019-07-24 11:30:14","I am getting a warning <RuntimeWarning: invalid value encountered in sqrt>","<python><python-2.7><python-3.x><numpy><math>","2","3","1","","","CC BY-SA 4.0","0"
"29771168","1","29771407","","2015-04-21 11:38:16","","8","39956","<p>I have done my code this far but it is not working properly with remove()..can anyone help me..</p>

<pre><code>'''
Created on Apr 21, 2015

@author: Pallavi
'''
from pip._vendor.distlib.compat import raw_input
print (""Enter Query"")
str=raw_input()  

fo = open(""stopwords.txt"", ""r+"")
str1 = fo.read();
list=str1.split(""\n"");
fo.close()
words=str.split("" "");
for i in range(0,len(words)):
    for j in range(0,len(list)):
        if(list[j]==words[i]):
            print(words[i])
            words.remove(words(i))
</code></pre>

<p>Here is the error:</p>

<pre><code>Enter Query
let them cry try diesd
let them try
Traceback (most recent call last):
  File ""C:\Users\Pallavi\workspace\py\src\parser.py"", line 17, in &lt;module&gt;
    if(list[j]==words[i]):
IndexError: list index out of range
</code></pre>
","4353375","","1870151","","2015-04-21 12:14:09","2020-04-22 13:08:59","How to remove words from a list in python","<python-3.x>","4","3","3","","","CC BY-SA 3.0","0"
"42441211","1","42441391","","2017-02-24 14:32:08","","4","39765","<p>I wrote a script to find spelling mistakes in SO questions' titles.
I used it for about a month.This was working fine. </p>

<p>But now, when I try to run it, I am getting this.</p>

<pre><code>Traceback (most recent call last):
  File ""copyeditor.py"", line 32, in &lt;module&gt;
    find_bad_qn(i)
  File ""copyeditor.py"", line 15, in find_bad_qn
    html = urlopen(url)
  File ""/usr/lib/python3.4/urllib/request.py"", line 161, in urlopen
    return opener.open(url, data, timeout)
  File ""/usr/lib/python3.4/urllib/request.py"", line 469, in open
    response = meth(req, response)
  File ""/usr/lib/python3.4/urllib/request.py"", line 579, in http_response
    'http', request, response, code, msg, hdrs)
  File ""/usr/lib/python3.4/urllib/request.py"", line 507, in error
    return self._call_chain(*args)
  File ""/usr/lib/python3.4/urllib/request.py"", line 441, in _call_chain
    result = func(*args)
  File ""/usr/lib/python3.4/urllib/request.py"", line 587, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
</code></pre>

<p>This is my code</p>

<pre><code>import json
from urllib.request import urlopen
from bs4 import BeautifulSoup
from enchant import DictWithPWL
from enchant.checker import SpellChecker

my_dict = DictWithPWL(""en_US"", pwl=""terms.dict"")
chkr = SpellChecker(lang=my_dict)
result = []


def find_bad_qn(a):
    url = ""https://stackoverflow.com/questions?page="" + str(a) + ""&amp;sort=active""
    html = urlopen(url)
    bsObj = BeautifulSoup(html, ""html5lib"")
    que = bsObj.find_all(""div"", class_=""question-summary"")
    for div in que:
        link = div.a.get('href')
        name = div.a.text
        chkr.set_text(name.lower())
        list1 = []
        for err in chkr:
            list1.append(chkr.word)
        if (len(list1) &gt; 1):
            str1 = ' '.join(list1)
            result.append({'link': link, 'name': name, 'words': str1})


print(""Please Wait.. it will take some time"")
for i in range(298314,298346):
    find_bad_qn(i)
for qn in result:
    qn['link'] = ""https://stackoverflow.com"" + qn['link']
for qn in result:
    print(qn['link'], "" Error Words:"", qn['words'])
    url = qn['link']
</code></pre>

<p><strong>UPDATE</strong></p>

<p>This is the url causing the problem.Even though this url exists.</p>

<p><a href=""https://stackoverflow.com/questions?page=298314&amp;sort=active"">https://stackoverflow.com/questions?page=298314&amp;sort=active</a></p>

<p>I tried changing the range to some lower values. It works fine now.</p>

<p>Why this happened with above url?</p>
","6281993","","6281993","","2017-02-24 14:37:54","2020-08-21 06:07:29","Python: urllib.error.HTTPError: HTTP Error 404: Not Found","<python><python-3.x><urllib>","3","3","2","","","CC BY-SA 3.0","0"
"37376516","1","37376566","","2016-05-22 16:01:34","","19","39699","<p>I have a set of three variables x, y, z and I want to check if they <strong>all share the same value</strong>. In my case, the value will either by 1 or 0, but I only need to know if they are all the same. Currently I'm using</p>

<pre><code>if 1 == x and  1 == y and 1 == z: 
    sameness = True
</code></pre>

<p>Looking for the answer I've found:</p>

<pre><code>if 1 in {x, y, z}:
</code></pre>

<p>However, this operates as</p>

<pre><code>if 1 == x or  1 == y or 1 == z: 
    atleastOneMatch = True
</code></pre>

<p>Is it possible to check if 1 is in each: x, y, and z? 
Better yet, is there a more concise way of checking simply if x, y, and z are the same value?</p>

<p>(if it matters, I use python 3)</p>
","4443226","","4443226","","2018-04-22 12:53:50","2019-10-30 13:02:06","Python - Check if multiple variables have the same value","<python><python-3.x>","7","1","10","","","CC BY-SA 3.0","0"
"45808140","1","45808255","","2017-08-22 02:44:00","","51","39538","<p>I am making a code that simulates a pawn going around a monopoly board a million times.
I would like to have a tqdm progress bar that is updated every time a turn around the board is achieved.</p>

<p><strong>Below is my current code.</strong> I am using a while loop which stops when the number of turns around the board surpasses the desired number.</p>

<pre><code>import os
from openpyxl import Workbook
from monopolyfct import *


def main(runs, fileOutput):

    ### EXCEL SETUP ###
    theWorkbook = Workbook()                              # Creates the workbook interface.
    defaultSheet = theWorkbook.active                     # Creates the used worksheet.
    currentData = [""Current Table Turn"", ""Current Tile""]  # Makes EXCEL column titles.
    defaultSheet.append(currentData)                      # Appends column titles.

    ### CONTENT SETUP ###
    currentData = [1, 0]             # Sets starting position.
    defaultSheet.append(currentData) # Appends starting position.

    while currentData[0] &lt;= runs:

        ### ROLLING THE DICES PROCESS ###
        dices = twinDiceRoll()
        currentData[1] += dices[2]  # Updating the current tile

        ### SURPASSING THE NUMBER OF TILES ONBOARD ###
        if currentData[1] &gt; 37:   # If more than a table turn is achieved,
            currentData[0] += 1   # One more turn is registered
            currentData[1] -= 38  # Update the tile to one coresponding to a board tile.
        else:
            pass

        ### APPENDING AQUIRED DATA ###
        defaultSheet.append(currentData)

        ### MANAGIING SPECIAL TILES ###
        if currentData[1] == 2 or 15 or 31:   # Community chess
            pass                              #TODO: Make a mechanic simulating the community chest card draw and it's related action.
        elif currentData[1] == 5 or 20 or 34: # Chance
            pass                              #TODO: Make a mechanic simulating the chance card draw and it's related action.
        elif currentData[1] == 28:            # Go to Jail
            pass                              #TODO: Make a mechanic simulating the entire jail process

        ### TWIN DICE ROLL EXCEPTION ###
        if dices[3] is True:  # If the dices roll a double,
            pass              #TODO: Make a mechanic considering that three doubles sends one to Jail.


    ### STORING THE ACCUMULATED DATA ###
    theWorkbook.save(fileOutput)  # Compiles the data in a .xlxs file.


if __name__ == ""__main__"":
    terminalWidth = os.get_terminal_size().columns                                               # Gets current terminal width.
    space(3)
    print(""Python Monopoly Statistics Renderer"".upper().center(terminalWidth))                   # Prints the title.
    print(""(PMSR)"".center(terminalWidth))                                                        # Prints the acronym.
    space(2)
    runs = int(request(""For how many table turns do you want the simulation to run?""))           # Prompts for the desired run ammount
    #runs = 1000
    fileOutput = request(""What should be the name of the file in which statistics are stored?"")  # Prompts for the desired store filename
    #fileOutput = ""test""
    fileOutput += "".xlsx""                                                                        # Adds file extension to filename
    main(runs, fileOutput)
</code></pre>
","2325057","","","","","2020-01-21 14:30:48","Using tqdm progress bar in a while loop","<python><python-3.x><tqdm>","1","1","4","","","CC BY-SA 3.0","0"
"45529502","1","45531659","","2017-08-06 07:06:55","","0","39449","<p>I understand the questions but the thing that im not sure about is the part it says ""using functions"". 
here is my code and wonder if its acceptable;</p>

<pre><code>x= int(input(""Enter first number:""))
y= int(input(""Enter second number:""))
sum=x+y
average=sum/2
print(""Sum of the given two numbers is:"", sum)
print(""Average of the given numbers is:"", average)
</code></pre>
","6053069","","","","","2018-08-17 05:57:23","Write a program to find sum of two numbers using functions and calculate their average?","<python-3.x>","5","2","1","","","CC BY-SA 3.0","0"
"28328890","1","28328919","","2015-02-04 18:38:42","","24","39358","<p>I am using this awesome library called <a href=""http://docs.python-requests.org/en/latest"" rel=""noreferrer""><code>requests</code></a> to maintain python 2 &amp; 3 compatibility and simplify my application requests management.</p>

<p>I have a case where I need to parse a url and replace one of it's parameter. E.g:</p>

<pre><code>http://example.com?param1=a&amp;token=TOKEN_TO_REPLACE&amp;param2=c
</code></pre>

<p>And I want to get this:</p>

<pre><code>http://example.com?param1=a&amp;token=NEW_TOKEN&amp;param2=c
</code></pre>

<p>With the <code>urllib</code> I can achieve it this way:</p>

<pre><code>from urllib.parse import urlparse
from urllib.parse import parse_qs
from urllib.parse import urlencode

url = 'http://example.com?param1=a&amp;token=TOKEN_TO_REPLACE&amp;param2=c'

o = urlparse(url)
query = parse_qs(o.query)
if query.get('token'):
    query['token'] = ['NEW_TOKEN', ]
    new_query = urlencode(query, doseq=True)
    url.split('?')[0] + '?' + new_query

&gt;&gt;&gt; http://example.com?param2=c&amp;param1=a&amp;token=NEW_TOKEN
</code></pre>

<p>How can you achieve the same using the <code>requests</code> library?</p>
","768335","","100297","","2015-02-04 18:51:09","2019-09-16 15:03:28","Python-Requests, extract url parameters from a string","<python><python-3.x><python-requests>","2","0","9","","","CC BY-SA 3.0","0"
"43281886","1","","","2017-04-07 15:27:51","","38","39285","<p>I have this list:</p>

<pre><code>colors = [""R"", ""G"", ""B"", ""Y""]
</code></pre>

<p>and I want to get 4 random letters from it, but including repetition.</p>

<p>Running this will only give me 4 unique letters, but never any repeating letters:</p>

<pre><code>print(random.sample(colors,4))
</code></pre>

<p>How do I get a list of 4 colors, with repeating letters possible?</p>
","7833566","","1001643","","2017-04-27 05:44:20","2017-11-13 14:56:37","Get a random sample with replacement","<python><python-3.x><random>","4","0","3","","","CC BY-SA 3.0","0"
"39510830","1","39511176","","2016-09-15 12:17:31","","1","39282","<p>I recently started learning python and one of the first projects I did was to scrap updates from my son's classroom web page and send me notifications that they updated the site.  This turned out to be an easy project so I wanted to expand on this and create a script that would automatically check if any of our lotto numbers hit. Unfortunately I haven't been able to figure out how to get the data from the website. Here is one of my attempts from last night.</p>

<pre><code>from bs4 import BeautifulSoup
import urllib.request

webpage = ""http://www.masslottery.com/games/lottery/large-winningnumbers.html""

websource = urllib.request.urlopen(webpage)
soup = BeautifulSoup(websource.read(), ""html.parser"")

span = soup.find(""span"", {""id"": ""winning_num_0""})
print (span)

Output is here...
&lt;span id=""winning_num_0""&gt;&lt;/span&gt; 
</code></pre>

<p>The output listed above is also what I see if I ""view source"" with a web browser.  When I ""inspect Element"" with the web browser I can see the winning numbers in the inspect element panel.  Unfortunately I'm not even sure how/where the web browser is getting the data. is it loading from another page or a script in the background? I thought the following tutorial was going to help me but I wasn't able to get the data using similar commands. </p>

<p><a href=""http://zevross.com/blog/2014/05/16/using-the-python-library-beautifulsoup-to-extract-data-from-a-webpage-applied-to-world-cup-rankings/"" rel=""nofollow"">http://zevross.com/blog/2014/05/16/using-the-python-library-beautifulsoup-to-extract-data-from-a-webpage-applied-to-world-cup-rankings/</a></p>

<p>Any help is appreciated.
Thanks</p>
","6673366","","","","","2016-09-15 12:34:02","extract data from website using python","<python><python-3.x>","1","5","3","","","CC BY-SA 3.0","0"
"52110869","1","","","2018-08-31 07:50:57","","26","39119","<p>I would like not to use the non-tuple sequence for multidimensional indexing so that the script will support future release of Python when this changes.</p>

<p>Below is the code that i am using for plotting the graph:</p>

<pre><code>data = np.genfromtxt(Example.csv,delimiter=',', dtype=None, names=True, 
    converters={0: str2date})

p1, = host.plot(data[""column_1""], data[""column_2""], ""b-"", label=""column_2"")
p2, = par1.plot(data[""column_1""], data['column_3'], ""r-"", label=""column_3"")
p3, = par2.plot(data[""column_1""], data[""column_4""], ""g-"", label=""column_4"")

host.set_xlim([data[""column_1""][0], data[""column_1""][-1]])
host.set_ylim(data[""column_2""].min(), data[""column_2""].max())
par1.set_ylim(data[""column_3""].min(), data[""column_3""].max())
par2.set_ylim(data[""column_4""].min(), data[""column_4""].max())
</code></pre>
","9412297","","901925","","2018-08-31 20:56:47","2019-02-17 10:57:14","FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated use `arr[tuple(seq)]` instead of `arr[seq]`","<python><arrays><python-3.x><numpy><matplotlib>","3","6","4","","","CC BY-SA 4.0","0"
"37003862","1","37102815","","2016-05-03 12:14:39","","39","39075","<p>How can I upload a file to <a href=""https://cloud.google.com/storage/"" rel=""noreferrer"">Google Cloud Storage</a> from Python 3? Eventually Python 2, if it's infeasible from Python 3.</p>

<p>I've looked and looked, but haven't found a solution that actually works. I tried <a href=""https://github.com/boto/boto"" rel=""noreferrer"">boto</a>, but when I try to generate the necessary .boto file through <code>gsutil config -e</code>, it keeps saying that I need to configure authentication through <code>gcloud auth login</code>. However, I have done the latter a number of times, without it helping.</p>
","265261","","5780109","","2020-08-31 23:27:03","2020-08-31 23:27:03","How to upload a file to Google Cloud Storage on Python 3?","<python><python-3.x><google-cloud-storage><boto><google-cloud-platform>","4","1","14","","","CC BY-SA 4.0","0"
"29828477","1","29828599","","2015-04-23 15:50:19","","3","39064","<p>I have this code, and its meant to change the text of the <code>Instruction</code> label when the item button is pressed. It doesn't for some reason, and I'm not entirely sure why. I've tried creating another button in the <code>press()</code> function with the same names and parameters except a different text.</p>

<pre><code>import tkinter
import Theme
import Info

Tk = tkinter.Tk()
message = 'Not pressed.'

#Sets window Options
Tk.wm_title(Info.Title)
Tk.resizable(width='FALSE', height='FALSE')
Tk.wm_geometry(""%dx%d%+d%+d"" % (720, 480, 0, 0))


#Method run by item button
def press():
    message = 'Button Pressed'
    Tk.update()

#item button
item = tkinter.Button(Tk, command=press).pack()

#label
Instruction = tkinter.Label(Tk, text=message, bg=Theme.GUI_hl2, font='size, 20').pack()

#Background
Tk.configure(background=Theme.GUI_bg)
Tk.mainloop()
</code></pre>
","3121122","","","user2555451","2015-04-23 15:56:59","2018-10-02 03:09:13","How to change Tkinter label text on button press","<python><button><python-3.x><tkinter><label>","2","0","1","","","CC BY-SA 3.0","0"
"45981317","1","45982631","","2017-08-31 12:19:54","","6","39038","<p>I am using python 3.6.2 and using Emacs 25 for the development of a PyQt5 project in Ubuntu and it's running with root privileges. This works fine but I'm getting</p>

<pre><code>QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-root'
</code></pre>

<p>from the command line for each run.</p>

<p>It would great, if you let me understand what this is and the possible solution for to avoid the same.</p>

<p>The code </p>

<pre><code>class MainWindow(QtWidgets.QMainWindow, Ui_MainWindow):

  def __init__(self, parent=None):
    super(MainWindow, self).__init__(parent=parent)
    self.setupUi(self)
    # TODO: board connection
    self.comPort.addItems([str(port) for port in display_SerialPorts()])
    self.comPort.highlighted.connect(self.boardConnet)


  def boardConnet(self):
    baudrate = 9600
    port = self.comPort.currentText()
    ser = serial.Serial(
        port, baudrate, timeout=1)  # open first serial port
    ser.close()
    ser.open()
</code></pre>

<p>Thanks in advance for your time – if I’ve missed out anything, over- or under-emphasised a specific point let me know in the comments.</p>
","5209865","","369450","","2017-08-31 12:22:29","2017-08-31 19:37:11","Why do I get warning ""QStandardPaths: XDG_RUNTIME_DIR not set"" every time for a PyQt5 project","<python><python-3.x><ubuntu><pyqt><pyqt5>","1","1","1","","","CC BY-SA 3.0","0"
"42130504","1","","","2017-02-09 07:19:34","","22","38994","<p>I have installed the pyPdf module successfully using the command pip install pydf but when I use the module using the import command I get the following error:</p>

<pre><code>enC:\Anaconda3\lib\site-packages\pyPdf\__init__.py in &lt;module&gt;()
1 from pdf import PdfFileReader, PdfFileWriter
  2 __all__ = [""pdf""]
ImportError: No module named 'pdf'
</code></pre>

<p>What should I do? I have installed the pdf module as well but still the error does not go away.</p>
","3923482","","","","","2019-06-12 11:44:48","Unable to use pypdf module","<python-3.x><pypdf>","6","0","0","","","CC BY-SA 3.0","0"
"31252939","1","31253184","","2015-07-06 18:20:37","","41","38972","<p>I have a list of namedtuples named <code>Books</code> and am trying to increase the <code>price</code> field by 20% which does change the value of <code>Books</code>. I tried to do:</p>

<pre><code>from collections import namedtuple
Book = namedtuple('Book', 'author title genre year price instock')
BSI = [
       Book('Suzane Collins','The Hunger Games', 'Fiction', 2008, 6.96, 20),
       Book('J.K. Rowling', ""Harry Potter and the Sorcerer's Stone"", 'Fantasy', 1997, 4.78, 12)]
for item in BSI:
    item = item.price*1.10
print(item.price)
</code></pre>

<p>But I keep getting :</p>

<pre><code> Traceback (most recent call last):
 print(item.price)
 AttributeError: 'float' object has no attribute 'price'
</code></pre>

<p>I understand that I cannot set the fields in a namedtuple. How do I go about updating <code>price</code>?</p>

<p>I tried to make it into a function:</p>

<pre><code>def restaurant_change_price(rest, newprice):
    rest.price = rest._replace(price = rest.price + newprice)
    return rest.price

print(restaurant_change_price(Restaurant(""Taillevent"", ""French"", ""343-3434"", ""Escargots"", 24.50), 25))
</code></pre>

<p>but I get an error with replace saying:</p>

<pre><code> rest.price = rest._replace(price = rest.price + newprice)
 AttributeError: can't set attribute
</code></pre>

<p>Can someone let me know why this is happening?</p>
","5083429","","5083429","","2015-07-08 19:12:14","2019-02-05 16:46:06","Changing values of a list of namedtuples","<python><list><python-3.x><tuples>","3","0","7","","","CC BY-SA 3.0","0"
"47518669","1","49932170","","2017-11-27 19:45:35","","32","38944","<p>I'm doing something like this:</p>

<pre><code>import pathlib

p = pathlib.Path(""temp/"").mkdir(parents=True, exist_ok=True)

with p.open(""temp.""+fn, ""w"", encoding =""utf-8"") as f:
    f.write(result)
</code></pre>

<blockquote>
  <p>Error message: AttributeError: 'NoneType' object has no attribute 'open'</p>
</blockquote>

<p>Obviously, based on the error message, <code>mkdir</code> returns <code>None</code>. </p>

<p>Jean-Francois Fabre suggested this correction: </p>

<pre><code>p = pathlib.Path(""temp/"")
p.mkdir(parents=True, exist_ok=True)

with p.open(""temp.""+fn, ""w"", encoding =""utf-8"") as f:
    ...
</code></pre>

<p>This triggered a new error message:</p>

<blockquote>
  <p>File ""/Users/user/anaconda/lib/python3.6/pathlib.py"", line 1164, in open
      opener=self._opener)<br>
  TypeError: an integer is required (got type str)</p>
</blockquote>
","8138305","","4850040","","2017-12-05 12:04:38","2020-06-12 16:07:26","create new folder in python with pathlib and write files into it","<python-3.x><pathlib>","4","2","2","","","CC BY-SA 3.0","0"
"43146528","1","43146653","","2017-03-31 17:30:57","","54","38882","<p>Consider the following list:</p>

<pre><code>a_list = ['🤔 🙈 me así, bla es se 😌 ds 💕👭👙']
</code></pre>

<p>How can I extract in a new list all the emojis inside <code>a_list</code>?:</p>

<pre><code>new_lis = ['🤔 🙈 😌 💕 👭 👙']
</code></pre>

<p>I tried to use regex, but I do not have all the possible emojis encodings.</p>
","4140027","","","","","2020-09-15 20:52:40","How to extract all the emojis from text?","<python><python-3.x><emoji>","13","1","8","","","CC BY-SA 3.0","0"
"31964930","1","31964966","","2015-08-12 12:14:25","","19","38828","<p>I am trying to do exactly the following:</p>

<pre><code>&gt;&gt;&gt; x = (1,2)
&gt;&gt;&gt; y = 'hello'
&gt;&gt;&gt; '%d,%d,%s' % (x[0], x[1], y)
'1,2,hello'
</code></pre>

<p>However, I have a long <code>x</code>, more than two items, so I tried:</p>

<pre><code>&gt;&gt;&gt; '%d,%d,%s' % (*x, y)
</code></pre>

<p>but it is syntax error. What would be the proper way of doing this without indexing like the first example?</p>
","547820","","42223","","2017-12-22 14:36:17","2017-12-22 14:36:17","Python string formatting with percent sign","<python><string><python-3.x><string-formatting>","3","2","1","","","CC BY-SA 3.0","0"
"36050848","1","36050918","","2016-03-17 02:55:31","","8","38658","<p>I'm doing homework and I have all except this last problem figured out. I can't seem to figure out how to get a character not to show up if it's inserted at a larger insertion point than the string has. This is the problem I'm working on:</p>
<blockquote>
<p>Write a program insert.py that simulates insert behavior on a string.</p>
<p>The program takes three inputs: a character to be inserted, its position, and a string into which a character is to be inserted. The program prints the new version of the string. For example, if the arguments are: -, 2, below, then the program prints be-low. If a position passed to the program is outside of the bounds of the original string (in this case &lt;0 or &gt;5 -nothing would append on the end of below), then print the original string, without any changes (Note: an insert value of 0 OR the length of the string will add to the start or append to the end, i.e. T, 3, can will be canT; T, 0, can will be Tcan, and T, 4, can will be can).</p>
</blockquote>
<p>This is what I have done so far:</p>
<pre><code>C = input(&quot;Choose your charecter to insert. &quot;)
P = int(input(&quot;Choose your character's position. &quot;))
S = input(&quot;Choose your string. &quot;)
st = S[:P] + C + S[P:]

print(st)
print(C, P, S)
</code></pre>
","6069572","","13328195","","2020-10-13 09:30:58","2020-10-13 09:30:58","Inserting characters in strings in python","<python><python-3.x>","4","0","3","","","CC BY-SA 4.0","0"
"40701961","1","40701981","","2016-11-20 08:11:58","","13","38608","<p>I'm trying to find an elegant way to find the max value in a two-dimensional array.
for example for this array:</p>

<pre><code>[0, 0, 1, 0, 0, 1] [0, 1, 0, 2, 0, 0][0, 0, 2, 0, 0, 1][0, 1, 0, 3, 0, 0][0, 0, 0, 0, 4, 0]
</code></pre>

<p>I would like to extract the value '4'.
I thought of doing a max within max but I'm struggling in executing it.</p>
","7184688","","2225682","","2016-11-20 08:15:03","2020-09-05 21:39:10","Finding the Max value in a two dimensional Array","<arrays><list><python-3.x><max><iterable>","4","0","1","","","CC BY-SA 3.0","0"
"30952079","1","30952626","","2015-06-20 08:44:10","","16","38429","<p>I want to make a call using my GSM modem. So I wrote the below program:</p>

<pre><code>import time
import serial

recipient = ""+98xxxxxxxxxx""

phone = serial.Serial(""COM10"",  115200, timeout=5)
try:
    time.sleep(0.5)
    phone.write(b'ATZ\r')
    time.sleep(1)
    phone.write(b'ATD""'+recipient.encode() +b'""\r')
    while(1):
        print(phone.readline())
    time.sleep(0.5)
finally:
    phone.close()
</code></pre>

<p>But when I run it I receive this output:</p>

<pre><code>&gt;&gt;&gt; ================================ RESTART ================================
&gt;&gt;&gt; 
b'ATZ\r\r\n'
b'OK\r\n'
b'ATDxxxxxxxxxx\r\r\n'
b'NO CARRIER\r\n'
</code></pre>

<p><strong>What does this ""NO CARRIER"" error means?</strong></p>

<p><em>Note that I can send SMS successfully.</em></p>

<hr>

<p>This is the program that I use to send SMS:</p>

<pre><code>import time
import serial

recipient = ""+98xxxxxxxxxx""
message = ""Test""

phone = serial.Serial(""COM10"",  115200, timeout=5)


try:
    time.sleep(0.5)
    phone.write(b'ATZ\r')
    time.sleep(0.5)
    phone.write(b'AT+CMGF=1\r')
    time.sleep(0.5)
    phone.write(b'AT+CMGS=""' + recipient.encode() + b'""\r')
    time.sleep(0.5)
    phone.write(message.encode() + b""\r"")
    time.sleep(0.5)
    phone.write(bytes([26]))
    time.sleep(0.5)
finally:
    phone.close()
</code></pre>
","3580433","","3580433","","2015-06-20 09:10:24","2019-09-05 12:50:27","Receiving ""NO CARRIER"" error while tring to make a call using GSM modem in Python","<python><python-3.x><gsm><at-command><modem>","1","2","6","","","CC BY-SA 3.0","0"
"50161551","1","50163474","","2018-05-03 18:18:51","","18","38299","<p>Using a Raspberry Pi using Debian 4.14.34-v7+, I am trying to get <code>pipenv</code> set up with Python 3.6.5 as the default version of Python. I first install Python 3.6 by compiling it on the Pi (hours...). After making a 'robot' directory, I then install <code>pipenv</code> with <code>sudo pip3 install pipenv</code> and <code>pipenv install --three</code>.</p>

<p>Then I start the shell and open up Python, getting Python 3.5.3&hellip;</p>



<pre class=""lang-none prettyprint-override""><code>pi@raspberrypi:~/robot $ pipenv shell
Spawning environment shell (/bin/bash). Use 'exit' to leave.
. /home/pi/.local/share/virtualenvs/robot-XZ3Md9g0/bin/activate
pi@raspberrypi:~/robot $ . /home/pi/.local/share/virtualenvs/robot-XZ3Md9g0/bin/activate
(robot-XZ3Md9g0) pi@raspberrypi:~/robot $ python
Python 3.5.3 (default, Jan 19 2017, 14:11:04) 
[GCC 6.3.0 20170124] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; exit()
</code></pre>

<p>I then change the virtualenv by <code>pipenv --python 3.6</code>, but after it correctly (apparently) installs, I immediately get the warning that pipenv still expects Python 3.5&hellip;</p>

<pre class=""lang-none prettyprint-override""><code>(robot-XZ3Md9g0) pi@raspberrypi:~/robot $ pipenv --python 3.6
Virtualenv already exists!
Remove existing virtualenv? [Y/n]: y
Removing existing virtualenv…
Creating a virtualenv for this project…
Using /usr/local/bin/python3.6m (3.6.5) to create virtualenv…
⠋Running virtualenv with interpreter /usr/local/bin/python3.6m
Using base prefix '/usr/local'
New python executable in /home/pi/.local/share/virtualenvs/robot-XZ3Md9g0/bin/python3.6m
Also creating executable in /home/pi/.local/share/virtualenvs/robot-XZ3Md9g0/bin/python
Installing setuptools, pip, wheel...done.

Virtualenv location: /home/pi/.local/share/virtualenvs/robot-XZ3Md9g0
Warning: Your Pipfile requires python_version 3.5, but you are using 3.6.5 (/home/pi/.local/share/v/r/bin/python).
  $ pipenv check will surely fail.
(robot-XZ3Md9g0) pi@raspberrypi:~/robot $ python
Python 3.6.5 (default, May  3 2018, 11:25:17) 
[GCC 6.3.0 20170516] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; 
</code></pre>

<p>How do I set up <code>pipenv</code> to look for Python 3.6, when I first create the virtualenv? I can manually go in and edit the Pipfile, but that seems to be defeating the purpose of having <code>pipenv</code> take care of things for me.</p>
","4565870","","241211","","2020-05-06 17:28:03","2020-09-06 19:07:52","Set python version when creating virtualenv using pipenv","<python-3.x><pipenv>","4","0","7","","","CC BY-SA 4.0","0"
"43465082","1","43485264","","2017-04-18 06:28:21","","4","38197","<p>so i am trying to make my discord bot delete all messages in the text channel because i like it when it's clean but i cant figure out how to do it this is what i have tried </p>

<pre><code>@CLIENT.command()
async def Clear(message):
    return await CLIENT.delete_message(message)
</code></pre>

<p>can't seem to figure it out can someone help thanks &lt;3
i have tried other things and looked at other posts but i have only found out that the bot will delete the message everytime i type (not what im looking for)</p>
","7127740","","","","","2020-10-01 09:04:10","python Discord.py delete all messages in a text channel","<python><python-3.x><discord>","2","0","0","","","CC BY-SA 3.0","0"
"32532900","1","37093626","","2015-09-11 22:15:45","","54","38182","<p>Trying to create a super user for my database:</p>

<pre><code>manage.py createsuperuser
</code></pre>

<p>Getting a sad recursive message:</p>

<p><em>Superuser creation skipped due to not running in a TTY. You can run <code>manage.py createsuperuser</code> in your project to create one manually.</em></p>

<p>Seriously Django? Seriously?</p>

<p>The only information I found for this was the one listed above but it didn't work:
<a href=""https://stackoverflow.com/questions/26980003/unable-to-create-superuser-in-django-due-to-not-working-in-tty"">Unable to create superuser in django due to not working in TTY</a></p>

<p>And this other one here, which is basically the same:
<a href=""https://stackoverflow.com/questions/14059573/cant-create-super-user-django"">Can&#39;t Create Super User Django</a></p>
","1903784","","-1","","2017-05-23 12:18:03","2020-10-16 07:13:03","Not able to create super user with Django manage.py","<python><django><python-3.x><django-admin><django-1.8>","6","6","18","","","CC BY-SA 3.0","0"
"29804843","1","","","2015-04-22 17:26:53","","9","38175","<p>I am currently learning python through a video tutorial on youtube, and have come up against a formula I cannot seem to grasp, as nothing looks right to me.   The basic concept of the excersise is to make a mortgage calculator that asks the user to input 3 pieces of information,   Loan Amount, Interest Rate,  and Loan Term (years)</p>

<p>then it calculates the monthly payments to the user.  here is my code: </p>

<pre><code>__author__ = 'Rick'
# This program calculates monthly repayments on an interest rate loan/mortgage.

loanAmount = input(""How much do you want to borrow? \n"")
interestRate = input(""What is the interest rate on your loan? \n"")
repaymentLength = input(""How many years to repay your loan? \n"")

#converting the string input variables to float
loanAmount = float(loanAmount)
interestRate = float(interestRate)
repaymentLength = float(repaymentLength)

#working out the interest rate to a decimal number
interestCalculation = interestRate / 100

print(interestRate)
print(interestCalculation)

#working out the number of payments over the course of the loan period.
numberOfPayments = repaymentLength*12

#Formula
#M = L[i(1+i)n] / [(1+i)n-1]

#   * M = Monthly Payment (what were trying to find out)
#   * L = Loan Amount (loanAmount)
#   * I = Interest Rate (for an interest rate of 5%, i = 0.05 (interestCalculation)
#   * N = Number of Payments (repaymentLength)

monthlyRepaymentCost = loanAmount * interestCalculation * (1+interestCalculation) * numberOfPayments / ((1+interestCalculation) * numberOfPayments - 1)
#THIS IS FROM ANOTHER BIT OF CODE THAT IS SUPPOSE TO BE RIGHT BUT ISNT---
# repaymentCost = loanAmount * interestRate * (1+ interestRate) * numberOfPayments  / ((1 + interestRate) * numberOfPayments -1)

#working out the total cost of the repayment over the full term of the loan
totalCharge = (monthlyRepaymentCost * numberOfPayments) - loanAmount


print(""You want to borrow £"" + str(loanAmount) + "" over "" + str(repaymentLength) + "" years, with an interest rate of "" + str(interestRate) + ""%!"")

print(""Your monthly repayment will be £"" + str(monthlyRepaymentCost))

print(""Your monthly repayment will be £%.2f "" % monthlyRepaymentCost)

print(""The total charge on this loan will be £%.2f !"" % totalCharge)
</code></pre>

<p>Everything works, but the value it throws out at the end is completely wrong... a £100 loan with an interest rate of 10% over 1 year shouldn't be making me pay £0.83 per month.  Any help in getting my head around this equation to help me understand would be greatly appreciated.  </p>
","4820672","","984421","","2018-02-10 20:58:36","2018-12-14 17:13:39","Calculating mortgage interest in Python","<python><python-3.x><formula><calculator>","6","10","4","","","CC BY-SA 3.0","0"
"47606873","1","47607044","","2017-12-02 10:25:23","","9","37945","<p>I installed ""imbalanced-learn"" (version 0.3.1) on ANACONDA Navigator. 
When I ran an example from the imbalanced-learn website using Jupyter (Python 3), I got an message regarding ""ModuleNotFoundError"".  No module named 'imblearn"". </p>

<pre><code>from imblearn.datasets import make_imbalance
from imblearn.under_sampling import NearMiss
from imblearn.pipeline import make_pipeline
from imblearn.metrics import classification_report_imbalanced
</code></pre>

<p>How could I resolve this? </p>
","4492738","","4685471","","2020-05-16 14:48:11","2020-09-15 12:39:22","Jupyter: No module named 'imblearn"" after installation","<python-3.x><anaconda><imblearn>","10","0","2","","","CC BY-SA 3.0","0"
"28193025","1","","","2015-01-28 13:04:15","","58","37899","<p>I have a Flask (v0.10.1) application using Flask-SQLAlchemy (v2.0) and I'm trying to configure Pylint to check it. Running with Python 3.4.2.</p>

<p>First error was:</p>

<pre><code> Instance of 'SQLAlchemy' has no 'Table' member (no-member)
</code></pre>

<p>And I fixed this one ignoring the check for member attributes on SQLAlchemy:</p>

<pre><code>ignored-classes=SQLAlchemy
</code></pre>

<p>But I'm having a problem with the query member on entities:</p>

<pre><code>Class 'UserToken' has no 'query' member (no-member)
</code></pre>

<p>Is there any way to fix this issue without having to ignore no-member errors on every query call?</p>

<hr>

<p>Flask bootstrap:</p>

<pre><code>from flask import Flask
from flask_sqlalchemy import SQLAlchemy

db = SQLAlchemy()
app = Flask(__name__)
db.init_app(app)
app.run()
</code></pre>

<p>UserToken entity:</p>

<pre><code>from app import db

class UserToken(db.Model):
    user_token_id = db.Column(db.Integer, primary_key=True, index=True)
    token_auth = db.Column(db.String(64), unique=True, nullable=False, index=True)
</code></pre>

<p>The controller:</p>

<pre><code>from entities import UserToken

token = UserToken.query.filter(
    UserToken.token_auth == token_hash,
).first()
</code></pre>
","4275410","","4275410","","2015-01-28 13:37:27","2020-08-01 20:46:31","Pylint can't find SQLAlchemy query member","<python><python-3.x><sqlalchemy><flask-sqlalchemy><pylint>","13","1","17","","","CC BY-SA 3.0","0"
"39359245","1","39359270","","2016-09-06 23:35:09","","43","37859","<p>What is the most idiomatic/efficient way to convert from a modification time retrieved from <code>stat()</code> call to a <code>datetime</code> object? I came up with the following (python3):</p>

<pre><code>from datetime import datetime, timedelta, timezone
from pathlib import Path

path = Path('foo')
path.touch()
statResult = path.stat()
epoch = datetime(1970, 1, 1, tzinfo=timezone.utc)
modified = epoch + timedelta(seconds=statResult.st_mtime)
print('modified', modified)
</code></pre>

<p>Seems round a bout, and a bit surprising that I have to hard code the Unix epoch in there. Is there a more direct way?</p>
","999530","","","","","2019-08-22 16:43:01","From stat().st_mtime to datetime?","<python><python-3.x><datetime><stat><pathlib>","2","0","7","","","CC BY-SA 3.0","0"
"32364127","1","32364177","","2015-09-02 22:24:18","","2","37791","<p>I am trying to save the result of a function into a variable and print that variable on the screen, but when I print I see ""none"". <br>
How to repair this?</p>

<pre><code>import time;


def hours():
    localtime =  time.localtime(time.time())
    print (localtime.tm_hour)

def minutes():
    localtime =  time.localtime(time.time()) 
    print (localtime.tm_min)

def seconds():
   localtime =  time.localtime(time.time())     
   print (localtime.tm_sec)


hours()
minutes()
seconds()

var = hours()
print(var)
</code></pre>
","4578904","","266068","","2020-04-12 18:37:49","2020-04-12 18:37:49","Store function result into variable","<python><python-3.x>","4","3","3","","","CC BY-SA 4.0","0"
"31161635","1","31184395","","2015-07-01 12:41:12","","3","37784","<p>I tried to install ephem module on my Windows 8.1 using </p>

<pre><code>pip install ephem
</code></pre>

<p>but I get this error:</p>

<p><code>Microsoft Visual C++ 10.0 is required (Unable to find vcvarsall.bat).</code></p>

<p>I'm using Python 3.4.3 
Please tell me exactly what I should do to make this work. I went through many solutions but it didn't help me.</p>

<p>These are a few:</p>

<p><a href=""https://stackoverflow.com/questions/28251314/error-microsoft-visual-c-10-0-is-required-unable-to-find-vcvarsall-bat"">Error: Microsoft Visual C++ 10.0 is required (Unable to find vcvarsall.bat)</a></p>

<p><a href=""https://stackoverflow.com/questions/27670365/python-pip-install-error-unable-to-find-vcvarsall-bat-tried-all-solutions"">Python Pip install Error: Unable to find vcvarsall.bat. Tried all solutions</a></p>
","","user4988789","-1","user4988789","2017-05-23 11:45:44","2020-02-18 09:14:16","Microsoft Visual C++ 10.0 is required (Unable to find vcvarsall.bat)","<python-3.x><pip>","2","2","4","","","CC BY-SA 3.0","0"
"34499400","1","34499904","","2015-12-28 19:20:24","","41","37424","<p>How can one add a new coroutine to a running asyncio loop? Ie. one that is already executing a set of coroutines.</p>

<p>I guess as a workaround one could wait for existing coroutines to complete and then initialize a new loop (with the additional coroutine). But is there a better way?</p>
","1256394","","1256394","","2015-12-29 06:37:53","2020-02-16 19:06:18","how to add a coroutine to a running asyncio loop?","<python><python-3.x><asynchronous><python-asyncio>","4","0","13","","","CC BY-SA 3.0","0"
"33418903","1","","","2015-10-29 15:50:29","","1","37421","<p>What does this error mean? </p>

<pre><code>labels, freq = zip(*terms_hash)
ValueError: not enough values to unpack (expected 2, got 0) 
</code></pre>

<p>When I just print the terms out there is no error.</p>

<p>code:  </p>

<pre class=""lang-python prettyprint-override""><code>fname = 'stream.json'
with open(fname, 'r') as f:
    print('alle Hashtags')
    count_all = Counter()
    for line in f:
        tweet = json.loads(line)
# Count hashtags only

        terms_hash = [term for term in preprocess(tweet['text']) 
              if term.startswith('#')]

        # Update the counter
        count_all.update(terms_hash)

        terms_hash = count_all.most_common(5)

        labels, freq = zip(*terms_hash)
        data = {'data': freq, 'x': labels}
        bar = vincent.Bar(data, iter_idx='x')
        bar.to_json('term_freq.json')    


    # Print the first 5 most frequent words
    print(count_all.most_common(5))
</code></pre>
","4853434","","3001761","","2015-10-29 15:53:45","2015-10-29 15:55:57","python: ValueError: not enough values to unpack (expected 2, got 0)","<python><python-3.x><tweets>","1","4","0","","","CC BY-SA 3.0","0"
"33674033","1","33674108","","2015-11-12 14:41:35","","26","37351","<p>I know how to convert a dictionary into an list in Python, but somehow when I try to get, say, the sum of the resulting list, I get the error <code>'dict_values' object is not subscriptable</code>. Also, I plan to sum only several items in the list.</p>

<pre><code>dict = {A:1, B:2, C:3, D:4}
arr = dict.values()
the_sum = sum(arr[1:3])
</code></pre>

<p>Upon closer inspection, I noticed that when the resulting list is printed out, it always gives <code>dict_values(......)</code> as the output which I can't remove. How do I get around this?</p>
","5311374","","2867928","","2019-02-07 09:00:41","2019-12-12 19:41:37","Python: how to convert a dictionary into a subscriptable array?","<python><python-3.x><dictionary>","4","3","3","","","CC BY-SA 4.0","0"
"49039436","1","49039535","","2018-02-28 22:09:20","","26","37153","<p>I have a project which I want to structure like this:</p>

<pre><code>myproject
  __init__.py
  api
    __init__.py
    api.py
  backend
    __init__.py
    backend.py
  models
    __init__.py
    some_model.py
</code></pre>

<p>Now, I want to import the module <code>some_model.py</code> in both <code>api.py</code> and <code>backend.py</code>. How do I properly do this?</p>

<p>I tried:</p>

<pre><code>from models import some_model
</code></pre>

<p>but that fails with <code>ModuleNotFoundError: No module named 'models'</code>.</p>

<p>I also tried:</p>

<pre><code>from ..models import some_model
</code></pre>

<p>which gave me <code>ValueError: attempted relative import beyond top-level package</code>. </p>

<p>What am I doing wrong here? How can I import a file from a different directory, which is not a subdirectory?</p>
","4437417","","355230","","2020-01-26 17:23:12","2020-10-14 23:26:55","How to import a module from a different folder?","<python><python-3.x><python-packaging>","4","0","6","","","CC BY-SA 4.0","0"
"44476790","1","44476996","","2017-06-10 18:37:15","","2","37143","<p>I am trying to segment hand from the depth image attached in this question. In the process, I wrote the below code which process the histogram of the image first and then does median filtering on the processed image. But this code is giving an error continuously even after trying hard to solve it.  </p>

<pre><code>    import numpy as np
    import matplotlib
    import matplotlib.pyplot as plt
    import matplotlib.image as mpimg
    import scipy
    from scipy import ndimage, misc
    from scipy.misc import imshow
    import skimage
    img = mpimg.imread('C:/Users/Prachi/Desktop/kinect_leap_dataset/acquisitions/P1/G1/1_depth.png')
    plt.hist(img.ravel(), bins=256, range=(0.0, 1.0))
    plt.show()
    imgplot = plt.imshow(img, clim=(0.064, 0.068))
    #plt.axis('off')
    plt.show()
    mod_img = ndimage.median_filter(imgplot, 20)
    plt.imshow(mod_img)
    plt.show()
</code></pre>

<p>This is the error I am getting. Kindly help in solving this error. I have checked each and every thread regarding this error, but was not successful in solving it. 
It seems that the code is doing median filtering but is not able to display the image as the error is occuring on the line:</p>

<pre><code>plt.imshow(mod_img)

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-129-8482175fdf0e&gt; in &lt;module&gt;()
     16 plt.show()
     17 mod_img = ndimage.median_filter(imgplot, 20)
---&gt; 18 plt.imshow(mod_img)
     19 plt.show()

C:\Users\Prachi\AppData\Local\Programs\Python\Python36-32\Anaconda3\lib\site-packages\matplotlib\pyplot.py in imshow(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, data, **kwargs)
   3155                         filternorm=filternorm, filterrad=filterrad,
   3156                         imlim=imlim, resample=resample, url=url, data=data,
-&gt; 3157                         **kwargs)
   3158     finally:
   3159         ax._hold = washold

C:\Users\Prachi\AppData\Local\Programs\Python\Python36-32\Anaconda3\lib\site-packages\matplotlib\__init__.py in inner(ax, *args, **kwargs)
   1895                     warnings.warn(msg % (label_namer, func.__name__),
   1896                                   RuntimeWarning, stacklevel=2)
-&gt; 1897             return func(ax, *args, **kwargs)
   1898         pre_doc = inner.__doc__
   1899         if pre_doc is None:

C:\Users\Prachi\AppData\Local\Programs\Python\Python36-32\Anaconda3\lib\site-packages\matplotlib\axes\_axes.py in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)
   5122                               resample=resample, **kwargs)
   5123 
-&gt; 5124         im.set_data(X)
   5125         im.set_alpha(alpha)
   5126         if im.get_clip_path() is None:

C:\Users\Prachi\AppData\Local\Programs\Python\Python36-32\Anaconda3\lib\site-packages\matplotlib\image.py in set_data(self, A)
    594         if (self._A.dtype != np.uint8 and
    595                 not np.can_cast(self._A.dtype, np.float)):
--&gt; 596             raise TypeError(""Image data can not convert to float"")
    597 
    598         if (self._A.ndim not in (2, 3) or

TypeError: Image data can not convert to float
</code></pre>

<p>As I cannot attach image so here are the details of the image. </p>

<p>The image (gray) is the '1_depth.png' from the dataset ""Microsoft Kinect and Leap Motion"" at </p>

<p><a href=""http://lttm.dei.unipd.it/downloads/gesture/"" rel=""nofollow noreferrer"">http://lttm.dei.unipd.it/downloads/gesture/</a></p>
","7917358","","7917358","","2017-06-10 18:51:45","2020-10-23 22:55:00","TypeError: Image data can not convert to float on plt.imshow()","<python-3.x><image-processing><matplotlib><jupyter-notebook>","3","0","","","","CC BY-SA 3.0","0"
"38189070","1","","","2016-07-04 16:30:51","","46","37064","<p>I am trying to do some vanilla pattern recognition with an LSTM using Keras to predict the next element in a sequence.</p>

<p>My data look like this:</p>

<p><a href=""https://i.stack.imgur.com/NnEvI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NnEvI.png"" alt=""My data""></a></p>

<p>where the label of the training sequence is the last element in the list: <code>X_train['Sequence'][n][-1]</code>.</p>

<p>Because my <code>Sequence</code> column can have a variable number of elements in the sequence, I believe an RNN to be the best model to use. Below is my attempt to build an LSTM in Keras:</p>

<pre><code># Build the model

# A few arbitrary constants...
max_features = 20000
out_size = 128

# The max length should be the length of the longest sequence (minus one to account for the label)
max_length = X_train['Sequence'].apply(len).max() - 1

# Normal LSTM model construction with sigmoid activation
model = Sequential()
model.add(Embedding(max_features, out_size, input_length=max_length, dropout=0.2))
model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))
model.add(Dense(1))
model.add(Activation('sigmoid'))

# try using different optimizers and different optimizer configs
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
</code></pre>

<p>And here's how I attempt to train my model:</p>

<pre><code># Train the model
for seq in X_train['Sequence']:
    print(""Length of training is {0}"".format(len(seq[:-1])))
    print(""Training set is {0}"".format(seq[:-1]))
    model.fit(np.array([seq[:-1]]), [seq[-1]])
</code></pre>

<p>My output is this:</p>

<pre><code>Length of training is 13
Training set is [1, 3, 13, 87, 1053, 28576, 2141733, 508147108, 402135275365, 1073376057490373, 9700385489355970183, 298434346895322960005291, 31479360095907908092817694945]
</code></pre>

<p>However, I get the following error:</p>

<pre><code>Exception: Error when checking model input: expected embedding_input_1 to have shape (None, 347) but got array with shape (1, 13)
</code></pre>

<p>I believe my training step is correctly setup, so my model construction must be wrong. Note that 347 is <code>max_length</code>.</p>

<p>How can I correctly build a variable-length input LSTM in Keras? I'd prefer not to pad the data. Not sure if it's relevant, but I'm using the Theano backend.</p>
","2883245","","2883245","","2016-07-07 10:30:20","2017-08-07 12:03:44","How do I create a variable-length input LSTM in Keras?","<python-3.x><keras><lstm><recurrent-neural-network><variable-length>","3","2","23","","","CC BY-SA 3.0","0"
"36103034","1","37819509","","2016-03-19 14:38:54","","17","36977","<p>When I try to import <code>psycopg2</code> it show below log for me: </p>

<pre><code>Traceback (most recent call last):
  File ""D:/Desktop/learn/python/webcatch/appserver/testpgsql.py"", line 2, in &lt;module&gt;
    import psycopg2
  File ""D:/Desktop/learn/python/webcatch/appserver/webcatch/lib/site-packages/psycopg2-2.6.1-py3.5-win32.egg/psycopg2/__init__.py"", line 50, in &lt;module&gt;
    from psycopg2._psycopg import BINARY, NUMBER, STRING, DATETIME, ROWID
ImportError: No module named 'psycopg2._psycopg'
</code></pre>

<p>How can I solve it?
My platform is win10 (64) and version is python 3.5</p>
","4754280","","5827215","","2016-03-19 15:22:45","2019-12-08 11:53:08","ImportError: No module named 'psycopg2._psycopg'","<python-3.x><psycopg2><importerror>","9","4","2","","","CC BY-SA 3.0","0"
"32417338","1","32417349","","2015-09-05 20:21:34","","1","36934","<p>So I was doing while loops and I noticed something strange.</p>

<pre><code>count = 0

while count &lt;= 5:
    count += 1
    print(count)
</code></pre>

<p>output:</p>

<pre><code>1
2
3
4
5
6
</code></pre>

<p>it's not that I don't understand while loops. It's that how come the count is printed up to six? when it's supposed to print <code>count</code> only if <code>count</code> is less than or equal to 5?</p>

<p>and well 6 is beyond 5. why is this? </p>

<p>I know I could do</p>

<pre><code>count = 0

    while count != 5:
        count += 1
        print(count)
</code></pre>

<p>but I just want to understand why does putting <code>&lt;=</code> behave in an odd way?</p>
","5166790","","100297","","2015-09-05 20:42:04","2018-08-30 11:24:32","while loop and less than or equal to sign (Python)","<python><python-3.x><while-loop>","5","0","","","","CC BY-SA 3.0","0"
"31279446","1","33615230","","2015-07-07 21:17:42","","59","36922","<p>I have a project having the structure</p>

<pre><code>/example
../prediction
....__init__.py
....a.py
</code></pre>

<p>PYTHONPATH is pointed to /example</p>

<p>now I open the python in terminal and type</p>

<pre><code>import prediction
</code></pre>

<p>it succeeded, but if I type</p>

<pre><code>import prediction.a
</code></pre>

<p>it returns error</p>

<pre><code>ImportError: No module named 'prediction.a'; 'prediction' is not a package
</code></pre>

<p>why is that? isn't that already imported as a package</p>
","2999675","","","","","2016-06-23 10:14:42","Import error, No module named xxxx","<python><python-3.x>","1","5","2","2019-06-10 14:46:53","","CC BY-SA 3.0","0"
"49776619","1","","","2018-04-11 13:36:23","","14","36901","<p>I'm learning flask web microframework and after initialization of my database I run <code>flask db init</code> I run <code>flask db migrate</code>, to migrate my models classes to the database and i got an error.  I work on Windows 10, the database is MySQL, and extensions install are <code>flask-migrate</code>, <code>flask-sqlalchemy</code>, <code>flask-login</code>.</p>

<pre><code>(env) λ flask db migrate
Traceback (most recent call last):
  File ""c:\python36\Lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\python36\Lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\aka\Dev\dream-team\env\Scripts\flask.exe\__main__.py"", line 9, in &lt;module&gt;
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask\cli.py"", line 513, in main
    cli.main(args=args, prog_name=name)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask\cli.py"", line 380, in main
    return AppGroup.main(self, *args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 697, in main
    rv = self.invoke(ctx)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 1066, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 1066, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 895, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 535, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask\cli.py"", line 257, in decorator
    return __ctx.invoke(f, *args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\click\core.py"", line 535, in invoke
    return callback(*args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask_migrate\cli.py"", line 90, in migrate
    rev_id, x_arg)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\flask_migrate\__init__.py"", line 197, in migrate
    version_path=version_path, rev_id=rev_id)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\command.py"", line 176, in revision
    script_directory.run_env()
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\script\base.py"", line 427, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\util\pyfiles.py"", line 81, in load_python_file
    module = load_module_py(module_id, path)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\alembic\util\compat.py"", line 83, in load_module_py
    spec.loader.exec_module(module)
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 678, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 219, in _call_with_frames_removed
  File ""migrations\env.py"", line 87, in &lt;module&gt;
    run_migrations_online()
  File ""migrations\env.py"", line 70, in run_migrations_online
    poolclass=pool.NullPool)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\__init__.py"", line 465, in engine_from_config
    return create_engine(url, **options)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\__init__.py"", line 424, in create_engine
    return strategy.create(*args, **kwargs)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\strategies.py"", line 50, in create
    u = url.make_url(name_or_url)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\url.py"", line 211, in make_url
    return _parse_rfc1738_args(name_or_url)
  File ""c:\users\aka\dev\dream-team\env\lib\site-packages\sqlalchemy\engine\url.py"", line 270, in _parse_rfc1738_args
    ""Could not parse rfc1738 URL from string '%s'"" % name)
sqlalchemy.exc.ArgumentError: Could not parse rfc1738 URL from string 'mysql/dt_admin:dt2016@localhost/dreamteam_db'
</code></pre>
","8559808","","8712097","","2018-04-11 20:26:02","2018-04-13 15:53:02","sqlalchemy.exc.ArgumentError: Could not parse rfc1738 URL from string","<python-3.x><flask><flask-sqlalchemy><flask-login><flask-migrate>","2","2","1","","","CC BY-SA 3.0","0"
"33128325","1","33134213","","2015-10-14 14:36:09","","92","36897","<p>How can I define a class with <code>await</code> in the constructor or class body?</p>

<p>For example what I want:</p>

<pre class=""lang-py prettyprint-override""><code>import asyncio

# some code


class Foo(object):

    async def __init__(self, settings):
        self.settings = settings
        self.pool = await create_pool(dsn)

foo = Foo(settings)
# it raises:
# TypeError: __init__() should return None, not 'coroutine'
</code></pre>

<p>or example with class body attribute:</p>

<pre class=""lang-py prettyprint-override""><code>class Foo(object):

    self.pool = await create_pool(dsn)  # Sure it raises syntax Error

    def __init__(self, settings):
        self.settings = settings

foo = Foo(settings)
</code></pre>

<p>My solution (But I would like to see a more elegant way)</p>

<pre class=""lang-py prettyprint-override""><code>class Foo(object):

    def __init__(self, settings):
        self.settings = settings

    async def init(self):
        self.pool = await create_pool(dsn)

foo = Foo(settings)
await foo.init()
</code></pre>
","1026990","","2073595","","2015-10-14 18:18:50","2020-07-29 20:59:53","How to set class attribute with await in __init__","<python><python-3.x><python-asyncio>","5","4","37","","","CC BY-SA 3.0","0"
"38651672","1","","","2016-07-29 06:12:34","","18","36783","<p>The <a href=""https://www.python.org/downloads/windows/"" rel=""noreferrer"">downloads page</a> for Python for Windows offers a <em>""web-based""</em> installer and an <em>""executable""</em> installer. What's the difference?</p>
","2751481","","1709587","","2018-02-06 21:09:44","2019-01-30 01:29:15","Difference between web-based and executable installers for Python 3 on Windows","<python><windows><python-3.x>","3","1","3","","","CC BY-SA 3.0","0"
"34739315","1","34739687","","2016-01-12 09:02:48","","39","36710","<p>I am new to Python programming. Can anybody provide an explanation on what a *.pyw file is and how it works. </p>
","5766600","","119775","","2017-09-06 07:54:39","2020-02-12 06:49:28",".pyw files in python program","<python><python-3.x>","2","1","11","","","CC BY-SA 3.0","0"
"33053641","1","","","2015-10-10 11:48:33","","8","36704","<p>With:</p>

<pre><code>sentence= input(""Enter a sentence"")
keyword= input(""Input a keyword from the sentence"")
</code></pre>

<p>I want to find the position of the keyword in the sentence. So far, I have this code which gets rid of the punctuation and makes all letters lowercase:</p>

<pre><code>punctuations = '''!()-[]{};:'""\,&lt;&gt;./?@#$%^&amp;*_~'''#This code defines punctuation
#This code removes the punctuation
no_punct = """" 
for char in sentence:
   if char not in punctuations:
       no_punct = no_punct + char

no_punct1 =(str.lower (no_punct)
</code></pre>

<p>I know need a piece of code which actually finds the position of the word.</p>
","5430794","","2867928","","2018-01-10 10:41:17","2018-01-10 10:41:17","Finding the position of a word in a string","<python><string><python-3.x><find>","2","1","2","","","CC BY-SA 3.0","0"
"57381430","1","57382914","","2019-08-06 17:43:46","","51","36577","<p>I installed TensorFlow 1.10.1 but when I tried to import TensorFlow it said that I need TensorFlow version 1.10.0. Thus, I installed it and now I get the following warnings:</p>

<pre><code>&gt;&gt;&gt; import tensorflow
C:\Users\PC\Anaconda3\envs\tut\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
C:\Users\PC\Anaconda3\envs\tut\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
C:\Users\PC\Anaconda3\envs\tut\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
C:\Users\PC\Anaconda3\envs\tut\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
C:\Users\PC\Anaconda3\envs\tut\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
C:\Users\PC\Anaconda3\envs\tut\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
C:\Users\PC\Anaconda3\envs\tut\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
C:\Users\PC\Anaconda3\envs\tut\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
C:\Users\PC\Anaconda3\envs\tut\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
C:\Users\PC\Anaconda3\envs\tut\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
C:\Users\PC\Anaconda3\envs\tut\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
C:\Users\PC\Anaconda3\envs\tut\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
</code></pre>
","11423611","","9805238","","2019-08-07 09:32:42","2020-10-15 03:42:09","""synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'."" problem in TensorFlow","<python><python-3.x><numpy><tensorflow><artificial-intelligence>","7","1","7","","","CC BY-SA 4.0","0"
"52265978","1","52267075","","2018-09-10 21:52:11","","17","36403","<p>I'm trying to delete a file that I uploaded on Google colab using the following code:</p>

<pre><code>from google.colab import files
uploaded = files.upload()
</code></pre>

<p>How to delete the file now? e.g If the file's name is 'sample.jpg' .</p>
","8185479","","","","","2020-09-25 10:06:29","How to delete a locally uploaded file on google colab?","<python-3.x><keras><jupyter-notebook><google-colaboratory>","6","0","4","","","CC BY-SA 4.0","0"
"51721695","1","","","2018-08-07 07:58:20","","9","36388","<p>Visual studio 2015 is not installed
I have installed all the requirements before building dlib in python:
Cmake</p>

<p>python 3.6 and other bindings.</p>

<p>while installing dlib using pip:</p>

<pre><code>pip install dlib
</code></pre>

<p>Got an error:</p>

<pre><code>Collecting dlib
  Using cached https://files.pythonhosted.org/packages/df/aa/6a9bb2a763107bb2606d6ee1aa65fcd3b51375a9ef6436e9c9280b0dd63c/dlib-19.15.0.tar.gz
Building wheels for collected packages: dlib
  Running setup.py bdist_wheel for dlib ... error
  Complete output from command ""c:\users\vikas tiwari\anaconda3\python.exe"" -u -c ""import setuptools, tokenize;__file__='C:\\Users\\VIKAST~1\\AppData\\Local\\Temp\\pip-install-4f4b8868\\dlib\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" bdist_wheel -d C:\Users\VIKAST~1\AppData\Local\Temp\pip-wheel-nnayeybw --python-tag cp36:
  running bdist_wheel
  running build
  running build_py
  package init file 'dlib\__init__.py' not found (or not a regular file)
  running build_ext
  Building extension for Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]
  Invoking CMake setup: 'cmake C:\Users\VIKAST~1\AppData\Local\Temp\pip-install-4f4b8868\dlib\tools\python -DCMAKE_LIBRARY_OUTPUT_DIRECTORY=C:\Users\VIKAST~1\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\lib.win-amd64-3.6 -DPYTHON_EXECUTABLE=c:\users\vikas tiwari\anaconda3\python.exe -DCMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE=C:\Users\VIKAST~1\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\lib.win-amd64-3.6 -A x64'
  -- Building for: Visual Studio 14 2015
  -- Selecting Windows SDK version  to target Windows 10.0.17134.
  CMake Error in CMakeLists.txt:
    Failed to run MSBuild command:

      C:/Program Files (x86)/MSBuild/14.0/bin/MSBuild.exe

    to get the value of VCTargetsPath:

      Microsoft (R) Build Engine version 14.0.25420.1
      Copyright (C) Microsoft Corporation. All rights reserved.

      Build started 8/7/2018 1:07:47 PM.
      Project ""C:\Users\Vikas Tiwari\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\temp.win-amd64-3.6\Release\CMakeFiles\3.12.0\VCTargetsPath.vcxproj"" on node 1 (default targets).
      C:\Users\Vikas Tiwari\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\temp.win-amd64-3.6\Release\CMakeFiles\3.12.0\VCTargetsPath.vcxproj(14,2): error MSB4019: The imported project ""C:\Microsoft.Cpp.Default.props"" was not found. Confirm that the path in the &lt;Import&gt; declaration is correct, and that the file exists on disk.
      Done Building Project ""C:\Users\Vikas Tiwari\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\temp.win-amd64-3.6\Release\CMakeFiles\3.12.0\VCTargetsPath.vcxproj"" (default targets) -- FAILED.

      Build FAILED.

      ""C:\Users\Vikas Tiwari\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\temp.win-amd64-3.6\Release\CMakeFiles\3.12.0\VCTargetsPath.vcxproj"" (default target) (1) -&gt;
        C:\Users\Vikas Tiwari\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\temp.win-amd64-3.6\Release\CMakeFiles\3.12.0\VCTargetsPath.vcxproj(14,2): error MSB4019: The imported project ""C:\Microsoft.Cpp.Default.props"" was not found. Confirm that the path in the &lt;Import&gt; declaration is correct, and that the file exists on disk.

          0 Warning(s)
          1 Error(s)

      Time Elapsed 00:00:00.03


    Exit code: 1



  -- Configuring incomplete, errors occurred!
  See also ""C:/Users/Vikas Tiwari/AppData/Local/Temp/pip-install-4f4b8868/dlib/build/temp.win-amd64-3.6/Release/CMakeFiles/CMakeOutput.log"".
  Traceback (most recent call last):
    File ""&lt;string&gt;"", line 1, in &lt;module&gt;
    File ""C:\Users\VIKAST~1\AppData\Local\Temp\pip-install-4f4b8868\dlib\setup.py"", line 257, in &lt;module&gt;
      'Topic :: Software Development',
    File ""c:\users\vikas tiwari\anaconda3\lib\site-packages\setuptools\__init__.py"", line 129, in setup
      return distutils.core.setup(**attrs)
    File ""c:\users\vikas tiwari\anaconda3\lib\distutils\core.py"", line 148, in setup
      dist.run_commands()
    File ""c:\users\vikas tiwari\anaconda3\lib\distutils\dist.py"", line 955, in run_commands
      self.run_command(cmd)
    File ""c:\users\vikas tiwari\anaconda3\lib\distutils\dist.py"", line 974, in run_command
      cmd_obj.run()
    File ""c:\users\vikas tiwari\anaconda3\lib\site-packages\wheel\bdist_wheel.py"", line 202, in run
      self.run_command('build')
    File ""c:\users\vikas tiwari\anaconda3\lib\distutils\cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""c:\users\vikas tiwari\anaconda3\lib\distutils\dist.py"", line 974, in run_command
      cmd_obj.run()
    File ""c:\users\vikas tiwari\anaconda3\lib\distutils\command\build.py"", line 135, in run
      self.run_command(cmd_name)
    File ""c:\users\vikas tiwari\anaconda3\lib\distutils\cmd.py"", line 313, in run_command
      self.distribution.run_command(command)
    File ""c:\users\vikas tiwari\anaconda3\lib\distutils\dist.py"", line 974, in run_command
      cmd_obj.run()
    File ""C:\Users\VIKAST~1\AppData\Local\Temp\pip-install-4f4b8868\dlib\setup.py"", line 133, in run
      self.build_extension(ext)
    File ""C:\Users\VIKAST~1\AppData\Local\Temp\pip-install-4f4b8868\dlib\setup.py"", line 170, in build_extension
      subprocess.check_call(cmake_setup, cwd=build_folder)
    File ""c:\users\vikas tiwari\anaconda3\lib\subprocess.py"", line 291, in check_call
      raise CalledProcessError(retcode, cmd)
  subprocess.CalledProcessError: Command '['cmake', 'C:\\Users\\VIKAST~1\\AppData\\Local\\Temp\\pip-install-4f4b8868\\dlib\\tools\\python', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=C:\\Users\\VIKAST~1\\AppData\\Local\\Temp\\pip-install-4f4b8868\\dlib\\build\\lib.win-amd64-3.6', '-DPYTHON_EXECUTABLE=c:\\users\\vikas tiwari\\anaconda3\\python.exe', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE=C:\\Users\\VIKAST~1\\AppData\\Local\\Temp\\pip-install-4f4b8868\\dlib\\build\\lib.win-amd64-3.6', '-A', 'x64']' returned non-zero exit status 1.

  ----------------------------------------
  Failed building wheel for dlib
  Running setup.py clean for dlib
Failed to build dlib
Installing collected packages: dlib
  Running setup.py install for dlib ... error
    Complete output from command ""c:\users\vikas tiwari\anaconda3\python.exe"" -u -c ""import setuptools, tokenize;__file__='C:\\Users\\VIKAST~1\\AppData\\Local\\Temp\\pip-install-4f4b8868\\dlib\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\VIKAST~1\AppData\Local\Temp\pip-record-aol24hcc\install-record.txt --single-version-externally-managed --compile:
    running install
    running build
    running build_py
    package init file 'dlib\__init__.py' not found (or not a regular file)
    running build_ext
    Building extension for Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)]
    Invoking CMake setup: 'cmake C:\Users\VIKAST~1\AppData\Local\Temp\pip-install-4f4b8868\dlib\tools\python -DCMAKE_LIBRARY_OUTPUT_DIRECTORY=C:\Users\VIKAST~1\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\lib.win-amd64-3.6 -DPYTHON_EXECUTABLE=c:\users\vikas tiwari\anaconda3\python.exe -DCMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE=C:\Users\VIKAST~1\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\lib.win-amd64-3.6 -A x64'
    -- Building for: Visual Studio 14 2015
    -- Selecting Windows SDK version  to target Windows 10.0.17134.
    CMake Error in CMakeLists.txt:
      Failed to run MSBuild command:

        C:/Program Files (x86)/MSBuild/14.0/bin/MSBuild.exe

      to get the value of VCTargetsPath:

        Microsoft (R) Build Engine version 14.0.25420.1
        Copyright (C) Microsoft Corporation. All rights reserved.

        Build started 8/7/2018 1:07:50 PM.
        Project ""C:\Users\Vikas Tiwari\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\temp.win-amd64-3.6\Release\CMakeFiles\3.12.0\VCTargetsPath.vcxproj"" on node 1 (default targets).
        C:\Users\Vikas Tiwari\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\temp.win-amd64-3.6\Release\CMakeFiles\3.12.0\VCTargetsPath.vcxproj(14,2): error MSB4019: The imported project ""C:\Microsoft.Cpp.Default.props"" was not found. Confirm that the path in the &lt;Import&gt; declaration is correct, and that the file exists on disk.
        Done Building Project ""C:\Users\Vikas Tiwari\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\temp.win-amd64-3.6\Release\CMakeFiles\3.12.0\VCTargetsPath.vcxproj"" (default targets) -- FAILED.

        Build FAILED.

        ""C:\Users\Vikas Tiwari\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\temp.win-amd64-3.6\Release\CMakeFiles\3.12.0\VCTargetsPath.vcxproj"" (default target) (1) -&gt;
          C:\Users\Vikas Tiwari\AppData\Local\Temp\pip-install-4f4b8868\dlib\build\temp.win-amd64-3.6\Release\CMakeFiles\3.12.0\VCTargetsPath.vcxproj(14,2): error MSB4019: The imported project ""C:\Microsoft.Cpp.Default.props"" was not found. Confirm that the path in the &lt;Import&gt; declaration is correct, and that the file exists on disk.

            0 Warning(s)
            1 Error(s)

        Time Elapsed 00:00:00.03


      Exit code: 1



    -- Configuring incomplete, errors occurred!
    See also ""C:/Users/Vikas Tiwari/AppData/Local/Temp/pip-install-4f4b8868/dlib/build/temp.win-amd64-3.6/Release/CMakeFiles/CMakeOutput.log"".
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""C:\Users\VIKAST~1\AppData\Local\Temp\pip-install-4f4b8868\dlib\setup.py"", line 257, in &lt;module&gt;
        'Topic :: Software Development',
      File ""c:\users\vikas tiwari\anaconda3\lib\site-packages\setuptools\__init__.py"", line 129, in setup
        return distutils.core.setup(**attrs)
      File ""c:\users\vikas tiwari\anaconda3\lib\distutils\core.py"", line 148, in setup
        dist.run_commands()
      File ""c:\users\vikas tiwari\anaconda3\lib\distutils\dist.py"", line 955, in run_commands
        self.run_command(cmd)
      File ""c:\users\vikas tiwari\anaconda3\lib\distutils\dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""c:\users\vikas tiwari\anaconda3\lib\site-packages\setuptools\command\install.py"", line 61, in run
        return orig.install.run(self)
      File ""c:\users\vikas tiwari\anaconda3\lib\distutils\command\install.py"", line 545, in run
        self.run_command('build')
      File ""c:\users\vikas tiwari\anaconda3\lib\distutils\cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""c:\users\vikas tiwari\anaconda3\lib\distutils\dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""c:\users\vikas tiwari\anaconda3\lib\distutils\command\build.py"", line 135, in run
        self.run_command(cmd_name)
      File ""c:\users\vikas tiwari\anaconda3\lib\distutils\cmd.py"", line 313, in run_command
        self.distribution.run_command(command)
      File ""c:\users\vikas tiwari\anaconda3\lib\distutils\dist.py"", line 974, in run_command
        cmd_obj.run()
      File ""C:\Users\VIKAST~1\AppData\Local\Temp\pip-install-4f4b8868\dlib\setup.py"", line 133, in run
        self.build_extension(ext)
      File ""C:\Users\VIKAST~1\AppData\Local\Temp\pip-install-4f4b8868\dlib\setup.py"", line 170, in build_extension
        subprocess.check_call(cmake_setup, cwd=build_folder)
      File ""c:\users\vikas tiwari\anaconda3\lib\subprocess.py"", line 291, in check_call
        raise CalledProcessError(retcode, cmd)
    subprocess.CalledProcessError: Command '['cmake', 'C:\\Users\\VIKAST~1\\AppData\\Local\\Temp\\pip-install-4f4b8868\\dlib\\tools\\python', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY=C:\\Users\\VIKAST~1\\AppData\\Local\\Temp\\pip-install-4f4b8868\\dlib\\build\\lib.win-amd64-3.6', '-DPYTHON_EXECUTABLE=c:\\users\\vikas tiwari\\anaconda3\\python.exe', '-DCMAKE_LIBRARY_OUTPUT_DIRECTORY_RELEASE=C:\\Users\\VIKAST~1\\AppData\\Local\\Temp\\pip-install-4f4b8868\\dlib\\build\\lib.win-amd64-3.6', '-A', 'x64']' returned non-zero exit status 1.

    ----------------------------------------
Command """"c:\users\vikas tiwari\anaconda3\python.exe"" -u -c ""import setuptools, tokenize;__file__='C:\\Users\\VIKAST~1\\AppData\\Local\\Temp\\pip-install-4f4b8868\\dlib\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record C:\Users\VIKAST~1\AppData\Local\Temp\pip-record-aol24hcc\install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in C:\Users\VIKAST~1\AppData\Local\Temp\pip-install-4f4b8868\dlib\
</code></pre>

<p>I really don't have any idea what's the real issue here. Do i have to install Visual Studio to compile c++ files of dlib ?
Don't have VS 2015 installed right now!!</p>
","10190567","","","","","2019-04-10 16:03:06","Dlib installation error?","<python-3.x><visual-studio><cmake><dlib>","3","0","1","2019-07-02 07:05:52","","CC BY-SA 4.0","0"
"32673412","1","","","2015-09-19 21:28:29","","21","36367","<p>I am not getting desired output of this program?</p>

<pre><code>from sys import argv

script, first, second, third = argv

print (""The script is called:"", script)
print (""Your first variable is:"", first)
print (""Your second variable is:"", second)
print (""Your third variable is:"", third)
</code></pre>

<p>How to use cmd to pass these arguments?</p>
","5353274","","999858","","2015-09-19 21:50:28","2019-01-10 19:14:19","How to pass command line arguments in Python 3.x?","<python-3.x>","2","0","5","","","CC BY-SA 3.0","0"
"49721089","1","49721133","","2018-04-08 18:09:22","","28","36301","<p>I am working with Django for a first time and I'm trying to build an API and I am following some tutorials and examples and it works right, but I am running the project now in a Raspberry Pi after install all the requirements and the project is failing with the following error:</p>

<pre><code>    Performing system checks...

Unhandled exception in thread started by &lt;function check_errors.&lt;locals&gt;.wrapper at 0xb547adb0&gt;
Traceback (most recent call last):
  File ""/home/pi/.local/lib/python3.5/site-packages/django/utils/autoreload.py"", line 225, in wrapper
    fn(*args, **kwargs)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/management/commands/runserver.py"", line 120, in inner_run
    self.check(display_num_errors=True)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/management/base.py"", line 364, in check
    include_deployment_checks=include_deployment_checks,
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/management/base.py"", line 351, in _run_checks
    return checks.run_checks(**kwargs)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/checks/registry.py"", line 73, in run_checks
    new_errors = check(app_configs=app_configs)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/checks/urls.py"", line 13, in check_url_config
    return check_resolver(resolver)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/core/checks/urls.py"", line 23, in check_resolver
    return check_method()
  File ""/home/pi/.local/lib/python3.5/site-packages/django/urls/resolvers.py"", line 397, in check
    for pattern in self.url_patterns:
  File ""/home/pi/.local/lib/python3.5/site-packages/django/utils/functional.py"", line 36, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/urls/resolvers.py"", line 536, in url_patterns
    patterns = getattr(self.urlconf_module, ""urlpatterns"", self.urlconf_module)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/utils/functional.py"", line 36, in __get__
    res = instance.__dict__[self.name] = self.func(instance)
  File ""/home/pi/.local/lib/python3.5/site-packages/django/urls/resolvers.py"", line 529, in urlconf_module
    return import_module(self.urlconf_name)
  File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 673, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 673, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
  File ""/home/pi/Projects/openvpn-monitor/openvpnmonitor/urls.py"", line 24, in &lt;module&gt;
    url(r'^api/', include('openvpnmonitor.api.urls')),
  File ""/home/pi/.local/lib/python3.5/site-packages/django/urls/conf.py"", line 34, in include
    urlconf_module = import_module(urlconf_module)
  File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 673, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 673, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
  File ""/home/pi/Projects/openvpn-monitor/openvpnmonitor/api/urls.py"", line 16, in &lt;module&gt;
    urlpatterns += router.urls
  File ""/home/pi/.local/lib/python3.5/site-packages/rest_framework/routers.py"", line 101, in urls
    self._urls = self.get_urls()
  File ""/home/pi/.local/lib/python3.5/site-packages/rest_framework/routers.py"", line 363, in get_urls
    urls = super(DefaultRouter, self).get_urls()
  File ""/home/pi/.local/lib/python3.5/site-packages/rest_framework/routers.py"", line 261, in get_urls
    routes = self.get_routes(viewset)
  File ""/home/pi/.local/lib/python3.5/site-packages/rest_framework/routers.py"", line 176, in get_routes
    extra_actions = viewset.get_extra_actions()
AttributeError: type object 'SessionViewSet' has no attribute 'get_extra_actions'
</code></pre>

<p>My views.py has the following code:</p>

<pre><code>from django.shortcuts import render

from rest_framework import viewsets
from .models import Session
from .serializers import SessionSerializer

from rest_framework.views import APIView, Response


class SessionViewSet(APIView):
    queryset = Session.objects.all()
    serializer_class = SessionSerializer

    def get(self, request, format=None):
        return Response(""test"")
</code></pre>

<p>I really don't know why is working on my laptop but it is not working on my Raspberry Pi.</p>

<p>Has this happened to someone or anyone knows why is happening this?</p>

<p>Thank you so much!</p>

<p>Edit:</p>

<p>Here is my urls.py</p>

<pre><code>from django.conf.urls import url
from rest_framework import routers
from openvpnmonitor.api.views import SessionViewSet

router = routers.DefaultRouter()
router.register(r'sessions', SessionViewSet)

urlpatterns = [
    url(r'sessions', SessionViewSet.as_view()),
    url(r'^docs/', schema_view),
]

urlpatterns += router.urls
</code></pre>
","4990714","","4990714","","2018-04-08 19:53:13","2020-08-12 03:36:48","Django viewset has not attribute 'get_extra_actions'","<python><django><python-3.x><django-rest-framework>","6","0","7","","","CC BY-SA 3.0","0"
"46820625","1","46820836","","2017-10-18 23:18:57","","15","36299","<p>I'm using Kali dist so I have already installed Python 2.7, 3.5 and 3.6. Commands 'python' and 'pip' are associated with Python 2.7. But the 'python3' uses Python 3.6 while pip3 is installing packages for Python 3.5.<br>
When I tried to create an venv:</p>

<pre><code>pip3 -p python3.6 virtualenv myenv
</code></pre>

<p>I've got an error:</p>

<pre><code>no such option: -p
</code></pre>

<p>How can I associate pip3 with Python 3.6 instead of Python 3.5?</p>
","","user8798404","","","","2020-07-16 07:02:08","How to use pip3 for python 3.6 instead of python 3.5?","<python><python-3.x><pip>","5","4","4","","","CC BY-SA 3.0","0"
"33469015","1","","","2015-11-02 00:42:45","","4","36266","<p>Just started learning Python. And i'm having trouble using the Pyperclip module.</p>

<p>When I tried to use the <code>pip install pyperclip</code> in the command line, it shows up this error:</p>

<pre><code>pip install pyperclip
          ^
SyntaxError: invalid syntax
</code></pre>

<p>I am running Python 3.5 (32 bit) on a Windows 7 desktop.</p>
","4850122","","1413133","","2015-11-02 00:44:49","2020-09-06 09:15:36","Python 3.5 Pyperclip module import failure","<python><python-3.x><pyperclip>","9","0","2","","","CC BY-SA 3.0","0"
"34528107","1","34529150","","2015-12-30 10:56:35","","7","36186","<p>I have just installed Python 3.5.1 on my Mac (running the latest version of OSX). My system came with Python 2.7 installed. When I type <code>IDLE</code> at the Terminal prompt my system pulls up the original Python 2.7 rather than the newly installed Python 3.5. How do I get my system to default to Python 3.5.1 when I open the IDLE window from Terminal?</p>
","3798654","","202229","","2017-05-01 10:10:24","2019-05-07 15:29:18","How do I make Python 3.5 my default version on MacOS?","<python><macos><python-2.7><python-3.x>","7","1","8","","","CC BY-SA 3.0","0"
"32613579","1","","","2015-09-16 16:03:48","","0","36140","<p>I am trying to write a program that reads an integer and displays, using asterisks, a filled diamond of the given side length. For Example, if the side length is 4, the program should display</p>

<pre><code>   *
  ***
 *****
*******
 *****
  ***
   *
</code></pre>

<p>Here is what I am trying to do. It is executing, but I can't seem to get the spaces right for the program to show the diamond shape properly....</p>

<pre><code>userInput = int(input(""Please input side length of diamond: ""))

if userInput &gt; 0:
    for i in range(userInput):
        for s in range(userInput -3, -2, -1):
            print("" "", end="""")
        for j in range(i * 2 -1):
            print(""*"", end="""")
        print()
    for i in range(userInput, -1, -1):
        for j in range(i * 2 -1):
            print(""*"", end="""")
        print()
</code></pre>

<p>Thank you!</p>
","","user5343168","","user5343168","2015-09-16 16:08:52","2019-02-14 14:01:14","Creating a diamond pattern using loops","<python><python-3.x>","5","5","3","","","CC BY-SA 3.0","0"
"37427683","1","37560251","","2016-05-25 04:10:22","","8","36113","<p>Is there an inbuilt module to search for a file in the current directory, as well as all the super-directories?</p>

<p>Without the module, I'll have to list all the files in the current directory, search for the file in question, and recursively move up if the file isn't present. Is there an easier way to do this?</p>
","4593127","","4593127","","2018-06-26 13:11:21","2019-11-13 15:52:23","Python: search for a file in current directory and all it's parents","<python><python-3.x><python-2.7>","6","3","0","","","CC BY-SA 3.0","0"
"38646040","1","38646285","","2016-07-28 20:22:22","","14","36084","<p>I've been attempting to fit this data by a Linear Regression, following a tutorial on bigdataexaminer. Everything was working fine up until this point. I imported LinearRegression from sklearn, and printed the number of coefficients just fine. This was the code before I attempted to grab the coefficients from the console.</p>

<pre><code>import numpy as np
import pandas as pd
import scipy.stats as stats
import matplotlib.pyplot as plt
import sklearn
from sklearn.datasets import load_boston
from sklearn.linear_model import LinearRegression

boston = load_boston()
bos = pd.DataFrame(boston.data)
bos.columns = boston.feature_names
bos['PRICE'] = boston.target

X = bos.drop('PRICE', axis = 1)

lm = LinearRegression()
</code></pre>

<p>After I had all this set up I ran the following command, and it returned the proper output:</p>

<pre><code>In [68]: print('Number of coefficients:', len(lm.coef_)

Number of coefficients: 13
</code></pre>

<p>However, now if I ever try to print this same line again, or use 'lm.coef_', it tells me coef_ isn't an attribute of LinearRegression, right after I JUST used it successfully, and I didn't touch any of the code before I tried it again.</p>

<pre><code>In [70]: print('Number of coefficients:', len(lm.coef_))

Traceback (most recent call last):

 File ""&lt;ipython-input-70-5ad192630df3&gt;"", line 1, in &lt;module&gt;
print('Number of coefficients:', len(lm.coef_))

AttributeError: 'LinearRegression' object has no attribute 'coef_'
</code></pre>
","6626436","","2285236","","2016-07-28 20:31:13","2020-08-09 18:27:29","AttributeError: LinearRegression object has no attribute 'coef_'","<python><python-3.x><scikit-learn><linear-regression><attributeerror>","1","6","1","","","CC BY-SA 3.0","0"
"42463866","1","42464564","","2017-02-26 01:59:07","","8","35952","<p>I have python 3.6 installed, I have a python extension installed on Visual Studio code but I still can't use pip on Visual Studio code. It says it is not a recognised command. Any help please?</p>

<p>Update: I tried installing pip manually but a file in <code>python2.7</code> keeps stopping. What's bothersome is that I uninstalled python 2.7 ages ago and I've currently removed every folder with it but <code>python-V</code> still says I have <code>python2.7.6</code> installed. </p>

<p>I'm on windows 10</p>
","6897944","","6897944","","2017-02-26 02:54:27","2020-02-08 18:34:16","How to use pip with Visual Studio Code","<python-3.x><visual-studio-code>","1","12","1","","","CC BY-SA 3.0","0"
"44941757","1","44944205","","2017-07-06 06:55:32","","16","35862","<p>I am fresh to flask and was trying to build a blog on my own, and I ran into an issue with SQLite operation error. I have researched similar problem on Github and Stackoverflow but none of the typical typo or error in old questions happens to me. It would be appreciated and really great if anyone can help me because this problem is like killing me and already cost me two days, I feel really bad.</p>

<p>In the code I have defined the table name which is ""users_table"" and run ""db.create_all()"" at the beginning to create the table, but the error keeps occurring with ""no such table user_table"" each time when a commit happens for updating user info.</p>

<p>This is how I test the SQLite operation:</p>

<pre><code>(under /project) python3 manage.py shell
&gt;&gt;&gt; u = User(email='foo@bar.com', username='foobar', password='player')
&gt;&gt;&gt; db.create_all()
&gt;&gt;&gt; db.session.add(u)
&gt;&gt;&gt; db.session.commit()  # with following error message
Traceback (most recent call last):
  File ""C:\...\Python\Python36-32\lib\site-packages\sqlalchemy\engine\base.py"", line 1182, in _execute_context
  context)
  File ""C:\...\Python\Python36-32\lib\site-packages\sqlalchemy\engine\default.py"", line 470, in do_execute
  cursor.execute(statement, parameters)
sqlite3.OperationalError: no such table: users_table
...
...
  sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: users_table
</code></pre>

<p>I have minimized the code into following four sections, which can reoccur the error message:</p>

<p>/project/app/__init__.py:</p>

<pre><code>from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from config import config

db = SQLAlchemy()

def create_app(config_name):
    app = Flask(__name__)
    app.config.from_object(config[config_name])
    config[config_name].init_app(app)
    db.init_app(app)
    return app
</code></pre>

<p>/project/app/models.py:</p>

<pre><code>import os
from flask_sqlalchemy import SQLAlchemy
from werkzeug.security import generate_password_hash
from flask import Flask

basedir = os.path.abspath(os.path.dirname(__file__))

app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + os.path.join(basedir, 'data.sqlite')
app.config['SQLALCHEMY_COMMIT_ON_TEARDOWN'] = True
db = SQLAlchemy(app)


class User(db.Model):
    __tablename__ = 'users_table'
    id = db.Column(db.Integer, primary_key=True)
    email = db.Column(db.String(64), unique=True, index=True)
    username = db.Column(db.String(64), unique=True, index=True)
    password_hash = db.Column(db.String(128))

    def __repr__(self):
        return '&lt;User %r&gt;' % self.username

    @property
    def password(self):
        raise AttributeError('Password is not a readable attribute')

    @password.setter
    def password(self, password):
        self.password_hash = generate_password_hash(password)
</code></pre>

<p>project/config.py:</p>

<pre><code>import os
basedir = os.path.abspath(os.path.dirname(\__file__))

class Config:
    SECRET_KEY = os.environ.get('SECRET_KEY') or 'fhuaioe7832of67^&amp;*T#oy93'
    SQLALCHEMY_COMMIT_ON_TEARDOWN = True

    @staticmethod
    def init_app(app):
        pass

class DevelopmentConfig(Config):
    DEBUG = True
    SQLALCHEMY_DATABASE_URI = 'sqlite:///' + os.path.join(basedir, 'data.sqlite')

config = {
    'development': DevelopmentConfig,
    'default': DevelopmentConfig,
}
</code></pre>

<p>project/manage.py:</p>

<pre><code>import os
from app import create_app, db
from app.models import User
from flask_script import Manager, Shell

app = create_app(os.getenv('FLASK_CONFIG') or 'default')
manager = Manager(app)

def make_shell_context():
    return dict(app=app, db=db, User=User)

manager.add_command(""shell"", Shell(make_context=make_shell_context))


if __name__ == '__main__':
    manager.run()
</code></pre>
","5182223","","2681632","","2017-07-06 07:04:41","2019-11-18 23:19:23","sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table","<python-3.x><sqlite><flask><sqlalchemy><flask-sqlalchemy>","2","1","6","","","CC BY-SA 3.0","0"
"34630669","1","","","2016-01-06 10:20:20","","23","35826","<p>I am new to Python. I have tried to ran this code but I am getting an error message for ImportError: No module named 'HTMLParser'. I am using Python 3.x. Any reason why this is not working ?</p>

<pre><code>#Import the HTMLParser model
from HTMLParser import HTMLParser

#Create a subclass and override the handler methods
class MyHTMLParser(HTMLParser):

#Function to handle the processing of HTML comments
    def handle_comment(self,data):
        print (""Encountered comment: "", data)
        pos = self.getpos()
        print (""At line: "", pos[0], ""position "", pos[1])

def main():
    #Instantiate the parser and feed it some html
    parser= MyHTMLParser()

    #Open the sample file and read it
    f = open(""myhtml.html"")
    if f.mode== ""r"":
        contents= f.read()  #read the entire FileExistsError
        parser.feed()


if __name__== ""__main__"":
    main()
</code></pre>

<p>I am getting the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\bm250199\workspace\test\htmlparsing.py"", line 3, in &lt;module&gt;
    from HTMLParser import HTMLParser
ImportError: No module named 'HTMLParser'
</code></pre>
","2625433","","","","","2016-01-06 10:24:09","Python: ImportError: No module named 'HTMLParser'","<python-3.x>","1","1","5","2016-01-06 12:14:01","","CC BY-SA 3.0","0"
"57715289","1","60301124","","2019-08-29 17:34:34","","10","35798","<p>I am trying to send an email with python, but it keeps saying <code>ssl.SSLError: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1056)</code>. Here is my code:</p>

<pre class=""lang-py prettyprint-override""><code>server = smtplib.SMTP_SSL('smtp.mail.com', 587)
server.login(""something0@mail.com"", ""password"")
server.sendmail(
""something0@mail.com"", 
""something@mail.com"", 
""email text"")
server.quit()
</code></pre>

<p>Do you know what is wrong?</p>
","10993097","","4788546","","2019-08-30 20:12:13","2020-10-17 08:53:22","How to fix ssl.SSLError: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:1056)?","<python><python-3.x><ssl><python-3.7><smtplib>","1","2","1","","","CC BY-SA 4.0","0"
"28325553","1","","","2015-02-04 15:48:15","","1","35764","<p>The following python code throws this error message, and I can't tell why, my tabs seem to be in line: </p>

<pre><code>File ""test.py"", line 12
    pass
       ^ TabError: inconsistent use of tabs and spaces in indentation
</code></pre>

<p>My code:</p>

<pre><code>class eightPuzzle(StateSpace):
    StateSpace.n = 0

    def __init__(self, action, gval, state, parent = None):
        StateSpace.__init__(self, action, gval, parent)
        self.state = state

    def successors(self) :
        pass
</code></pre>
","4529287","","98057","","2015-02-04 16:01:01","2020-03-04 08:46:14","Tab Error in Python","<python><python-3.x><indentation>","4","5","","","","CC BY-SA 3.0","0"
"36489042","1","36489257","","2016-04-07 23:35:49","","4","35731","<p>I am numerically setting up a mesh grid for the x-grid and x-vector and also time grid but again I have set up an array for <code>x</code> (position) which should only be between 0 and 20 and <code>t</code> (time) would be from 0 until 1000 thus in order to solve a Heat equation. But every time I want for e.g., I make the number of steps 10, I get an error:</p>

<pre><code>""Traceback (most recent call last):
File ""/home/universe/Desktop/Python/Heat_1.py"", line 33, in &lt;module&gt;
x[i] = a + i*h
IndexError: index 10 is out of bounds for axis 0 with size 10""
</code></pre>

<p>Here is my code:</p>

<pre><code>from math import sin,pi
import numpy
import numpy as np

#Constant variables
N = int(input(""Number of intervals in x (&lt;=20):""))
M = int(input(""Number of time steps (&lt;=1000):"" ))

#Some initialised varibles
a = 0.0
b = 1.0
t_min = 0.0
t_max = 0.5

# Array Variables
x = np.linspace(a,b, M)
t = np.linspace(t_min, t_max, M) 


#Some scalar variables
n = []                         # the number of x-steps
i, s = [], []                  # The position and time

# Get the number of x-steps to use
for n in range(0,N):
    if n &gt; 0 or n &lt;= N:
         continue

# Get the number of time steps to use
for m in range(0,M):
    if m &gt; 0 or n &lt;= M:
         continue

# Set up x-grid  and x-vector
h =(b-a)/n
for i in range(0,N+1):
    x[i] = a + i*h

# Set up time-grid
k = (t_max - t_min)/m
for s in range(0, M+1):
    t[s] = t_min + k*s

print(x,t)
</code></pre>
","6113143","","837534","","2016-04-08 08:14:00","2016-04-08 08:14:00","IndexError: index 10 is out of bounds for axis 0 with size 10","<arrays><python-2.7><python-3.x><numpy><differential-equations>","1","1","","","","CC BY-SA 3.0","0"
"34269772","1","50038614","","2015-12-14 14:43:24","","134","35727","<p>Consider following piece of code:</p>

<pre><code>from collections import namedtuple
point = namedtuple(""Point"", (""x:int"", ""y:int""))
</code></pre>

<p>The Code above is just a way to demonstrate as to what I am trying to achieve.
I would like to make <code>namedtuple</code> with type hints. </p>

<p>Do you know any elegant way how to achieve result as intended?</p>
","4805412","","1014938","","2019-03-17 13:35:19","2019-06-16 16:06:35","Type hints in namedtuple","<python><python-3.x><type-hinting><namedtuple><python-dataclasses>","2","1","13","","","CC BY-SA 3.0","0"
"29957477","1","29957722","","2015-04-30 01:24:12","","5","35696","<p>I know that there are many variations of this question, but I could not find one like mine. When I try to import the module <code>illustris_python</code> I get the error <code>ImportError: No module named 'util'</code> The module util is in the directory below the module <code>snapshot.py</code> that needs it so I am confused as to why python sees one module , but not the other.
 I have included the import call as well as traceback below. </p>

<pre><code>Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 24 2015, 22:44:40) [MSC v.1600 64 bit (AMD64)]
Type ""copyright"", ""credits"" or ""license"" for more information.

IPython 3.0.0 -- An enhanced Interactive Python.
?         -&gt; Introduction and overview of IPython's features.
%quickref -&gt; Quick reference.
help      -&gt; Python's own help system.
object?   -&gt; Details about 'object', use 'object??' for extra details.
%guiref   -&gt; A brief reference about the graphical user interface.

In [1]: import illustris_python as il
Traceback (most recent call last):

  File ""&lt;ipython-input-1-ff06d24b4811&gt;"", line 1, in &lt;module&gt;
    import illustris_python as il

  File ""C:\WinPython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site-     packages\illustris_python\__init__.py"", line 3, in &lt;module&gt;
    from . import *

  File ""C:\WinPython-64bit-3.4.3.2\python-3.4.3.amd64\lib\site- packages\illustris_python\snapshot.py"", line 6, in &lt;module&gt;
    from util import partTypeNum

ImportError: No module named 'util'


In [2]: 
</code></pre>

<p><img src=""https://i.stack.imgur.com/G0EVq.png"" alt=""Screen shot showing location of util""></p>
","908924","","1903116","","2015-04-30 01:32:48","2017-06-22 03:18:26","ImportError: No module named 'util'","<python><python-3.x><import><python-import>","2","9","1","","","CC BY-SA 3.0","0"
"50453428","1","","","2018-05-21 16:58:32","","35","35653","<p>Currently, I can download files as individual files with the command </p>

<pre><code>files.download(file_name)
</code></pre>

<p>I also tried uploading them to the drive with the below code snippet but it is uploading them as individual files.</p>

<pre><code>uploaded = drive.CreateFile({'title': file_name})
uploaded.SetContentString('Sample upload file content')
uploaded.Upload()
print('Uploaded file with ID {}'.format(uploaded.get('id')))
</code></pre>

<p>How can I download multiple files as a folder to my local computer? Or how can I upload these files as a folder to my google drive?</p>
","4286626","","3924118","","2019-01-09 20:03:34","2019-03-25 02:20:32","How do I download multiple files or an entire folder from Google Colab?","<python-3.x><tensorflow><google-colaboratory>","4","0","9","","","CC BY-SA 4.0","0"
"47661536","1","47661654","","2017-12-05 19:47:38","","54","35622","<p>I'm new to Django and am trying to create the back end code for a music application on my website.</p>

<p>I have created the correct view in my views.py file (in the correct directory) as shown below:</p>

<pre><code>def detail(request, album_id):
    return HttpResponse(""&lt;h1&gt;Details for Album ID:"" + str(album_id) + ""&lt;/h1&gt;"")
</code></pre>

<p>however, when creating the url or path for this (shown below)</p>

<pre><code>#/music/71/ (pk)
path(r'^(?P&lt;album_id&gt;[0-9])/$', views.detail, name='detail'),
</code></pre>

<p>I am experiencing a warning on my terminal stating:</p>

<pre><code>?: (2_0.W001) Your URL pattern '^(?P&lt;album_id&gt;[0-9])/$' [name='detail'] has a route that contains '(?P&lt;', begins with a '^', or ends with a '$'. This was likely an oversight when migrating to django.urls.path().
</code></pre>

<p>and whenever the <code>/music/</code> (for which the path works) is followed by a number, such as <code>/music/1</code> (which is what I want to be able to do) the page cannot be found and the terminal gives the above warning.</p>

<p>It may be a simple error and just me being stupid but I'm new to Django and python regex statements, so any help is appreciated.</p>
","9058173","","113962","","2018-03-27 09:43:58","2020-09-11 11:51:02","Django 2.0 path error ?: (2_0.W001) has a route that contains '(?P<', begins with a '^', or ends with a '$'","<python><django><python-3.x><django-views><django-urls>","7","3","9","","","CC BY-SA 3.0","0"
"55033372","1","","","2019-03-06 22:43:57","","10","35600","<pre><code>import socket
import os

user_url = input(""Enter url: "")

host_name = user_url.split(""/"")[2]
mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
mysock.connect((host_name, 80))
cmd = 'GET ' + user_url + ' HTTP/1.0\r\n\r\n'.encode()
mysock.send(cmd)

while True:
    data = mysock.recv(512)
    if len(data) &lt; 1:
        break
     print(data.decode(),end='\n')

mysock.close()
</code></pre>

<p>For some reason im gettin this error</p>

<blockquote>
  <h2>Enter url: <a href=""http://data.pr4e.org/romeo.txt"" rel=""noreferrer"">http://data.pr4e.org/romeo.txt</a></h2>

<pre><code> 7 mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
 8 mysock.connect((host_name, 80))
 9 cmd = 'GET ' + user_url + ' HTTP/1.0\r\n\r\n'.encode()
 TypeError: can only concatenate str (not ""bytes"") to str
</code></pre>
</blockquote>

<p>Any ideas what im doing wrong with it?Encoding and decoding seems right to me, and i've trasnfered it using \n before .encode().
This is for a class</p>
","10986780","","","","","2019-03-06 22:47:53","can only concatenate str (not ""bytes"") to str","<python><python-3.x><string><byte>","2","0","1","","","CC BY-SA 4.0","0"
"49580313","1","49580476","","2018-03-30 19:31:30","","9","35544","<p>So I'm looking to create a log file for my discord bot which is built with python.</p>

<p>I have a few set commands which output the the console through the print command, I have added a date and time to the print outputs so it can be tracked when the bot is running, however is it easy to make it save the print outs to a file as well? That way I can make a log file to track different days and what was called for.</p>

<p>Console Output:
<a href=""https://i.stack.imgur.com/roeHW.png"" rel=""noreferrer"">Screenshot_1.png</a></p>

<p>Example of a print command in my code: </p>

<p>async def coin(ctx):</p>

<pre><code>author = ctx.message.author
choice = random.randint(1,2)
if choice == 1:
    await bot.say(""Heads"")
    print(currentTime() + "" - Coin Requested by "" + str(author) + "" It Landed on Heads!"")
elif choice == 2:
    await bot.say(""Tails"")
    print(currentTime() + "" - Coin Requested by "" + str(author) + "" It Landed on Tails!"")
</code></pre>

<p>I have tried looking online at some other questions but I get quite confused looking at them as there is no clear explanation as to whats happening and how I can configure it to work for my code.</p>
","9566887","","9566887","","2018-03-30 19:56:49","2020-07-09 22:54:41","Create a log file","<python><python-3.x><logging>","4","0","2","","","CC BY-SA 3.0","0"
"28265841","1","28266008","","2015-02-01 17:54:33","","7","35537","<p>I'm fairly new to python. I'm using Ubuntu 14.04 and have both python 2.7.6 and python 3.4.0 installed. I was trying to install BeautifulSoup but couldn't because I get an error saying </p>

<pre><code>The program 'pip' is currently not installed. 
</code></pre>

<p>I found that it comes bundles with python 3.4. I tried to install pip using <code>sudo easy_install pip</code> as mentioned in another question on stackoverflow. But this gives an error <code>sudo: easy_install: command not found</code>.</p>

<p>What is the problem? </p>
","3674059","","","","","2019-12-08 20:06:12","pip, easy_install commands not working in Ubuntu. Python 2.7 and 3.4 are installed","<python-2.7><python-3.x><pip><python-3.4><easy-install>","3","8","1","","","CC BY-SA 3.0","0"
"40000495","1","40000564","","2016-10-12 13:54:34","","35","35452","<p>I am trying to encode a dictionary containing a string of bytes with <code>json</code>, and getting a <code>is not JSON serializable error</code>:</p>
<pre><code>import base64
import json

data = {}
encoded = base64.b64encode(b'data to be encoded')
data['bytes'] = encoded

print(json.dumps(data))
</code></pre>
<p>The error I get:</p>
<pre class=""lang-none prettyprint-override""><code>TypeError: b'ZGF0YSB0byBiZSBlbmNvZGVk\n' is not JSON serializable
</code></pre>
<p>How can I correctly encode my dictionary containing bytes with JSON?</p>
","4262324","","3064538","","2020-06-23 17:53:02","2020-06-23 17:53:02","How to encode bytes in JSON? json.dumps() throwing a TypeError","<python><json><python-3.x>","1","0","5","","","CC BY-SA 4.0","0"
"41405001","1","41405002","","2016-12-31 02:50:24","","3","35450","<p>I was using Python 3.5.2 with Pygame 1.9.2 and everything was working fine. Then I decided to try the recently released Python 3.6.0 with a fresh install, but when I tried to reinstall Pygame with this:</p>

<pre><code>pip install pygame
</code></pre>

<p>it returned the following:</p>

<pre><code>Collecting pygame
  Downloading pygame-1.9.2.tar.gz (3.0MB)
    100% |████████████████████████████████| 3.0MB 292kB/s
    Complete output from command python setup.py egg_info:


    WARNING, No ""Setup"" File Exists, Running ""config.py""
    Using WINDOWS configuration...

    Path for SDL not found.
    Too bad that is a requirement! Hand-fix the ""Setup""
    Path for FONT not found.
    Path for IMAGE not found.
    Path for MIXER not found.
    Path for PNG not found.
    Path for JPEG not found.
    Path for PORTMIDI not found.
    Path for COPYLIB_tiff not found.
    Path for COPYLIB_z not found.
    Path for COPYLIB_vorbis not found.
    Path for COPYLIB_ogg not found.

    If you get compiler errors during install, doublecheck
    the compiler flags in the ""Setup"" file.


    Continuing With ""setup.py""
    Error with the ""Setup"" file,
    perhaps make a clean copy from ""Setup.in"".
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""C:\Users\rodri\AppData\Local\Temp\pip-build-328x7lme\pygame\setup.py"", line 165, in &lt;module&gt;
        extensions = read_setup_file('Setup')
      File ""c:\python\lib\distutils\extension.py"", line 171, in read_setup_file
        line = expand_makefile_vars(line, vars)
      File ""c:\python\lib\distutils\sysconfig.py"", line 410, in expand_makefile_vars
        s = s[0:beg] + vars.get(m.group(1)) + s[end:]
    TypeError: must be str, not NoneType

    ----------------------------------------
Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\rodri\AppData\Local\Temp\pip-build-328x7lme\pygame\
</code></pre>
","7311392","","","","","2017-10-03 01:30:33","Pygame installation for Python 3.6.0","<python-3.x><pygame>","2","0","1","","","CC BY-SA 3.0","0"
"51150153","1","51150623","","2018-07-03 08:40:59","","10","35449","<p>While practicing Simple Linear Regression Model I got this error, 
I think there is something wrong with my data set.</p>

<p><a href=""https://i.stack.imgur.com/Eb0od.jpg"" rel=""noreferrer"">Here is my data set:</a></p>

<p><a href=""https://i.stack.imgur.com/O5wJN.jpg"" rel=""noreferrer"">Here is independent variable X:</a></p>

<p><a href=""https://i.stack.imgur.com/fuJUA.jpg"" rel=""noreferrer"">Here is dependent variable Y:</a></p>

<p><a href=""https://i.stack.imgur.com/OtkGZ.jpg"" rel=""noreferrer"">Here is X_train</a></p>

<p><a href=""https://i.stack.imgur.com/yQu8K.jpg"" rel=""noreferrer"">Here Is Y_train</a></p>

<p>This is error body:</p>

<pre><code>ValueError: Expected 2D array, got 1D array instead:
array=[ 7.   8.4 10.1  6.5  6.9  7.9  5.8  7.4  9.3 10.3  7.3  8.1].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>

<p>And this is My code:</p>

<pre><code>import pandas as pd
import matplotlib as pt

#import data set

dataset = pd.read_csv('Sample-data-sets-for-linear-regression1.csv')
x = dataset.iloc[:, 1].values
y = dataset.iloc[:, 2].values

#Spliting the dataset into Training set and Test Set
from sklearn.cross_validation import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=0)

#linnear Regression

from sklearn.linear_model import LinearRegression

regressor = LinearRegression()
regressor.fit(x_train,y_train)

y_pred = regressor.predict(x_test)
</code></pre>

<p>Thank you</p>
","10018194","","4576607","","2018-07-03 14:47:50","2020-09-24 08:43:54","ValueError: Expected 2D array, got 1D array instead:","<python-3.x><scikit-learn><linear-regression>","5","0","0","","","CC BY-SA 4.0","0"
"41535571","1","41536036","","2017-01-08 17:27:28","","35","35432","<p>I am currently participating in an Udacity course that instructs students on programming using Python. One of the projects has students rename photo files (remove any numbers in the name) in a directory in order to have the files arranged alphabetically, after which a secret message will be spelled out. For instance, if a file name is <code>""48athens""</code>, the program seeks to remove the numbers, leaving only <code>""athens""</code> as the file name.</p>

<p>I am using Python 3.6, while the course instructor is using Python 2.7. I should likely be using Python 2.7 so as to simplify the learning process. However, for now I will keep using Python 3.6.</p>

<p>The way in which the instructor has the files renamed is using the <code>.translate</code> function, which takes two arguments in Python 2.x, while Python 3.x only takes one argument. It removes any numbers (0 through 9) from the file names.</p>

<pre><code>import os

def rename_files(): #Obtain the file names from a folder.
    file_list = os.listdir(r""C:\Users\Dennis\Desktop\OOP\prank\prank"")
    print (file_list)
    saved_path = os.getcwd()
    os.chdir(r""C:\Users\Dennis\Desktop\OOP\prank\prank"")
    for file_name in file_list: #Rename the files inside of the folder.
        os.rename(file_name, file_name.translate(None, ""0123456789""))
    os.chdir(saved_path)

rename_files()
</code></pre>

<p>However, this does not work in Python 3.x, as it says that:</p>

<pre><code>TypeError: translate() takes exactly one argument (2 given)
</code></pre>

<p>Thankfully, I found another way using someone's assistance. However, I'm not really sure how it works. Can someone explain the <code>str.maketrans</code> function to me, and what the first two blank arguments in quotes are for? My thought is that it's saying: for the first two characters in the file name, remove any numbers (0 through 9). Is that correct? For instance, in <code>""48athens""</code>, remove the first two characters (4 and 8) if they are numbers between 0 and 9.</p>

<pre><code>import os

def rename_files(): #Obtain the file names from a folder.
    file_list = os.listdir(r""C:\Users\Dennis\Desktop\OOP\prank\prank"")
    print (file_list)
    saved_path = os.getcwd()
    os.chdir(r""C:\Users\Dennis\Desktop\OOP\prank\prank"")
    for file_name in file_list: #Rename the files inside of the folder.
        os.rename(file_name, file_name.translate(str.maketrans('','','0123456789')))
    os.chdir(saved_path)

rename_files()
</code></pre>

<p><strong>My Understanding of the Documentation:</strong></p>

<blockquote>
  <p><code>static str.maketrans(x[, y[, z]])</code> 
  This static method returns a translation table usable for <code>str.translate()</code>.</p>
</blockquote>

<p>It's saying that the arguments passed to <code>str.maketrans</code>, along with the actual function <code>str.maketrans</code>, will make a table that says, ""If this character appears, replace it with this character."" However, I'm not sure what the brackets are for.</p>

<blockquote>
  <p>If there is only one argument, it must be a dictionary mapping Unicode
  ordinals (integers) or characters (strings of length 1) to Unicode
  ordinals, strings (of arbitrary lengths) or None. Character keys will
  then be converted to ordinals.</p>
</blockquote>

<p>It's saying that it can only change integers, or characters in strings of length one, to other integers or strings (of any length you want). But I believe I have three arguments, not one.</p>

<blockquote>
  <p>If there are two arguments, they must be strings of equal length, and
  in the resulting dictionary, each character in x will be mapped to the
  character at the same position in y. If there is a third argument, it
  must be a string, whose characters will be mapped to None in the
  result.</p>
</blockquote>

<p>I have three arguments <code>('', '', '0123456789')</code>. I think <code>x</code> is the first <code>''</code>, and <code>y</code> is the second <code>''</code>. I have the third argument, which is a string <code>'0123456789'</code>, but I don't understand what it means to be mapped to <code>'None'</code>. </p>
","6598376","","4952130","","2017-01-08 18:26:21","2020-07-28 20:20:08","How to explain the str.maketrans function in Python 3.6?","<python><string><python-3.x>","4","2","22","","","CC BY-SA 3.0","0"
"44891070","1","44891216","","2017-07-03 17:22:45","","69","35430","<p>When I run these methods</p>

<pre><code>s.isdigit()
s.isnumeric()
s.isdecimal()
</code></pre>

<p>I always got as output or all True, or all False for each value of s (which is of course a string). 
What's​ the difference between the three? Can you provide an example that gives two trues and one false (or viceversa)?</p>
","8225026","","4774918","","2018-06-10 13:27:20","2020-02-22 00:14:32","What's the difference between str.isdigit, isnumeric and isdecimal in python?","<python><string><python-3.x>","5","0","20","","","CC BY-SA 4.0","0"
"47253638","1","","","2017-11-12 20:32:21","","4","35380","<p>I’m trying to download the package <code>statsmodels</code> by running in command prompt(admin) this command:</p>

<pre><code>pip3 install statsmodels
</code></pre>

<p>and I get this error </p>

<blockquote>
  <p>“error: Microsoft Visual C++ 14.0 is required. Get it with ""Microsoft
  Visual C++ Build Tools"":
  <a href=""http://landinghub.visualstudio.com/visual-cpp-build-tools"" rel=""nofollow noreferrer"">http://landinghub.visualstudio.com/visual-cpp-build-tools</a>”</p>
</blockquote>

<p>Please note that I already installed Visual Studio 2015 and I also have Visual Studio 2013 installed on my machine.</p>
","7687251","","5457306","","2018-07-23 13:22:48","2020-01-09 12:59:44","error: Microsoft Visual C++ 14.0 is required when installing python package","<python><python-3.x><package><c++14><statsmodels>","2","3","7","","","CC BY-SA 4.0","0"
"41061824","1","","","2016-12-09 13:42:20","","8","35364","<p>I have a problem with installing csv package in pycharm (running under python 3.5.2) </p>

<p>When I try to install it I get an error saying </p>

<p>Executed command:
<code>pip install --user csv</code></p>

<p>Error occurred:
Non-zero exit code (1)</p>

<p>Could not find a version that satisfies the requirement csv (from versions: )
No matching distribution found for csv</p>

<p>I updated the pip package to version 9.0.1 but still nothing.</p>

<p>When I run the following code:</p>

<pre><code>import csv
f = open('fl.csv')
csv_f = csv.reader(f)
f.close()
</code></pre>

<p>I get this error:</p>

<p>csv_f = csv.reader(f)
AttributeError: module 'csv' has no attribute 'reader'</p>

<p>I thought it was because I could not install the ""csv package""</p>

<p>also I tried running:</p>

<pre><code>import csv
print(dir(csv))
</code></pre>

<p>and the print result is:</p>

<p>['<strong>doc</strong>', '<strong>loader</strong>', '<strong>name</strong>', '<strong>package</strong>', '<strong>path</strong>', '<strong>spec</strong>']</p>

<p>A lot of methods including csv.reader are missing</p>

<p>This is pretty much all the useful information up until comment #29</p>
","4005127","","4005127","","2016-12-09 15:06:41","2020-05-26 21:06:43","install csv package in pycharm","<python><python-3.x>","5","1","","","","CC BY-SA 3.0","0"
"54665842","1","54708388","","2019-02-13 08:43:40","","47","35362","<p>I have installed Ancaconda3 and Tensorflow. When I try to import Tensorflow in python shell I receive the following error:</p>

<blockquote>
  <p>ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'
  ImportError: numpy.core.multiarray failed to import</p>
  
  <p>The above exception was the direct cause of the following exception:</p>
  
  <p>Traceback (most recent call last):   File """", line 980, in _find_and_load SystemError:
   returned a result with
  an error set ImportError: numpy.core._multiarray_umath failed to
  import ImportError: numpy.core.umath failed to import</p>
</blockquote>

<p>I am not sure what the problem is as numpy is installed on my system and can be successfully imported in python.
I am using Windows10.</p>

<p>Thanks in advance.</p>
","6104563","","5495381","","2019-02-13 09:02:10","2019-05-29 08:52:48","When importing tensorflow, I get the following error: No module named 'numpy.core._multiarray_umath'","<python-3.x><numpy><tensorflow><anaconda>","3","1","4","","","CC BY-SA 4.0","0"
"51700960","1","51701040","","2018-08-06 05:19:18","","50","35362","<p>I am trying to run this code:</p>

<pre><code>import web

urls = (
    '/', 'index'
)

if __name__ == ""__main__"":
    app = web.application(urls, globals())
    app.run()
</code></pre>

<p>But it gives me this error everytime</p>

<pre><code>C:\Users\aidke\Desktop&gt;python app.py
Traceback (most recent call last):
  File ""C:\Users\aidke\AppData\Local\Programs\Python\Python37-32\lib\site-packages\web\utils.py"", line 526, in take
    yield next(seq)
StopIteration

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""app.py"", line 14, in &lt;module&gt;
    app = web.application(urls, globals())
  File ""C:\Users\aidke\AppData\Local\Programs\Python\Python37-32\lib\site-packages\web\application.py"", line 62, in __init__
    self.init_mapping(mapping)
  File ""C:\Users\aidke\AppData\Local\Programs\Python\Python37-32\lib\site-packages\web\application.py"", line 130, in init_mapping
    self.mapping = list(utils.group(mapping, 2))
  File ""C:\Users\aidke\AppData\Local\Programs\Python\Python37-32\lib\site-packages\web\utils.py"", line 531, in group
    x = list(take(seq, size))
RuntimeError: generator raised StopIteration
</code></pre>

<p>I tried someone else's code and the exact same thing happened.  Additionally I tried reinstalling web.py(experimental) but it still didn't work.</p>
","10184894","","6573902","","2020-03-27 07:19:57","2020-03-27 07:19:57","""RuntimeError: generator raised StopIteration"" every time I try to run app","<python><python-3.x><runtime-error><python-3.7><stopiteration>","5","1","12","","","CC BY-SA 4.0","0"
"36108824","1","36108883","","2016-03-20 00:15:42","","9","35319","<p>I'm working on this task in python, but I'm not sure if I'm adding the elements to a list the right way. So basically I'm suppose to create a create_list function the takes the size of the list and prompt the user for that many values and store each value into the list. The create_list function should return this newly created list. Finally the main() function should prompt the user for the number of values to be entered, pass that value to the create_list function to set up the list, and then call the get_total function to print the total of the list. Please tell me what I'm missing or doing wrong. Thank you so much in advance.</p>

<pre><code>def main():
    # create a list
    myList = []

    number_of_values = input('Please enter number of values: ')

    # Display the total of the list  elements.
    print('the list is: ', create_list(number_of_values))
    print('the total is ', get_total(myList))

    # The get_total function accepts a list as an
    # argument returns the total sum of the values in
    # the list

def get_total(value_list):

    total = 0

    # calculate the total of the list elements
    for num in value_list:
        total += num

    #Return the total.
    return total

def create_list(number_of_values):

    myList = []
    for num in range(number_of_values):
        num = input('Please enter number: ')
        myList.append(num)

    return myList

main()
</code></pre>
","4084160","","4084160","","2016-03-20 20:55:59","2019-09-08 03:10:22","How to add elements to a List in python?","<python><python-3.x>","7","16","1","","","CC BY-SA 3.0","0"
"34463901","1","","","2015-12-25 15:04:22","","7","35292","<p>I am working on database connectivity in Python 3.4.
There are two columns in my database.</p>

<p>Below is the query which gives me all the data from two columns in shown format
QUERY:</p>

<pre><code>cur.execute("""""" select * from filehash """""")
data=cur.fetchall()
print(data)
</code></pre>

<p>OUTPUT:</p>

<pre><code>[('F:\\test1.py', '12345abc'), ('F:\\test2.py', 'avcr123')]
</code></pre>

<p>To iterate through this output, my code is as below</p>

<pre><code>cur.execute("""""" select * from filehash """""")
data=cur.fetchall()

i=0
j=1

for i,row in data:
    print(row[i])
    print(row[j])
    i=i+1
</code></pre>

<p>This gives me below error</p>

<pre><code>print(row[i])
TypeError: string indices must be integers
</code></pre>

<p>Let me know how can we work on individual values of <code>fetchall()</code></p>
","5716883","","3620003","","2015-12-25 15:05:34","2018-06-15 04:27:21","How to Iterate through cur.fetchall() in Python","<python><python-3.x>","5","2","3","","","CC BY-SA 3.0","0"
"43516019","1","43516323","","2017-04-20 09:49:53","","15","35256","<p>I would like to make a browse folder button using tkinter and to store the path into a variable. So far i am able to print the path but i am not able to store it in a variable. Can you please advise?</p>

<p>Below i attach the code that i use.</p>

<pre><code>from tkinter import filedialog
from tkinter import *

def browse_button():
    filename = filedialog.askdirectory()
    print(filename)
    return filename


root = Tk()
v = StringVar()
button2 = Button(text=""Browse"", command=browse_button).grid(row=0, column=3)

mainloop()
</code></pre>

<p>Thanks you in advance!</p>
","7593646","","","","","2017-04-20 10:02:38","python tkInter browse folder button","<python><python-3.x><tkinter>","1","2","7","","","CC BY-SA 3.0","0"
"31662681","1","31663422","","2015-07-27 20:29:52","","20","35209","<p>My <code>index.html</code> looks like this</p>

<pre><code>&lt;form name=""myForm"" action="""" method=""post"" onsubmit=""""&gt;
&lt;p&gt;
&lt;input type=""radio"" name=""options"" id=""option1""&gt; Option1 &lt;br&gt;
&lt;input type=""radio"" name=""options"" id=""option2""&gt; Option2 &lt;br&gt;
&lt;input type=""radio"" name=""options"" id=""option3""&gt; Option3 &lt;br&gt;
&lt;/p&gt;
&lt;p&gt;&lt;input type=submit value=Next&gt;&lt;/p&gt;
&lt;/form&gt;
</code></pre>

<p>I need to get the selected button. But since they all have the same name I cannot do it by writing <code>request.form['option']</code>. If I make their names different, users can make multiple selections.</p>

<p>Isn't there a way to get a button's state by it's id ? If no, what's the simplest way to handle this form ?</p>
","3925804","","","","","2020-06-17 04:35:02","flask handle form with radio buttons","<python-3.x><flask><radio-button>","2","0","3","","","CC BY-SA 3.0","0"
"33137503","1","","","2015-10-14 23:41:48","","18","35186","<p>I'm very new to <code>Python</code> and am trying to install the <a href=""https://pypi.python.org/pypi/FuncDesigner"">FuncDesigner</a> package. It gives the following error: </p>

<p>Generator expression must be parenthesized if not sole argument and points to the following line:</p>

<pre><code>kw = {'skipArrayCast':True} if isComplexArray else {}
r = ooPoint((v, x[S.oovar_indexes[i]:S.oovar_indexes[i+1]]) for i, v in enumerate(S._variables), **kw)
</code></pre>

<p>Any ideas what to change the line starting with <strong>""r = ""</strong> to to get it to work?</p>

<p>I'm using a <code>Python 3</code> version.</p>
","356931","","4583854","","2015-10-15 00:12:08","2015-10-15 00:12:08","Generator expression must be parenthesized if not sole argument","<python><python-3.x>","1","0","3","","","CC BY-SA 3.0","0"
"51154871","1","","","2018-07-03 12:38:17","","34","35127","<p>I have Python 3.7.0 and I installed PyQt5 with this command:</p>

<pre><code>pip install PyQt5
</code></pre>

<p>I have returned this error:</p>

<pre><code>    main.py"", line 4, in &lt;module&gt;
    from PyQt5.QtWebEngineWidgets import *
ModuleNotFoundError: No module named 'PyQt5.QtWebEngineWidgets'
</code></pre>

<p>In Eclipse I have this configuration:</p>

<p><a href=""https://i.stack.imgur.com/J7p4J.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/J7p4J.png"" alt=""enter image description here""></a></p>

<p>What may be wrong?</p>

<p>Thanks</p>
","453712","","4195725","","2018-07-03 13:53:04","2020-02-19 18:15:38","Python 3.7.0 No module named 'PyQt5.QtWebEngineWidgets'","<python><python-3.x><pyqt5>","4","6","9","","","CC BY-SA 4.0","0"
"40981120","1","40981455","","2016-12-05 18:42:40","","8","35086","<p>I have installed python 3.5, and need to install pywin (pywin32)</p>

<p>however, pip cannot find it.  Note, i have just PIP install'ed send2trash and gitpython successfully</p>

<pre><code> Could not find a version that satisfies the requirement pywin32 (from versions: )
</code></pre>

<p>A few possibly relevant data points:</p>

<ul>
<li>new install of python 3.5</li>
<li>windows 7 x64</li>
<li>python 2.7 was previously installed on the machine</li>
<li>as mentioned, several other packages were installed fine via PIP</li>
<li>running these commands from git-bash, which came from the git windows installer, installed some time ago.
-- I have gnu grep in my path, so i believe i selected the git installer option to put the whole mysys toolchain in my path</li>
</ul>

<p>full --verbose output:</p>

<pre><code>C:\Users\USER&gt;pip install  pywin32    --proxy http://proxy.COMPANY.com:8080
Collecting pywin32
  Could not find a version that satisfies the requirement pywin32 (from versions: )
No matching distribution found for pywin32

C:\Users\USER&gt;pip install  pywin32    --proxy http://proxy.COMPANY.com:8080 --verbose
Config variable 'Py_DEBUG' is unset, Python ABI tag may be incorrect
Config variable 'WITH_PYMALLOC' is unset, Python ABI tag may be incorrect
Collecting pywin32
  1 location(s) to search for versions of pywin32:
  * https://pypi.python.org/simple/pywin32/
  Getting page https://pypi.python.org/simple/pywin32/
  Looking up ""https://pypi.python.org/simple/pywin32/"" in the cache
  Current age based on date: 61
  Freshness lifetime from max-age: 600
  Freshness lifetime from request max-age: 600
  The response is ""fresh"", returning cached response
  600 &gt; 61
  Analyzing links from page https://pypi.python.org/simple/pywin32/
  Could not find a version that satisfies the requirement pywin32 (from versions: )
Cleaning up...
No matching distribution found for pywin32
Exception information:
Traceback (most recent call last):
  File ""c:\users\USER\appdata\local\programs\python\python35\lib\site-packages\pip\basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""c:\users\USER\appdata\local\programs\python\python35\lib\site-packages\pip\commands\install.py"", line 324, in run
    requirement_set.prepare_files(finder)
  File ""c:\users\USER\appdata\local\programs\python\python35\lib\site-packages\pip\req\req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""c:\users\USER\appdata\local\programs\python\python35\lib\site-packages\pip\req\req_set.py"", line 554, in _prepare_file
    require_hashes
  File ""c:\users\USER\appdata\local\programs\python\python35\lib\site-packages\pip\req\req_install.py"", line 278, in populate_link
    self.link = finder.find_requirement(self, upgrade)
  File ""c:\users\USER\appdata\local\programs\python\python35\lib\site-packages\pip\index.py"", line 514, in find_requirement
    'No matching distribution found for %s' % req
pip.exceptions.DistributionNotFound: No matching distribution found for pywin32 
</code></pre>
","309433","","","","","2020-07-25 02:54:04","Python PIP cannot find pywin32 (on windows)","<python><python-3.x><pip>","6","0","2","","","CC BY-SA 3.0","0"
"29896309","1","29924949","","2015-04-27 12:50:23","","8","35009","<p>I am trying to set up a proxy server in Anaconda because my firewall does not allow me to run online commands such as</p>

<blockquote>
  <p>conda update</p>
</blockquote>

<p>I see online that I should create a <strong>.condarc</strong> file that contains the proxy address. Unfortunately, </p>

<ol>
<li><p>I dont know how to create that file (is it a text file?)</p></li>
<li><p>and where to put it?! (in which folder? in the Anaconda folder?)</p></li>
</ol>

<p>Any help appreciated
Thanks!</p>
","1609428","","1609428","","2018-04-25 18:06:26","2018-04-25 18:06:26","how to create a .condarc file for Anaconda?","<python-3.x><python-2.7><proxy><anaconda><conda>","2","0","1","","","CC BY-SA 3.0","0"
"31840195","1","32367706","","2015-08-05 18:32:16","","14","34827","<p>When I switch my PyCharm to use Python 3.4.3 I am getting the error:</p>

<blockquote>
  <p>Invalid Python SDK</p>
</blockquote>

<p>Also PyCharm does not automatically find the Python 3.4 interpreter for me, even though it is on desired path <code>/Library/Frameworks/Python.framework/Versions/3.4/bin/python3.4</code>.</p>

<p>See screenshot:</p>

<p><a href=""https://i.stack.imgur.com/LfJVU.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/LfJVU.png"" alt=""enter image description here""></a></p>

<p>Although the interpreter does work.
I am able to get correct output as expected, but code completion related to python 3.4 is not working.</p>

<p>e.g. <code>print(""hello world!)</code> still shows an error on the editor, but the console shows the correct output.</p>
","3905226","","1983854","","2017-07-14 09:42:05","2019-08-18 07:11:01","Invalid Python SDK Error while using python 3.4 on PyCharm","<python-3.x><pycharm>","4","0","","","","CC BY-SA 3.0","0"
"39854841","1","40877143","","2016-10-04 14:24:27","","21","34796","<p>I have followed a few tutorials around but I am not able to get this code block to run, I did the necessary switches from StringIO to BytesIO (I believe?)</p>

<p>I am unsure why 'banana' is printing nothing, I think the errors might be red herrings? is it something to do with me following a python2.7 tutorial and trying to translate it to python3?</p>

<pre><code>errors: File ""/Users/foo/PycharmProjects/Try/Pdfminer.py"", line 28, in &lt;module&gt;
    banana = convert(""A1.pdf"")
  File ""/Users/foo/PycharmProjects/Try/Pdfminer.py"", line 19, in convert
    infile = file(fname, 'rb')
NameError: name 'file' is not defined
</code></pre>

<p>script</p>

<pre><code>from io import BytesIO

from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfpage import PDFPage

def convert(fname, pages=None):
    if not pages:
        pagenums = set()
    else:
        pagenums = set(pages)

    output = BytesIO()
    manager = PDFResourceManager()
    converter = TextConverter(manager, output, laparams=LAParams())
    interpreter = PDFPageInterpreter(manager, converter)

    infile = file(fname, 'rb')
    for page in PDFPage.get_pages(infile, pagenums):
        interpreter.process_page(page)
    infile.close()
    converter.close()
    text = output.getvalue()
    output.close
    return text

banana = convert(""A1.pdf"")
print(banana)
</code></pre>

<p>The same thing happens with this variant:</p>

<pre><code>from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfpage import PDFPage
from io import BytesIO

def convert_pdf_to_txt(path):
    rsrcmgr = PDFResourceManager()
    retstr = BytesIO()
    codec = 'utf-8'
    laparams = LAParams()
    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)
    fp = file(path, 'rb')
    interpreter = PDFPageInterpreter(rsrcmgr, device)
    password = """"
    maxpages = 0
    caching = True
    pagenos=set()

    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):
        interpreter.process_page(page)

    text = retstr.getvalue()

    fp.close()
    device.close()
    retstr.close()
    return text

Banana = convert_pdf_to_txt(""A1.pdf"")
print(Banana)
</code></pre>

<p>I have tried searching for this (most of the pdfminer code is from <a href=""https://www.binpress.com/tutorial/manipulating-pdfs-with-python/167"" rel=""noreferrer"">this</a> or <a href=""https://stackoverflow.com/questions/26494211/extracting-text-from-a-pdf-file-using-pdfminer-in-python"">this</a>) but having no luck.</p>

<p>Any insight is appreciated.</p>

<p>Cheers</p>
","6310052","","-1","","2017-05-23 12:34:42","2020-05-17 19:23:29","Pdfminer python 3.5","<python-3.x><pdf><text><extract><pdfminer>","4","2","6","","","CC BY-SA 3.0","0"
"30612298","1","30617106","","2015-06-03 06:21:51","","8","34729","<p>I have tried PyTTS (deprecated) and PyTTSx (the most recommended) and two Google TTS solutions (gTTS and another one by some guy named Hung Truong) but none of them worked under Python 3.4. It seems they haven't been ported to 3.x.</p>

<p>I searched here on StackOverflow and Google, but all the proposed TTS solutions don't work under Python 3. I'm on Windows 7.</p>
","4932902","","4932902","","2015-06-04 22:49:59","2020-06-28 08:32:46","Text-to-speech (TTS) module that works under Python 3","<python><python-3.x><text-to-speech>","4","0","4","","","CC BY-SA 3.0","0"
"52725278","1","52725410","","2018-10-09 16:14:14","","21","34653","<p>I have below try-except to catch JSON parse errors:</p>

<pre><code>with open(json_file) as j:
    try:
        json_config = json.load(j)
    except ValueError as e:
        raise Exception('Invalid json: {}'.format(e))
</code></pre>

<p>Why is <code>During handling of the above exception, another exception occurred</code> printed out, and how do I resolve it?</p>

<pre><code>json.decoder.JSONDecodeError: Expecting ',' delimiter: line 103 column 9 (char 1093)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
&lt;....&gt;
raise Exception('Invalid json: {}'.format(e))
Exception: Invalid json: Expecting ',' delimiter: line 103 column 9 (char 1093)
</code></pre>
","2748513","","3889449","","2018-10-09 16:29:36","2020-08-18 19:46:13","During handling of the above exception, another exception occurred","<python><python-3.x>","3","1","5","","","CC BY-SA 4.0","0"
"28534460","1","28545721","","2015-02-16 04:18:59","","18","34464","<p>I'm transforming <a href=""https://gist.github.com/guinslym/5ce47460a31fe4c4046b#file-original_xml-xml"" rel=""noreferrer"">an xml</a> document with <a href=""https://gist.github.com/guinslym/5ce47460a31fe4c4046b#file-test_xslt-xslt"" rel=""noreferrer"">xslt</a>. While doing it with python3 I had this following error. But I don't have any errors with python2</p>

<pre><code>-&gt; % python3 cstm/artefact.py
Traceback (most recent call last):
  File ""cstm/artefact.py"", line 98, in &lt;module&gt;
    simplify_this_dataset('fisheries-service-des-peches.xml')
  File ""cstm/artefact.py"", line 85, in simplify_this_dataset
    xslt_root = etree.XML(xslt_content)
  File ""lxml.etree.pyx"", line 3012, in lxml.etree.XML (src/lxml/lxml.etree.c:67861)
  File ""parser.pxi"", line 1780, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102420)
ValueError: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.

#!/usr/bin/env python3
# vim:fileencoding=UTF-8:ts=4:sw=4:sta:et:sts=4:ai
# -*- coding: utf-8 -*-

from lxml import etree

def simplify_this_dataset(dataset):
    """"""Create A simplify version of an xml file
    it will remove all the attributes and assign them as Elements instead
    """"""
    module_path = os.path.dirname(os.path.abspath(__file__))
    data = open(module_path+'/data/ex-fire.xslt')
    xslt_content = data.read()
    xslt_root = etree.XML(xslt_content)
    dom = etree.parse(module_path+'/../CanSTM_dataset/'+dataset)
    transform = etree.XSLT(xslt_root)
    result = transform(dom)
    f = open(module_path+ '/../CanSTM_dataset/otra.xml', 'w')
    f.write(str(result))
    f.close()
</code></pre>
","2581266","","2581266","","2015-02-16 06:25:22","2019-12-06 14:43:32","lxml.etree.XML ValueError for Unicode string","<python><python-3.x><unicode><python-3.4>","3","0","6","","","CC BY-SA 3.0","0"
"46493419","1","","","2017-09-29 16:43:29","","14","34457","<p>I originally tried to use <code>generator</code> syntax when writing a custom generator for training a Keras model. So I <code>yield</code>ed from <code>__next__</code>. However, when I would try to train my mode with <code>model.fit_generator</code> I would get an error that my generator was not an iterator. The fix was to change <code>yield</code> to <code>return</code> which also necessitated rejiggering the logic of <code>__next__</code> to track state. It's quite cumbersome compared to letting <code>yield</code> do the work for me.</p>

<p>Is there a way I can make this work with <code>yield</code>? I will need to write several more iterators that will have to have very clunky logic if I have to use a <code>return</code> statement.</p>
","8452765","","","","","2020-03-16 02:56:33","Use a generator for Keras model.fit_generator","<python-3.x><iterator><keras><generator>","4","1","7","","","CC BY-SA 3.0","0"
"39038358","1","39038455","","2016-08-19 11:48:30","","90","34439","<p>On <a href=""https://www.codewars.com/"" rel=""nofollow noreferrer"">Codewars.com</a> I encountered the following task:</p>
<blockquote>
<p>Create a function <code>add</code> that adds numbers together when called in succession. So <code>add(1)</code> should return <code>1</code>, <code>add(1)(2)</code> should return <code>1+2</code>, ...</p>
</blockquote>
<p>While I'm familiar with the basics of Python, I've never encountered a function that is able to be called in such succession, i.e. a function <code>f(x)</code> that can be called as <code>f(x)(y)(z)...</code>. Thus far, I'm not even sure how to interpret this notation.</p>
<p>As a mathematician, I'd suspect that <code>f(x)(y)</code> is a function that assigns to every <code>x</code> a function <code>g_{x}</code> and then returns <code>g_{x}(y)</code> and likewise for <code>f(x)(y)(z)</code>.</p>
<p>Should this interpretation be correct, Python would allow me to dynamically create functions which seems very interesting to me. I've searched the web for the past hour, but wasn't able to find a lead in the right direction. Since I don't know how this programming concept is called, however, this may not be too surprising.</p>
<p>How do you call this concept and where can I read more about it?</p>
","6734723","","9871463","","2020-09-22 08:38:31","2020-09-22 08:38:31","Function chaining in Python","<python><function><python-3.x>","6","7","30","","","CC BY-SA 4.0","0"
"40312013","1","40312924","","2016-10-28 19:29:39","","15","34394","<p>I have different types of data. most of them are <code>int</code> and sometimes <code>float</code>. The <code>int</code> is different in size so 8/ 16/ 32 bits are the sizes.<br>
For this situation I'm creating a numerical type converter. therefore i check the type by using <code>isinstence()</code>. This because I have read that <code>isinstance()</code> is less worse than <code>type()</code>.</p>

<p>The point is that a lot of data i get is numpy arrays. I use spyder as IDE and then i see by the variables also a type. but when i type <code>isinstance(var,'type i read')</code> i get <code>False</code>.  </p>

<p>I did some checks:</p>

<pre><code>a = 2.17 
b = 3 
c = np.array(np.random.rand(2, 8))
d = np.array([1])
</code></pre>

<p>for there <code>isinstance(var,type)</code> i get:</p>

<pre><code>isinstance(a, float)
True
isinstance(b, int)
True
isinstance(c, float)  # or isinstance(c, np.float64)
False
isinstance(d, int)  # or isinstance(c, np.int32)
False
</code></pre>

<p><code>c</code> and <code>d</code> are True when i ask</p>

<pre><code>isinstance(c, np.ndarray)
True
isinstance(d, np.ndarray)
True
</code></pre>

<p>i can check with step in the <code>ndarray</code> by </p>

<pre><code>isinstance(c[i][j], np.float64)
True
isinstance(d[i], np.int32)
True
</code></pre>

<p>but this means that for every dimension i have to add a new index otherwise it is <code>False</code> again.
I can check there type with <code>dtype</code> like <code>c.dtype == 'float64'</code>...</p>

<p>Oke so for what i have find and tried...
My questions are basicly:</p>

<ul>
<li>how is the <code>var.dtype</code> method compared to <code>isinstance()</code> and <code>type()</code> (worst/ better etc)? </li>
<li>if <code>var.dtype</code> is even worse as <code>isinstance()</code> is there some method in the <code>isinstance()</code> without all the manual indexing? (autoindexing etc)? </li>
</ul>
","3621065","","3621065","","2016-10-28 20:20:30","2020-09-28 20:49:36","check type within numpy array","<python-3.x><numpy><isinstance>","4","5","2","","","CC BY-SA 3.0","0"
"48796169","1","48798075","","2018-02-14 21:03:46","","15","34387","<p>I am following this tensorflow <a href=""https://www.tensorflow.org/get_started/get_started_for_beginners"" rel=""noreferrer"">tutorial</a> after two days setting up the environment I finally could run <code>premade_estimator.py</code> using cmd</p>

<p><a href=""https://i.stack.imgur.com/DmlmX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/DmlmX.png"" alt=""""></a></p>

<p>but when I try to run the same code in a jupyter notebook I am getting this error:</p>

<blockquote>
<pre><code>usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]
                             [--train_steps TRAIN_STEPS]

ipykernel_launcher.py: error: unrecognized arguments: -f C:\Users\david\AppData\Roaming\jupyter\runtime\kernel-4faecb24-6e87-40b4-bf15-5d24520d7130.json
</code></pre>
  
  <p>An exception has occurred, use %tb to see the full traceback.</p>

<pre><code>SystemExit: 2

C:\Anaconda3\envs\python3x\lib\site-packages\IPython\core\interactiveshell.py:2918: 
UserWarning: To exit: use 'exit', 'quit', or Ctrl-D. warn(""To exit: use 'exit', 'quit', or Ctrl-D."", stacklevel=1)
</code></pre>
</blockquote>

<p>I have tried to fix it without success using:</p>

<pre><code>pip install --ignore-installed --upgrade jupyter

pip install ipykernel
python -m ipykernel install

conda install notebook ipykernel
ipython kernelspec install-self
</code></pre>

<p>Any idea will be appreciate! Thanks!</p>
","8828524","","8828524","","2018-02-14 23:49:50","2020-06-19 17:15:48","How to fix ipykernel_launcher.py: error: unrecognized arguments in jupyter?","<python><python-3.x><tensorflow><jupyter-notebook><jupyter>","5","0","7","","","CC BY-SA 3.0","0"
"35001229","1","","","2016-01-25 20:00:05","","13","34361","<p>I am working with data extracted from SFDC using simple-salesforce package.
I am using Python3 for scripting and Spark 1.5.2.</p>

<p>I created an rdd containing the following data:</p>

<pre><code>[('Id', 'a0w1a0000003xB1A'), ('PackSize', 1.0), ('Name', 'A')]
[('Id', 'a0w1a0000003xAAI'), ('PackSize', 1.0), ('Name', 'B')]
[('Id', 'a0w1a00000xB3AAI'), ('PackSize', 30.0), ('Name', 'C')]
...
</code></pre>

<p>This data is in RDD called v_rdd</p>

<p>My schema looks like this:</p>

<pre><code>StructType(List(StructField(Id,StringType,true),StructField(PackSize,StringType,true),StructField(Name,StringType,true)))
</code></pre>

<p>I am trying to create DataFrame out of this RDD:</p>

<pre><code>sqlDataFrame = sqlContext.createDataFrame(v_rdd, schema)
</code></pre>

<p>I print my DataFrame:</p>

<pre><code>sqlDataFrame.printSchema()
</code></pre>

<p>And get the following:</p>

<pre><code>+--------------------+--------------------+--------------------+
|                  Id|  PackSize|                          Name|
+--------------------+--------------------+--------------------+
|[Ljava.lang.Objec...|[Ljava.lang.Objec...|[Ljava.lang.Objec...|
|[Ljava.lang.Objec...|[Ljava.lang.Objec...|[Ljava.lang.Objec...|
|[Ljava.lang.Objec...|[Ljava.lang.Objec...|[Ljava.lang.Objec...|
</code></pre>

<p>I am expecting to see actual data, like this:</p>

<pre><code>+------------------+------------------+--------------------+
|                Id|PackSize|                          Name|
+------------------+------------------+--------------------+
|a0w1a0000003xB1A  |               1.0|       A            |
|a0w1a0000003xAAI  |               1.0|       B            |
|a0w1a00000xB3AAI  |              30.0|       C            |
</code></pre>

<p>Can you please help me identify what I am doing wrong here.</p>

<p>My Python script is long, I am not sure it would be convenient for people to sift through it, so I posted only parts I am having issue with. </p>

<p>Thank a ton in advance!</p>
","3596896","","","","","2018-05-17 19:36:44","Create DataFrame from list of tuples using pyspark","<python-3.x><pyspark><spark-dataframe>","1","0","6","","","CC BY-SA 3.0","0"
"42584282","1","42585214","","2017-03-03 16:45:15","","3","34309","<p>I have variable 'x_data' sized 360x190, I am trying to select particular rows of data. </p>

<pre><code> x_data_train = []
 x_data_train = np.append([x_data_train,
                           x_data[0:20,:],
                           x_data[46:65,:],
                           x_data[91:110,:],
                           x_data[136:155,:],
                           x_data[181:200,:],
                           x_data[226:245,:],
                           x_data[271:290,:],
                           x_data[316:335,:]],axis = 0)
</code></pre>

<p>I get the following error : 
TypeError: append() missing 1 required positional argument: 'values'</p>

<p>where did I go wrong ?</p>

<p>If I am using </p>

<pre><code>x_data_train = []
x_data_train.append(x_data[0:20,:])
x_data_train.append(x_data[46:65,:])
x_data_train.append(x_data[91:110,:])
x_data_train.append(x_data[136:155,:])
x_data_train.append(x_data[181:200,:])
x_data_train.append(x_data[226:245,:])
x_data_train.append(x_data[271:290,:])
x_data_train.append(x_data[316:335,:])
</code></pre>

<p>the size of the output is 8 instead of 160 rows.</p>

<p>Update: </p>

<p>In matlab, I will load the text file and x_data will be variable having 360 rows and 190 columns.
If I want to select 1 to 20 , 46 to 65, ... rows of data , I simply give 
<code>x_data_train = xdata([1:20,46:65,91:110,136:155,181:200,226:245,271:290,316:335], :);</code>
the resulting x_data_train will be the array of my desired. </p>

<p>How can do that in python because it results array of 8 subsets of array for 20*192 each, but I want it to be one array 160*192</p>
","7457101","","7457101","","2017-03-04 03:29:18","2017-03-07 04:49:34","TypeError: append() missing 1 required positional argument: 'values'","<python-3.x><append>","2","3","0","","","CC BY-SA 3.0","0"
"32795460","1","32795481","","2015-09-26 09:07:15","","21","34184","<p>I am having problems making the modules 'json' and 'urllib.request' work together in a simple Python script test. Using Python 3.5 and here is the code:</p>

<pre><code>import json
import urllib.request

urlData = ""http://api.openweathermap.org/data/2.5/weather?q=Boras,SE""
webURL = urllib.request.urlopen(urlData)
print(webURL.read())
JSON_object = json.loads(webURL.read()) #this is the line that doesn't work
</code></pre>

<p>When running script through command line the error I am getting is ""<strong>TypeError:the JSON object must be str, not 'bytes'</strong>"". I am new to Python so there is most likely a very easy solution to is. Appreciate any help here. </p>
","5378857","","","","","2018-06-14 07:51:17","Loading JSON object in Python using urllib.request and json modules","<python><python-3.x>","2","0","7","","","CC BY-SA 3.0","0"
"45448994","1","45451208","","2017-08-01 23:11:48","","23","34121","<p>I have a page that i need to get the source to use with BS4, but the middle of the page takes 1 second(maybe less) to load the content, and requests.get catches the source of the page before the section loads, how can I wait a second before getting the data?</p>

<pre><code>r = requests.get(URL + self.search, headers=USER_AGENT, timeout=5 )
    soup = BeautifulSoup(r.content, 'html.parser')
    a = soup.find_all('section', 'wrapper')
</code></pre>

<p><a href=""http://legendas.tv/busca/walking%20dead%20s03e02"" rel=""noreferrer"">The page</a></p>

<pre><code>&lt;section class=""wrapper"" id=""resultado_busca""&gt;
</code></pre>
","8261997","","","","","2020-06-16 13:00:57","Wait page to load before getting data with requests.get in python 3","<python-3.x><web-scraping><beautifulsoup><python-requests>","4","0","8","","","CC BY-SA 3.0","0"
"40143289","1","40225614","","2016-10-19 23:46:46","","36","34101","<p>I was going through the Python documentation for <code>asyncio</code> and I'm wondering why most examples use <code>loop.run_until_complete()</code> as opposed to <code>Asyncio.ensure_future()</code>.</p>

<p>For example: <a href=""https://docs.python.org/dev/library/asyncio-task.html"" rel=""noreferrer"">https://docs.python.org/dev/library/asyncio-task.html</a></p>

<p>It seems <code>ensure_future</code> would be a much better way to demonstrate the advantages of non-blocking functions. <code>run_until_complete</code> on the other hand, blocks the loop like synchronous functions do. </p>

<p>This makes me feel like I should be using <code>run_until_complete</code> instead of a combination of <code>ensure_future</code>with <code>loop.run_forever()</code> to run multiple co-routines concurrently.</p>
","1462505","","","","","2018-04-17 22:14:39","Why do most asyncio examples use loop.run_until_complete()?","<python-3.x><python-asyncio>","1","3","6","","","CC BY-SA 3.0","0"
"40598248","1","","","2016-11-14 21:37:19","","1","33954","<p>Why does my code not remove the last empty element in the list?</p>

<pre><code>templist = ['', 'hello', '', 'hi', 'mkay', '', '']

for element in templist:
    if element == '':
        templist.remove(element)

print (templist)
</code></pre>

<p>Output:</p>

<pre><code>['hello', 'hi', 'mkay', '']
</code></pre>
","6130540","","","","","2017-08-01 22:37:20","Removing all empty elements from a list","<python><list><python-3.x>","4","2","2","2016-11-14 21:51:46","","CC BY-SA 3.0","0"
"54274298","1","54291461","","2019-01-20 07:02:49","","5","33936","<p>I am writing a facial recognition program and I keep getting this error when I try to train my recognizer</p>

<pre><code>TypeError: Expected cv::UMat for argument 'labels'
</code></pre>

<p>my code is </p>

<pre><code>def detect_face(img):
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + ""haarcascade_frontalface_default.xml"")
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5);
    if (len(faces)==0):
        return None, None
    (x, y, w, h) = faces[0]
    return gray[y:y+w, x:x+h], faces[0]

def prepare_training_data():
    faces = []
    labels = []
    for img in photo_name_list: #a collection of file locations as strings
        image = cv2.imread(img)
        face, rect = detect_face(image)
        if face is not None:
            faces.append(face)
            labels.append(""me"")
    return faces, labels

def test_photos():
    face_recognizer = cv2.face.LBPHFaceRecognizer_create()
    faces, labels = prepare_training_data()
    face_recognizer.train(faces, np.ndarray(labels))
</code></pre>

<p>labels is list of labels for each photo in the image list returned from prepare_training_data, and I convert it to a numpy array because I read that is what train() needs it to be.</p>
","10096110","","","","","2019-01-21 13:53:11","OpenCV 4 TypeError: Expected cv::UMat for argument 'labels'","<python><python-3.x><numpy><opencv><facial-identification>","1","0","1","","","CC BY-SA 4.0","0"
"37675280","1","37675536","","2016-06-07 09:19:31","","1","33925","<p>I'm trying to do a code where is generates a random string, but I have only just started coding so I don't want anything to the code to be too complicated.</p>

<pre><code>import random, string
randomthing = random.choice(string)
print(randomthing(10))
</code></pre>

<p>But it keeps saying the length (len) is not defined. What should I do?</p>
","6186549","","3100115","","2016-06-07 09:20:45","2018-06-27 18:35:42","How to generate a random string","<python><python-3.x>","5","4","","2016-06-07 12:45:27","","CC BY-SA 3.0","0"
"32166147","1","","","2015-08-23 11:28:32","","0","33894","<p>I just tried to install pygame for python 3.4 my windows computer, but apparently I need to extract the whl file? I have seen other questions like this one on stackoverflow and it says to write in commant prompt:</p>

<pre><code>pip install package-name.whl
</code></pre>

<p>However, when I tried this, I got in return:</p>

<pre><code>'pip' is not recognized as an internal of external command, operable program, or batch file.
</code></pre>

<p>Is this because I have Windows 10? Please help!</p>
","5257032","","","","","2019-12-13 10:48:38","How to install pygame windows 10?","<python-3.x><pygame>","7","3","","","","CC BY-SA 3.0","0"
"32660387","1","32660421","","2015-09-18 20:19:18","","5","33857","<p>I am using python 3.3 with tkinter, and the package python3-tk is installed. In most docs the old ""import tkFont"" is used, which is not working any more.</p>

<p>This is supposed to work:</p>

<pre><code>from tkinter import font
appHighlightFont = font.Font(family='Helvetica', size=12, weight='bold')
font.families()
</code></pre>

<p>However, I get this exception on the second line:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/lib/python3.3/tkinter/font.py"", line 92, in __init__
    root.tk.call(""font"", ""create"", self.name, *font)
AttributeError: 'NoneType' object has no attribute 'tk'
</code></pre>

<p>I checked <a href=""http://infohost.nmt.edu/tcc/help/pubs/tkinter/web/fonts.html"" rel=""noreferrer"">http://infohost.nmt.edu/tcc/help/pubs/tkinter/web/fonts.html</a> and <a href=""http://www.tkdocs.com/tutorial/fonts.html"" rel=""noreferrer"">http://www.tkdocs.com/tutorial/fonts.html</a> which were the most useful tkinter docs so far.</p>

<p>Unfortunately I still can't figure out what I am doing wrong.</p>
","5351930","","4486699","","2016-01-02 06:36:07","2018-05-28 03:38:57","Python3 Tkinter fonts not working","<python><python-3.x><fonts><tkinter><python-import>","2","1","1","","","CC BY-SA 3.0","0"
"35509744","1","","","2016-02-19 16:10:48","","5","33677","<p>I do not understand how a list can not be iterable. We initialized the list and used it to add pieces of a jigsaw puzzle inside the list. </p>

<p>The user is to input the amount of rows and column they want the board to be. Each row is a new piece and each column is a new piece. Each piece is a list of 3 lists to look like a matrix. My idea was to create a list pieces=[] and append the list with each new generated piece.</p>

<p>My problem is that i can't get the str() to print the board out. I was thinking we should print the first list of the 3 lists of each piece in the pieces list and then for the middle list of the 3 lists and then the last list of the three lists. And start at a new line. I do know how to implement it though.</p>

<p>Or I was thinking i could just print each piece alone and then add pieces to that string to print out the board. So i would not be using the pieces=[] list. I don't know which is best or how to implement them.</p>

<p>I tried looking at other questions but I could not find an answer, I try to print the string representation of the class:</p>

<pre><code>from random import randint
from pprint import pprint


class Jigsaw:

    def __init__(self, r, c):
        self.pieces = []
        self.row = r
        self.col = c

    def __str__(self):
        """"""
        Writes the state and value to a string.
        """"""
        s ="" ""
        d = """"
        f = """"
        g = """"
        for row in self.pieces:
        #print(row)
            for col in row:
               #print(col)
                d = d +"" ""+ format(col[0])
        print(d)
        for row in self.pieces:
            #print(row)
            for col in row:
            #print(col)
                f = f +"" "" + format(col[1])
        print(f)

        for row in self.pieces:
            #print(row)
            for col in row:
                #print(col)
                g = g +"" ""+ format(col[2])
        print(g)



    #     return pce
    # result = 'ball : %s' \
    #     % (pce)
        #return r

    def __repr__(self):
        """"""
        Returns the string representation.
        """"""
        return str(self)

    def puzzle_board(self):
        for c in range(self.row):
            for d in range(self.col):
                self.pieces.append(self.add_piece)

        return self.pieces

    def add_piece(self):
        a = PieceGenerator()
        b = self.a_piece(a.idn,a.top,a.left,a.right,a.bottom)
        self.pieces.append(b)
        return(b)


    def mis(self):
        self.add_piece()

     # def boardShuffle(self,board):

    def a_piece(self, id, top,left,right,bottom):
        """"""
        Returns the piece values.
        """"""
        pce = [["" "", top, "" ""], [left, id, right], ["" "", bottom, "" ""]]
        return(pce)

    # def puzzleSolve(self,board):

class PieceGenerator:
    """"""
    Creates new puzzle pieces with their values.
    """"""
    idn = 0 #Global variable to keep track of all the pieces
    def __init__(self):
        """"""
        A piece have top, bottom, right, left values and puzzle piece it self
        """"""
        self.top = randint(10,99)
        self.bottom = randint(10,99)
        self.right = randint(10,99)
        self.left = randint(10,99)
        PieceGenerator.idn += 1

print(Jigsaw(3,5).puzzle_board())
</code></pre>

<p>Here is the error I get:</p>

<pre><code>Traceback (most recent call last):
   File ""C:/Users/HP/PycharmProjects/untitled/.idea/jigsaw.py"", line 102, in    &lt;module&gt;
    print(Jigsaw(3,5).puzzle_board())
   File ""C:/Users/HP/PycharmProjects/untitled/.idea/jigsaw.py"", line 56, in  __repr__
    return str(self)
   File ""C:/Users/HP/PycharmProjects/untitled/.idea/jigsaw.py"", line 22, in __str__
    for col in row:
TypeError: 'method' object is not iterable
</code></pre>
","5322478","","5589510","","2016-02-19 16:24:29","2016-02-19 16:24:29","Why do I get this error ""TypeError: 'method' object is not iterable""?","<python><python-3.x>","1","1","2","","","CC BY-SA 3.0","0"
"52027384","1","","","2018-08-26 14:48:59","","6","33550","<p>I have installed Cuda using following command on Anaconda </p>

<p>conda install -c anaconda cudatoolkit</p>

<p>Earlier I also have used following command to install Tensorflow GPU version </p>

<p>conda install -c anaconda tensorflow-gpu</p>

<p>However, Tensorflow-gpu is not activated and when I run the  following script:</p>

<pre><code>    from tensorflow.python.client import device_lib
    print(device_lib.list_local_devices())
</code></pre>

<p>name: ""/device:CPU:0""</p>

<p>device_type: ""CPU""</p>

<p>memory_limit: 268435456</p>

<p>locality 
{
}
incarnation: 12015853371339101357
]</p>
","2838082","","","","","2018-08-27 09:22:48","How to check if cuda is installed correctly on Anaconda","<python-3.x><tensorflow><cuda><anaconda>","1","0","1","","","CC BY-SA 4.0","0"
"34846413","1","34848474","","2016-01-18 02:31:11","","26","33517","<p>I am creating a python movie player/maker, and I want to find the number of lines in a multiple line string. I was wondering if there was any built in function or function I could code to do this:</p>

<pre><code>x = """"""
line1
line2 """"""

getLines(x)
</code></pre>
","5338726","","","","","2020-01-23 11:20:59","Find how many lines in string","<python><python-3.x>","4","2","2","","","CC BY-SA 3.0","0"
"44574548","1","44722672","","2017-06-15 18:32:15","","20","33443","<p>I need to fetch a list of items from S3 using Boto3, but instead of returning default sort order (descending) I want it to return it via reverse order.</p>

<p>I know you can do it via awscli:</p>

<pre><code>aws s3api list-objects --bucket mybucketfoo --query ""reverse(sort_by(Contents,&amp;LastModified))""
</code></pre>

<p>and its doable via the UI console (not sure if this is done client side or server side)</p>

<p>I cant seem to see how to do this in Boto3.</p>

<p>I am currently fetching all the files, and then sorting...but that seems overkill, especially if I only care about the 10 or so most recent files.</p>

<p>The filter system seems to only accept the Prefix for s3, nothing else.</p>
","957913","","","","","2020-03-15 16:29:10","Boto3 S3, sort bucket by last modified","<python><python-3.x><amazon-web-services><amazon-s3><boto3>","7","5","5","","","CC BY-SA 3.0","0"
"42908334","1","42913743","","2017-03-20 15:59:57","","22","33433","<p>I'm trying to make a function with the arguments <code>(a,tol=1e-8)</code> that returns a boolean value that tells the user whether or not the matrix is symmetric (symmetric matrix is equal to its transpose). So far I have:</p>

<pre><code>def check_symmetric(a, tol=1e-8):
if np.transpose(a, axes=axes) == np.transpose(a, axes=axes):
    return True
def sqr(s):
    rows = len(s)
    for row in sq:
        if len(row) != rows:
            return False
    return True
if a != sqr(s):
    raise ValueError
</code></pre>

<p>although I keep getting an <code>axes isn't defined</code> message so I'm pretty sure that doesn't work at all...... the tests I'd like to pass are:</p>

<pre><code>e = np.eye(4)
f = np.diag([1], k=3)
g = e[1:, :]

print(check_symmetric(e))
print(not check_symmetric(e + f))
print(check_symmetric(e + f * 1e-9))
print(not check_symmetric(e + f * 1e-9, 1e-10))
try:
    check_symmetric(g)
    print(False)
except ValueError:
    print(True)
</code></pre>

<p>Any help is appreciated, thanks!</p>
","7740822","","","","","2019-07-15 20:58:49","Checking if a matrix is symmetric in Numpy","<python><python-3.x><numpy><matrix>","2","3","4","","","CC BY-SA 3.0","0"
"33322659","1","33322792","","2015-10-24 19:55:05","","7","33408","<p>I need to use <strong><em>requests</em></strong> in my code but it says that it's not installed. I get the following error: <code>No module named 'requests'</code>. It's actually installed and works in python 2.7: <code>Requirement already satisfied (use --upgrade to upgrade): requests in /Library/Python/2.7/site-packages</code>. I searched a lot and tried to reinstall it, to <a href=""https://stackoverflow.com/questions/11994337/need-help-installing-requests-for-python-3"">download missing libraries</a>, and etc... but nothing helped. How can I make it work on python 3.5 ?</p>
","5114355","","-1","","2017-05-23 12:26:10","2016-09-26 08:46:01","No module named 'requests' Python 3.5.0","<python-3.x><python-requests>","2","1","1","","","CC BY-SA 3.0","0"
"31848293","1","31848403","","2015-08-06 06:28:48","","47","33386","<p>I had a script in Python2 that was working great. </p>

<pre><code>def _generate_signature(data):
   return hmac.new('key', data, hashlib.sha256).hexdigest()
</code></pre>

<p>Where data was the output of <code>json.dumps</code>. </p>

<p>Now, if I try to run the same kind of code in Python 3, I get the following:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/lib/python3.4/hmac.py"", line 144, in new
    return HMAC(key, msg, digestmod)
  File ""/usr/lib/python3.4/hmac.py"", line 42, in __init__
    raise TypeError(""key: expected bytes or bytearray, but got %r"" %type(key).__name__)
TypeError: key: expected bytes or bytearray, but got 'str'
</code></pre>

<p>If I try something like transforming the key to bytes like so:</p>

<pre><code>bytes('key')
</code></pre>

<p>I get</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
TypeError: string argument without an encoding
</code></pre>

<p>I'm still struggling to understand the encodings in Python 3.</p>
","4467480","","1245190","","2017-09-02 14:24:15","2020-07-01 18:05:09","Python3 and hmac . How to handle string not being binary","<python><string><python-3.x><encoding><hmac>","3","1","8","","","CC BY-SA 3.0","0"
"30712020","1","30713019","","2015-06-08 14:34:14","","14","33373","<p>I'm trying to write a list of data bytes to a CSV file. Since it's a list of byte strings, I used the below code:</p>

<pre><code>with open(r""E:\Avinash\Python\extracting-drug-data\out.csv"", ""wb"") as w:
    writer = csv.writer(w)
    writer.writerows(bytes(datas, 'UTF-8'))
</code></pre>

<p>But it results in the following error:</p>

<blockquote>
  <p>TypeError: encoding or errors without a string argument</p>
</blockquote>

<p><code>datas</code> is a list of byte strings.</p>

<pre><code>print(datas)
</code></pre>

<p>yields</p>

<pre><code>[b'DB08873', b' MOLSDFPDBSMILESInChIView Structure \xc3\x97Structure for DB08873 (Boceprevir) Close', b'394730-60-0', b'LHHCSNFAOIFYRV-DOVBMPENSA-N', b'Organic acids and derivatives  ', b'Food increases exposure of boceprevir by up to 65% relative to fasting state. However, type of food and time of meal does not affect bioavailability of boceprevir and thus can be taken without regards to food.  \r\nTmax = 2 hours;\r\nTime to steady state, three times a day dosing = 1 day;\r\nCmax]
</code></pre>

<p>I want the above list to be printed as first row in a CSV file with the decoding of Unicode chars. That is, <code>\xc3\x97</code> should be converted to it's corresponding character.</p>
","3297613","","2050","","2019-01-29 01:27:38","2019-01-29 01:27:38","TypeError: encoding or errors without a string argument","<python><python-3.x>","2","4","","","","CC BY-SA 4.0","0"
"52851994","1","52852053","","2018-10-17 09:52:47","","9","33297","<p>I have a list called <strong>fileList</strong> containing thousands of filenames and sizes something like this:</p>

<pre><code>['/home/rob/Pictures/some/folder/picture one something.jpg', '143452']
['/home/rob/Pictures/some/other/folder/pictureBlah.jpg', '473642']
['/home/rob/Pictures/folder/blahblahpicture filename.jpg', '345345']
</code></pre>

<p>I want to copy the files using fileList[0] as the source but to another whole destination.  Something like:</p>

<pre><code>copyFile(fileList[0], destinationFolder)
</code></pre>

<p>and have it copy the file to that location.</p>

<p>When I try this like so:</p>

<pre><code>for item in fileList:
    copyfile(item[0], ""/Users/username/Desktop/testPhotos"")
</code></pre>

<p>I get an error like the following:</p>

<pre><code>with open(dst, 'wb') as fdst:
IsADirectoryError: [Errno 21] Is a directory: '/Users/username/Desktop/testPhotos'
</code></pre>

<p>What could be something I could look at to get this working? I'm using Python&nbsp;3 on a Mac and on Linux.</p>
","4894051","","63550","","2018-10-17 17:07:46","2019-04-25 12:41:35","Copy a file from one location to another in Python","<python><python-3.x><file-copying>","4","2","5","","","CC BY-SA 4.0","0"
"40354399","1","40354450","","2016-11-01 05:00:42","","26","33296","<p>I have installed  <code>Python 3.4.1</code> in windows desktop, but i don't have pip. I am trying to install a module from <code>https://www.reportlab.com/reportlabplus/installation/</code> it shows <code>pip install rlextra -i https://www.reportlab.com/pypi/</code>.  And then if I execute the reported command, it shows the following</p>
<pre><code>C:\Python34&gt;pip install rlextra -i https://www.reportlab.com/pypi/
'pip' is not recognized as an internal or external command,
operable program or batch file.
</code></pre>
<p>Edit: This question is not about how to  install pip, instead why you need pip ?</p>
","2278097","","13890306","","2020-08-02 15:04:10","2020-08-02 15:04:10","What is pip in Python?","<python><python-3.x><module>","3","0","4","2016-11-01 21:30:41","","CC BY-SA 4.0","0"
"59265097","1","59269003","","2019-12-10 10:27:45","","20","33282","<p>I am under Windows 10, 64 bits.</p>

<p>I tried several time to update Spyder 4.0.0 with both the Anaconda Prompt and the Anaconda Navigator.
It failed. I uninstalled Anaconda and reinstalled it.</p>

<p>Then I ran the Anaconda Prompt as an Administrator and executed :</p>

<pre><code>conda update spyder
</code></pre>

<p>The version of Spyder was 3.3.6.
I tried this command :</p>

<pre><code>conda install spyder=4.0.0
</code></pre>

<p>The prompt returned :</p>

<pre><code>Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: |
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
Finding conflicts:   0%|                                                                       | 0/360 [00:00&lt;?, ?it/s]|Examining hpack:  42%|█████████████████████████▌                                   | 151/360 [00:00&lt;00:00, 1468.80it/s]/Examining jupyter_core:  56%|██████████████████████████████▍                       | 203/360 [00:00&lt;00:00, 1468.80it/-
- mparing specs that have this dependency:   0%|                                                | 0/20 [00:00&lt;?, ?it/\
Examining jupyter_core:  57%|██████████████████████████████▌                       | 204/360 [00:20&lt;00:00, 1468.80it/s]|| mparing specs that have this dependency:  10%|████                                    | 2/20 [00:18&lt;02:47,  9.32s/i/ |
Comparing specs that have this dependency:  55%|█████████████████████▍                 | 11/20 [01:07&lt;00:55,  6.16s/i-
Examining tbb:  73%|███████████████████████████████████████████████▎                 | 262/360 [02:35&lt;01:19,  1.24it/s]-Examining jedi:  81%|███████████████████████████████████████████████████▉            | 292/360 [02:35&lt;00:54,  1.24it/- \
Examining jupyter_client:  90%|████████████████████████████████████████████████▍     | 323/360 [06:03&lt;04:23,  7.12s/i\ \
Comparing specs that have this dependency:   7%|██▋                                     | 1/15 [00:10&lt;02:27, 10.51s/i-
Comparing specs that have this dependency:  13%|█████▎                                  | 2/15 [00:21&lt;02:19, 10.73s/i/ |
Comparing specs that have this dependency:  27%|██████████▋                             | 4/15 [00:31&lt;01:25,  7.77s/i\ \
Finding conflict paths:   0%|                                                                    | 0/7 [00:00&lt;?, ?it// -
Comparing specs that have this dependency:  73%|████████████████████████████▌          | 11/15 [01:21&lt;00:29,  7.40s/i\ |
Finding shortest conflict path for jupyter_client[version='&gt;=4.2']:  67%|██████████▋     | 4/6 [00:10&lt;00:01,  1.35it/| -
Comparing specs that have this dependency:  80%|███████████████████████████████▏       | 12/15 [01:34&lt;00:23,  7.91s/i- |
failed                                                                                                                 /
                                                                                                                       \
UnsatisfiableError: The following specifications were found to be incompatible with a past
explicit spec that is not an explicit spec in this operation (spyder):

  - spyder=4.0.0

The following specifications were found to be incompatible with each other:



Package jedi conflicts for:
spyder=4.0.0 -&gt; python-language-server[version='&gt;=0.31.2,&lt;0.32.0'] -&gt; jedi[version='&gt;=0.14.1,&lt;0.16']
jedi
jupyterlab_server -&gt; notebook -&gt; ipykernel -&gt; ipython[version='&gt;=4.0'] -&gt; jedi[version='&gt;=0.10']
anaconda==2019.10 -&gt; jedi==0.15.1[build='py37_0|py36_0|py27_0']
ipython -&gt; jedi[version='&gt;=0.10']
qtconsole -&gt; ipykernel[version='&gt;=4.1'] -&gt; ipython[version='&gt;=4.0'] -&gt; jedi[version='&gt;=0.10']
_ipyw_jlab_nb_ext_conf -&gt; ipywidgets -&gt; ipython[version='&gt;=4.0.0'] -&gt; jedi[version='&gt;=0.10']
jupyter_console -&gt; ipython -&gt; jedi[version='&gt;=0.10']
spyder-kernels -&gt; ipykernel -&gt; ipython[version='&gt;=4.0'] -&gt; jedi[version='&gt;=0.10']
widgetsnbextension -&gt; notebook[version='&gt;=4.4.1'] -&gt; ipykernel -&gt; ipython[version='&gt;=4.0'] -&gt; jedi[version='&gt;=0.10']
jupyterlab -&gt; notebook[version='&gt;=4.3.1'] -&gt; ipykernel -&gt; ipython[version='&gt;=4.0'] -&gt; jedi[version='&gt;=0.10']
jupyter -&gt; ipykernel -&gt; ipython[version='&gt;=4.0'] -&gt; jedi[version='&gt;=0.10']
spyder=4.0.0 -&gt; qtconsole[version='&gt;=4.6.0'] -&gt; ipykernel[version='&gt;=4.1'] -&gt; ipython[version='&gt;=5.0'] -&gt; jedi[version='&gt;=0.10']
ipywidgets -&gt; ipython[version='&gt;=4.0.0'] -&gt; jedi[version='&gt;=0.10']
notebook -&gt; ipykernel -&gt; ipython[version='&gt;=4.0'] -&gt; jedi[version='&gt;=0.10']
ipykernel -&gt; ipython[version='&gt;=4.0'] -&gt; jedi[version='&gt;=0.10']
anaconda==2019.10 -&gt; spyder==3.3.6=py27_0 -&gt; jedi[version='&gt;=0.10|&gt;=0.9']
spyder=4.0.0 -&gt; jedi=0.14.1
Package jupyter_client conflicts for:
_ipyw_jlab_nb_ext_conf -&gt; jupyterlab -&gt; notebook[version='&gt;=4.3.1'] -&gt; nbconvert -&gt; jupyter_client[version='&gt;=4.2']
widgetsnbextension -&gt; notebook[version='&gt;=4.4.1'] -&gt; nbconvert -&gt; jupyter_client[version='&gt;=4.2']
jupyterlab -&gt; notebook[version='&gt;=4.3.1'] -&gt; nbconvert -&gt; jupyter_client[version='&gt;=4.2']
ipywidgets -&gt; widgetsnbextension[version='&gt;=3.3.0,&lt;3.4.0'] -&gt; notebook[version='&gt;=4.4.1'] -&gt; jupyter_client[version='&gt;=5.2.0|&gt;=5.3.1|&gt;=5.3.4']
jupyterlab_server -&gt; notebook -&gt; jupyter_client[version='&gt;=5.2.0|&gt;=5.3.1|&gt;=5.3.4']
anaconda==2019.10 -&gt; ipykernel==4.10.0=py27_0 -&gt; jupyter_client[version='&gt;=4.1|&gt;=5.2.0|&gt;=5.2.3|&gt;=5.3.1']
jupyter -&gt; ipykernel -&gt; jupyter_client[version='&gt;=4.1|&gt;=4.2|&gt;=5.2.0|&gt;=5.3.1|&gt;=5.3.4']
ipywidgets -&gt; widgetsnbextension[version='&gt;=3.3.0,&lt;3.4.0'] -&gt; notebook[version='&gt;=4.4.1'] -&gt; nbconvert -&gt; jupyter_client[version='&gt;=4.2']
spyder=4.0.0 -&gt; qtconsole[version='&gt;=4.6.0'] -&gt; ipykernel[version='&gt;=4.1'] -&gt; jupyter_client
jupyterlab -&gt; notebook[version='&gt;=4.3.1'] -&gt; jupyter_client[version='&gt;=5.2.0|&gt;=5.3.1|&gt;=5.3.4']
spyder=4.0.0 -&gt; nbconvert -&gt; jupyter_client[version='&gt;=4.1|&gt;=4.2|&gt;=5.3.4']
ipywidgets -&gt; ipykernel[version='&gt;=4.5.1'] -&gt; jupyter_client
anaconda==2019.10 -&gt; jupyter_client==5.3.3[build='py27_1|py36_1|py37_1']
spyder-kernels -&gt; ipykernel -&gt; jupyter_client
jupyter_console -&gt; jupyter_client
spyder-kernels -&gt; jupyter_client[version='&gt;=5.2.3|&gt;=5.3.4']
notebook -&gt; nbconvert -&gt; jupyter_client[version='&gt;=4.2']
jupyter_client
notebook -&gt; jupyter_client[version='&gt;=5.2.0|&gt;=5.3.1|&gt;=5.3.4']
nbconvert -&gt; jupyter_client[version='&gt;=4.2']
_ipyw_jlab_nb_ext_conf -&gt; ipywidgets -&gt; ipykernel[version='&gt;=4.5.1'] -&gt; jupyter_client[version='&gt;=5.2.0|&gt;=5.3.1|&gt;=5.3.4']
jupyterlab_server -&gt; notebook -&gt; nbconvert -&gt; jupyter_client[version='&gt;=4.2']
widgetsnbextension -&gt; notebook[version='&gt;=4.4.1'] -&gt; jupyter_client[version='&gt;=5.2.0|&gt;=5.3.1|&gt;=5.3.4']
ipykernel -&gt; jupyter_client
qtconsole -&gt; ipykernel[version='&gt;=4.1'] -&gt; jupyter_client
qtconsole -&gt; jupyter_client[version='&gt;=4.1']
Package jupyter_core conflicts for:
anaconda==2019.10 -&gt; jupyter_core==4.5.0=py_0
spyder=4.0.0 -&gt; nbconvert -&gt; jupyter_client[version='&gt;=4.2'] -&gt; jupyter_core[version='&gt;=4.6.0']
nbconvert -&gt; jupyter_client[version='&gt;=4.2'] -&gt; jupyter_core[version='&gt;=4.6.0']
jupyterlab -&gt; notebook[version='&gt;=4.3.1'] -&gt; jupyter_core[version='&gt;=4.4.0|&gt;=4.6.0']
ipywidgets -&gt; ipykernel[version='&gt;=4.5.1'] -&gt; jupyter_client -&gt; jupyter_core[version='&gt;=4.4.0|&gt;=4.6.0']
anaconda-project -&gt; anaconda-client -&gt; nbformat[version='&gt;=4.4.0'] -&gt; jupyter_core
jupyter_core
anaconda-navigator -&gt; anaconda-client[version='&gt;=1.6'] -&gt; nbformat[version='&gt;=4.4.0'] -&gt; jupyter_core
jupyter -&gt; notebook -&gt; jupyter_core[version='&gt;=4.4.0|&gt;=4.6.0']
_ipyw_jlab_nb_ext_conf -&gt; jupyterlab -&gt; notebook[version='&gt;=4.3.1'] -&gt; jupyter_core[version='&gt;=4.4.0|&gt;=4.6.0']
spyder-kernels -&gt; jupyter_client[version='&gt;=5.2.3'] -&gt; jupyter_core[version='&gt;=4.6.0']
widgetsnbextension -&gt; notebook[version='&gt;=4.4.1'] -&gt; jupyter_core[version='&gt;=4.4.0|&gt;=4.6.0']
notebook -&gt; jupyter_core[version='&gt;=4.4.0|&gt;=4.6.0']
ipywidgets -&gt; nbformat[version='&gt;=4.2.0'] -&gt; jupyter_core
anaconda-client -&gt; nbformat[version='&gt;=4.4.0'] -&gt; jupyter_core
nbformat -&gt; jupyter_core
jupyter_console -&gt; jupyter_client -&gt; jupyter_core[version='&gt;=4.6.0']
anaconda==2019.10 -&gt; jupyter_client==5.3.3=py27_1 -&gt; jupyter_core[version='&gt;=4.4.0']
qtconsole -&gt; jupyter_core
ipykernel -&gt; jupyter_client -&gt; jupyter_core[version='&gt;=4.6.0']
spyder=4.0.0 -&gt; nbconvert -&gt; jupyter_core
jupyter_client -&gt; jupyter_core[version='&gt;=4.6.0']
nbconvert -&gt; jupyter_core
jupyterlab_server -&gt; notebook -&gt; jupyter_core[version='&gt;=4.4.0|&gt;=4.6.0']
qtconsole -&gt; jupyter_client[version='&gt;=4.1'] -&gt; jupyter_core[version='&gt;=4.6.0']
Package spyder-kernels conflicts for:
spyder=4.0.0 -&gt; spyder-kernels[version='&gt;=1.8.1,&lt;2.0.0']
spyder-kernels
anaconda==2019.10 -&gt; spyder-kernels==0.5.2[build='py37_0|py36_0|py27_0']
anaconda==2019.10 -&gt; spyder==3.3.6=py27_0 -&gt; spyder-kernels[version='&gt;=0.5.0,&lt;1']
Package qtconsole conflicts for:
anaconda==2019.10 -&gt; qtconsole==4.5.5=py_0
jupyter -&gt; qtconsole
anaconda==2019.10 -&gt; jupyter==1.0.0=py27_7 -&gt; qtconsole[version='&gt;=4.2']
spyder=4.0.0 -&gt; qtconsole[version='&gt;=4.6.0']
qtconsole
</code></pre>

<p>When launching the Anaconda Navigator, the Spyder version was still on 3.3.6.</p>

<p>What am I missing to benefit the last version of Spyder ?</p>

<p><strong>EDIT 1:</strong></p>

<p>When I try to use the Anaconda Navigator, it says :</p>

<blockquote>
  <p>spyder 4.0.0 cannot be installed on this environment</p>
</blockquote>

<p>And it proposes other environments to install but I want Spyder to be updated, not installed somewhere else.</p>

<p><strong>EDIT 2:</strong>
I successfuly change conda to 4.6, but I get these two errors :</p>

<pre><code>FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\brichard\\AppData\\Local\\Continuum\\anaconda3\\Menu\\spyder_shortcut.jsodone
</code></pre>

<p>and :</p>

<pre><code>CondaError: Cannot link a source that does not exist. C:\Users\brichard\AppData\Local\Continuum\anaconda3\Scripts\conda.exe 
Running `conda clean --packages` may resolve your problem.
</code></pre>

<p>The <code>conda clean --packages</code> command does not help since the same errors are raised after it is executed.</p>
","12213202","","12213202","","2019-12-16 10:11:49","2020-10-30 19:42:29","Trouble updating to Spyder 4.0.0","<python-3.x><anaconda><conda><spyder>","8","2","3","","","CC BY-SA 4.0","0"
"49883177","1","49883466","","2018-04-17 16:29:15","","55","33210","<p><strong>Especially</strong> when using recursive code there are massive improvements with <code>lru_cache</code>.       I do understand that a cache is a space that stores data that has to be served fast and saves the computer from recomputing.</p>

<p>How does the <strong>Python</strong> <code>lru_cache</code> from functools work internally? </p>

<p>I'm Looking for a specific answer, does it use dictionaries like the rest of Python? Does it only store the <code>return</code> value?</p>

<p>I know that <strong>Python</strong> is heavily built on top of dictionaries, however, I couldn't find a specific answer to this question. Hopefully, someone can simplify this answer for all the users on <strong><em>StackOverflow</em></strong>.</p>
","6929467","","6929467","","2018-04-17 16:54:02","2020-07-23 06:20:12","How does Lru_cache (from functools) Work?","<python><python-3.x><numpy><caching><lru>","3","0","16","","","CC BY-SA 3.0","0"
"34818782","1","","","2016-01-15 19:50:44","","9","33202","<p>I am using Python; and I need to iterate through JSON objects and retrieve nested values. A snippet of my data follows:</p>

<pre><code> ""bills"": [
{
  ""url"": ""http:\/\/maplight.org\/us-congress\/bill\/110-hr-195\/233677"",
  ""jurisdiction"": ""us"",
  ""session"": ""110"",
  ""prefix"": ""H"",
  ""number"": ""195"",
  ""measure"": ""H.R. 195 (110\u003csup\u003eth\u003c\/sup\u003e)"",
  ""topic"": ""Seniors' Health Care Freedom Act of 2007"",
  ""last_update"": ""2011-08-29T20:47:44Z"",
  ""organizations"": [
    {
      ""organization_id"": ""22973"",
      ""name"": ""National Health Federation"",
      ""disposition"": ""support"",
      ""citation"": ""The National Health Federation (n.d.). \u003ca href=\""http:\/\/www.thenhf.com\/government_affairs_federal.html\""\u003e\u003ccite\u003e Federal Legislation on Consumer Health\u003c\/cite\u003e\u003c\/a\u003e. Retrieved August 6, 2008, from The National Health Federation."",
      ""catcode"": ""J3000""
    },
    {
      ""organization_id"": ""27059"",
      ""name"": ""A Christian Perspective on Health Issues"",
      ""disposition"": ""support"",
      ""citation"": ""A Christian Perspective on Health Issues (n.d.). \u003ca href=\""http:\/\/www.acpohi.ws\/page1.html\""\u003e\u003ccite\u003ePart E - Conclusion\u003c\/cite\u003e\u003c\/a\u003e. Retrieved August 6, 2008, from ."",
      ""catcode"": ""X7000""
    },
    {
      ""organization_id"": ""27351"",
      ""name"": ""Natural Health Roundtable"",
      ""disposition"": ""support"",
      ""citation"": ""Natural Health Roundtable (n.d.). \u003ca href=\""http:\/\/naturalhealthroundtable.com\/reform_agenda\""\u003e\u003ccite\u003eNatural Health Roundtable SUPPORTS the following bills\u003c\/cite\u003e\u003c\/a\u003e. Retrieved August 6, 2008, from Natural Health Roundtable."",
      ""catcode"": ""J3000""
    }
  ]
},
</code></pre>

<p>I need to go through each object in ""bills"" and get ""session"", ""prefix"", etc. and I also need go through each ""organizations"" and get ""name"", ""disposition"", etc. I have the following code:</p>

<pre><code>import csv
import json

path = 'E:/Thesis/thesis_get_data'

with open (path + ""/"" + 'maplightdata110congress.json',""r"") as f:
data = json.load(f)
a = data['bills']
b = data['bills'][0][""prefix""]
c = data['bills'][0][""number""]

h = data['bills'][0]['organizations'][0]
e = data['bills'][0]['organizations'][0]['name']
f = data['bills'][0]['organizations'][0]['catcode']
g = data['bills'][0]['organizations'][0]['catcode']

for i in a:
    for index in e:
          print ('name')
</code></pre>

<p>and it returns the string 'name' a bunch of times.</p>

<p>Suggestions? </p>
","5211377","","5211377","","2016-01-15 19:56:51","2019-08-07 11:40:25","iterate through nested JSON object and get values with Python","<json><python-3.x><for-loop>","2","0","6","","","CC BY-SA 3.0","0"
"48342143","1","","","2018-01-19 13:23:22","","2","33098","<p>hello i am new to sklearn in python and iam trying to learn it and use this module to predict some numbers based on two features here is the error i am getting:  </p>

<blockquote>
  <p>ValueError: only 2 non-keyword arguments accepted</p>
</blockquote>

<p>and here is my code:</p>

<pre><code>    from sklearn.linear_model import LinearRegression
    import numpy as np

    trainingData = np.array([[861, 16012018], [860, 12012018], [859, 9012018], [858, 5012018], [857, 2012018], [856, 29122017], [855, 26122017], [854, 22122017], [853, 19122017]])
    trainingScores = np.array([11,18,23,33,34,6],[10,19,21,33,34,1], [14,18,22,23,31,6],[16,22,29,31,33,10],[21,24,27,30,31,6],[1,14,15,20,27,7],[1,9,10,11,15,8],[2,9,27,31,35,1],[7,13,18,22,33,2])

    clf = LinearRegression(fit_intercept=True)
    clf.fit(trainingScores,trainingData)

   predictionData = np.array([862, 19012018 ])
   x=clf.predict(predictionData)
   print(x)
</code></pre>
","5037966","","","","","2020-05-19 00:29:49","how to resolve this ValueError: only 2 non-keyword arguments accepted sklearn python","<python-3.x><scikit-learn><valueerror>","2","4","","","","CC BY-SA 3.0","0"
"31036098","1","31036156","","2015-06-24 20:13:58","","60","32984","<p>In Python 2, <code>floor()</code> returned a float value. Although not obvious to me, I found a few explanations clarifying  why it may be useful to have <code>floor()</code> return float (for cases like <code>float('inf')</code> and <code>float('nan')</code>). </p>

<p>However, in Python 3, <code>floor()</code> returns integer (and returns overflow error for the special cases mentioned before).</p>

<p>So what is the difference, if any, between <code>int()</code> and <code>floor()</code> now?</p>
","1470459","","2301450","","2015-06-24 20:16:21","2020-05-01 09:56:20","What is the difference between int() and floor() in Python 3?","<python><python-3.x><floating-point>","2","0","11","","","CC BY-SA 3.0","0"
"34447623","1","34525188","","2015-12-24 05:17:52","","37","32925","<p>How can I wrap an open binary stream – a Python 2 <code>file</code>, a Python 3 <code>io.BufferedReader</code>, an <code>io.BytesIO</code> – in an <code>io.TextIOWrapper</code>?</p>

<p>I'm trying to write code that will work unchanged:</p>

<ul>
<li>Running on Python 2.</li>
<li>Running on Python 3.</li>
<li>With binary streams generated from the standard library (i.e. I can't control what type they are)</li>
<li>With binary streams made to be test doubles (i.e. no file handle, can't re-open).</li>
<li>Producing an <code>io.TextIOWrapper</code> that wraps the specified stream.</li>
</ul>

<p>The <code>io.TextIOWrapper</code> is needed because its API is expected by other parts of the standard library. Other file-like types exist, but don't provide the right API.</p>

<h1>Example</h1>

<p>Wrapping the binary stream presented as the <code>subprocess.Popen.stdout</code> attribute:</p>

<pre class=""lang-python prettyprint-override""><code>import subprocess
import io

gnupg_subprocess = subprocess.Popen(
        [""gpg"", ""--version""], stdout=subprocess.PIPE)
gnupg_stdout = io.TextIOWrapper(gnupg_subprocess.stdout, encoding=""utf-8"")
</code></pre>

<p>In unit tests, the stream is replaced with an <code>io.BytesIO</code> instance to control its content without touching any subprocesses or filesystems.</p>

<pre class=""lang-python prettyprint-override""><code>gnupg_subprocess.stdout = io.BytesIO(""Lorem ipsum"".encode(""utf-8""))
</code></pre>

<p>That works fine on the streams created by Python 3's standard library. The same code, though, fails on streams generated by Python 2:</p>

<pre class=""lang-python prettyprint-override""><code>[Python 2]
&gt;&gt;&gt; type(gnupg_subprocess.stdout)
&lt;type 'file'&gt;
&gt;&gt;&gt; gnupg_stdout = io.TextIOWrapper(gnupg_subprocess.stdout, encoding=""utf-8"")
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'file' object has no attribute 'readable'
</code></pre>

<h1>Not a solution: Special treatment for <code>file</code></h1>

<p>An obvious response is to have a branch in the code which tests whether the stream actually is a Python 2 <code>file</code> object, and handle that differently from <code>io.*</code> objects.</p>

<p>That's not an option for well-tested code, because it makes a branch that unit tests – which, in order to run as fast as possible, must not create any <em>real</em> filesystem objects – can't exercise.</p>

<p>The unit tests will be providing test doubles, not real <code>file</code> objects. So creating a branch which won't be exercised by those test doubles is defeating the test suite.</p>

<h1>Not a solution: <code>io.open</code></h1>

<p>Some respondents suggest re-opening (e.g. with <code>io.open</code>) the underlying file handle:</p>

<pre class=""lang-python prettyprint-override""><code>gnupg_stdout = io.open(
        gnupg_subprocess.stdout.fileno(), mode='r', encoding=""utf-8"")
</code></pre>

<p>That works on both Python 3 and Python 2:</p>

<pre class=""lang-python prettyprint-override""><code>[Python 3]
&gt;&gt;&gt; type(gnupg_subprocess.stdout)
&lt;class '_io.BufferedReader'&gt;
&gt;&gt;&gt; gnupg_stdout = io.open(gnupg_subprocess.stdout.fileno(), mode='r', encoding=""utf-8"")
&gt;&gt;&gt; type(gnupg_stdout)
&lt;class '_io.TextIOWrapper'&gt;
</code></pre>

<pre class=""lang-python prettyprint-override""><code>[Python 2]
&gt;&gt;&gt; type(gnupg_subprocess.stdout)
&lt;type 'file'&gt;
&gt;&gt;&gt; gnupg_stdout = io.open(gnupg_subprocess.stdout.fileno(), mode='r', encoding=""utf-8"")
&gt;&gt;&gt; type(gnupg_stdout)
&lt;type '_io.TextIOWrapper'&gt;
</code></pre>

<p>But of course it <strong>relies on re-opening a real file</strong> from its file handle. So it fails in unit tests when the test double is an <code>io.BytesIO</code> instance:</p>

<pre class=""lang-python prettyprint-override""><code>&gt;&gt;&gt; gnupg_subprocess.stdout = io.BytesIO(""Lorem ipsum"".encode(""utf-8""))
&gt;&gt;&gt; type(gnupg_subprocess.stdout)
&lt;type '_io.BytesIO'&gt;
&gt;&gt;&gt; gnupg_stdout = io.open(gnupg_subprocess.stdout.fileno(), mode='r', encoding=""utf-8"")
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
io.UnsupportedOperation: fileno
</code></pre>

<h1>Not a solution: <code>codecs.getreader</code></h1>

<p>The standard library also has the <code>codecs</code> module, which provides wrapper features:</p>

<pre class=""lang-python prettyprint-override""><code>import codecs

gnupg_stdout = codecs.getreader(""utf-8"")(gnupg_subprocess.stdout)
</code></pre>

<p>That's good because it doesn't attempt to re-open the stream. But it fails to provide the <code>io.TextIOWrapper</code> API. Specifically, it <strong>doesn't inherit <code>io.IOBase</code></strong> and <strong>doesn't have the <code>encoding</code> attribute</strong>:</p>

<pre class=""lang-python prettyprint-override""><code>&gt;&gt;&gt; type(gnupg_subprocess.stdout)
&lt;type 'file'&gt;
&gt;&gt;&gt; gnupg_stdout = codecs.getreader(""utf-8"")(gnupg_subprocess.stdout)
&gt;&gt;&gt; type(gnupg_stdout)
&lt;type 'instance'&gt;
&gt;&gt;&gt; isinstance(gnupg_stdout, io.IOBase)
False
&gt;&gt;&gt; gnupg_stdout.encoding
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/usr/lib/python2.7/codecs.py"", line 643, in __getattr__
    return getattr(self.stream, name)
AttributeError: '_io.BytesIO' object has no attribute 'encoding'
</code></pre>

<p>So <code>codecs</code> doesn't provide objects which substitute for <code>io.TextIOWrapper</code>.</p>

<h1>What to do?</h1>

<p>So how can I write code that works for both Python 2 and Python 3, with both the test doubles and the real objects, which <strong>wraps an <code>io.TextIOWrapper</code> around the already-open byte stream</strong>?</p>
","70157","","70157","","2018-04-23 06:18:26","2018-04-23 06:18:26","Wrap an open stream with io.TextIOWrapper","<python><unit-testing><python-3.x><character-encoding><python-2.x>","6","2","6","","","CC BY-SA 3.0","0"
"36827962","1","36829884","","2016-04-24 19:35:39","","75","32883","<h1>Problem</h1>

<p>PEP8 has a rule about putting imports at the top of a file:</p>

<blockquote>
  <p>Imports are always put at the top of the file, just after any module comments and docstrings, and before module globals and constants.</p>
</blockquote>

<p>However, in certain cases, I might want to do something like:</p>

<pre><code>import sys
sys.path.insert("".."", 0)

import my_module
</code></pre>

<p>In this case, the <code>pep8</code> command line utility flags my code:</p>

<blockquote>
  <p>E402 module level import not at top of file</p>
</blockquote>

<p>What is the best way to achieve PEP8 compliance with <code>sys.path</code> modifications?</p>

<h1>Why</h1>

<p>I have this code because I'm following <a href=""https://github.com/kennethreitz/samplemod"">the project structure</a> given in <a href=""http://docs.python-guide.org/en/latest/writing/structure/#structure-of-the-repository"">The Hitchhiker's Guide to Python</a>.</p>

<p>That guide suggests that I have a <code>my_module</code> folder, separate from a <code>tests</code> folder, both of which are in the same directory. If I want to access <code>my_module</code> from <code>tests</code>, I think I need to add <code>..</code> to the <code>sys.path</code></p>
","4414003","","","","","2019-07-09 23:58:48","PEP8 – import not at top of file with sys.path","<python><python-3.x><pep8>","6","7","18","","","CC BY-SA 3.0","0"
"34794634","1","","","2016-01-14 16:25:44","","14","32807","<p>Would it be possible to use a variable as a function name in python?
For example:</p>

<pre><code>list = [one, two, three]
for item in list:
    def item():
         some_stuff()
</code></pre>
","5787526","","113962","","2016-01-14 16:37:27","2019-01-17 11:01:08","How to use a variable as function name in Python","<python><python-3.x>","6","5","4","","","CC BY-SA 3.0","0"
"49705047","1","49705239","","2018-04-07 07:33:53","","11","32716","<p>I have a question about the function of yahoo finance using the pandas data reader. I'm using for months now a list with stock tickers and execute it in the following lines:</p>

<pre><code>import pandas_datareader as pdr
import datetime

stocks = [""stock1"",""stock2"",....]
start = datetime.datetime(2012,5,31)
end = datetime.datetime(2018,3,1)

f = pdr.DataReader(stocks, 'yahoo',start,end)
</code></pre>

<p>Since yesterday i get the error ""IndexError: list index out of range"", which appears only if I try to get multiple stocks.</p>

<p>Has anything changed in recent days which I have to consider or do you have a better solution for my problem?</p>
","8759280","","","","","2020-05-19 14:42:55","Downloading mutliple stocks at once from yahoo finance python","<python><python-3.x><yahoo-finance><pandas-datareader>","4","0","4","","","CC BY-SA 3.0","1"
"45432471","1","45432535","","2017-08-01 08:16:59","","6","32715","<p>I have a django (but i think it's nor revelant here) project where I try to add a script i did before. So I put it in a subdirectory of my project, and i have this structure (I know it's a little bit of a mess at the moment but it won't stay exactly like that)</p>

<p><a href=""https://i.stack.imgur.com/k1rFy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/k1rFy.png"" alt=""enter image description here""></a></p>

<p>From <strong>views.py</strong> i want to import <strong>main.py</strong> (Especially the function <strong>excelToXml</strong>) . After searches on internet i found that code that i copied in views.py . If I undestood it right it add to the variable $PATH the directory parent of <strong>first_page</strong> and though, every subdirectory </p>

<pre><code>CURRENT = os.path.dirname(os.path.abspath(__file__))
PARENT = os.path.dirname(CURRENT)
sys.path.append(PARENT)
from ExcelToXML.main import excelToXml
</code></pre>

<p>I also created a file <strong>__init.py_</strong>_ in the directory ExcelToXML, this file is left empty.</p>

<p>However even I did all that i still get this error when i run the django server</p>

<blockquote>
  <p>File ""c:\Users\CRA\AppData\Local\Programs\Python\Python36-32\Lib\site-packages\django\bin\DevisVersOpen\DevisVersOpen\urls.py"", line 18, in module</p>
  
  <blockquote>
    <p>from first_page import views</p>
  </blockquote>
  
  <p>File
  ""c:\Users\CRA\AppData\Local\Programs\Python\Python36-32\Lib\site-packages\django\bin\DevisVersOpen\first_page\views.py"",
  line 13, in module</p>
  
  <blockquote>
    <p>from ExcelToXML.main import excelToXml</p>
  </blockquote>
  
  <p>ModuleNotFoundError: No module named 'ExcelToXML'</p>
</blockquote>

<p>I didn't find any solution that I could understand on internet so I really don't know how to solve this</p>
","8237744","","","","","2020-10-21 11:06:19","Importing module not working","<python><django><python-3.x><module>","4","0","1","","","CC BY-SA 3.0","0"
"48371856","1","48371928","","2018-01-21 21:23:37","","16","32695","<p>If I have got something like this:</p>

<pre><code>D = {'a': 97, 'c': 0 , 'b':0,'e': 94, 'r': 97 , 'g':0}
</code></pre>

<p>If I want for example to count the number of occurrences for the ""0"" as a value without having to iterate the whole list, is that even possible and how?</p>
","7310034","","2867928","","2019-01-15 14:29:05","2019-01-15 14:29:05","count the number of occurrences of a certain value in a dictionary in python?","<python><python-3.x><dictionary><counting>","3","8","1","","","CC BY-SA 3.0","0"
"38640109","1","52388406","","2016-07-28 15:02:13","","72","32675","<p>I am using the logistic regression function from sklearn, and was wondering what each of the solver is actually doing behind the scenes to solve the optimization problem.</p>
<p>Can someone briefly describe what &quot;newton-cg&quot;, &quot;sag&quot;, &quot;lbfgs&quot; and &quot;liblinear&quot; are doing?</p>
","3436204","","1839439","","2020-09-11 15:35:29","2020-09-11 15:35:29","logistic regression python solvers' defintions","<python><python-3.x><scikit-learn><logistic-regression>","1","1","50","","","CC BY-SA 4.0","0"
"39434821","1","39439119","","2016-09-11 09:44:39","","9","32674","<p>I lost my credentials.. so I'm creating this new thread. The old question it here if it helps: <a href=""https://stackoverflow.com/questions/39424703/how-to-click-a-button-to-vote-with-python"">How to click a button to vote with python</a></p>

<p>I'd like to change this line:</p>

<pre><code>&lt;a data-original-title=""I&amp;nbsp;like&amp;nbsp;this&amp;nbsp;faucet"" href=""#"" class=""vote-link up"" data-faucet=""39274"" data-vote=""up"" data-toggle=""tooltip"" data-placement=""top"" title=""""&gt;&lt;span class=""glyphicon glyphicon-thumbs-up""&gt;&lt;/span&gt;&lt;/a&gt;
</code></pre>

<p>to this:</p>

<pre><code>&lt;a data-original-title=""I&amp;nbsp;like&amp;nbsp;this&amp;nbsp;faucet"" href=""#"" class=""vote-link up voted"" data-faucet=""39274"" data-vote=""up"" data-toggle=""tooltip"" data-placement=""top"" title=""""&gt;&lt;span class=""glyphicon glyphicon-thumbs-up""&gt;&lt;/span&gt;&lt;/a&gt;
</code></pre>

<p>So that the vote is set changing <code>vote-link up</code> to <code>vote-link up voted</code>.</p>

<p>But the problem is that in that site, there are severals items to vote, and the element ""data-faucet"" changes. If I use this script:</p>

<pre><code>from selenium import webdriver

driver = webdriver.Firefox()
driver.get(""linkurl"")
element = driver.find_element_by_css_selector("".vote-link.up"")
element_attribute_value = element.get_attribute(""data-faucet"")
if element_attribute_value == ""39274"":
    print (""Value: {0}"".format(element_attribute_value))
driver.quit()
</code></pre>

<p>But it obsiusly doesn't print anything, cause the first attribute value has another number. How can I select my line with the input of the number of <code>data-faucet</code> element, so I can replace it with <code>vote-link up voted</code>?</p>

<p>I only can do this selenium? Is there another way without using a real browser?</p>

<p>Anyway, this is the structure of the webpage:</p>

<pre><code>&lt;html&gt;
&lt;head&gt;&lt;/head&gt;
&lt;body role=""document""&gt;
&lt;div id=""static page"" class=""container-fluid""&gt;
&lt;div id=""page"" class=""row""&gt;&lt;/div&gt;
&lt;div id=""faucets-list""&gt;
&lt;tbody&gt;
&lt;tr class=""""&gt;&lt;/tr&gt;
&lt;tr class=""""&gt;&lt;/tr&gt;
&lt;tr class=""""&gt;&lt;/tr&gt;
&lt;tr class=""""&gt;&lt;/tr&gt;
# an infinite number of nodes, until there's mine
&lt;tr class=""""&gt;
&lt;td class=""vote-col""&gt;
&lt;div class=""vote-box""&gt;
&lt;div class=""vote-links""&gt;
&lt;a class=""vote-link up"" data-original-title=""I like this faucet"" href=""#"" data-faucet""39274"" data-vote""up"" data-toggle""tooltip"" data-placement=""top"" title=""""&gt;&lt;/a&gt;
</code></pre>

<p>The site is this: <a href=""https://faucetbox.com/en/list/BTC"" rel=""noreferrer"">https://faucetbox.com/en/list/BTC</a></p>
","6817268","","-1","","2017-05-23 12:32:08","2016-09-11 18:37:12","How to change element class attribute value using selenium","<python><python-3.x><selenium><selenium-webdriver><css-selectors>","1","3","5","","","CC BY-SA 3.0","0"
"38346619","1","38347774","","2016-07-13 08:38:40","","4","32661","<p>I need to parse various text sources and then print / store it somewhere.</p>

<p>Every time a non ASCII character is encountered, I can't correctly print it as it gets converted to bytes, and I have no idea how to view the correct characters.</p>

<p>(I'm quite new to Python, I come from PHP where I never had any utf-8 issues)</p>

<p>The following is a code example:</p>

<pre><code>#!/usr/bin/python
# -*- coding: utf-8 -*-

import codecs
import feedparser

url = ""http://feeds.bbci.co.uk/japanese/rss.xml""
feeds = feedparser.parse(url)
title = feeds['feed'].get('title').encode('utf-8')

print(title)

file = codecs.open(""test.txt"", ""w"", ""utf-8"")
file.write(str(title))
file.close()
</code></pre>

<p>I'd like to print and write in a file the RSS title (BBC Japanese - ホーム) but instead the result is this:</p>

<blockquote>
  <p>b'BBC Japanese - \xe3\x83\x9b\xe3\x83\xbc\xe3\x83\xa0'</p>
</blockquote>

<p>Both on screen and file. Is there a proper way to do this ?</p>
","196688","","460845","","2016-07-13 09:36:55","2019-12-21 13:41:14","How to handle utf-8 text with Python 3?","<python-3.x><utf-8><character-encoding>","1","0","","","","CC BY-SA 3.0","0"
"29139350","1","","","2015-03-19 07:44:46","","43","32638","<p>I am using a list <code>p = [[1,2,3],[4,5,6]]</code></p>

<p>If I do :</p>

<pre><code>&gt;&gt;&gt;d=zip(p)
&gt;&gt;&gt;list(d)
[([1, 2, 3],), ([4, 5, 6],)]
</code></pre>

<p>Though, what I actually want is obtained using this:</p>

<pre><code>&gt;&gt;&gt;d=zip(*p)
&gt;&gt;&gt;list(d)
[(1, 4), (2, 5), (3, 6)]
</code></pre>

<p>I have found out that adding a '*' before the list name gives my required output, but I can't make out the difference in their operation. Can you please explain the difference?</p>
","3735438","","2781698","","2015-03-19 07:49:19","2018-10-31 08:35:49","Difference between zip(list) and zip(*list)","<python><python-3.x>","6","0","11","2015-03-19 10:25:41","","CC BY-SA 3.0","0"
"36826839","1","36826908","","2016-04-24 17:55:59","","42","32524","<p>I've been struggling with imports in my package for the last hour.</p>

<p>I've got a directory structure like so:</p>

<pre><code>main_package
 |
 | __init__.py
 | folder_1
 |  | __init__.py
 |  | folder_2
 |  |  | __init__.py
 |  |  | script_a.py
 |  |  | script_b.py
 |
 | folder_3
 |  | __init__.py
 |  | script_c.py
</code></pre>

<p>I want to access code in <code>script_b.py</code> as well as code from <code>script_c.py</code> from <code>script_a.py</code>. How can I do this?</p>

<p>If I put a simple <code>import script_b</code> inside <code>script_a.py</code>, when I run</p>

<pre><code>from main_package.folder_1.folder_2 import script_b
</code></pre>

<p>I am met with an</p>

<pre><code>ImportError: no module named ""script_b""
</code></pre>

<p>For accessing <code>script_c.py</code>, I have no clue. I wasn't able to find any information about accessing files two levels up, but I know I can import files one level up with</p>

<pre><code>from .. import some_module
</code></pre>

<p>How can I access both these files from <code>script_a.py</code>?</p>
","4414003","","","","","2020-08-26 15:10:31","Python relative-import script two levels up","<python><python-3.x><python-import><directory-structure><relative-import>","1","1","7","","","CC BY-SA 3.0","0"
"39125532","1","54376484","","2016-08-24 14:04:03","","77","32515","<p>I'm on a <a href=""https://en.wikipedia.org/wiki/IPython#Project_Jupyter"" rel=""noreferrer"">Jupyter Notebook</a> server (v4.2.2) with Python 3.4.2 and
I want to use the global name <code>__file__</code>, because the notebook will be cloned from other users and in one section I have to run:</p>

<pre><code>def __init__(self, trainingSamplesFolder='samples', maskFolder='masks'):
    self.trainingSamplesFolder = self.__getAbsPath(trainingSamplesFolder)
    self.maskFolder = self.__getAbsPath(maskFolder)

def __getAbsPath(self, path):
    if os.path.isabs(path):
        return path
    else:
        return os.path.join(os.path.dirname(__file__), path)
</code></pre>

<p>The <code>__getAbsPath(self, path)</code> checks if a <code>path</code> param is a relative or absolute path and returns the absolute version of <code>path</code>. So I can use the returned <code>path</code> safely later.</p>

<p>But I get the error</p>

<blockquote>
  <p>NameError: name <code>'__file__'</code> is not defined</p>
</blockquote>

<p>I searched for this error online and found the ""solution"" that I should better use <code>sys.argv[0]</code>, but <code>print(sys.argv[0])</code> returns</p>

<blockquote>
  <p><code>/usr/local/lib/python3.4/dist-packages/ipykernel/__main__.py</code></p>
</blockquote>

<p>But the correct notebook location should be <code>/home/ubuntu/notebooks/</code>.</p>

<p>Thanks for the reference <em><a href=""https://stackoverflow.com/questions/12544056/how-to-i-get-the-current-ipython-notebook-name"">How do I get the current IPython Notebook name</a></em> from Martijn Pieters (comments) the last answer (not accepted) fits perfect for my needs:</p>

<p><code>print(os.getcwd())</code></p>

<blockquote>
  <p>/home/ubuntu/notebooks</p>
</blockquote>
","3524844","","7469434","","2020-01-08 14:41:11","2020-01-08 14:41:11","__file__ does not exist in Jupyter Notebook","<python-3.x><path><jupyter>","4","2","10","","","CC BY-SA 4.0","0"
"46768088","1","46768117","","2017-10-16 10:25:52","","36","32301","<p>The class:</p>

<pre><code>class Book(object):
    def __init__(self, title, author):
        self.title = title
        self.author = author

    def get_entry(self):
        return ""{0} by {1} on {}"".format(self.title, self.author, self.press)
</code></pre>

<p>Create an instance of my book from it:</p>

<pre><code>In [72]: mybook = Book('HTML','Lee')
In [75]: mybook.title
Out[75]: 'HTML'
In [76]: mybook.author
Out[76]: 'Lee'
</code></pre>

<p>Please notice that I didn't initialize attribute 'self.press',while use it in the get_entry method.Go ahead to type in data.</p>

<pre><code>mybook.press = 'Murach'
mybook.price = 'download'
</code></pre>

<p>Till now, I can specify all the data input with <code>vars</code></p>

<pre><code>In [77]: vars(mybook)
Out[77]: {'author': 'Lee', 'title': 'HTML',...}
</code></pre>

<p>I hardtype lot of data about mybook in the console.When try to call get_entry method, errors report.</p>

<pre><code>mybook.get_entry()
ValueError: cannot switch from manual field specification to automatic field numbering.
</code></pre>

<p>All this going in interactive mode on console.I cherish the data inputed, further to pickle <code>mybook</code> object in file. However, it is flawed. How can rescue it in the interactive mode.
or I have to restart all over again.</p>
","7301792","","6451573","","2019-08-06 21:08:28","2020-10-11 14:35:59","ValueError: cannot switch from manual field specification to automatic field numbering","<python><python-3.x><string-formatting>","3","0","6","","","CC BY-SA 3.0","0"
"30550774","1","30586290","","2015-05-30 20:00:11","","8","32300","<p>I'm trying to write a python class to display data in a tabular format. I'm sure there are classes out there already to do the same thing, however, I'm using this exercise as a way to teach myself Python and tkinter. For the most part, I have the class working the way I want it to, however I cannot get the header and data cells to fill their entire cell, while being aligned left. Here is what my class currently generates for a table:</p>

<p><img src=""https://i.imgur.com/DwaFjM2.png"" alt=""How the table currently looks""></p>

<p>I went ahead and changed the sticky on the cells to be (W,E) rather than just W, in order to show how I want the table to look, except each cell left justified. Below is what I'm shooting for:</p>

<p><img src=""https://i.imgur.com/ipmJIWI.png"" alt=""How I want the table to look""></p>

<p>Based on the research I've done, it would seem I need to be using the <code>weight</code> attribute of <code>grid_columnconfigure</code> and <code>grid_rowconfigure</code>, however every way I have tried using them I cannot, get it to work.</p>

<p>Here is the code for my class (I am using Python 3.4):</p>

<pre><code>from tkinter import *
from tkinter import ttk
from tkinter import font

class TableData:

    def __init__(self,parent,attributes,columns,data):
        self.parent = parent
        self.tableName = StringVar()
        self.tableName.set(attributes['tableName'])
        self.columns = columns
        self.columnCount = 0
        self.borderColor = attributes['borderColor']
        self.titleBG = attributes['titleBG']
        self.titleFG = attributes['titleFG']
        self.titleFontSize = attributes['titleFontSize']
        self.headerBG = attributes['headerBG']
        self.headerFG = attributes['headerFG']
        self.headerFontSize = attributes['headerFontSize']
        self.dataRowColor1 = attributes['dataRowColor1']
        self.dataRowColor2 = attributes['dataRowColor2']
        self.dataRowFontSize = attributes['dataRowFontSize']
        self.dataRowFG = attributes['dataRowFG']
        self.data = data
        self.tableDataFrame = ttk.Frame(self.parent)
        self.tableDataFrame.grid(row=0,column=0)
        self.initUI()

    def countColumns(self):
        cnt = 0
        for i in self.columns:
            cnt += 1

        self.columnCount = cnt

    def buildTableTitle(self):
        tableTitleFont = font.Font(size=self.titleFontSize)
        Label(self.tableDataFrame,textvariable=self.tableName,bg=self.titleBG,fg=self.titleFG,font=tableTitleFont, highlightbackground=self.borderColor,highlightthickness=2).grid(row=0,columnspan=self.columnCount,sticky=(W,E), ipady=3)

    def buildHeaderRow(self):
        colCount = 0
        tableHeaderFont = font.Font(size=self.headerFontSize)
        for col in self.columns:
            Label(self.tableDataFrame,text=col,font=tableHeaderFont,bg=self.headerBG,fg=self.headerFG,highlightbackground=self.borderColor,highlightthickness=1).grid(row=1,column=colCount,sticky=W, ipady=2, ipadx=5)
            colCount += 1

    def buildDataRow(self):
        tableDataFont = font.Font(size=self.dataRowFontSize)
        rowCount = 2
        for row in self.data:
            if rowCount % 2 == 0:
                rowColor = self.dataRowColor2
            else:
                 rowColor = self.dataRowColor1
            colCount = 0
            for col in row:
                Label(self.tableDataFrame,text=col,bg=rowColor,fg=self.dataRowFG,font=tableDataFont,highlightbackground=self.borderColor,highlightthickness=1).grid(row=rowCount,column=colCount,sticky=W,ipady=1, ipadx=5)
                colCount += 1
            rowCount += 1

    def initUI(self):
        self.countColumns()
        self.buildTableTitle()
        self.buildHeaderRow()
        self.buildDataRow()
</code></pre>

<p>Here is a test file referencing the TableData class:</p>

<pre><code>from tkinter import *
from tkinter import ttk
from tableData import TableData
import sqlite3

root = Tk()
root.geometry('1000x400')

mainframe = ttk.Frame(root).grid(row=0,column=0)

attributes = {}
attributes['tableName'] = 'Title'
attributes['borderColor'] = 'black'
attributes['titleBG'] = '#1975D1'
attributes['titleFG'] = 'white'
attributes['titleFontSize'] = 16
attributes['headerBG'] = 'white'
attributes['headerFG'] = 'black'
attributes['headerFontSize'] = 12
attributes['dataRowColor1'] = 'white'
attributes['dataRowColor2'] = 'grey'
attributes['dataRowFontSize'] = 10
attributes['dataRowFG'] = 'black'

columns = ['Col 1', 'Column 2', 'Column 3','Column    4']

results = [('1','Key','Desc','Attribute'),('2','Key Column','Description Column','AttributeColumn')]

table = TableData(mainframe,attributes,columns,results)

root.mainloop()
</code></pre>

<p>Thanks in advance for any insight. Please, let me know if there is any other info that would be helpful.</p>
","821141","","821141","","2015-05-30 23:24:36","2019-11-03 20:54:11","How to left justify Python tkinter grid columns while filling entire cell","<python><python-3.x><tkinter><grid>","4","5","1","","","CC BY-SA 3.0","0"
"32171005","1","32180369","","2015-08-23 20:09:44","","1","32282","<p>I am trying to work out how to get a value from a tkinter entry box, and then store it as a integer. This is what I have:</p>

<pre><code>   AnswerVar = IntVar()
AnswerBox = Entry(topFrame)
AdditionQuestionLeftSide = random.randint(0, 10)
AdditionQuestionRightSide = random.randint(0, 10)
AdditionQuestionRightSide = Label(topFrame, text= AdditionQuestionRightSide).grid(row=0,column=0)
AdditionSign = Label(topFrame, text=""+"").grid(row=0,column=1)
AdditionQuestionLeftSide= Label(topFrame, text= AdditionQuestionLeftSide).grid(row=0,column=2)
EqualsSign = Label(topFrame, text=""="").grid(row=0,column=3)
AnswerBox.grid(row=0,column=4)
answerVar = AnswerBox.get()

root.mainloop() 
(


    )
</code></pre>

<p>I want to take then input from AnswerBox, and store it in the integer variable ""answer"". How can I do this?</p>

<p>Thanks</p>
","","user2868524","","user2868524","2015-08-24 11:02:42","2019-11-07 12:43:12","How to get an integer from a tkinter entry box?","<python><python-3.x><tkinter>","2","0","","","","CC BY-SA 3.0","0"
"36230492","1","36230538","","2016-03-26 01:12:46","","46","32169","<p>I have a module in a <code>fibo.py</code> file which has the following functions -  </p>

<pre><code>#fibonacci numbers module

def fib(n):    # write Fibonacci series up to n
    a, b = 0, 1
    while b &lt; n:
        print(b, end=' ')
        a, b = b, a+b
    print()

def fib2(n): # return Fibonacci series up to n
    result = []
    a, b = 0, 1
    while b &lt; n:
        result.append(b)
        a, b = b, a+b
    return result
</code></pre>

<p>Now when I run the module from the cli python3 as - </p>

<pre><code>&gt; python3 -m fibo.py
</code></pre>

<p>I get the error </p>

<pre><code>Error while finding spec for 'fibo.py' (&lt;class 'AttributeError'&gt;:
'module' object has no attribute '__path__')
</code></pre>

<p>The <code>__path__</code> variable has has the current dir . I am not sure how to fix this.</p>
","273812","","4720935","","2018-12-18 10:13:15","2019-05-02 04:32:37","Error while finding spec for 'fibo.py' (<class 'AttributeError'>: 'module' object has no attribute '__path__')","<python-3.x><python-module>","3","0","15","","","CC BY-SA 4.0","0"
"29587179","1","29587282","","2015-04-12 07:42:24","","15","32159","<p>I have a pickle file, with </p>

<pre><code>&gt;&gt;&gt; with open(""wikilinks.pickle"", ""rb"") as f:
...     titles, links = pickle.load(f)
... 
&gt;&gt;&gt; len(titles)
13421
</code></pre>

<p>I can load it in python3.
However, when I try to load it in python2, I get this message:
Traceback (most recent call last):</p>

<pre><code>  File ""&lt;stdin&gt;"", line 2, in &lt;module&gt;
  File ""/usr/lib/python2.7/pickle.py"", line 1378, in load
    return Unpickler(file).load()
  File ""/usr/lib/python2.7/pickle.py"", line 858, in load
    dispatch[key](self)
  File ""/usr/lib/python2.7/pickle.py"", line 886, in load_proto
    raise ValueError, ""unsupported pickle protocol: %d"" % proto
ValueError: unsupported pickle protocol: 3
</code></pre>

<p>So how to load it in python2?</p>
","3921356","","","","","2017-04-08 06:05:58","Load pickle file(comes from python3) in python2","<python-2.7><python-3.x><pickle>","2","0","","","","CC BY-SA 3.0","0"
"32808893","1","32808937","","2015-09-27 14:19:35","","53","32157","<pre><code>#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import time

async def foo():
  await time.sleep(1)

foo()
</code></pre>

<p>I couldn't make this dead simple example to run:</p>

<pre><code>RuntimeWarning: coroutine 'foo' was never awaited foo()
</code></pre>
","1263403","","100297","","2015-09-28 00:22:02","2019-01-29 11:04:20","How to use async/await in Python 3.5?","<python><python-3.x><async-await><coroutine><python-3.5>","2","1","10","","","CC BY-SA 3.0","0"
"54302103","1","54302420","","2019-01-22 06:00:13","","19","32155","<p>I am trying to create a webcrawler using Selenium, but I get this error when I try to create the webdriver object.</p>

<pre><code>selenium.common.exceptions.SessionNotCreatedException: Message: session not created: Chrome version must be between 70 and 73
(Driver info: chromedriver=2.45.615291 (ec3682e3c9061c10f26ea9e5cdcf3c53f3f74387),platform=Windows NT 6.1.7601 SP1 x86_64)
</code></pre>

<p>I downloaded the latest version of <a href=""http://chromedriver.chromium.org/downloads"" rel=""noreferrer"">chromedriver</a> (2.45) which requires Chrome 70-73. My current Chrome version is 68.0.3440.106 (Official Build) (64-bit), which is the latest. I tried downloading an ""older"" <a href=""https://www.slimjet.com/chrome/google-chrome-old-version.php"" rel=""noreferrer"">chrome version</a> (71) and when I tried installing it, the installer indicated that I had a newer version already installed.</p>

<p>There doesn't seem to be any previous Chromedriver releases available for download, even though the website says there is. I couldn't find them.</p>

<p>I don't quite understand how version 71 is older than 68?</p>

<p>Is there a Chrome version newer than 68 actually available, or an older version of chromedriver i can use with Chrome 68?</p>

<p>Does anyone have any other suggestions?</p>

<p>This is the code that i'm trying to execute:</p>

<pre><code>import time
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
user = 'XXXXXXX'
pwd = 'XXXXXXX'
chromedriver = ""...\...\...\chromedriver.exe""
driver = webdriver.Chrome(executable_path=chromedriver) # Error occurs at this line
driver.get(""http://www.facebook.com"")
assert ""Facebook"" in driver.title
time.sleep(5) # So i can see something!
elem = driver.find_element_by_id(""email"")
elem.send_keys(user)
time.sleep(5) # So i can see something!
elem = driver.find_element_by_id(""pass"")
elem.send_keys(pwd)
time.sleep(5) # So i can see something!
elem.send_keys(Keys.RETURN)
driver.close()
</code></pre>
","2168171","","7429447","","2019-01-22 09:36:56","2020-10-28 08:42:09","selenium.common.exceptions.SessionNotCreatedException: Message: session not created: Chrome version must be between 70 and 73 with ChromeDriver","<python-3.x><google-chrome><selenium-webdriver><webdriver><selenium-chromedriver>","11","0","5","","","CC BY-SA 4.0","0"
"29218750","1","29218792","","2015-03-23 19:28:57","","20","32099","<p>I wonder if there is <em>simple</em> way to remove <em>one</em> or <em>more</em> dictionary element(s) from a python dictionary <em>by value</em>.</p>

<p>We have a dictionary called <code>myDict</code>:</p>

<pre><code>myDict = {1:""egg"", ""Answer"":42, 8:14, ""foo"":42}
</code></pre>

<p>and want to <em>remove</em> all items which <em>values</em> are equal to <code>42</code>.</p>

<p>Implementation suggestion:</p>

<ol>
<li><p>Get a list of all keys of a certain value in <code>myDict</code><br />(See for instance <a href=""https://stackoverflow.com/questions/8023306/get-key-by-value-in-dictionary"">get key by value in dictionary</a>.)</p></li>
<li><p>Delete this dict element or elements (based on the found keys) from <code>myDict</code><br />(For more information, see <a href=""https://stackoverflow.com/questions/5844672/delete-an-element-from-a-dictionary"">Delete an element from a dictionary</a>.)</p></li>
</ol>

<p>So, what do you think now is the <strong>most elegant</strong> and <strong>most “pythonic”</strong> way to implement this problem in Python?</p>
","4594443","","-1","","2017-05-23 12:32:47","2019-08-09 06:34:02","What is the best way to remove a dictionary item by value in python?","<python><python-2.7><python-3.x><dictionary>","5","2","4","2019-08-09 11:11:48","","CC BY-SA 3.0","0"
"34275140","1","34275363","","2015-12-14 19:32:54","","28","32029","<p>Given the following heatmap, how would I remove the axis titles ('month' and 'year')?</p>

<pre><code>import seaborn as sns

# Load the example flights dataset and conver to long-form
flights_long = sns.load_dataset(""flights"")
flights = flights_long.pivot(""month"", ""year"", ""passengers"")

# Draw a heatmap with the numeric values in each cell
sns.heatmap(flights, annot=True, fmt=""d"", linewidths=.5)
</code></pre>

<p><a href=""https://i.stack.imgur.com/ZqjKn.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ZqjKn.png"" alt=""current graph""></a></p>
","5569314","","1840471","","2018-01-08 20:34:58","2018-01-08 20:35:19","Hide Axis Titles in Seaborn","<python-3.x><seaborn>","1","0","2","","","CC BY-SA 3.0","0"
"38072278","1","38072494","","2016-06-28 09:27:21","","9","31893","<p>I'm trying to understand how to use <code>super</code> in python</p>

<pre><code>class people:   
 name = ''  
 age = 0  
  __weight = 0  

 def __init__(self,n,a,w):  
    self.name = n  
    self.age = a  
    self.__weight = w  
def speak(self):  
    print(""%s is speaking: I am %d years old"" %(self.name,self.age))  


class student(people):  
 grade = ''  
 def __init__(self,n,a,w,g):  
    #people.__init__(self,n,a,w)  
    super(student,self).__init__(self,n,a,w)
    self.grade = g  

 def speak(self):  
    print(""%s is speaking: I am %d years old,and I am in grade %d""%(self.name,self.age,self.grade))  


s = student('ken',20,60,3)  
s.speak()
</code></pre>

<p>The above code gets following error:</p>

<pre><code>---------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-147-9da355910141&gt; in &lt;module&gt;()
     10 
     11 
---&gt; 12 s = student('ken',20,60,3)
     13 s.speak()

&lt;ipython-input-147-9da355910141&gt; in __init__(self, n, a, w, g)
      3     def __init__(self,n,a,w,g):
      4         #people.__init__(self,n,a,w)
----&gt; 5         super(student).__init__(self,n,a,w)
      6         self.grade = g
      7 

TypeError: must be type, not classobj
</code></pre>

<p>I'm confused about why I cannot use <code>super(student,self).__init__(self,n,a,w)</code> in this case, and why I have to use <code>people.__init__(self,n,a,w)</code></p>

<p>Any help?</p>
","1015633","","3890632","","2016-06-28 09:34:58","2016-06-28 09:57:21","super in python 2.7","<python><python-2.7><python-3.x><subprocess>","1","2","1","2016-06-28 21:58:47","","CC BY-SA 3.0","0"
"33283603","1","33291200","","2015-10-22 14:33:51","","11","31776","<p>Using Python 3.4.3 on Windows.</p>

<p>My script runs a little java program in console, and should get the ouput:</p>

<pre><code>import subprocess
p1 = subprocess.Popen([ ... ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)
out, err = p1.communicate(str.encode(""utf-8""))
</code></pre>

<p>This leads to a normal </p>

<blockquote>
  <p>'UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 135: character maps to &lt; undefined>'.</p>
</blockquote>

<p>Now I want to ignore errors:</p>

<pre><code>out, err = p1.communicate(str.encode(encoding=""utf-8"", errors=""ignore""))
</code></pre>

<p>This leads to a more interesting error I found no help for using google:</p>

<blockquote>
  <p>TypeError: descriptor 'encode' of 'str' object needs an argument</p>
</blockquote>

<p>So it seems that python does not even know anymore what the arguments for str.encode(...) are. The same also applies when you leave out the errors part.</p>
","2441026","","","","","2015-10-22 21:42:55","Python popen() - communicate( str.encode(encoding=""utf-8"", errors=""ignore"") ) crashes","<python><python-3.x><encoding><subprocess><popen>","2","1","2","","","CC BY-SA 3.0","0"
"48937457","1","","","2018-02-22 21:26:00","","9","31751","<p>My question looks like this:                                </p>

<pre><code>  10-7//2*3+1 
</code></pre>

<p>I am supposed to solve the equation.</p>

<p>And my answer seems to come out as 8 when using PEMDAS</p>

<pre><code>First its's 2*3 = 6; 10-7//6+1
second = 7//6= 1; 10-1+1
Third = 10-8 = 8;
</code></pre>

<p>But when putting it into python, I get a 2. Why?</p>

<p>It seems the programs order is as such:</p>

<pre><code>first: 7//2=3; 10-3*3+1
second: 3*3=9; 10-9+1
third:10-9+1= 2; 2
</code></pre>

<p>I don't get it</p>
","9069813","","100297","","2018-02-22 22:13:11","2020-09-17 20:06:28","How do order of operations go on Python?","<python><python-3.x><math>","5","2","3","","","CC BY-SA 3.0","0"
"38298652","1","38298682","","2016-07-11 03:16:49","","18","31738","<p>I am running MacOS X with python 3. The folder and files have 755 but I have also tested it in 777 with no luck. My question is if I have the right permissions why does it not let me run without sudo. Or are my settings incorrect?</p>

<pre><code>cris-mbp:ProjectFolder cris$ python3 zbo.py 
Traceback (most recent call last):
  File ""zbo.py"", line 9, in &lt;module&gt;
    app.run(host=""127.0.0.1"",port=81,debug=True)
  File ""/usr/local/lib/python3.5/site-packages/flask/app.py"", line 843, in run
    run_simple(host, port, self, **options)
  File ""/usr/local/lib/python3.5/site-packages/werkzeug/serving.py"", line 677, in run_simple
    s.bind((hostname, port))
PermissionError: [Errno 13] Permission denied
cris-mbp:ProjectFolder cris$ sudo python3 zbo.py 
 * Running on http://127.0.0.1:81/ (Press CTRL+C to quit)
 * Restarting with stat
 * Debugger is active!
 * Debugger pin code: 106-133-233
</code></pre>
","1026880","","","","","2018-06-19 12:52:28","PermissionError: [Errno 13] Permission denied Flask.run()","<python><python-3.x><flask>","3","0","2","","","CC BY-SA 3.0","0"
"37489280","1","37489342","","2016-05-27 17:30:01","","21","31644","<p>How to list file names which are only from a particular directory? I don't want file names from sub-directory or anything. </p>
","3597719","","","","","2018-01-26 00:49:53","Python3 list files from particular directory","<python><linux><file><python-3.x><filenames>","1","1","1","2016-05-27 19:39:41","","CC BY-SA 3.0","0"
"36994839","1","36995008","","2016-05-03 02:56:22","","30","31629","<p>The <code>pickle</code> reference <a href=""https://docs.python.org/3/library/pickle.html#what-can-be-pickled-and-unpickled"" rel=""noreferrer"">states that</a> the set of objects which can be pickled is rather limited. Indeed, I have a function which returns a dinamically-generated class, and I found I can't pickle instances of that class:</p>

<pre><code>&gt;&gt;&gt; import pickle
&gt;&gt;&gt; def f():
...     class A: pass
...     return A
... 
&gt;&gt;&gt; LocalA = f()
&gt;&gt;&gt; la = LocalA()
&gt;&gt;&gt; with open('testing.pickle', 'wb') as f:
...     pickle.dump(la, f, pickle.HIGHEST_PROTOCOL)
... 
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 2, in &lt;module&gt;
AttributeError: Can't pickle local object 'f.&lt;locals&gt;.A'
</code></pre>

<p>Such objects are too complicated for <code>pickle</code>. Ok. Now, what's magic is that, if I try to pickle a similar object, but of a derived class, it works!</p>

<pre><code>&gt;&gt;&gt; class DerivedA(LocalA): pass
... 
&gt;&gt;&gt; da = DerivedA()
&gt;&gt;&gt; with open('testing.pickle', 'wb') as f:
...     pickle.dump(da, f, pickle.HIGHEST_PROTOCOL)
...
&gt;&gt;&gt;
</code></pre>

<p>What's happening here? If this is so easy, why doesn't <code>pickle</code> use this workaround to implement a <code>dump</code> method that allows ""local objects"" to be pickled?</p>
","3067276","","2379433","","2016-05-03 17:59:00","2018-10-19 12:29:40","I can ""pickle local objects"" if I use a derived class?","<python><python-3.x><nested><pickle>","4","0","9","","","CC BY-SA 3.0","0"
"35302215","1","35302242","","2016-02-09 21:07:46","","6","31524","<p>How do I use the Python <code>in</code> operator to check my list/tuple <code>sltn</code> contains each of the integers 0, 1, and 2?</p>

<p>I tried the following, why are they both wrong:</p>

<pre><code># Approach 1
if (""0"",""1"",""2"") in sltn:
     kwd1 = True

# Approach 2
if any(item in sltn for item in (""0"", ""1"", ""2"")):
     kwd1 = True
</code></pre>

<hr>

<p>Update: why did I have to convert <code>(""0"", ""1"", ""2"")</code> into either the tuple <code>(1, 2, 3)</code>? or the list <code>[1, 2, 3]</code>?</p>
","5905348","","202229","","2019-07-01 18:55:47","2019-07-01 18:55:47","How to use Python 'in' operator to check my list/tuple contains each of the integers 0, 1, 2?","<python><python-3.x><contains>","4","8","3","","","CC BY-SA 4.0","0"
"49353905","1","","","2018-03-18 23:30:00","","17","31523","<p>I am running a telegram bot in python and i am using python3.6 on raspbian ( pi3 )</p>

<p>Below is my imports:</p>

<pre><code>from __future__ import (absolute_import, division,
                    print_function, unicode_literals)
from builtins import (
    bytes, dict, int, list, object, range, str,
    ascii, chr, hex, input, next, oct, open,
    pow, round, super,
    filter, map, zip)
from uuid import uuid4

import re
import telegram

from telegram.utils.helpers import escape_markdown

from telegram import InlineQueryResultArticle, ParseMode, \
    InputTextMessageContent
from telegram.ext import Updater, InlineQueryHandler, CommandHandler
import logging
import random
import telepot
import unicodedata
import json
import requests
import bs4
from bs4 import BeautifulSoup
</code></pre>

<p>When i try to run my bot with sudo python3 bot.py i get </p>

<pre><code>ImportError: No module named 'future'
</code></pre>

<p>I have searched and found many answers on this but none have worked for me such as <code>pip install future</code> and <code>pip3 install future</code> The module does show in my lib for python 3.6 <a href=""https://i.stack.imgur.com/zvTXE.png"" rel=""noreferrer"">future in lib</a></p>

<p>Any idea why it still says <code>No module named future</code>? ? </p>
","9382114","","","","","2019-08-07 11:41:11","No module named future","<python-3.x><raspbian>","4","3","","","","CC BY-SA 3.0","0"
"33827179","1","33827268","","2015-11-20 12:50:37","","13","31506","<p>i'm always getting this error when running something like this:</p>

<pre><code>from decimal import *
getcontext().prec =30

b=(""2/3"")

Decimal(b)
</code></pre>

<p>Error:</p>

<pre><code>Traceback (most recent call last):
  File ""Test.py"", line 6, in &lt;module&gt;
    Decimal(b)
decimal.InvalidOperation: [&lt;class 'decimal.ConversionSyntax'&gt;]
</code></pre>

<p>Also, why do i get this result from the console?</p>

<pre><code>&gt;&gt;&gt; Decimal(2/3)
Decimal('0.66666666666666662965923251249478198587894439697265625')
</code></pre>

<p>Thanks</p>
","5238305","","953482","","2015-11-20 12:57:30","2019-07-16 13:40:03","Python decimal.InvalidOperation error","<python><python-3.x><math>","2","0","0","","","CC BY-SA 3.0","0"
"30516414","1","30516779","","2015-05-28 20:23:42","","17","31497","<p>I am using Windows 7 and Python 3.4.3. I would like to run this simple helloworld.py file in my browser:</p>

<pre><code>print('Content-Type: text/html')
print( '&lt;html&gt;')
print( '&lt;head&gt;&lt;/head&gt;')
print( '&lt;body&gt;')
print( '&lt;h2&gt;Hello World&lt;/h2&gt;')
print( '&lt;/body&gt;&lt;/html&gt;')
</code></pre>

<p>What I do is:</p>

<p>1) Go to command line <code>C:\Python</code> (where python is installed)</p>

<p>2) run: <code>python -m http.server</code></p>

<p>3) Got to Firefox and type <code>http://localhost:8000/hello.py</code></p>

<p>However, instead of ""Hello World"", the browser just prints the content of the hello.py file.</p>

<p>How can I fix it?</p>
","1014142","","984421","","2017-11-30 22:09:01","2020-03-30 00:27:18","How to run CGI ""hello world"" with python http.server","<python><http><python-3.x><cgi>","3","13","6","","","CC BY-SA 3.0","0"
"36859564","1","36868320","","2016-04-26 08:37:01","","23","31497","<p>I want to debug a project written in python3 in Visual Studio Code, but I can't seem to find any way of specifying interpreter or python version in the launch.json file.</p>

<p>It works great for Python 2, so the question is, how do I make it work for Python 3?</p>
","92326","","","","","2020-09-08 23:49:10","How can I debug Python3 code in Visual Studio Code","<python><python-3.x><debugging><visual-studio-code>","6","0","5","","","CC BY-SA 3.0","0"
"28019402","1","28019615","","2015-01-19 06:58:10","","7","31486","<p>I'm new to Tkinter, and I tried creating an app with the grid layout manager. However, I can't seem to find a way to utilize it the way I want to. What I need to do is simulate a grid full of 'cells' so that I can place, for example, a label in cell (3,8) or a button in cell (5,1). Here is an example of what I've tried:</p>

<pre><code>import tkinter as tk
root = tk.Tk()

label = tk.Label(root, text = 'Label')
label.grid(column = 3, row = 8)

button = tk.Button(root, text = 'Button')
button.grid(column = 5, row = 1)
</code></pre>

<p>Tkinter keeps the relative position of each widget (i.e. <code>button</code> is above and to the right of <code>label</code>), but it ignores any space in between. I've searched a bit for this problem and found out about <code>weight</code>, but that doesn't seem to be what I'm looking for; I'd like something independent of the <code>label</code> and <code>button</code>'s layouts. Thanks for the help!</p>
","3546190","","","","","2015-01-19 11:45:24","Tkinter.grid spacing options?","<python><python-3.x><tkinter><grid-layout>","2","0","3","","","CC BY-SA 3.0","0"
"50508262","1","50508618","","2018-05-24 11:29:57","","2","31470","<p>I am using python to automate a piezoelectric droplet generator. For each value of a pulse length, a suitable value of voltage will be there to produce a signal to give away a droplet. This value of voltage keeps changing in every run(for example, + or -10). So I have a database of different value of voltages for every pulse length. </p>

<p>I would like to know some things about using lookup tables in python. For my task, I want to pick a random pulse length from 15 to 70, and associate this value with a particular range of voltages from the database (for example: for a value 17, I would like the program to access the lookup table and return a range of voltages 35-50). Is it possible to take the entire range and not just a single value. Since, I am new to coding and  python, I am not really sure. Any help is welcome. Thank you.</p>
","7428743","","7428743","","2018-05-24 11:38:35","2020-02-07 22:16:43","Using Look Up Tables in Python","<python><python-3.x><lookup-tables>","3","7","3","","","CC BY-SA 4.0","0"
"58258003","1","","","2019-10-06 14:03:05","","23","31460","<p>I am following <a href=""https://pythonprogramming.net/creating-tfrecord-files-tensorflow-object-detection-api-tutorial/"" rel=""noreferrer"">this tutorial</a> and doing a project on custom object-detection using tensorflow.</p>

<p>So when I tried to create TF record for the train images using the following command</p>

<p><code>python3 generate_tfrecord.py --csv_input=data/train_labels.csv --output_path=data/train.record</code></p>

<p>I get the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""generate_tfrecord.py"", line 23, in &lt;module&gt;
    flags = tf.app.flags
AttributeError: module 'tensorflow' has no attribute 'app'
</code></pre>

<p>How can I resolve this error?</p>
","11364786","","6084317","","2019-11-21 03:50:21","2020-07-29 18:00:48","AttributeError: module 'tensorflow' has no attribute 'app'","<python-3.x><tensorflow><object-detection>","2","0","","","","CC BY-SA 4.0","0"
"28745153","1","28745374","","2015-02-26 14:35:00","","16","31371","<p>I have installed both Python 3.5 and Beautifulsoup4. When I try to import bs4, I get the error below. Is there any fix for that? Or should I just install Python 3.4 instead? 
Please be very explicit - I am new to programming. Many thanks!</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Python 3.5\lib\sit-packages\bs4\__init__.py"", line 30, in    &lt;module&gt;
   from .builder import builder_registry, ParserRejectionMarkup
  File ""C:\Python 3.5\lib\sit-packages\bs4\__init__.py"", line 308, in &lt;module&gt;
   from . import _htmlparser
  File ""C:\Python 3.5\lib\sit-packages\bs4\_htmlparser.py"", line 7, in &lt;module&gt;
   from html.parser import ( 
ImportError: cannot import name 'HTMLParseError'
</code></pre>
","4610654","","771848","","2016-12-19 20:50:04","2019-09-16 12:16:15","Importing bs4 in Python 3.5","<python><python-3.x><beautifulsoup><html-parsing><python-3.5>","3","0","1","","","CC BY-SA 3.0","0"
"34751794","1","34752072","","2016-01-12 19:02:43","","22","31341","<p>I'm not sure how to display the name of my instance in AWS EC2 using <code>boto3</code></p>

<p>This is some of the code I have:</p>

<pre><code>import boto3

ec2 = boto3.resource('ec2', region_name='us-west-2')
vpc = ec2.Vpc(""vpc-21c15555"")
for i in vpc.instances.all():
    print(i)
</code></pre>

<p>What I get in return is </p>

<pre><code>...
...
...
ec2.Instance(id='i-d77ed20c')
</code></pre>

<p><a href=""https://i.stack.imgur.com/uq24o.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/uq24o.png"" alt=""enter image description here""></a></p>

<p>I can change <code>i</code> to be <code>i.id</code> or <code>i.instance_type</code> but when I try <code>name</code> I get:</p>

<p><code>AttributeError: 'ec2.Instance' object has no attribute 'name'</code></p>

<p>What is the correct way to get the instance name?</p>
","1815710","","5451492","","2016-01-12 19:52:20","2018-10-03 07:29:49","Displaying EC2 Instance name using Boto 3","<python><python-3.x><amazon-web-services><amazon-ec2><boto3>","2","0","4","","","CC BY-SA 3.0","0"
"38175170","1","38175276","","2016-07-03 23:17:14","","9","31329","<p>My code looks as follows:</p>

<pre><code>md = input(""MD5 Hash: "")
if len(md) != 32:
    print(""Don't MD5 Hash."")
else:
    liste = input(""Wordlist: "")
    ac = open(liste).readlines()
    for new in ac:
        new = new.split()
        hs = hashlib.md5(new).hexdigest()
        if hs == md:
            print(""MD5 HASH CRACKED : "",new)
        else:
            print(""Sorry :( Don't Cracked."")
</code></pre>

<p>But, I get this error when I run it:</p>

<pre><code>    hs = hashlib.md5(new).hexdigest()
TypeError: object supporting the buffer API required
</code></pre>

<p>How do I solve this? ""b"" bytes?</p>
","6545147","","4952130","","2016-07-03 23:47:22","2016-07-04 12:31:53","Python MD5 Cracker ""TypeError: object supporting the buffer API required""","<python><python-3.x><md5>","1","0","","","","CC BY-SA 3.0","0"
"34860982","1","","","2016-01-18 17:49:12","","25","31300","<p>I have a problem with the code and can not figure out how to move forward.</p>

<pre><code>tweet = ""I am tired! I like fruit...and milk""
clean_words = tweet.translate(None, "",.;@#?!&amp;$"")
words = clean_words.split()

print tweet
print words
</code></pre>

<p>Output:</p>

<pre><code>['I', 'am', 'tired', 'I', 'like', 'fruitand', 'milk']
</code></pre>

<p>What I would like is to replace the punctuation with white space but do not know what function or cycle use. Can anyone help me please?</p>
","5805744","","1903116","","2016-01-18 18:01:50","2019-09-25 10:21:18","replace the punctuation with whitespace","<python><string><python-3.x>","4","0","3","","","CC BY-SA 3.0","0"
"45836356","1","45836997","","2017-08-23 09:51:07","","8","31267","<p>I'm trying to print the key and value using the below code : </p>

<pre><code>data = {""Key1"" : ""Value1"", ""Key2"" : ""Value2""}
print(data.keys()[1])   
print(data.values()[1])
</code></pre>

<p>I get the error </p>

<pre><code>'dict_keys' object does not support indexing
</code></pre>

<p>What is wrong with the mentioned code?</p>

<p>Accepted Output :</p>

<pre><code>Key2
Value2
</code></pre>
","","user7800892","","user7800892","2017-08-23 09:52:38","2020-03-14 06:32:41","Return the first key in Dictionary - Python 3","<python><python-3.x>","7","5","3","","","CC BY-SA 3.0","0"
"46854330","1","49328226","","2017-10-20 17:27:49","","6","31226","<p>I have been working on this error for a long time now. I have Python 3.6 and Python 2.7. I have tried to install opencv 2 and 3 in Python 2.7 and Python 3.6 respectively. I know the python interpreter I am using and I can interchange between them when I want. </p>

<p>When I run Python interpreter and write <code>import cv2</code> it does import it. When I run the code from command prompt it says <em>ModuleNotFoundError: No module named 'cv2'</em>.
The module is installed. The cv2.pyd file is in <code>C:\Python27\Lib\site-packages</code> I have attached a screen shot which shows the modules in Python27</p>

<p><a href=""https://i.stack.imgur.com/4srYG.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4srYG.png"" alt=""enter image description here""></a> </p>

<p>I have used <code>pip install opencv-python</code>. I have downloaded the module from different sites and manually copy pasted it in the correct folder. Nothing works and I am seriously short of ideas now.</p>

<p><strong>EDIT:</strong> I am on windows 10 with python 3.6 installed through anaconda and python 2.7 installed directly. Both have their variables set in the path</p>
","2609743","","","","","2019-11-15 03:04:58","ModuleNotFoundError: No module named 'cv2'","<python><python-2.7><python-3.x><opencv><cv2>","2","5","1","","","CC BY-SA 3.0","0"
"32093559","1","32144048","","2015-08-19 10:57:05","","10","31193","<p>I create two python files, and the directory/file relations is as follows:</p>

<pre><code>mytest---
     |---mycommon.py
     |---myMainDir---
                     |----myMain.py
</code></pre>

<p>In mycommon.py:</p>

<pre><code>def myFunc(a):
    ...
</code></pre>

<p>And in myMain.py:</p>

<pre><code>import sys
sys.path.append(os.path.join(os.path.dirname(os.path.abspath('__file__')), '..'))
import mycommon.py
mycommon.myFunc(""abc"")
</code></pre>

<p>Then I created exe using pyinstaller:</p>

<pre><code>pyinstall.py -F mytest\myMainDir\myMain.py
</code></pre>

<p>MyMain.exe is created, but when run, is tells that can not find <code>mycommon</code> module.</p>
","5030270","","5030270","","2015-08-19 12:58:17","2019-01-11 03:20:22","exe-file created by pyinstaller, not find self-defined modules while running","<python><python-2.7><python-3.x><python-2.6><pyinstaller>","2","2","5","","","CC BY-SA 3.0","0"
"35702996","1","35703151","","2016-02-29 15:14:20","","10","31173","<p>I am very new to <code>python</code> and just installed <code>Eric6</code>  I am wanting to search a folder (and all sub dirs) to print the filename of any file that has the extension of <code>.pdf</code>  I have this as my syntax, but it errors saying</p>

<blockquote>
  <p>The debugged program raised the exception unhandled FileNotFoundError<br>
  ""[WinError 3] The system can not find the path specified 'C:'""<br>
  File: C:\Users\pcuser\EricDocs\Test.py, Line: 6</p>
</blockquote>

<p>And this is the syntax I want to execute:</p>

<pre><code>import os

results = []
testdir = ""C:\Test""
for folder in testdir:
  for f in os.listdir(folder):
    if f.endswith('.pdf'):
        results.append(f)

print (results)
</code></pre>
","5998303","","1561176","","2016-02-29 15:19:22","2020-07-19 12:56:21","Printing File Names","<python><python-3.x>","8","5","2","","","CC BY-SA 3.0","0"
"40280762","1","","","2016-10-27 09:19:07","","2","31022","<p>i have problem to try pymysql.</p>

<ol>
<li>my computer is using windows 10</li>
<li>use python 3.5 </li>
<li>i have installed pymysql via <code>pip install pymsql</code> and success, no error. i have checked with <code>pip list</code> and get <code>PyMySQL (0.7.9)</code></li>
</ol>

<p>but when i run <code>import PyMySQL</code> get error no module named ""PyMySQL"" where is the problem?</p>

<p>edited :</p>

<p>but if i try <code>import pymysql</code>, i got error :</p>

<pre><code>Traceback (most recent call last):                                                                                                 
  File ""pymysql1.py"", line 1, in &lt;module&gt;                                                                                          
    import pymysql                                                                                                                 
  File ""C:\Program Files (x86)\Python35-32\lib\site-packages\pymysql\__init__.py"", line 29, in &lt;module&gt;                            
    from .err import (                                                                                                             
  File ""C:\Program Files (x86)\Python35-32\lib\site-packages\pymysql\err.py"", line 1, in &lt;module&gt;                                  
    import struct                                                                                                                  
  File ""C:\python\struct.py"", line 2, in &lt;module&gt;                                                                                  
    pack('hhl', 1, 2, 3)                                                                                                           
NameError: name 'pack' is not defined                
</code></pre>
","3600555","","3600555","","2016-10-28 15:21:22","2018-03-05 11:47:16","Error No Module named ""PyMySQL"" Windows 10","<python-3.x><pymysql>","3","0","","","","CC BY-SA 3.0","0"
"30032078","1","30033462","","2015-05-04 14:00:34","","10","30980","<p>I know this title look familiar to some old questions, but I’ve looked at every single one of them, none of them solves. And here is my codes:</p>

<pre><code>TypeError: __init__() takes 1 positional argument but 3 were given

Traceback (most recent call last)
File ""C:\Users\hp user\virtual_flask\lib\site-packages\flask\app.py"", line 1836, in __call__
return self.wsgi_app(environ, start_response)
File ""C:\Users\hp user\virtual_flask\lib\site-packages\flask\app.py"", line 1820, in wsgi_app
response = self.make_response(self.handle_exception(e))
File ""C:\Users\hp user\virtual_flask\lib\site-packages\flask\app.py"", line 1403, in handle_exception
reraise(exc_type, exc_value, tb)
File ""C:\Users\hp user\virtual_flask\lib\site-packages\flask\_compat.py"", line 33, in reraise
raise value
File ""C:\Users\hp user\virtual_flask\lib\site-packages\flask\app.py"", line 1817, in wsgi_app
response = self.full_dispatch_request()
File ""C:\Users\hp user\virtual_flask\lib\site-packages\flask\app.py"", line 1477, in full_dispatch_request
rv = self.handle_user_exception(e)
File ""C:\Users\hp user\virtual_flask\lib\site-packages\flask\app.py"", line 1381, in handle_user_exception
reraise(exc_type, exc_value, tb)
File ""C:\Users\hp user\virtual_flask\lib\site-packages\flask\_compat.py"", line 33, in reraise
raise value
File ""C:\Users\hp user\virtual_flask\lib\site-packages\flask\app.py"", line 1475, in full_dispatch_request
rv = self.dispatch_request()
File ""C:\Users\hp user\virtual_flask\lib\site-packages\flask\app.py"", line 1461, in dispatch_request
return self.view_functions[rule.endpoint](**req.view_args)
File ""C:\Users\hp user\PycharmProjects\flask123\views.py"", line 19, in create
create_post = Post(my_form.title.data, my_form.text.data)
TypeError: __init__() takes 1 positional argument but 3 were given
The debugger caught an exception in your WSGI application. You can now look at the traceback which led to the error.
To switch between the interactive traceback and the plaintext one, you can click on the ""Traceback"" headline. From the text traceback you can also create a paste of it. For code execution mouse-over the frame you want to debug and click on the console icon on the right side.

You can execute arbitrary Python code in the stack frames and there are some extra helpers available for introspection:

dump() shows all variables in the frame
dump(obj) dumps all that's known about the object
</code></pre>

<p>My classes are as follow:</p>

<p>Models.py</p>

<pre><code>from app import db
from datetime import datetime


class Post(db.Model):
    post_id = db.Column(db.Integer, primary_key=True)
    title = db.Column(db.String(100))
    text = db.Column(db.Text())
    created_time = db.Column(db.DateTime())

    def __init__(self, title, text, created_time=None):
        self.title = title
        self.text = text
        if created_time is None:
            self.created_time = datetime.utcnow()
        else:
            self.created_time = created_time
</code></pre>

<p>views.py</p>

<pre><code>from app import app, db
from flask import render_template, request, url_for
from forms import CreateForm
from models import Post


@app.route('/')
def homepage():
    return render_template('base.html')


@app.route('/create', methods=['GET', 'POST'])
def create():
    form = CreateForm(csrf_enabled=False)
    if request.method == 'GET':
        return render_template('create.html', form=form)
    if request.method == 'POST':
        if form.validate_on_submit():
            create_post = Post(form.title.data, form.text.data)
            db.session.add(create_post)
            db.session.commit()
    return redirect(url_for('homepage'))
</code></pre>

<p>I have tried with all possible solution and checked my code for spelling mistakes.But  I found none. </p>
","4751368","","1307905","","2015-05-07 07:11:38","2018-09-24 13:13:17","TypeError: __init__() takes 1 positional argument but 3 were given","<python><python-3.x><flask>","1","1","1","","","CC BY-SA 3.0","0"
"48260412","1","53923249","","2018-01-15 09:48:30","","14","30979","<p>I have installed pyspark recently. It was installed correctly. When I am using following simple program in python, I am getting an error.</p>

<pre><code>&gt;&gt;from pyspark import SparkContext
&gt;&gt;sc = SparkContext()
&gt;&gt;data = range(1,1000)
&gt;&gt;rdd = sc.parallelize(data)
&gt;&gt;rdd.collect()
</code></pre>

<p>while running the last line I am getting error whose key line seems to be</p>

<pre><code>[Stage 0:&gt;                                                          (0 + 0) / 4]18/01/15 14:36:32 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py"", line 123, in main
    (""%d.%d"" % sys.version_info[:2], version))
Exception: Python in worker has different version 2.7 than that in driver 3.5, PySpark cannot run with different minor versions.Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
</code></pre>

<p>I have the following variables in .bashrc</p>

<pre><code>export SPARK_HOME=/opt/spark
export PYTHONPATH=$SPARK_HOME/python3
</code></pre>

<p>I am using Python 3.</p>
","9099959","","2308683","","2018-12-25 14:41:58","2020-05-16 19:41:54","environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON","<python><python-3.x><apache-spark><pyspark>","7","1","5","","","CC BY-SA 3.0","0"
"28864307","1","","","2015-03-04 20:31:40","","10","30939","<p>Thanks for reading this in the first place.</p>

<p>I'm trying to install Django. So here's what I do in the command line:</p>

<pre><code>C:\&gt;python34 pip install Django
</code></pre>

<p>And here's what I get:</p>

<pre><code>C:\Python34\python.exe: can't open file 'pip': [Errno 2] No such file or directory
</code></pre>

<p>If I do the same from the site-packages directory:</p>

<pre><code>C:\Python34\Lib\site-packages&gt;python34 pip install Django
Traceback (most recent call last):
  File ""C:\Python34\lib\runpy.py"", line 170, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Python34\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""pip\__main__.py"", line 2, in &lt;module&gt;
    from .runner import run
SystemError: Parent module '' not loaded, cannot perform relative import
</code></pre>

<p>I used pip before, and it worked fine, but now I don't know how to run it properly... I tried to find an answer first, but I don't understand any of them. Probably because I'm relatively new to all this. Could anyone explain in first-grade-level-language what I need to do to get this right?</p>
","4633931","","2011147","","2015-03-05 19:44:38","2017-04-30 17:57:34","Pip install: can't open file pip, or Parent module '' not loaded","<python><django><python-3.x><installation><pip>","3","0","3","","","CC BY-SA 3.0","0"
"48163641","1","","","2018-01-09 07:56:24","","32","30937","<p>It's my first time trying to deploy a Django app(django 2.0.1)(Python 3.6) to pythonanywhere, it is a simple portfolio app with no models, no bootstrap.
Just Django, HTML, CSS &amp; Javascript.</p>

<p>After pulling it from the Github repo onto pythnanywhere with their bash console, I run :</p>

<pre><code>python manage.py migrate
</code></pre>

<p>&amp; was hit with this error :</p>

<pre><code>Traceback (most recent call last):
File ""manage.py"", line 22, in &lt;module&gt;
execute_from_command_line(sys.argv)
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/site-
packages/django/core/management/__init__.py"", line 371, in 
execute_from_command_line
utility.execute()
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/site-
packages/django/core/management/__init__.py"", line 365, in execute
self.fetch_command(subcommand).run_from_argv(self.argv)
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/site-
packages/django/core/management/__init__.py"", line 216, in fetch_command
klass = load_command_class(app_name, subcommand)
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/site-
packages/django/core/management/__init__.py"", line 36, in load_command_class
module = import_module('%s.management.commands.%s' % (app_name, name))
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/importlib/__init__.py"", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
File ""&lt;frozen importlib._bootstrap&gt;"", line 978, in _gcd_import
File ""&lt;frozen importlib._bootstrap&gt;"", line 961, in _find_and_load
File ""&lt;frozen importlib._bootstrap&gt;"", line 950, in _find_and_load_unlocked
File ""&lt;frozen importlib._bootstrap&gt;"", line 655, in _load_unlocked
File ""&lt;frozen importlib._bootstrap_external&gt;"", line 678, in exec_module
File ""&lt;frozen importlib._bootstrap&gt;"", line 205, in _call_with_frames_removed
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/site-
packages/django/core/management/commands/migrate.py"", line 12, in &lt;module&gt;
from django.db.migrations.autodetector import MigrationAutodetector
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/site-
packages/django/db/migrations/autodetector.py"", line 11, in &lt;module&gt;
from django.db.migrations.questioner import MigrationQuestioner
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/site-
packages/django/db/migrations/questioner.py"", line 9, in &lt;module&gt;
from .loader import MigrationLoader
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/site-packages/django/db/migrations/loader.py"", line 8, in &lt;module&gt;
from django.db.migrations.recorder import MigrationRecorder
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/site-packages/django/db/migrations/recorder.py"", line 9, in &lt;module&gt;
class MigrationRecorder:
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/site-packages/django/db/migrations/recorder.py"", line 22, in MigrationRecorder
class Migration(models.Model):
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/site-packages/django/db/models/base.py"", line 100, in __new__
app_config = apps.get_containing_app_config(module)
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/site-packages/django/apps/registry.py"", line 244, in get_containing_app_config
self.check_apps_ready()
File ""/home/Limerin555/.virtualenvs/projectenv/lib/python3.6/site-packages/django/apps/registry.py"", line 127, in check_apps_ready
raise AppRegistryNotReady(""Apps aren't loaded yet."")
django.core.exceptions.AppRegistryNotReady: Apps aren't loaded yet.
</code></pre>

<p>I tired looking for solutions everywhere I could possibly find but nothing really helps, I've even tried adding this line to my settings.py :</p>

<pre><code>import django
django.setup()
</code></pre>

<p>underneath this line :</p>

<pre><code>SECRET_KEY = os.environ.get(""SECRET_KEY"")
</code></pre>

<p>as suggested from <a href=""https://stackoverflow.com/questions/34114427/django-upgrading-to-1-9-error-appregistrynotready-apps-arent-loaded-yet"">this post</a>, but to no avail.</p>

<p>Here is my settings.py :</p>

<pre><code>import os

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
TEMPLATE_DIR = os.path.join(BASE_DIR, ""templates"")
STATIC_DIR = os.path.join(BASE_DIR, ""static"")

SECRET_KEY = os.environ.get('SECRET_KEY')

import django
django.setup()

DEBUG = False

ALLOWED_HOSTS = [""limerin555.pythonanywhere.com""]

INSTALLED_APPS = [
    'django.contrib.contenttypes',
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'portfolio_showcase',
]

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
]

ROOT_URLCONF = 'limerin.urls'

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [TEMPLATE_DIR,],
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'limerin.wsgi.application'


DATABASE_PATH = os.path.join(BASE_DIR, 'db.sqlite3')

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': DATABASE_PATH,
    }
}


AUTH_PASSWORD_VALIDATORS = [
    {
    'NAME': 
'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 
'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 
'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 
'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'Asia/Singapore'

USE_I18N = True

USE_L10N = True

USE_TZ = True


STATIC_URL = '/static/'
STATICFILES_DIRS = [
    STATIC_DIR,
]
</code></pre>

<p>I am really lost on this, hoping someone can help shed some light on what's the real problem here.</p>
","9039847","","","","","2020-09-20 06:29:36","django.core.exceptions.AppRegistryNotReady: Apps aren't loaded yet. (django 2.0.1)(Python 3.6)","<python><django><python-3.x><pythonanywhere><django-2.0>","8","3","3","","","CC BY-SA 3.0","0"
"39997714","1","40774361","","2016-10-12 11:41:25","","45","30930","<p>I've been trying to use Airflow to schedule a DAG.
One of the DAG includes a task which loads data from s3 bucket.</p>

<p>For the purpose above I need to setup s3 connection. But UI provided by airflow isn't that intutive (<a href=""http://pythonhosted.org/airflow/configuration.html?highlight=connection#connections"" rel=""noreferrer"">http://pythonhosted.org/airflow/configuration.html?highlight=connection#connections</a>). Any one succeeded setting up the s3 connection if so are there any best practices you folks follow?</p>

<p>Thanks.</p>
","7007498","","7007498","","2016-10-18 08:40:49","2020-05-25 11:39:18","Airflow s3 connection using UI","<python-3.x><airflow>","8","1","20","","","CC BY-SA 3.0","0"
"36287720","1","36291428","","2016-03-29 14:49:16","","32","30898","<p>I am struggling to find out how I can get my aws_access_key_id and aws_secret_access_key dynamically from my code. </p>

<p>In boto2 I could do the following: <code>boto.config.get_value('Credentials', 'aws_secret_access_key')</code> but I can't seem to find a similar method in boto3. I was able to find the keys if I look in <code>boto3.Session()._session._credentials</code> but that seems like the mother of all hacks to me and I would rather not go down that road.</p>
","218183","","659298","","2016-03-29 17:38:47","2020-04-16 15:56:10","Boto3: get credentials dynamically?","<python-3.x><amazon-web-services><boto3>","3","6","9","","","CC BY-SA 3.0","0"
"27732075","1","27732098","","2015-01-01 15:30:33","","4","30827","<p><strong>Edited :</strong></p>

<p><strong>I use corporate env , on my home env the script works ok</strong></p>

<p>I'm trying to get response from some weather site . I made a simple python script . </p>

<pre><code>import urllib.request
import json

req=urllib.request.Request(""http://api.wunderground.com/api/e0b319c6f7e8115a/geolookup/conditions/q/IA/Cedar_Rapids.json"")
 response=urllib.request.urlopen(req)
 data = json.loads(response.read().decode())  #&lt;--according to @falsetru fix
 print (data)
</code></pre>

<p><strong>When I paste the URL into browser, I get response , but in python  I get error</strong>
Please help</p>

<p>Full error</p>

<pre><code>Traceback (most recent call last):
File ""C:\Python34\weather.py"", line 14, in &lt;module&gt;
response=urllib.request.urlopen(req)
File ""C:\Python34\lib\urllib\request.py"", line 153, in urlopen
return opener.open(url, data, timeout)
File ""C:\Python34\lib\urllib\request.py"", line 455, in open
response = self._open(req, data)
File ""C:\Python34\lib\urllib\request.py"", line 473, in _open
'_open', req)
File ""C:\Python34\lib\urllib\request.py"", line 433, in _call_chain
result = func(*args)
File ""C:\Python34\lib\urllib\request.py"", line 1202, in http_open
return self.do_open(http.client.HTTPConnection, req)
File ""C:\Python34\lib\urllib\request.py"", line 1177, in do_open
r = h.getresponse()
File ""C:\Python34\lib\http\client.py"", line 1172, in getresponse
response.begin()
File ""C:\Python34\lib\http\client.py"", line 351, in begin
version, status, reason = self._read_status()
File ""C:\Python34\lib\http\client.py"", line 313, in _read_status
line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
File ""C:\Python34\lib\socket.py"", line 371, in readinto
return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host
</code></pre>

<p>Thanks</p>
","410999","","410999","","2015-01-01 16:04:51","2019-02-05 16:52:00","Python 3 , Windows 7 :ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host","<python><json><url><python-3.x>","2","0","2","","","CC BY-SA 3.0","0"
"39201093","1","39206941","","2016-08-29 07:51:23","","10","30792","<p>BACKGROUND:</p>

<p>The AWS operation to list IAM users returns a max of 50 by default.  </p>

<p>Reading the docs (links) below I ran following code and returned a complete set data by setting the ""MaxItems"" to 1000.</p>

<pre><code>paginator = client.get_paginator('list_users')
response_iterator = paginator.paginate(
 PaginationConfig={
     'MaxItems': 1000,
     'PageSize': 123})
for page in response_iterator:
    u = page['Users']
    for user in u:
        print(user['UserName'])
</code></pre>

<p><a href=""http://boto3.readthedocs.io/en/latest/guide/paginators.html"" rel=""noreferrer"">http://boto3.readthedocs.io/en/latest/guide/paginators.html</a>
<a href=""https://boto3.readthedocs.io/en/latest/reference/services/iam.html#IAM.Paginator.ListUsers"" rel=""noreferrer"">https://boto3.readthedocs.io/en/latest/reference/services/iam.html#IAM.Paginator.ListUsers</a></p>

<p>QUESTION:</p>

<p>If the ""MaxItems"" was set to 10, for example, what would be the best method to loop through the results? the</p>

<p>I tested with the following but it only loops 2 iterations before 'IsTruncated' == False and results in ""KeyError: 'Marker'"". Not sure why this is happening because I know there are over 200 results.</p>

<pre><code>marker = None

while True:
    paginator = client.get_paginator('list_users')
    response_iterator = paginator.paginate( 
        PaginationConfig={
            'MaxItems': 10,
            'StartingToken': marker})
    #print(response_iterator)
    for page in response_iterator:
        u = page['Users']
        for user in u:
            print(user['UserName'])
            print(page['IsTruncated'])
            marker = page['Marker']
            print(marker)
        else:
            break
</code></pre>

<p>THanks.</p>
","1420181","","6017840","","2016-08-29 16:15:51","2020-09-27 15:24:56","How to use Boto3 pagination","<python-3.x><amazon-web-services><boto3>","1","1","3","","","CC BY-SA 3.0","0"
"46141097","1","46141143","","2017-09-10 13:16:16","","12","30786","<p>Why does this statement produce an error even though a string is actually a list of character constants?</p>

<pre><code>string_name = """"  
string_name.append(""hello word"")  
</code></pre>

<p>The reason I expect this to work is because when we use for-loop, we are allowed to use this statement:</p>

<pre><code>for i in string_name:  
    ...
</code></pre>

<p>I think <code>string_name</code> is considered as a list here(?)</p>
","8044527","","202229","","2020-01-09 14:21:30","2020-09-12 17:21:36","Why doesn't .append() method work on strings, don't they behave like lists?","<string><python-3.x><append>","2","1","1","","","CC BY-SA 4.0","0"
"38076220","1","38078544","","2016-06-28 12:25:04","","17","30784","<p>I am making a Python project where I have to seek and retreive data from a database.<br>
I tried making a class, in which I declare the connection and do my queries, here is moreless what I have so far.  </p>

<pre><code>import MySQLdb
dbc =(""localhost"",""root"",""1234"",""users"")
class sql:
    db = MySQLdb.connect(dbc[0],dbc[1],dbc[2],dbc[3])
    cursor = db.cursor()

    def query(self,sql):
        sql.cursor.execute(sql)
        return sql.cursor.fetchone()

    def rows(self):
        return sql.cursor.rowcount

sqlI = sql()
print(sqlI.query(""SELECT `current_points` FROM `users` WHERE `nick` = 'username';""))
</code></pre>

<p>So, the main problem is that the variable <code>db</code> and <code>cursor</code> are not callable from other def's/functions from the same Class. What I'd like to get, is a polished query, where I can make queries and retreive it's content. This would summarize my code, therefore I should do.</p>
","5625667","","5625667","","2016-06-29 08:49:50","2020-06-01 20:57:47","Python MySQLdb - Connection in a class","<python><python-3.x><mysql-python>","4","3","17","","","CC BY-SA 3.0","0"
"48817461","1","","","2018-02-15 22:38:39","","2","30752","<p>I am trying to have a user input whether or not they like spicy food and the output is supposed to be a boolean but I don't seem to be getting an output with my code below:</p>

<pre><code>def likes_spicyfood():
    spicyfood = bool(input(""Do you like spicy food? True or False?""))
    if spicyfood == (""True""):
        print(""True"")
    if spicyfood == (""False""):
        print(""False"")
        return(likes_spicyfood)
</code></pre>
","5556453","","3001761","","2018-02-15 22:40:13","2020-06-04 15:15:47","User input boolean in python","<python><python-3.x><input><boolean>","9","6","","","","CC BY-SA 3.0","0"
"41862682","1","41862825","","2017-01-25 22:17:13","","14","30702","<p>I was using the Anaconda <code>3.5</code> <a href=""https://www.continuum.io/downloads#windows"" rel=""noreferrer"">distro</a> in a Windows 10 machine. Due to dependencies in libraries that I want to work with, I had to have the <code>2.7</code> version installed as well. </p>

<p>The good news is that the libraries I needed can now work with the <code>2.7</code> version smoothly and Visual Studio 2015 automagically detected my new Python environment. </p>

<p>The problem comes when using the command line. Upon issuing the command</p>

<pre><code>conda info --envs
</code></pre>

<p>I get</p>

<pre><code>root                  *  C:\Users\norah\Anaconda2
</code></pre>

<p>i.e. a single environment (to my understanding and search so far, according to <a href=""http://conda.pydata.org/docs/using/envs.html"" rel=""noreferrer"">this</a> I should see two envs listed). This means I can't use <code>conda</code> to acquire new packages for my <code>Python3.5</code> installation, at least not at the command line as I used to since <code>conda</code> only refers to <code>Python2.7</code>. The same goes for the GUI version, Anaconda navigator (I'm not very fond of the GUI version but I tried it out). </p>

<p>There's also no way of launching python3 from the command line since </p>

<pre><code>$python
</code></pre>

<p>always fires up python2.7 and issuing <code>$python3</code> or <code>$python3.5</code> in the command line doesn't seem to work (nor would adding the path of python3 to the system since the actual executable has the same name as python2 i.e. <code>python.exe</code>) </p>

<p>Is my system taken over by Python2.7? <strong>Is anyone here using them both smoothly and if so could you please elaborate on how to do that</strong>? Was it a ""no no"" move to install both versions of Anaconda?</p>
","4224575","","4224575","","2017-01-27 10:32:34","2017-01-31 17:47:08","How can I use both Anaconda versions (2.7 & 3.5)?","<python><python-2.7><python-3.x><anaconda>","3","3","12","","","CC BY-SA 3.0","0"
"46715902","1","46715977","","2017-10-12 17:47:11","","7","30700","<p>I have the following string:</p>

<pre><code>'2017-08-15T13:34:35Z'
</code></pre>

<p>How to convert this string to object that I can call  <code>.isoformat()</code>?</p>

<pre><code>someobject = convert('2017-08-15T13:34:35Z')
someobject.isoformat()
</code></pre>

<p>How to implement <code>convert()</code>?</p>
","","user6611764","364696","","2017-10-12 18:00:34","2017-10-12 18:00:34","How to convert string to datetime?","<python><python-3.x><datetime><date-parsing>","2","1","3","2017-10-12 17:57:52","","CC BY-SA 3.0","0"
"35833593","1","35833941","","2016-03-06 22:48:13","","6","30675","<p>I am using python 3.5 and I am trying to install NumPy but when I try to install from command prompt using command: pip install numpy</p>

<p>I get a whole lot of errors. The main error though seems to be at the end: error: Unable to find vcvarsall.bat</p>

<p>I have also tried downloading the numpy binaries from <a href=""http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy"" rel=""noreferrer"">http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy</a> and installing using command: pip install numpy-1.10.4+mkl-cp35-cp35m-win_amd64.whl</p>

<p>But I get an error that says: numpy-1.10.4+mkl-cp35-cp35m-win_amd64.whl is not a supported wheel on this platform.</p>

<p>Then I found a numpy-1.10.2-win32-superpack-python3.4.exe file here <a href=""https://sourceforge.net/projects/numpy/files/NumPy/1.10.2/"" rel=""noreferrer"">https://sourceforge.net/projects/numpy/files/NumPy/1.10.2/</a> but when I try to run that I get an error that I require Python version 3.4.</p>

<p>I looked and cannot find an installer for python 3.5.exe, do I need to uninstall python 3.5 and install 3.4 instead? Or is there anyway to install numpy for Python 3.5?</p>

<p>I am using Windows 10/64 bit.</p>
","5145932","","","","","2016-12-21 13:49:27","Install NumPy for Python 3.5","<python-3.x><numpy>","1","2","","","","CC BY-SA 3.0","0"
"33535625","1","33665924","","2015-11-05 02:25:50","","0","30669","<p>I am using OS X El Capitan and my default Python version is 2.7.10. </p>

<p>How can I change the default version to Python 3.5 for Terminal use?</p>
","5527294","","2227834","","2015-11-07 04:43:19","2016-03-25 15:38:59","Python 3.5 for OS X El Capitan","<macos><python-3.x>","2","2","1","","","CC BY-SA 3.0","0"
"42623049","1","","","2017-03-06 10:22:05","","20","30660","<pre><code>+--------+-----------------------------------+
|   OS   |           Ubuntu 12.04            |
+--------+-----------------------------------+
| Python | 2.7, 3.2 and source installed 3.6 |
+--------+-----------------------------------+
</code></pre>

<p>Since there are 2 versions of Python 3, anything installed from the repository doesn't work for Python 3.6. The latest version of Python in the repositories is 3.2, so I need source installs or through pip3.6.</p>

<p>After starting <code>python3.6</code> I tried to <code>import tkinter</code>, which gave the following error. Even though <code>help('modules')</code> returned a list of modules which included tkinter.</p>

<pre><code> import tkinter
 ModuleNotFoundError: No module named '_tkinter'
</code></pre>

<p>I tried doing the same in <code>python3.2</code> and there were <strong>no errors</strong>.  <code>tkinter._tkinter</code> gave the location of tkinter library for python3.2</p>

<p>I cd'd into the <code>python3.6</code> directory which has all library files and indeed it was missing the <code>tkinter.so</code> object file.</p>

<p>How do I fix the error? </p>

<p>I would like to get tkinter/tkagg working as it seems all the modules/ package are already installed.</p>

<p>After googling some more, I found I need to build <code>python3.6</code> again, but this time with Tcl/Tk options while running configure. I'd rather not. It takes 1hr approx to install <code>python3.6</code> from scratch. </p>

<p><del>Is there some other way where I can tell <code>python3.6</code> where Tcl/Tk is located?</del></p>

<p>The problem isn't telling python where tcl/tk is. After messing around with python3.6's source code, and then comparing python3.6 with python3.2, I found out that tkinter calls _tkinter which isn't a python file, it's an .so(shared object) file that python builds during installation via setup.py that uses gcc, that somehow may involve distutils.</p>

<p>The new and more appropriate question is how do I build,
<code>_tkinter.cpython-36m-i386-linux-gnu.so</code> from tcl/tk?</p>

<p>Note : I do have tcl/tk installed, which I have confirmed using tclsh and wish.</p>
","5070281","","5070281","","2017-03-06 19:21:01","2020-05-20 11:20:01","Install tkinter in python3.6 on Ubuntu","<python><python-3.x><tkinter><tcl><python-3.6>","6","6","6","","","CC BY-SA 3.0","0"
"48299396","1","48360061","","2018-01-17 10:56:49","","6","30630","<p>Currently I'm trying to convert my tkinter python script to a exe file. I'm using cx_freeze to do this. When i try to add an aditional file, it somehow is not working anymore. In the minimum example I'm using below you can see the method I've used.</p>

<pre><code>import tkinter as tk

import numpy.core._methods, numpy.lib.format 

class Main(tk.Tk):

    def __init__(self, *args, **kwargs):
        tk.Tk.__init__(self, *args, **kwargs)

        self.geometry(""700x400"")
        self.wm_iconbitmap('test.ico')

        container = tk.Frame(self)

        container.pack(side=""top"", fill=""both"", expand = True)

        container.grid_rowconfigure(0, weight=1)
        container.grid_columnconfigure(0, weight=1)

        self.frames = {}

        for F in (StartPage, PageOne):

            frame = F(container, self)
            self.frames[F] = frame
            frame.grid(row=0, column=0, sticky=""nsew"")

        self.show_frame(StartPage)

    def show_frame(self, cont):
        frame = self.frames[cont]
        frame.tkraise()        
        frame.update_page() # &lt;-- update data on page when you click button

    def get_page(self, page_class):
        return self.frames[page_class]


class StartPage(tk.Frame):

    def __init__(self, parent, controller):
        tk.Frame.__init__(self, parent)
        self.controller = controller 

        label1 = tk.Label(self, text=""What are the sizes?"")
        label1.pack()

        L1 = tk.Label(self, text=""Length :"")
        L1.pack()

        self.E1 = tk.Entry(self)
        self.E1.pack()

        button = tk.Button(self, text=""Next"", command=lambda: controller.show_frame(PageOne))
        button.pack()

    def update_page(self): # empty method but I need it
        pass   

class PageOne(tk.Frame):

    def __init__(self, parent, controller):
        tk.Frame.__init__(self, parent)
        self.controller = controller

        label1 = tk.Label(self, text=""You have insert"")
        label1.pack()

        # create empty label at start
        self.label2 = tk.Label(self, text="""")
        self.label2.pack()

        button = tk.Button(self, text=""Back"", command=lambda: controller.show_frame(StartPage))
        button.pack()

    def update_page(self):
        # update label when page is changed
        page1 = self.controller.get_page(StartPage) 
        var = page1.E1.get()
        self.label2['text'] = var


app = Main()
app.mainloop() 
</code></pre>

<p>The second script:</p>

<pre><code>import cx_Freeze
import sys
import matplotlib 
import os 
import numpy.core._methods
import numpy.lib.format

base = None 

if sys.platform=='win32':
    base = ""Win32GUI""


executables = [cx_Freeze.Executable(""Show_file.py"")]    

cx_Freeze.setup(
        name = ""Name"",
        options = {""build_exe"":{""packages"":[""tkinter"",""matplotlib""],""include_files"":[""test.ico""]}},
        version=""0.01"",
        executables=executables) 
</code></pre>

<p>When I try to build the exe file, it works when i do not add an icon. However in case I try to add an icon, the exe does not open anymore. Furthermore, when I try to add a database excel file, I get the message that such a file does not exist. Allo the files are in the correct folder. That is not the problem. </p>

<p>Could somebody help me with this problem?</p>

<p>Many thanks in advance. </p>
","8899328","","","","","2019-01-25 09:09:53","Converting tkinter to exe","<python-3.x><tkinter><cx-freeze>","2","2","5","","","CC BY-SA 3.0","0"
"46093890","1","","","2017-09-07 10:20:23","","3","30566","<p>I am trying to update a dictionary value in Python. I'm trying to update a value in the dict by subtracting 3 from the value.</p>

<pre><code>if buildings == 1:
        workpower -= 3
        if workpower &gt;= 0:
            farmplace1 = int(input(""where do you want it?""))
            farmplace2 = int(input(""where do you want it?""))
            board[farmplace1][farmplace2] = ""F""    
            my_dict['d'] -= 3;
</code></pre>

<p>I get the following error </p>

<pre><code>IndentationError: unindent does not match any outer indentation level
</code></pre>

<p>What is wrong with the code?</p>
","8573936","","4889267","","2017-09-07 10:43:24","2017-09-07 10:49:14","Python IndentationError: unindent does not match any outer indentation level","<python><python-3.x><dictionary><indentation>","2","2","4","2017-09-07 10:49:12","","CC BY-SA 3.0","0"
"30467495","1","","","2015-05-26 19:42:27","","25","30546","<p>I'm trying to istall mysql server on a windows 7 machine - that has python 3.4.3 installed. However, when trying to install the python connectors for 3.4, the installer fails to recognize the python installation, saying <code>python 3.4 is not installed</code>. </p>

<p>Has anyone solved this issue before? I'ts driving me nuts...</p>
","963564","","","","","2020-04-06 20:54:53","mysql installer fails to recognize python 3.4","<mysql><python-3.x><installation>","12","0","2","","","CC BY-SA 3.0","0"
"42720875","1","42720892","","2017-03-10 14:34:45","","3","30389","<p>I've gone through many questions but couldn't find what i was looking for.
I have a list something like this:
    <code>[2, 3, 5, 7, 11]</code><br>
and I want to convert it into a dictionary in the format:
i.e. the values of the list should be keys and each associated value should be zero.
    <code>{2:0 , 3:0 , 5:0 , 7:0 , 11:0}</code>  </p>
","6669021","","6669021","","2017-03-10 14:36:13","2017-05-02 01:50:45","Converting a list to dictionary in python","<python><list><python-3.x><dictionary>","2","1","1","2017-03-10 14:40:46","","CC BY-SA 3.0","0"
"50737192","1","50737863","","2018-06-07 09:03:22","","36","30366","<p>Let's say I have defined a dataset in this way:</p>

<pre><code>filename_dataset = tf.data.Dataset.list_files(""{}/*.png"".format(dataset))
</code></pre>

<p>how can I get the number of elements that are inside the dataset (hence, the number of single elements that compose an epoch)?</p>

<p>I know that <code>tf.data.Dataset</code> already knows the dimension of the dataset, because the <code>repeat()</code> method allows repeating the input pipeline for a specified number of epochs. So it must be a way to get this information.</p>
","2891324","","2230844","","2020-03-05 17:16:48","2020-09-03 12:22:22","tf.data.Dataset: how to get the dataset size (number of elements in a epoch)?","<python><python-3.x><tensorflow><tensorflow-datasets>","10","9","5","","","CC BY-SA 4.0","0"
"34827566","1","","","2016-01-16 13:33:09","","44","30354","<ol>
<li>This is the hints,how can I resolve it?</li>
<li>I use Python 3.5.1 created a virtual envirement by virtualenv</li>
<li>The source code works well on my friend's computer machine</li>
</ol>

<p>Error: </p>

<pre><code>Traceback (most recent call last):
  File ""manage.py"", line 10, in &lt;module&gt;
    execute_from_command_line(sys.argv)
  File ""A:\Python3.5\lib\site-packages\django\core\management\__init__.py"", line 385, in execute_from_command_line
    utility.execute()
  File ""A:\Python3.5\lib\site-packages\django\core\management\__init__.py"", line 354, in execute
    django.setup()
  File ""A:\Python3.5\lib\site-packages\django\__init__.py"", line 18, in setup
    from django.utils.log import configure_logging
  File ""A:\Python3.5\lib\site-packages\django\utils\log.py"", line 13, in &lt;module&gt;
    from django.views.debug import ExceptionReporter, get_exception_reporter_filter
  File ""A:\Python3.5\lib\site-packages\django\views\debug.py"", line 10, in &lt;module&gt;
    from django.http import (HttpResponse, HttpResponseServerError,
  File ""A:\Python3.5\lib\site-packages\django\http\__init__.py"", line 4, in &lt;module&gt;
    from django.http.response import (
  File ""A:\Python3.5\lib\site-packages\django\http\response.py"", line 13, in &lt;module&gt;
    from django.core.serializers.json import DjangoJSONEncoder
  File ""A:\Python3.5\lib\site-packages\django\core\serializers\__init__.py"", line 23, in &lt;module&gt;
    from django.core.serializers.base import SerializerDoesNotExist
  File ""A:\Python3.5\lib\site-packages\django\core\serializers\base.py"", line 6, in &lt;module&gt;
    from django.db import models
  File ""A:\Python3.5\lib\site-packages\django\db\models\__init__.py"", line 6, in &lt;module&gt;
    from django.db.models.query import Q, QuerySet, Prefetch  # NOQA
  File ""A:\Python3.5\lib\site-packages\django\db\models\query.py"", line 13, in &lt;module&gt;
    from django.db.models.fields import AutoField, Empty
  File ""A:\Python3.5\lib\site-packages\django\db\models\fields\__init__.py"", line 18, in &lt;module&gt;
    from django import forms
  File ""A:\Python3.5\lib\site-packages\django\forms\__init__.py"", line 6, in &lt;module&gt;
    from django.forms.fields import *  # NOQA
  File ""A:\Python3.5\lib\site-packages\django\forms\fields.py"", line 18, in &lt;module&gt;
    from django.forms.utils import from_current_timezone, to_current_timezone
  File ""A:\Python3.5\lib\site-packages\django\forms\utils.py"", line 15, in &lt;module&gt;
    from django.utils.html import format_html, format_html_join, escape
  File ""A:\Python3.5\lib\site-packages\django\utils\html.py"", line 16, in &lt;module&gt;
    from .html_parser import HTMLParser, HTMLParseError
  File ""A:\Python3.5\lib\site-packages\django\utils\html_parser.py"", line 12, in &lt;module&gt;
    HTMLParseError = _html_parser.HTMLParseError
AttributeError: module 'html.parser' has no attribute 'HTMLParseError'
</code></pre>
","4965890","","3001761","","2016-01-16 13:35:01","2019-06-17 15:13:17","AttributeError: module 'html.parser' has no attribute 'HTMLParseError'","<python><django><python-3.x>","4","4","6","","","CC BY-SA 3.0","0"
"31421379","1","31421411","","2015-07-15 04:12:58","","483","30337","<p>If I do this:</p>

<pre><code>&gt;&gt;&gt; False in [False, True]
True
</code></pre>

<p>That returns <code>True</code>. Simply because <code>False</code> is in the list.</p>

<p>But if I do:</p>

<pre><code>&gt;&gt;&gt; not(True) in [False, True]
False
</code></pre>

<p>That returns <code>False</code>. Whereas <code>not(True)</code> is equal to <code>False</code>:</p>

<pre><code>&gt;&gt;&gt; not(True)
False
</code></pre>

<p>Why?</p>
","4232487","","1523776","","2015-11-28 21:48:14","2019-08-28 10:23:09","Why does ""not(True) in [False, True]"" return False?","<python><python-2.7><python-3.x>","8","2","57","","","CC BY-SA 3.0","0"
"28575359","1","","","2015-02-18 03:17:59","","11","30325","<p>I'm trying to update some code to python3, using <a href=""http://ldap3.readthedocs.org/en/latest/quicktour.html"" rel=""noreferrer"">ldap3</a> version '0.9.7.4'.
(<a href=""https://pypi.python.org/pypi/ldap3"" rel=""noreferrer"">https://pypi.python.org/pypi/ldap3</a>)</p>

<p>Previously, I used python-ldap with python2 to authenticate a user like this:</p>

<pre><code>import ldap
address = ""ldap://HOST:389""
con = ldap.initialize(address)
base_dn = ""ourDN=jjj""
con.protocol_version = ldap.VERSION3
search_filter = ""(uid=USERNAME)""
result = con.search_s(base_dn, ldap.SCOPE_SUBTREE, search_filter, None)  
user_dn = result[0][0]  # get the user DN
con.simple_bind_s(user_dn, ""PASSWORD"")
</code></pre>

<p>This properly returns <code>(97, [], 2, [])</code> on correct password, and raises <code>ldap.INVALID_CREDENTIALS</code> on a bind attempt using an incorrect password.</p>

<p>Using <code>ldap3</code> in python3 I'm doing the following:</p>

<pre><code>from ldap3 import Server, Connection, AUTH_SIMPLE, STRATEGY_SYNC, ALL
s = Server(HOST, port=389, get_info=ALL)
c = Connection(s, authentication=AUTH_SIMPLE, user=user_dn, password=PASSWORD, check_names=True, lazy=False, client_strategy=STRATEGY_SYNC, raise_exceptions=True)
c.open()
c.bind()
</code></pre>

<p>It's raising the following exception:</p>

<pre><code>ldap3.core.exceptions.LDAPInvalidCredentialsResult: LDAPInvalidCredentialsResult - 49 - invalidCredentials - [{'dn': '', 'message': '', 'type': 'bindResponse', 'result': 0, 'saslCreds': 'None', 'description': 'success', 'referrals': None}]
</code></pre>

<p>I'm using the <code>user_dn</code> value returned by python2's ldap search, since this appears to be working in python2.</p>

<p>How can I get this to bind properly using ldap3 in python3?</p>

<p>(<em>One thing strange, I noticed, is that the ldap3's LDAPInvalidCredentialsResult includes <code>'description': 'success'</code>.  I'm guessing this just means response successfully recieved...</em>)</p>
","24718","","1421222","","2018-01-20 10:39:35","2018-01-20 10:39:35","How to bind (authenticate) a user with ldap3 in python3","<python><python-3.x><ldap><ldap3>","1","7","3","","","CC BY-SA 3.0","0"
"47812372","1","48096286","","2017-12-14 11:35:00","","13","30302","<p>If I store the path that i want to open in a string called finalpath which looks something like this:
""./2.8 Movies/English/Die Hard Series""</p>

<p>then how do i open this in Windows Explorer?(Windows 10)(Python 3.6.2)</p>

<p>P.S I know many people have asked this question but I did not find them clear. Please answer soon.</p>
","8663663","","","","","2020-08-03 00:41:00","Python: How to open a folder on Windows Explorer(Python 3.6.2, Windows 10)","<windows><python-3.x><operating-system><windows-10><windows-explorer>","4","0","8","","","CC BY-SA 3.0","0"
"39268052","1","39268706","","2016-09-01 09:42:28","","33","30260","<p>Since Python 3.4, the <code>Enum</code> class exists.</p>

<p>I am writing a program, where some constants have a specific order and I wonder which way is the most pythonic to compare them:</p>

<pre><code>class Information(Enum):
    ValueOnly = 0
    FirstDerivative = 1
    SecondDerivative = 2
</code></pre>

<p>Now there is a method, which needs to compare a given <code>information</code> of <code>Information</code> with the different enums:</p>

<pre><code>information = Information.FirstDerivative
print(value)
if information &gt;= Information.FirstDerivative:
    print(jacobian)
if information &gt;= Information.SecondDerivative:
    print(hessian)
</code></pre>

<p>The direct comparison does not work with Enums, so there are three approaches and I wonder which one is preferred:</p>

<p>Approach 1: Use values:</p>

<pre><code>if information.value &gt;= Information.FirstDerivative.value:
     ...
</code></pre>

<p>Approach 2: Use IntEnum:</p>

<pre><code>class Information(IntEnum):
    ...
</code></pre>

<p>Approach 3: Not using Enums at all:</p>

<pre><code>class Information:
    ValueOnly = 0
    FirstDerivative = 1
    SecondDerivative = 2
</code></pre>

<p>Each approach works, Approach 1 is a bit more verbose, while Approach 2 uses the not recommended IntEnum-class, while  and Approach 3 seems to be the way one did this before Enum was added. </p>

<p>I tend to use Approach 1, but I am not sure. </p>

<p>Thanks for any advise!</p>
","1784952","","5014455","","2020-01-14 05:57:32","2020-03-18 15:15:05","How to compare Enums in Python?","<python><python-3.x><enums><compare>","3","3","1","","","CC BY-SA 3.0","0"
"41664806","1","","","2017-01-15 18:37:56","","10","30255","<p>With my code, I want to get the last two digits of an integer. But when I make x a positive number, it will take the first x digits, if it is a negative number, it will remove the first x digits.</p>

<p>Code:</p>

<pre><code>number_of_numbers = 1
num = 9
while number_of_numbers &lt;= 100:
  done = False
  num = num*10
  num = num+1
  while done == False:
    num_last = int(repr(num)[x])
    if num_last%14 == 0:
      number_of_numbers = number_of_numbers + 1
      done = True
    else:
      num = num + 1
print(num)
</code></pre>
","","user5428930","","","","2019-10-11 07:36:25","Last 2 digits of an integer? Python 3","<python><python-3.x><integer>","4","1","","","","CC BY-SA 3.0","0"
"29934449","1","29935140","","2015-04-29 04:35:08","","3","30165","<p>I am following a tutorial on installing the Requests library, a HTTP library for Python.   <a href=""http://docs.python-requests.org/en/latest/user/install/#install"" rel=""nofollow noreferrer"">Requests Installation Guide</a></p>
<p>The tutorial says to install the Requests library, simply run this command in my terminal</p>
<pre><code>pip install requests
</code></pre>
<p>I didn't which terminal to run this command in but I first tried Windows cmd after downloading pip, the package management system used to install and manage software packages written in Python(shown below)
<img src=""https://i.stack.imgur.com/hRr1a.png"" alt=""enter image description here"" /></p>
<p>I then tried the Python 3.4.2 terminal(shown below)</p>
<p><img src=""https://i.stack.imgur.com/nqyaS.png"" alt=""enter image description here"" /></p>
<p>Does anyone know which terminal to run this command in and what my syntax error is for that terminal(tried both)? To me it's weird because the Python terminal was able to recognize pip but not install.....</p>
","3761297","","-1","","2020-06-20 09:12:55","2017-01-19 20:39:56","Why am I getting an invalid syntax error?","<python><python-3.x><cmd><terminal><pip>","4","2","1","","","CC BY-SA 3.0","0"
"35355225","1","35355433","","2016-02-12 05:08:03","","12","30159","<p>I am really new to Python. I am currently working on an assignment for creating an HTML file using python. I understand how to read an HTML file into python and  then edit and save it. </p>

<pre><code>table_file = open('abhi.html', 'w')
table_file.write('&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;body&gt;')
table_file.close()
</code></pre>

<p>The problem with the above piece is it's just replacing the whole HTML file and putting the string inside write(). How can I edit the file and the same time keep it's content intact. I mean, writing something like this, but inside the <strong>body tags</strong></p>

<pre><code>&lt;link rel=""icon"" type=""image/png"" href=""img/tor.png""&gt;
</code></pre>

<p>I need the link to automatically go in between the opening and closing body tags.</p>
","4416786","","4979237","","2018-08-15 18:27:50","2018-08-15 18:27:50","Edit and create HTML file using Python","<python><html><python-3.x><python-import>","2","3","9","","","CC BY-SA 4.0","0"
"30773911","1","","","2015-06-11 07:07:43","","32","30120","<pre><code>[[1, '34', '44'], [1, '40', '30', '41'], [1, '41', '40', '42'], [1, '42', '41', '43'], [1, '43', '42', '44'], [1, '44', '34', '43']]
</code></pre>

<p>I have a list of lists. My aim is to check whether any one sublist has anything in common with other sublists(excluding the first index object to compare). If it has anything in common then unify those sublists. </p>

<p>For example, for this example my final answer should be something like:</p>

<pre><code>[[1, '34, '44', '40' '30', '41', '42', '43']]
</code></pre>

<p>I can understand that I should convert the sublists to sets and then use union() and intersection() operation. But what I am stuck with is to how to compare each set/sublist. I can't run a loop over the list and compare each sublist one by one as the contents of the list would be modified and this would lead to error.</p>

<p>What I want to know is there any efficient method to compare all the sublists(converted to sets) and get union of them?</p>
","4422502","","3510736","","2015-06-11 07:13:57","2020-08-26 18:09:28","Union of multiple sets in python","<python><list><python-3.x><set-union>","7","7","6","","","CC BY-SA 3.0","0"
"35410498","1","35410610","","2016-02-15 13:22:49","","7","30070","<p>In my python script I want to repeat a function every N minutes, and, of course, the main thread has to keep working as well. In the main thread I have this:</p>

<pre><code># something
# ......
while True:
  # something else
  sleep(1)
</code></pre>

<p>So how can I create a function (I guess, in another thread) which executes every N minutes? Should I use a timer, or Even, or just a Thread? I'm a bit confused.</p>
","5930137","","","","","2017-12-01 16:59:06","How to repeat a function every N minutes?","<python><multithreading><python-3.x><timer>","1","0","4","2016-02-15 17:43:33","","CC BY-SA 3.0","0"
"28373288","1","28373515","","2015-02-06 19:27:33","","4","30067","<p>I am writing a script to automate changing a particular set of text in one file into a particular set in another with a different name.</p>

<p>I want to get the name of the file using the <code>askopenfilename</code> function, but when I try to print the file name, it returns:</p>

<p><code>&lt;_io.TextIOWrapper name='/home/rest/of/file/path/that/I/actually/need.txt' mode='w' encoding='ANSI_X3.4-1968'&gt;</code></p>

<p>I need just the file name because the <code>&lt;_io.TextIOWrapper ...&gt;</code> is not sub scriptable.</p>

<p>Any suggestions to remove the extraneous bits?</p>
","4538582","","3924118","","2015-03-06 13:19:41","2015-03-06 13:19:41","Get file path from askopenfilename function in Tkinter","<python><python-3.x><tkinter>","1","0","1","","","CC BY-SA 3.0","0"
"53376099","1","53498623","","2018-11-19 13:51:58","","66","30041","<p>The standard library in 3.7 can recursively convert a dataclass into a dict (example from the docs):</p>

<pre><code>from dataclasses import dataclass, asdict
from typing import List

@dataclass
class Point:
     x: int
     y: int

@dataclass
class C:
     mylist: List[Point]

p = Point(10, 20)
assert asdict(p) == {'x': 10, 'y': 20}

c = C([Point(0, 0), Point(10, 4)])
tmp = {'mylist': [{'x': 0, 'y': 0}, {'x': 10, 'y': 4}]}
assert asdict(c) == tmp
</code></pre>

<p>I am looking for a way to turn a dict back into a dataclass when there is nesting. Something like <code>C(**tmp)</code> only works if the fields of the data class are simple types and not themselves dataclasses. I am familiar with <a href=""https://jsonpickle.github.io/"" rel=""noreferrer"">jsonpickle</a>, which however comes with a prominent security warning. </p>
","419338","","4531270","","2019-09-21 03:51:43","2020-05-04 13:42:37","Python dataclass from a nested dict","<python><python-3.x><python-dataclasses>","10","5","21","","","CC BY-SA 4.0","0"
"29045768","1","29052142","","2015-03-14 05:24:16","","8","30006","<p>Ok so here is part of my code (I have imported sys) </p>

<pre><code>if __name__ == '__main__':


MyCaesarCipher = CaesarCipher() #MyCaesarCipher IS a CaesarCipher()

if len(sys.argv) &gt;1:
    #what will it check? 

Done = False   
while not Done:
    print('C Clear All')
    print('L Load Encrypted File')
    print('R Read Decrypted File')
    print('S Store Encrypted File')
    print('W Write Decrypted File')
    print('O Output Encrypted Text')
    print('P Print Decrypted Text')
    print('E Encrypt Decrypted Text')
    print('D Decrypted Encrypted Text')
    print('Q Quit')
    print('----------------')
    print('Enter Choice&gt;')
</code></pre>

<p>So the thing is I want to  do is if the command line length is more than 1, the program runs as a script. </p>

<p>This is the instruction:</p>

<blockquote>
  <p>If no command line arguments are input, then the script enters menu
  mode. If more than 1 command line argument (anything other than script
  name) is provided during the run of the script it enters single run
  mode.</p>
</blockquote>

<p>I do not know what this means, though. </p>
","4460729","","","","","2015-03-15 10:55:04","How to use sys.argv in python to check length of arguments so it can run as script?","<python><python-3.x><sys>","5","0","2","","","CC BY-SA 3.0","0"
"32055768","1","32055799","","2015-08-17 16:52:22","","15","29991","<p>I am working on a Linux machine with Python version 3.2.3. 
Whenever I try to do <code>list.clear()</code> I get an exception </p>

<pre><code>&gt;&gt;&gt; l = [1, 2, 3, 4, 5, 6, 7]
&gt;&gt;&gt; l.clear()
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
AttributeError: 'list' object has no attribute 'clear'
</code></pre>

<p>At the same time on my Mac with Python 3.4.3 the same code runs smoothly. 
Can it be due to the difference between Python versions or is there something I'm missing? </p>
","2219578","","2301450","","2015-08-17 17:13:02","2016-12-04 20:29:24","Python3 AttributeError: 'list' object has no attribute 'clear'","<python><list><python-3.x><attributeerror><python-3.2>","1","0","4","","","CC BY-SA 3.0","0"
"40526496","1","40539365","","2016-11-10 11:27:17","","5","29989","<p>My aim is to have a scrollbar that stays at the right-side of a full-screen window, allowing the user to scroll up and down through various different widgets (such as labels &amp; buttons).
From other answers I've seen on this site, I've come to the conclusion that a scrollbar has to be assigned to a canvas in order for it to function properly, which I have tried to include in my code but have not had much success with.</p>

<p>The below code shows a simplified version of what I've managed to accomplish so far:</p>

<pre><code>from tkinter import *
root = Tk()
root.state(""zoomed"")
root.title(""Vertical Scrollbar"")
frame = Frame(root)
canvas = Canvas(frame)
Label(canvas, text = ""Test text 1\nTest text 2\nTest text 3\nTest text 4\nTest text 5\nTest text 6\nTest text 7\nTest text 8\nTest text 9"", font = ""-size 100"").pack()
scrollbar = Scrollbar(frame)
scrollbar.pack(side = RIGHT, fill = Y)
canvas.configure(yscrollcommand = scrollbar.set)
canvas.pack()
frame.pack()
root.mainloop()
</code></pre>

<p>I'm facing two issues when running this code:</p>

<p>One is that the scrollbar is inactive, and doesn't allow for the user to scroll down to view the rest of the text.</p>

<p>The other is that the scrollbar is attached to the right-side of the text, rather than the right-side of the window.</p>

<p>So far, none of the other answers I've found on this site have enabled me to amend my code to support a fully-functioning scrollbar for my program. I'd be very grateful for any help anyone reading this could provide.</p>
","2946537","","","","","2016-11-11 00:28:02","Vertical scrollbar for frame in Tkinter, Python","<python><python-3.x><tkinter><scrollbar><tkinter-canvas>","1","5","0","","","CC BY-SA 3.0","0"
"37958360","1","","","2016-06-22 04:05:15","","2","29951","<p>I'm trying to create a function that compares characters in the same position of two strings of same length and returns the count of their differences.</p>

<p>For instance,</p>

<pre><code>a = ""HORSE""
b = ""TIGER""
</code></pre>

<p>And it would return 5 (as all characters in the same position are different)</p>

<p>Here's what I've been working on.</p>

<pre><code>def Differences(one, two):
    difference = []
    for i in list(one):
        if list(one)[i] != list(two)[i]:
            difference = difference+1
    return difference
</code></pre>

<p>That gives an error ""List indices must be integers not strings""</p>

<p>And so I've tried turning it to int by using int(ord(</p>

<pre><code>def Differences(one, two):
    difference = 0
    for i in list(one):
        if int(ord(list(one)[i])) != int(ord(list(two)[i])):
            difference = difference+1
    return difference
</code></pre>

<p>Which also returns the same error.</p>

<p>When I print list(one)[1] != list(two)[1] it eithers returns True or False, as the comparison is correctly made.</p>

<p>Can you tell me how to correct my code for this purpose?</p>
","6497116","","4817649","","2018-02-05 10:38:17","2020-02-18 13:16:39","Comparing characters in strings","<python><string><python-3.x><comparison><character>","11","1","1","","","CC BY-SA 3.0","0"
"54164630","1","54166934","","2019-01-12 22:56:05","","42","29867","<p>My simple python code is this</p>

<pre><code>import cv2

img=cv2.imread('Materials/shapes.png')

blur=cv2.GaussianBlur(img,(3,3),0)
gray=cv2.cvtColor(blur,cv2.COLOR_BGR2GRAY)
returns,thresh=cv2.threshold(gray,80,255,cv2.THRESH_BINARY)

ret,contours,hierachy=cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)

for cnt in contours:

    area=cv2.contourArea(cnt) #contour area

    if (area&gt;1220):
        cv2.drawContours(img,[cnt],-1,(0,255,0),2)
        cv2.imshow('RGB',img)
        cv2.waitKey(1000)
        print(len(cnt))

import numpy as np

contours=np.array(contours)

print(contours)
</code></pre>

<p>This worked fine. But recently without me even making any changes. This was throwed to me</p>

<blockquote>
  <p>ret,contours,hierachy=cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)</p>
  
  <p>ValueError: not enough values to unpack (expected 3, got 2)</p>
</blockquote>

<p>Help me guys.</p>

<p>Thanks.</p>
","10822059","","10877389","","2019-01-13 02:28:08","2019-10-11 20:51:46","Want to find contours -> ValueError: not enough values to unpack (expected 3, got 2), this appears","<python><python-3.x><opencv-contour>","2","0","9","","","CC BY-SA 4.0","0"
"27892356","1","27892556","","2015-01-11 21:33:15","","22","29856","<p>Is there a way to add key-value-pair into kwargs during the function call?</p>

<pre><code>def f(**kwargs):
    print(kwargs)

# ...

pre_defined_kwargs = {'a': 1, 'b': 2}

f(**pre_defined_kwargs, c=3)
</code></pre>

<p>Or even change the existing arguments?</p>

<pre><code>f(**pre_defined_kwargs, b=3)  # replaces the earlier b=2
</code></pre>

<p>These two examples don't work, as they raise error</p>

<pre><code>&gt;&gt;&gt; f(**pre_defined_kwargs, c=3)
SyntaxError: invalid syntax
</code></pre>

<p>Pointing at the comma in between the arguments</p>
","2505645","","","","","2018-01-03 18:36:21","Add a parameter into kwargs during function call?","<python><python-3.x><keyword-argument>","2","2","1","","","CC BY-SA 3.0","0"
"38394598","1","38395145","","2016-07-15 11:05:52","","13","29821","<p>I am new to django and python. During url mapping to views i am getting following error: 
TypeError: view must be a callable or a list/tuple in the case of include().</p>

<p>Urls. py code:-</p>

<pre><code>from django.conf.urls import url
from django.contrib import admin


urlpatterns = [
    url(r'^admin/', admin.site.urls),
    url(r'^posts/$', ""posts.views.post_home""), #posts is module and post_home 
]                                              # is a function in view. 
</code></pre>

<p>views.py code:-</p>

<pre><code>from django.shortcuts import render
from django.http import HttpResponse
# Create your views here.
#function based views

def post_home(request):
    response = ""&lt;h1&gt;Success&lt;/h1&gt;""
    return HttpResponse(response)
</code></pre>

<p><strong>Traceback</strong></p>

<p><a href=""https://i.stack.imgur.com/NWbLo.png""><img src=""https://i.stack.imgur.com/NWbLo.png"" alt=""enter image description here""></a></p>
","6593806","","3762142","","2016-07-15 11:12:52","2020-05-23 12:57:05","TypeError: view must be a callable or a list/tuple in the case of include()","<django><python-2.7><python-3.x>","7","2","2","","","CC BY-SA 3.0","0"
"43889141","1","43889337","","2017-05-10 09:57:49","","7","29766","<p>So I just started learning Python 3 in school, and we had to make a function  that
takes <strong><em>a</em></strong> as a parameter, chooses a reasonable value of x, and returns an estimate of the square root of <strong><em>a</em></strong>.</p>

<p>We also had to make a function to test it. We had to write a function named test_square_root that prints a table, where The first column is a number, <strong><em>a</em></strong>; the second column is the square root of <strong><em>a</em></strong> computed with the first function;
the third column is the square root computed by math.sqrt; the fourth column is the absolute value of the difference between the two estimates.</p>

<p>I wrote the first function to find the square root, but I don't know how to make a table like that. I've read other questions on here about tables in Python3 but I still don't know how to apply them to my function.</p>

<pre><code>def mysqrt(a):
    for x in range(1,int(1./2*a)):
        while True:
            y = (x + a/x) / 2
            if y == x:
                break
            x = y
    print(x)
print(mysqrt(16))
</code></pre>
","7990937","","7990937","","2017-05-10 10:06:37","2020-07-19 02:12:20","Making a table in Python 3(beginner)","<python><python-3.x>","2","1","2","","","CC BY-SA 3.0","0"
"37796598","1","37796686","","2016-06-13 18:17:35","","10","29738","<p>Is there a simple way to sort files in a directory in python? The files I have in mind come in an ordering as </p>

<pre><code>file_01_001
file_01_005
...
file_02_002
file_02_006
...
file_03_003
file_03_007
...
file_04_004
file_04_008
</code></pre>

<p>What I want is something like</p>

<pre><code>file_01_001
file_02_002
file_03_003
file_04_004
file_01_005
file_02_006
...
</code></pre>

<p>I am currently opening them using <code>glob</code> for the directory as follows:</p>

<pre><code>for filename in glob(path):    
    with open(filename,'rb') as thefile:
        #Do stuff to each file
</code></pre>

<p>So, while the program performs the desired tasks, it's giving incorrect data if I do more than one file at a time, due to the ordering of the files. Any ideas?</p>
","6362677","","","","","2019-07-23 15:51:54","How to sort file names in a particular order using python","<python><sorting><python-3.x><io>","2","2","4","","","CC BY-SA 3.0","0"
"34168806","1","34168892","","2015-12-09 00:38:52","","12","29714","<p>I am still learning Python and currently solving a question on <a href=""https://www.hackerrank.com/challenges/python-tuples"" rel=""nofollow noreferrer"">Hackerrank</a> where I am thinking of converting input(String type) to tuple by using built-in function(tuple(input.split("" "")). </p>

<p>For example, myinput = ""2 3"", and I want to convert it to tuple,such as (2,3). However,if I do that, it will, of course, give me a tuple with String type, like ('2', '3').  I know that there are a lot of ways to solve the problem, but I'd like to know how to convert elements(str) in tuple to integer(in Python) in most efficient way.</p>

<p>See below for more details. </p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; myinput = ""2 3""
&gt;&gt;&gt; temp = myinput.split()
&gt;&gt;&gt; print(temp)
['2', '3']
&gt;&gt;&gt; mytuple = tuple(myinput)
&gt;&gt;&gt; print(mytuple)
('2', ' ', '3')
&gt;&gt;&gt; mytuple = tuple(myinput.split("" ""))
&gt;&gt;&gt; mytuple
('2', '3')
&gt;&gt;&gt; type(mytuple)
&lt;class 'tuple'&gt;
&gt;&gt;&gt; type(mytuple[0])
&lt;class 'str'&gt;
&gt;&gt;&gt; 
</code></pre>

<p>Thanks in advance.</p>
","3745094","","7404","","2019-05-31 17:35:01","2019-12-01 14:46:38","How to convert elements(string) to integer in tuple in Python","<python-3.x><tuples>","2","2","2","","","CC BY-SA 4.0","0"
"33925566","1","33930410","","2015-11-25 20:10:13","","10","29704","<p>I currently have Python 3.5 on my Windows machine. I'm trying to install a Python package using the command ""pip install"" but as soon as I hit enter nothing happens. The action hangs for such a long time and when I try to exit the command line, it freezes. How do I get pip install to work?</p>
","2847130","","","","","2016-06-21 08:32:20","Pip Install hangs","<python><python-3.x><package><pip>","2","3","2","","","CC BY-SA 3.0","0"
"49799057","1","","","2018-04-12 14:22:20","","16","29637","<p>I have one image and one co-ordinate (X, Y). How to draw a point with this co-ordinate on the image. I want to use Python OpenCV.</p>
","2738831","","","","","2020-05-02 18:53:33","How to Draw a point in an image using given co-ordinate with python opencv?","<python-3.x><image><opencv>","2","6","3","","","CC BY-SA 3.0","0"
"29030725","1","","","2015-03-13 11:10:51","","11","29620","<p>I'm trying to decode hex string to binary values. 
I found this below command on internet to get it done, </p>

<pre><code>string_bin = string_1.decode('hex')
</code></pre>

<p>but I got error saying</p>

<pre><code>'str' object has no attrubute 'decode'
</code></pre>

<p>I'm using python v3.4.1</p>
","4302701","","565635","","2015-03-13 11:13:27","2018-04-09 12:14:07","'str' object has no attribute 'decode'","<python><string><python-3.x><binary><hex>","2","0","5","","","CC BY-SA 3.0","0"
"33458074","1","","","2015-11-01 00:35:37","","2","29589","<p>I've posted this before, but I'm reposting it with more detailed information. </p>

<p>This is my assignment:
<a href=""https://i.stack.imgur.com/5nNfk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5nNfk.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/ZtfrW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZtfrW.png"" alt=""enter image description here""></a></p>

<p>And, so far, this is my code:</p>

<pre><code># Define a function that caculates average test scores
def calculate_average(score1, score2, score3, score4, score5):
    total = score1 + score2 + score3 + score4 + score5
    average = total % 5
    print(average)

# Define a function that determines a letter grade
# Based on the given score
def determine_grade(score):
    if score &lt; 60:
        print 'F'
    elif score =&gt; 60 or score &lt;= 69:
        print 'D'
    elif score =&gt; 70 or score &lt;= 79:
        print 'C'
    elif score =&gt; 80 or score &lt;= 89:
        print 'B'
    elif score =&gt; 90 or score &lt;= 100:
        print 'A'
# Define a function that prompts the user to input names and test scores
def input_data():
    score1 = input('Enter score 1:')
    name1 = input('Enter name 1:')
    score2 = input('Enter score 2:')
    name2 = input('Enter name 2:')
    score3 = input('Enter score 3:')
    name3 = input('Enter name 3:')
    score4 = input('Enter score 4:')
    name4 = input('Enter name 4:')
    score5 = input('Enter score 5:')
    name5 = input('Enter name 5:')


# Define a function that displays a letter grade for each score
# followed by the student's name and the average test score
def display_menu():
</code></pre>

<p>As I said in the first post, I don't have a real problem with anything except setting up that table. I'm really confused!</p>

<p>I asked my instructor (this is an online course, BTW), and he said: ""Its just using the print operator, like you have done in previous modules, your printing text , variables and the return value of a function.""</p>

<p>Now I'm just wondering where to start.</p>

<p><strong>EDIT:</strong> I've updated my code this:</p>

<pre><code># Define a function that prompts the user to input names and test scores
def input_data():
    score1 = input('Enter score 1:')
    name1 = input('Enter name 1:')
    score2 = input('Enter score 2:')
    name2 = input('Enter name 2:')
    score3 = input('Enter score 3:')
    name3 = input('Enter name 3:')
    score4 = input('Enter score 4:')
    name4 = input('Enter name 4:')
    score5 = input('Enter score 5:')
    name5 = input('Enter name 5:')

# Define a function that determines a letter grade
# Based on the given score
def determine_grade(score):
    if score &lt; 60:
        print ('F')
    elif 60 &lt;= score &lt;= 69:
        print ('D')
    elif  70 &lt;= score &lt;= 79:
        print ('C')
    elif 80 &lt;= score &lt;= 89:
        print ('B')
    elif 90 &lt;= score &lt;= 100:
        print ('A')

# Define a function that caculates average test scores
def calculate_average(score1, score2, score3, score4, score5):
    total = score1 + score2 + score3 + score4 + score5
    average = total / 5
    print(average)

# Define a function that displays a letter grade for each score
# followed by the student's name and the average test score
def display_menu():
    for x in range(10):
        print(""{:&lt;10}"".format(""{:0.1f}"".format(x)), end='')
    print (""Name\t\t\tnumeric grade\t\tletter grade"")
print (""---------------------------------------------------------------"")
print (""%s:\t\t\t%f\t\t%s"") % ('name1', 'score1', determine_grade)
print (""%s:\t\t\t%f\t\t%s"") % ('name2', 'score2', determine_grade)
print (""%s:\t\t\t%f\t\t%s"") % ('name3', 'score3', determine_grade)
print (""%s:\t\t\t%f\t\t%s"") % ('name4', 'score4', determine_grade)
print (""%s:\t\t\t%f\t\t%s"") % ('name5', 'score5', determine_grade)
print (""---------------------------------------------------------------"")
</code></pre>

<p>But when I run it:
<a href=""https://i.stack.imgur.com/ga6o8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ga6o8.png"" alt=""enter image description here""></a></p>

<p><strong>EDIT #2:</strong> This is my code currently:</p>

<pre><code># Define a function that prompts the user to input names and test scores
def input_data():
    score1 = input('Enter score 1:')
    name1 = input('Enter name 1:')
    score2 = input('Enter score 2:')
    name2 = input('Enter name 2:')
    score3 = input('Enter score 3:')
    name3 = input('Enter name 3:')
    score4 = input('Enter score 4:')
    name4 = input('Enter name 4:')
    score5 = input('Enter score 5:')
    name5 = input('Enter name 5:')

# Define a function that caculates average test scores
def calculate_average(score1, score2, score3, score4, score5):
    total = score1 + score2 + score3 + score4 + score5
    average = total / 5
    print(average)

# Define a function that determines a letter grade
# Based on the given score
def determine_grade(score):
    if score &lt; 60:
        print ('F')
    elif 60 &lt;= score &lt;= 69:
        print ('D')
    elif  70 &lt;= score &lt;= 79:
        print ('C')
    elif 80 &lt;= score &lt;= 89:
        print ('B')
    elif 90 &lt;= score &lt;= 100:
        print ('A')



# Define a function that displays a letter grade for each score
# followed by the student's name and the average test score
def display_menu():
   print (""Name\t\t\tnumeric grade\t\tlettergrade"")
print (""---------------------------------------------------------------"")
print (""%s:\t\t\t%f\t\t%s"" % ('name1', 93, 'A'))
print (""%s:\t\t\t%f\t\t%s"" % ('name2', 89, 'B'))
print (""%s:\t\t\t%f\t\t%s"" % ('name3', 76, 'C'))
print (""%s:\t\t\t%f\t\t%s"" % ('name4', 58, 'F'))
print (""%s:\t\t\t%f\t\t%s"" % ('name5', 98, 'A'))
print (""---------------------------------------------------------------"")
print (calculate_average)
</code></pre>

<p>And this is what happens when I run it:</p>

<p><a href=""https://i.stack.imgur.com/HTDSA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HTDSA.png"" alt=""enter image description here""></a></p>

<p>Now, I have mainly two problems:</p>

<p>1) How do I get the input statements to execute and enter the data BEFORE the table is displayed?</p>

<p>2) How do I convert the numbers displayed so that they are in the '.2f' format? (I've already tried a few ways and none of them worked).</p>

<p><strong>HOPEFULLY THE FINAL EDIT:</strong> I'm getting really close to the solution, but need help with a few more things. </p>

<p>Here is my code:</p>

<pre><code># Define a function that prompts the user to input names and test scores

score = input('Enter score 1:')
name1 = input('Enter name 1:')
score = input('Enter score 2:')
name2 = input('Enter name 2:')
score = input('Enter score 3:')
name3 = input('Enter name 3:')
score = input('Enter score 4:')
name4 = input('Enter name 4:')
score = input('Enter score 5:')
name5 = input('Enter name 5:')




# Define a function that determines a letter grade
# Based on the given score
def determine_grade(score):
    if score &lt; 60:
        print ('F')
    elif 60 &lt;= score &lt;= 69:
        print ('D')
    elif  70 &lt;= score &lt;= 79:
        print ('C')
    elif 80 &lt;= score &lt;= 89:
        print ('B')
    elif 90 &lt;= score &lt;= 100:
        print ('A')

determine_grade(score)


# Define a function that caculates average test scores
def calculate_average(score):
    total = score + score + score + score + score
    average = total / 5
    print(average)

calculate_average(score)

# Define a function that displays a letter grade for each score
# followed by the student's name and the average test score
def display_menu():
   print (""Name\t\t\tnumeric grade\t\tlettergrade"")
print (""---------------------------------------------------------------"")
print (""%s:\t\t\t%f\t\t%s"" % ('name1', 'score', determine_grade('score')))
print (""%s:\t\t\t%f\t\t%s"" % ('name2', 'score', determine_grade('score')))
print (""%s:\t\t\t%f\t\t%s"" % ('name3', 'score', determine_grade('score')))
print (""%s:\t\t\t%f\t\t%s"" % ('name4', 'score', determine_grade('score')))
print (""%s:\t\t\t%f\t\t%s"" % ('name5', 'score', determine_grade('score')))
print (""---------------------------------------------------------------"")
calculate_average(score)
</code></pre>

<p>And this is what happens when I press F5:</p>

<p><a href=""https://i.stack.imgur.com/Sv0Gh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Sv0Gh.png"" alt=""enter image description here""></a></p>

<p><strong>GUYS, I'M ALMOST DONE JUST NEED HELP WITH FORMATTING:</strong> 
I created another file so I could reorganize my code a bit, so that's why there's no comments. This is what I have:</p>

<pre><code>score1 = float(input('Enter score 1:'))
name1 = input('Enter name 1:')
score2 = float(input('Enter score 2:'))
name2 = input('Enter name 2:')
score3 = float(input('Enter score 3:'))
name3 = input('Enter name 3:')
score4 = float(input('Enter score 4:'))
name4 = input('Enter name 4:')
score5 = float(input('Enter score 5:'))
name5 = input('Enter name 5:')


def determine_letter_grade1(score1):
    if score1 &lt; 60.0:
        print ('F')
    elif 60.0 &lt;= score1 &lt;= 69.0:
        print ('D')
    elif  70.0 &lt;= score1 &lt;= 79.0:
        print ('C')
    elif 80.0 &lt;= score1 &lt;= 89.0:
        print ('B')
    elif 90.0 &lt;= score1 &lt;= 100.0:
        print ('A')

def determine_letter_grade2(score2):
    if score2 &lt; 60.0:
        print ('F')
    elif 60.0 &lt;= score2 &lt;= 69.0:
        print ('D')
    elif  70.0 &lt;= score2 &lt;= 79.0:
        print ('C')
    elif 80.0 &lt;= score2 &lt;= 89.0:
        print ('B')
    elif 90.0 &lt;= score2 &lt;= 100.0:
        print ('A')

def determine_letter_grade3(score3):
    if score3 &lt; 60.0:
        print ('F')
    elif 60.0 &lt;= score3 &lt;= 69.0:
        print ('D')
    elif  70.0 &lt;= score3 &lt;= 79.0:
        print ('C')
    elif 80.0 &lt;= score3 &lt;= 89.0:
        print ('B')
    elif 90.0 &lt;= score3 &lt;= 100.0:
        print ('A')

def determine_letter_grade4(score4):
    if score4 &lt; 60.0:
        print ('F')
    elif 60.0 &lt;= score4 &lt;= 69.0:
        print ('D')
    elif  70.0 &lt;= score4 &lt;= 79.0:
        print ('C')
    elif 80.0 &lt;= score4 &lt;= 89.0:
        print ('B')
    elif 90.0 &lt;= score4 &lt;= 100.0:
        print ('A')

def determine_letter_grade5(score5):
    if score5 &lt; 60.0:
        print ('F')
    elif 60.0 &lt;= score5 &lt;= 69.0:
        print ('D')
    elif  70.0 &lt;= score5 &lt;= 79.0:
        print ('C')
    elif 80.0 &lt;= score5 &lt;= 89.0:
        print ('B')
    elif 90.0 &lt;= score5 &lt;= 100.0:
        print ('A')

average = (score1 + score2 + score3 + score4 + score5) / 5.0

def determine_letter_grade_avg(average):
    if average &lt; 60.0:
        print ('F')
    elif 60.0 &lt;= average &lt;= 69.0:
        print ('D')
    elif  70.0 &lt;= average &lt;= 79.0:
        print ('C')
    elif 80.0 &lt;= average &lt;= 89.0:
        print ('B')
    elif 90.0 &lt;= average &lt;= 100.0:
        print ('A')

def display_menu():
    for x in range(10):
        print(""{:&lt;10}"".format(""{:0.1f}"".format(x)), end='')
    print (""Name\t\t\tnumeric grade\t\tletter grade"")
print (""---------------------------------------------------------------"")
print (""%s:\t\t\t%f\t\t%s"" % (name1, score1, determine_letter_grade1(score1)))
print (""%s:\t\t\t%f\t\t%s"" % (name2, score2, determine_letter_grade2(score2)))
print (""%s:\t\t\t%f\t\t%s"" % (name3, score3, determine_letter_grade3(score3)))
print (""%s:\t\t\t%f\t\t%s"" % (name4, score4, determine_letter_grade4(score4)))
print (""%s:\t\t\t%f\t\t%s"" % (name5, score5, determine_letter_grade5(score5)))
print (""---------------------------------------------------------------"")
print ('Average Score:', average, determine_letter_grade_avg(average))
</code></pre>

<p>And when I run it:</p>

<p><a href=""https://i.stack.imgur.com/MLpXR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MLpXR.png"" alt=""enter image description here""></a></p>
","5455163","","1555990","","2015-11-03 21:02:43","2015-11-03 21:02:43","How To Create A Table in Python","<python><function><python-3.x><tabstop>","3","6","2","","","CC BY-SA 3.0","0"
"51110525","1","","","2018-06-29 23:34:48","","4","29567","<p>I have binary file which is this format <code>(b'A\xd9\xa5\x1ab\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x0b\xda\xa5\x1ab\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\xcd\xdb\xa5\x1ab\x00\x00\x00\x04\x00\x00\x00\x01\x00\x00\x00\xff\xdb\xa5\x1ab\x00\x00\x00\x05\x00\x00\x00\x01\x00\x00\x00\xe9\xdc\xa5\x1ab\x00\x00\x00\x06\x00\x00\x00\x02\x00\x00\x00\xf7\xdc\xa5\x1ab\x00\x00\x00\x08\x00\x00\x00\x02\x00\x00\x00\x1b\xdd\xa5\x1a')</code> and I'm taking the file as user input and reading the file in the <code>read_file</code> variable( class bytes object).I need to convert it to ascii using an integer schema (int, int, int, int) each <code>int</code> of 4 bytes. I was trying doing this using <code>struct</code> library to unpack it. I wrote the following commands but it gave me following error:</p>

<p><strong>error</strong></p>

<pre><code>print(unpack(""IIII"", read_file))
struct.error: unpack requires a buffer of 16 bytes
</code></pre>

<p><strong>code</strong></p>

<pre><code>    for (dirpath, dirnames, filenames) in walk('/Users/amathur1/PycharmProjects/learningpython/NAWF_VRG_G'):
        count = 1
        for file in filenames:
            print(count, "" : "", file)
            count = count + 1
        print(""select file you want to convert"")
        input_file = input()
        print(""Selected file number is : "", input_file)

        #To open the selected file
    with open(dirpath + ""/"" + filenames[int(input_file) - 1], 'rb') as file:
        # Reading the selected file i.e. file
        read_file = file.read()
    print(unpack(""IIII"", read_file))
</code></pre>
","10012730","","5859957","","2018-06-30 02:21:43","2020-01-12 18:55:11","Struct.error: unpack requires a buffer of 16 bytes","<python-3.x>","2","1","1","","","CC BY-SA 4.0","0"
"36298231","1","36298241","","2016-03-30 01:51:44","","6","29533","<p>I am just wondering if this following if statement works:</p>

<pre><code>    value=[1,2,3,4,5,f]
    target = [1,2,3,4,5,6,f]
    if value[0] in target OR value[1] in target AND value[6] in target:
       print (""good"")
</code></pre>

<p>My goal is to make sure the following 2 requirements are all met at the same time:
1. value[6] must be in target
2. either value[0] or value[1] in target
Apologize if I made a bad example but my question is that if I could make three AND &amp; OR in one statement?  Many thanks!</p>
","6130082","","","","","2016-03-30 01:57:47","Python IF multiple ""and"" ""or"" in one statement","<python><python-2.7><python-3.x><if-statement>","1","4","1","","","CC BY-SA 3.0","0"
"44391671","1","44400442","","2017-06-06 13:41:33","","4","29473","<p>A month ago I encountered this Github: <a href=""https://github.com/taraslayshchuk/es2csv"" rel=""nofollow noreferrer"">https://github.com/taraslayshchuk/es2csv</a></p>

<p>I installed this package via pip3 in Linux ubuntu. When I wanted to use this package, I encountered the problem that this package is meant for python2. I dived into the code and soon I found the problem.</p>

<pre><code>                for line in open(self.tmp_file, 'r'):
                timer += 1
                bar.update(timer)
                line_as_dict = json.loads(line)
                line_dict_utf8 = {k: v.encode('utf8') if isinstance(v, unicode) else v for k, v in line_as_dict.items()}
                csv_writer.writerow(line_dict_utf8)
            output_file.close()
            bar.finish()
        else:
            print('There is no docs with selected field(s): %s.' % ','.join(self.opts.fields))
</code></pre>

<p>The code did a check for unicode, this is not necessary within python3 Therefore, I changed the code to the code below. As result, The package worked properly under Ubuntu 16.</p>

<pre><code>                for line in open(self.tmp_file, 'r'):
                timer += 1
                bar.update(timer)
                line_as_dict = json.loads(line)
                # line_dict_utf8 = {k: v.encode('utf8') if isinstance(v, unicode) else v for k, v in line_as_dict.items()}
                csv_writer.writerow(line_as_dict)
            output_file.close()
            bar.finish()
        else:
            print('There is no docs with selected field(s): %s.' % ','.join(self.opts.fields))
</code></pre>

<p>But a month later, it was necessary to get the es2csv package working on a Windows 10 operating system. After doing the exact same adjustments with es2csv under Windows 10, I received the following error message after I tried to run es2csv:</p>

<pre><code>    PS C:\&gt; es2csv -u 192.168.230.151:9200 -i scrapy -o database.csv -q '*'
Found 218 results
Run query [#######################################################################################################################] [218/218] [100%] [0:00:00] [Time: 0:00:00] [  2.3 Kidocs/s]
Write to csv [#                                                                                                                     ] [2/218] [  0%] [0:00:00] [ETA: 0:00:00] [  3.9 Kilines/s]T
raceback (most recent call last):
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\Scripts\es2csv-script.py"", line 11, in &lt;module&gt;
    load_entry_point('es2csv==5.2.1', 'console_scripts', 'es2csv')()
  File ""c:\users\admin\appdata\local\programs\python\python36\lib\site-packages\es2csv.py"", line 284, in main
    es.write_to_csv()
  File ""c:\users\admin\appdata\local\programs\python\python36\lib\site-packages\es2csv.py"", line 238, in write_to_csv
    csv_writer.writerow(line_as_dict)
  File ""c:\users\admin\appdata\local\programs\python\python36\lib\csv.py"", line 155, in writerow
    return self.writer.writerow(self._dict_to_list(rowdict))
  File ""c:\users\admin\appdata\local\programs\python\python36\lib\encodings\cp1252.py"", line 19, in encode
    return codecs.charmap_encode(input,self.errors,encoding_table)[0]
UnicodeEncodeError: 'charmap' codec can't encode characters in position 95-98: character maps to &lt;undefined&gt;
</code></pre>

<p>Does anyone has an idea how to fix this error message?</p>
","7785511","","","","","2019-01-08 18:51:07","python3 UnicodeEncodeError: 'charmap' codec can't encode characters in position 95-98: character maps to <undefined>","<python-3.x><elasticsearch-plugin><python-unicode>","1","0","3","","","CC BY-SA 3.0","0"
"49183801","1","53615877","","2018-03-08 22:49:43","","12","29473","<p>I'm running into trouble with the module urllib (Python 3.6). Every time I use the module, I get a page's worth of errors. </p>

<p>what's wrong with urllib and how to fix it?</p>

<pre><code>import urllib.request
url='https://www.goodreads.com/quotes/tag/artificial-intelligence'
u1 = urllib.request.urlopen(url)
print(u1)
</code></pre>

<p>That block of code likes to spit out this mouthful of stuff:</p>

<pre><code>  Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1318, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1026, in _send_output
    self.send(msg)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 964, in send
    self.connect()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py"", line 1400, in connect
    server_hostname=server_hostname)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 407, in wrap_socket
    _context=self, _session=session)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 814, in __init__
    self.do_handshake()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 1068, in do_handshake
    self._sslobj.do_handshake()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/ssl.py"", line 689, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/a-nguyen/Downloads/EzSorses/APAsauce.py"", line 3, in &lt;module&gt;
    u1 = urllib.request.urlopen(url)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 223, in urlopen
    return opener.open(url, data, timeout)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 526, in open
    response = self._open(req, data)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 544, in _open
    '_open', req)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 504, in _call_chain
    result = func(*args)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1361, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py"", line 1320, in do_open
    raise URLError(err)
urllib.error.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)&gt;
</code></pre>

<p>Seems like something's off with the module itself.</p>
","9132929","","4420967","","2018-03-09 08:39:16","2020-04-06 12:04:59","SSL: CERTIFICATE_VERIFY_FAILED with urllib","<python-3.x><urllib>","4","2","7","","","CC BY-SA 3.0","0"
"29105941","1","29115017","","2015-03-17 17:35:19","","17","29435","<p>I've been trying to modify the default Python compiler/run command in Geany.</p>

<p>Some searching indicated that I would need to modify `/usr/share/geany/filetypes.python's last two lines as follows</p>

<pre><code>#compiler=python -m py_compile ""%f""
#run_cmd=python ""%f""
compiler=python3 -c ""import py_compile; py_compile.compile('%f')""
run_cmd=python3 ""%f""
</code></pre>

<p>After restarting Geany however, <code>Build -&gt; Set Build Commands</code> still shows the old commands and attemping to run a py3 script causes errors.</p>
","3817250","","2915834","","2015-03-18 14:58:36","2019-06-29 19:38:46","How do I make Python3 the default Python in Geany","<python><python-3.x><geany>","2","0","5","","","CC BY-SA 3.0","0"
"39282309","1","","","2016-09-01 23:33:07","","3","29396","<p>I usually work on PC's but started to work on projects on my mac. I run Python 3 and when I started a new project I did the following:</p>

<p>1) In main project folder, installed virtualenv and activated it.</p>

<p>2) Install Django and Gunicorn</p>

<p>3) Did startproject</p>

<p>When I try to python3 manage.py startapp www I get an error that Django could not be imported. Below is what was in the terminal:</p>

<pre><code>(venv) AB:directory AB$ pip freeze
Django==1.10
gunicorn==19.6.0

(venv) AB:directory AB$ ls
directory   manage.py

(venv) AB:directory AB$ python3 manage.py startpap www

Traceback (most recent call last):
  File ""manage.py"", line 8, in &lt;module&gt;
    from django.core.management import execute_from_command_line
ImportError: No module named 'django'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""manage.py"", line 14, in &lt;module&gt;
    import django
ImportError: No module named 'django'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""manage.py"", line 17, in &lt;module&gt;
    ""Couldn't import Django. Are you sure it's installed and ""
ImportError: Couldn't import Django. Are you sure it's installed and available on your PYTHONPATH environment variable? Did you forget to activate a virtual environment?
</code></pre>
","3088202","","","","","2019-03-20 03:09:15","Couldn't import Django error when I try to startapp","<python><django><python-3.x>","3","6","2","","","CC BY-SA 3.0","0"
"48924165","1","","","2018-02-22 09:46:41","","21","29304","<p>I created a model using Keras library and saved the model as .json and its weights with .h5 extension. How can I download this onto my local machine?</p>

<p>to save the model I followed this <a href=""https://machinelearningmastery.com/save-load-keras-deep-learning-models/"" rel=""noreferrer"">link</a></p>
","9106969","","4566277","","2019-08-11 20:40:05","2020-02-27 05:12:16","google colaboratory, weight download (export saved models)","<python-3.x><google-colaboratory>","7","0","15","","","CC BY-SA 3.0","0"
"47113472","1","47859280","","2017-11-04 17:27:10","","15","29223","<p>I am trying to use Tensorboard but every time I run any program with Tensorflow, I get an error when I go to localhost:6006 to view the Visualization</p>

<p>Here is my code</p>

<pre><code>a = tf.add(1, 2,)
b = tf.multiply(a, 3)

with tf.Session() as sess:
    writer = tf.summary.FileWriter(""output"", sess.graph)
    print(sess.run(b))
    writer.close()
</code></pre>

<p>When I go to the command prompt and enter</p>

<pre><code>tensorboard --logdir=C:\path\to\output\folder
</code></pre>

<p>It returns with </p>

<pre><code>TensorBoard 0.1.8 at http://MYCOMP:6006 (Press CTRL+C to quit)
</code></pre>

<p>When I go to localhost:6006 it states</p>

<blockquote>
  <p>No dashboards are active for the current data set.
  Probable causes:
  - You haven’t written any data to your event files.
  - TensorBoard can’t find your event files.</p>
</blockquote>

<p>I have looked at this link (<a href=""https://stackoverflow.com/questions/46108507/tensorboard-no-dashboards-are-active-for-the-current-data-set"">Tensorboard: No dashboards are active for the current data set</a>) but it doesn't seem to fix this issue</p>

<p>And I am running this on Windows 10</p>

<p>What do I do to fix this issue? Am I giving the right path for Tensorboard in the command prompt? </p>

<p>Thank you in advance</p>
","7359915","","7359915","","2017-11-05 00:31:23","2020-03-23 17:55:15","Tensorboard Error: No dashboards are active for current data set","<python-3.x><tensorflow><tensorboard>","10","0","6","","","CC BY-SA 3.0","0"
"30469575","1","30469744","","2015-05-26 21:51:57","","28","29184","<p>I need to pickle a Python3 object to a string which I want to unpickle from an environmental variable in a Travis CI build. The problem is that I can't seem to find a way to pickle to a portable string (unicode) in Python3:</p>

<pre class=""lang-python prettyprint-override""><code>import os, pickle    

from my_module import MyPickleableClass


obj = {'cls': MyPickleableClass, 'other_stuf': '(...)'}

pickled = pickle.dumps(obj)

# raises TypeError: str expected, not bytes
os.environ['pickled'] = pickled

# raises UnicodeDecodeError: 'utf-8' codec can't decode byte 0xbb (...)
os.environ['pickled'] = pickled.decode('utf-8')

pickle.loads(os.environ['pickled'])
</code></pre>

<p>Is there a way to serialize complex objects like <code>datetime.datetime</code> to unicode or to some other string representation in Python3 which I can transfer to a different machine and deserialize?</p>

<h2>Update</h2>

<p>I have tested the solutions suggested by @kindall, but the <code>pickle.dumps(obj, 0).decode()</code> raises a <code>UnicodeDecodeError</code>. Nevertheless the <strong>base64</strong> approach works but it needed an <strong>extra decode/encode</strong> step. The solution works on both Python2.x and Python3.x.</p>

<pre class=""lang-python prettyprint-override""><code># encode returns bytes so it needs to be decoded to string
pickled = pickle.loads(codecs.decode(pickled.encode(), 'base64')).decode()

type(pickled)  # &lt;class 'str'&gt;

unpickled = pickle.loads(codecs.decode(pickled.encode(), 'base64'))
</code></pre>
","1493220","","1493220","","2015-05-27 11:39:00","2020-09-16 01:11:29","How to pickle and unpickle to portable string in Python 3","<python><python-3.x><serialization><unicode>","3","2","6","","","CC BY-SA 3.0","0"
"32675679","1","32676625","","2015-09-20 04:04:07","","9","29180","<p>Despite the many related questions, I can't find any that match my problem.  I'd like to change a binary string (for example, <code>""0110100001101001""</code>) into a byte array (same example, <code>b""hi""</code>).</p>

<p>I tried this:  </p>

<pre><code>bytes([int(i) for i in ""0110100001101001""])
</code></pre>

<p>but I got:</p>

<pre><code>b'\x00\x01\x01\x00\x01' #... and so on
</code></pre>

<p>What's the correct way to do this in Python 3?</p>
","2577654","","2577654","","2015-09-21 14:21:26","2015-09-21 14:21:26","Convert binary string to bytearray in Python 3","<python><python-3.x><binary><bytearray>","3","7","3","","","CC BY-SA 3.0","0"
"33633954","1","33634074","","2015-11-10 15:46:23","","12","29156","<p>I am simply trying to parse an XML file:</p>

<pre><code>import xml.etree.ElementTree as ET
tree = ET.parse('country_data.xml')
root = tree.getroot()
</code></pre>

<p>but this gives me:</p>

<pre><code>import xml.etree.ElementTree as ET
ImportError: No module named 'xml.etree'
</code></pre>

<p>I am using Python 3.5. I have tried to same code with Python 2.7 and 3.4 but I always get this error. I thought that the XML libraries come as standard. Also, I can see that in my Lib folder:</p>

<p><a href=""https://i.stack.imgur.com/8F9yz.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8F9yz.png"" alt=""enter image description here""></a></p>

<p>So why can't it pick up the module? I am really confused. Do I have to make some change in an environment variable somewhere?</p>

<p>Please help.</p>
","2334092","","837534","","2016-12-28 11:58:34","2020-08-17 15:46:26","Python Error : ImportError: No module named 'xml.etree'","<python><xml><python-3.x>","3","1","1","","","CC BY-SA 3.0","0"
"29075601","1","","","2015-03-16 11:22:52","","1","29145","<p>This should be an easy one for experienced coders out there... I am writing a program that outputs anagrams (from a given list) from a jumbled input. I believe my code includes all the necessary functions to produce the desired result, but I cannot figure out how to make the program run the respective functions in order. A sample output is:</p>

<pre><code>Please enter a jumbled word: lsitNe
Your words are:
silent
listen
enlist
tinsel
</code></pre>

<p>This is my output, currently:</p>

<pre><code>  Please enter a jumbled word: dff
Traceback (most recent call last):
  File ""/Users/edinnerman/Desktop/poop.py"", line 53, in &lt;module&gt;
    main()
  File ""/Users/edinnerman/Desktop/poop.py"", line 5, in main
    yay_anagrams = anagramlist(lword)
NameError: name 'lword' is not defined
</code></pre>

<p>How do I reformat my code to output all anagrams listed in a text file given a user input?
Here is my code:</p>

<pre><code> #0: Create a main function to bring all functions together for the output.
def main():
    textfile = open('words.txt', 'r').read()
    dictionary_of_words = filename(textfile)
    yay_anagrams = anagramlist(lword)
    double_whammy = binary_search(fresh_list, ltextfile)
    answer = output()

#1, 2: Take a filename as a parameter and ask user for word to unjumble. Lowercase all.
def filename(textfile):
    ltextfile = textfile.lower()
    scrambled_eggs = input(""Please enter a jumbled word: "")
    lword = scrambled_eggs.lower()

#3: Create an anagram list (already lowercase) for the jumbled word.
def anagramlist(lword):
    if lword == """":
        return([lword])
    else:
        yay_word = lword[1:]
        first_letter = lword[0]
        fresh_list = []
        for mixed_word in filename(yay_word):
            for pos in range(len(mixed_word) + 1):
                fresh_list.append(mixed_word[:pos] + first_letter[0] + mixed_word[pos:])
            return fresh_list

#4: Check if any words in anagram list match dictionary list.
def binary_search(fresh_list, ltextfile):
    for eachword in fresh_list:
        low = 0
        high = len(list) - 1 
    while low &lt;= high:
        mid = (low + high)//2
        item = list[mid]
    if fresh_list == item:
        return True
    elif fresh_list &lt; item:
        high = mid - 1
    elif fresh_list &gt; item:
        low = mid + 1
    return False

#5: Print appropriate statements.
def output():
    if fresh_list == True:
        newlist = set()
        for item in fresh_list:
            newlist.add(item)
        print(""Your words are:/n"", newlist)
    if fresh_list == False:
        print(""Your word cannot be unjumbled."")       
main()
</code></pre>

<p>Please excuse some of the minor coding errors I might have in this code. </p>
","4494585","","355230","","2018-12-18 12:20:32","2018-12-18 12:20:32","How to run multiple functions in order?","<python><function><python-3.x>","1","10","0","2015-03-16 13:11:22","","CC BY-SA 4.0","0"
"35957085","1","35957133","","2016-03-12 11:31:04","","4","29137","<p>i have the task to get the String <code>'AAAABBBCCDAABBB'</code> into a list like this: <code>['A','B','C','D','A','B']</code></p>

<p>I am working on this for 2 hours now, and i can't get the solution. This is my code so far: </p>

<pre><code>list = []

string = 'AAAABBBCCDAABBB'

i = 1

for i in string:
    list.append(i)

print(list)

for element in list:
    if list[element] == list[element-1]:
        list.remove(list[element])

print(list)
</code></pre>

<p>I am a newbie to programming, and the error ""TypeError: list indices must be integers or slices, not str"" always shows up...</p>

<p>I already changed the comparison</p>

<pre><code>if list[element] == list[element-1]
</code></pre>

<p>to</p>

<pre><code>if list[element] is list[element-1]
</code></pre>

<p>But the error stays the same. I already googled a few times, but there were always lists which didn't need the string-format, but i need it (am i right?).</p>

<p>Thank you for helping!
<em>NoAbL</em></p>
","6053642","","3203213","","2016-03-12 14:32:33","2016-03-12 14:32:33","Python3 TypeError: list indices must be integers or slices, not str","<string><list><python-3.x><int><typeerror>","4","2","2","","","CC BY-SA 3.0","0"
"53688225","1","","","2018-12-09 00:04:53","","5","29096","<p>I have the code below with cv2 . This code is downloaded from <a href=""https://github.com/dipakkr/3d-cnn-action-recognition"" rel=""noreferrer"">https://github.com/dipakkr/3d-cnn-action-recognition</a>. I want to use cv2.imshow to visualize the frames of the video it get. But I get the following error. What is the problem?
I am doubtful of whether this code is really able to read the video as what is returns as the output is an array of zeros. </p>

<pre><code>def video3d(self, filename, color=False, skip=True):

        cap = cv2.VideoCapture(filename)
        #ret, frame=cap.read()
        #cv2.imshow('frame', frame)
        nframe = cap.get(cv2.CAP_PROP_FRAME_COUNT) #Returns the specified VideoCapture property  ,,Number of frames in the video file

        print (nframe, ""nframe"")

        if skip:
            frames = [x * nframe / self.depth for x in range(self.depth)]
            print (frames, ""frameeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeees"")

        else:
            frames = [x for x in range(self.depth)]
            print (frames, ""frameseeeeeeeeeeeeeeeeeeeeeeeeeeeeeee2"")

        framearray = []

        for i in range(self.depth):
            cap.set(cv2.CAP_PROP_POS_FRAMES, frames[i])  #Sets a property in the VideoCapture. ,,0-based index of the frame to be decoded/captured next.
            ret, frame = cap.read()
            cv2.imshow(frame)
            print(ret, ""reeeeeeeeeeeeeeeeettttttttt"")
            print(frame ,""frame issssssssssss:"")
            frame = cv2.resize(frame, (self.height, self.width))
            print(frame, ""frame222 isssssssssssssss"")
            #cv2.imshow(frame)
            if color:
                framearray.append(frame)
            else:
                framearray.append(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))

        cap.release()
        return np.array(framearray)


X.append(vid3d.video3d(v_file_path, color=color, skip=skip))
</code></pre>

<p>Error:</p>

<pre><code>    main()
  File ""3dcnn.py"", line 151, in main
    args.output, args.color, args.skip)
  File ""3dcnn.py"", line 103, in loaddata
    X.append(vid3d.video3d(v_file_path, color=color, skip=skip))
  File ""/home/gxa131/Documents/final_project_computationalintelligence/3d-cnn-action-recognition/videoto3d.py"", line 34, in video3d
    cv2.imshow(frame)
TypeError: Required argument 'mat' (pos 2) not found
</code></pre>
","10725316","","10725316","","2018-12-09 00:11:06","2020-06-08 12:07:08","TypeError: Required argument 'mat' (pos 2) not found","<python><python-3.x><opencv><cv2>","3","0","1","","","CC BY-SA 4.0","0"
"36926077","1","36926259","","2016-04-28 21:48:00","","14","29052","<p>I am trying to pass an array to the python </p>

<pre><code>import sys
arr = sys.argv[1]
print(arr[2])
</code></pre>

<p>My command is </p>

<pre class=""lang-bash prettyprint-override""><code>python3 test.py [1,2,3,4,5] 0
</code></pre>

<p>I hope the result it </p>

<pre><code>2
</code></pre>

<p>However, it is</p>

<pre><code>,
</code></pre>
","5771655","","699305","","2016-04-28 22:16:18","2016-04-28 22:21:46","How to pass an array to python through command line","<python><python-3.x>","2","3","","2016-04-29 19:38:30","","CC BY-SA 3.0","0"
"42045362","1","42054155","","2017-02-04 20:40:33","","14","29042","<p>I have a program that's supposed to change the contrast, but I feel like it's not really changing the contrast.It changes some areas to red whereas I don't want it to. If you could tell me how to remove them, thank you.
Here is the code:</p>

<pre><code>from PIL import Image


def change_contrast(img, level):

    img = Image.open(""C:\\Users\\omar\\Desktop\\Site\\Images\\obama.png"")
    img.load()

    factor = (259 * (level+255)) / (255 * (259-level))
    for x in range(img.size[0]):
        for y in range(img.size[1]):
            color = img.getpixel((x, y))
            new_color = tuple(int(factor * (c-128) + 128) for c in color)
            img.putpixel((x, y), new_color)

    return img

result = change_contrast('C:\\Users\\omar\\Desktop\\Site\\Images\\test_image1.jpg', 100)
result.save('C:\\Users\\omar\\Desktop\\Site\\Images\\test_image1_output.jpg')
print('done')
</code></pre>

<p>And here is the image and its result:</p>

<p><a href=""https://i.stack.imgur.com/j5UID.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/j5UID.png"" alt=""obama.png""></a>
<a href=""https://i.stack.imgur.com/hONkZ.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hONkZ.jpg"" alt=""obama modified""></a></p>

<p>If this is the actual contrast method, feel free to tell me</p>
","7453178","","","","","2018-11-27 14:29:09","Change contrast of image in PIL","<python><python-3.x><python-imaging-library>","2","2","5","","","CC BY-SA 3.0","0"
"51940081","1","","","2018-08-21 00:28:44","","3","29034","<p>My following python code works for Python 2, </p>

<h3>Write header only once</h3>

<pre><code>if header_written == False:
    header = out_data.keys()
    writer.writerow(out_data.keys()) # write headers
    header_written = True
</code></pre>

<h3>Write values</h3>

<pre><code>writer.writerow(out_data.values()) #write rows
del out_data  #del object
del row_data #del dict object
</code></pre>

<p>but in Python 3, it returns the following error:</p>

<blockquote>
  <p>TypeError: a bytes-like object is required, not 'str'</p>
</blockquote>
","7780956","","7385033","","2019-06-10 16:34:01","2020-03-22 12:34:59","string - Python 3.5 write a bytes-like object is required, not 'str'","<string><python-3.x><types>","2","0","1","","","CC BY-SA 4.0","0"
"35028683","1","","","2016-01-27 04:05:01","","20","28995","<p>Trying to create a twitter bot that reads lines and posts them. Using Python3 and tweepy, via a virtualenv on my shared server space. This is the part of the code that seems to have trouble:</p>

<pre><code>#!/foo/env/bin/python3

import re
import tweepy, time, sys

argfile = str(sys.argv[1])

filename=open(argfile, 'r')
f=filename.readlines()
filename.close()
</code></pre>

<p>this is the error I get:</p>

<pre><code>UnicodeDecodeError: 'ascii' codec can't decode byte 0xfe in position 0: ordinal not in range(128)
</code></pre>

<p>The error specifically points to <code>f=filename.readlines()</code> as the source of the error. Any idea what might be wrong? Thanks.</p>
","5742881","","364696","","2016-12-14 22:05:55","2017-01-14 17:27:12","Python3 UnicodeDecodeError with readlines() method","<python><python-3.x><unicode><tweepy><sys>","3","4","4","","","CC BY-SA 3.0","0"
"28524378","1","","","2015-02-15 08:40:49","","33","28992","<p>In Python 2 I could do the following:</p>

<pre><code>import numpy as np    
f = lambda x: x**2
seq = map(f, xrange(5))
seq = np.array(seq)
print seq
# prints: [ 0  1  4  9 16]
</code></pre>

<p>In Python 3 it does not work anymore:</p>

<pre><code>import numpy as np    
f = lambda x: x**2
seq = map(f, range(5))
seq = np.array(seq)
print(seq)
# prints: &lt;map object at 0x10341e310&gt;
</code></pre>

<p>How do I get the old behaviour (converting the <code>map</code> results to <code>numpy</code> array)?</p>

<p><strong>Edit</strong>: As @jonrsharpe pointed out in his answer this could be fixed if I converted <code>seq</code> to a list first:</p>

<pre><code>seq = np.array(list(seq))
</code></pre>

<p>but I would prefer to avoid the extra call to <code>list</code>.</p>
","74342","","74342","","2015-02-15 09:06:12","2015-02-15 13:18:59","Convert map object to numpy array in python 3","<python><python-3.x><numpy>","2","2","4","","","CC BY-SA 3.0","0"
"32839561","1","32839651","","2015-09-29 08:42:59","","4","28978","<p>I have a list in python like the following:</p>

<pre><code>edge =  [[1, 2], [1, 3], [1, 4], [3, 4]]
</code></pre>

<p>I want to print <code>1</code> and <code>1</code> and <code>1</code> and <code>3</code>; aka the first element of each sub-list.</p>

<p>I use this code: </p>

<pre><code>for subList in edge:
    for firstItem in subList:
        print(firstItem)
</code></pre>

<p>But it prints all elements..</p>
","4975410","","6162307","","2018-05-14 06:15:27","2019-06-11 04:17:20","Python - How to get first item in list on list in python3","<list><python-3.x><element>","1","0","0","","","CC BY-SA 4.0","0"
"36606771","1","36607040","","2016-04-13 18:35:05","","7","28955","<p>I created this simple UI with qtDesigner and I want to update my label every 10 seconds with the value of a function, but I have no idea how to do this.I've tried different things but nothing worked.</p>

<pre><code>def example():
    ...
    return text
</code></pre>

<p>UI:</p>

<pre><code>class Ui_Form(object):
    def setupUi(self, Form):
        Form.setObjectName(""Form"")
        Form.resize(400, 300)
        self.label = QtWidgets.QLabel(Form)
        self.label.setGeometry(QtCore.QRect(165, 125, 61, 16))
        self.label.setObjectName(""label"")

        self.retranslateUi(Form)
        QtCore.QMetaObject.connectSlotsByName(Form)

    def retranslateUi(self, Form):
        _translate = QtCore.QCoreApplication.translate
        Form.setWindowTitle(_translate(""Form"", ""Form""))
        self.label.setText(_translate(""Form"", plsupdatethis)

if __name__ == ""__main__"":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    Form = QtWidgets.QWidget()
    ui = Ui_Form()
    ui.setupUi(Form)
    Form.show()
    sys.exit(app.exec_())
</code></pre>
","6200221","","1547004","","2016-04-13 18:43:04","2017-04-19 17:21:45","pyQt: How do I update a label?","<python><python-3.x><pyqt5><qt-designer><qtwidgets>","1","0","3","","","CC BY-SA 3.0","0"
"32922210","1","35477608","","2015-10-03 11:50:18","","27","28942","<p>The code you see above is just a sample but it works to reproduce this error:</p>

<pre><code>sqlalchemy.exc.IntegrityError: (raised as a result of Query-invoked autoflush; 
consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.IntegrityError) NOT NULL constraint failed: X.nn 
[SQL: 'INSERT INTO ""X"" (nn, val) VALUES (?, ?)'] [parameters: (None, 1)]
</code></pre>

<p>A mapped instance is still added to a session. The instance wants to know (which means query on the database) if other instances its own type exists having the same values. There is a second attribute/column (<code>_nn</code>). It is specified to <code>NOT NULL</code>. But by default it is <code>NULL</code>.</p>

<p>When the instance (like in the sample) is still added to the session a call to <code>query.one()</code> invoke a auto-flush. This flush create an <code>INSERT</code> which tries to store the instance. This fails because <code>_nn</code> is still null and violates the <code>NOT NULL</code> constraint.</p>

<p>That is what I understand currently.
But the question is why does it invoke an auto-flush? Can I block that?</p>

<pre><code>#!/usr/bin/env python3

import os.path
import os
import sqlalchemy as sa 
import sqlalchemy.orm as sao
import sqlalchemy.ext.declarative as sad
from sqlalchemy_utils import create_database

_Base = sad.declarative_base()
session = None


class X(_Base):
    __tablename__ = 'X'

    _oid = sa.Column('oid', sa.Integer, primary_key=True)
    _nn = sa.Column('nn', sa.Integer, nullable=False) # NOT NULL!
    _val = sa.Column('val', sa.Integer)

    def __init__(self, val):
        self._val = val

    def test(self, session):
        q = session.query(X).filter(X._val == self._val)
        x = q.one()
        print('x={}'.format(x))

dbfile = 'x.db'

def _create_database():
    if os.path.exists(dbfile):
        os.remove(dbfile)

    engine = sa.create_engine('sqlite:///{}'.format(dbfile), echo=True)
    create_database(engine.url)
    _Base.metadata.create_all(engine)
    return sao.sessionmaker(bind=engine)()


if __name__ == '__main__':
    session = _create_database()

    for val in range(3):
        x = X(val)
        x._nn = 0
        session.add(x)
    session.commit()

    x = X(1)
    session.add(x)
    x.test(session)
</code></pre>

<p>Of course a solution would be to <em>not</em> add the instance to the session before <code>query.one()</code> was called. This work. But in my real (but to complex for this question) use-case it isn't a nice solution.</p>
","4865723","","4865723","","2019-07-16 11:24:21","2019-07-16 11:24:21","Why does a query invoke a auto-flush in SQLAlchemy?","<python><sql><python-3.x><sqlalchemy>","1","0","5","","","CC BY-SA 3.0","0"
"54063285","1","","","2019-01-06 15:57:40","","27","28903","<p>I am using the Anaconda distribution with Python 3.7. Among the packages installed, I have numpy, pandas, etc.
In PyCharm IDE, I have set the Project Interpreter to be the path to the python.exe installed with Anaconda:
C:\Users\my_user_name\AppData\Local\Continuum\anaconda3\python.exe</p>
<p>However, when I try running a simple python script:</p>
<pre><code>import numpy as np
print(np.pi)
</code></pre>
<p>I get an error:</p>
<pre><code>ImportError: 
Importing the multiarray numpy extension module failed.  Most
likely you are trying to import a failed build of numpy.
If you're working with a numpy git repo, try `git clean -xdf` (removes all
files not under version control).  Otherwise reinstall numpy.

Original error was: DLL load failed: The specified module could not be found.

See the full error message below:

----------------------------------------------------------------------------------
Traceback (most recent call last):
  File &quot;C:\Users\my_user_name\AppData\Local\Continuum\anaconda3\lib\site-packages\numpy\core\__init__.py&quot;, line 16, in &lt;module&gt;
    from . import multiarray
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:/Users/my_user_name/Documents/TestMyApps/simpletest.py&quot;, line 1, in &lt;module&gt;
    import numpy as np
  File &quot;C:\Users\my_user_name\AppData\Local\Continuum\anaconda3\lib\site-packages\numpy\__init__.py&quot;, line 142, in &lt;module&gt;
    from . import add_newdocs
  File &quot;C:\Users\my_user_name\AppData\Local\Continuum\anaconda3\lib\site-packages\numpy\add_newdocs.py&quot;, line 13, in &lt;module&gt;
    from numpy.lib import add_newdoc
  File &quot;C:\Users\my_user_name\AppData\Local\Continuum\anaconda3\lib\site-packages\numpy\lib\__init__.py&quot;, line 8, in &lt;module&gt;
    from .type_check import *
  File &quot;C:\Users\my_user_name\AppData\Local\Continuum\anaconda3\lib\site-packages\numpy\lib\type_check.py&quot;, line 11, in &lt;module&gt;
    import numpy.core.numeric as _nx
  File &quot;C:\Users\my_user_name\AppData\Local\Continuum\anaconda3\lib\site-packages\numpy\core\__init__.py&quot;, line 26, in &lt;module&gt;
    raise ImportError(msg)
ImportError: 
Importing the multiarray numpy extension module failed.  Most
likely you are trying to import a failed build of numpy.
If you're working with a numpy git repo, try `git clean -xdf` (removes all
files not under version control).  Otherwise reinstall numpy.

Original error was: DLL load failed: The specified module could not be found.


Process finished with exit code 1

----------------------------------------------------------------------------------
</code></pre>
<p>I have tried uninstalling and reinstalling numpy using Anaconda Prompt with:
conda uninstall numpy, and then: conda install numpy [I now have numpy 1.15.4].
The re-install seem 'successful' (at least according to Anaconda Prompt), but, I keep getting an error in PyCharm.</p>
<p>The strange part, is when I directly go into Anaconda Prompt, and type:</p>
<pre><code>python
import numpy as np
print(np.pi)
</code></pre>
<p>I do not get any error, and I am able to see the <a href=""https://imgur.com/a/Z6qHBIY"" rel=""nofollow noreferrer"">correct result printed out</a>.
At first, I thought that perhaps, when using PyCharm, I am somehow pointing to a different executable, but I checked in both Anaconda Prompt and Python Console of Pycharm, using:</p>
<pre><code>sys.executable
</code></pre>
<p>and they both show the same path:</p>
<pre><code>'C:\\Users\\my_user_name\\AppData\\Local\\Continuum\\anaconda3\\python.exe'
</code></pre>
<p>Note that in PyCharm, when I try running a script where I am not importing any library, or when the library imported are just like sys or os, the scripts run fine. However, when I try running any other script that involves importing a library that somehow depends on numpy, it fails as well (i.e. scripts where I import pandas, etc.)
Those same scripts work fine in Anaconda Prompt.</p>
<p>I am at a loss here, any help would be very appreciated!</p>
","7669666","","12526884","","2020-07-23 00:30:33","2020-07-23 00:30:33","numpy is already installed with Anaconda but I get an ImportError (DLL load failed: The specified module could not be found)","<python><python-3.x><numpy><pycharm>","5","2","9","","","CC BY-SA 4.0","0"
"39541394","1","39541559","","2016-09-16 23:31:36","","5","28881","<p>I'm trying to experiment with simple code to send an email from a Python script. I keep getting an error that the module 'email.MIMEMultipart' does not exist. To simplify the question/answer process, I can narrow it down even further. From the Python environment prompt I can enter</p>
<h1>&gt;&gt;&gt;import email</h1>
<h1>&gt;&gt;&gt; dir(email)</h1>
<p>It will list a bunch of modules in the email module, but none of the MIME modules are there. I can see them from WindowsExplorer in the same lib folder as all the other modules. After searching other questions I did see that I had named my test program 'email.py' which I see now is a big no-no. I deleted it and verified there isn't also an 'email.pyc'.</p>
<p>I'm using Windows 10, Python 3.5.2. I've also already reinstalled Python with no improvement.</p>
<p>Can anyone tell me what else I should check?
This is the actual code:</p>
<pre><code> import smtplib
 from email.MIMEMultipart import MIMEMultipart
</code></pre>
","6776724","","-1","","2020-06-20 09:12:55","2020-02-05 17:40:48","Why am I getting the error: No module named 'email.MIMEMultipart'?","<python><python-3.x><email><module>","1","1","1","","","CC BY-SA 3.0","0"
"46064070","1","46064138","","2017-09-05 22:16:20","","11","28846","<p>In Python 3 I'm getting error <code>TypeError: a bytes-like object is required, not 'bytearray'</code></p>

<p>I have a bytearray, which looks like this:</p>

<pre><code> &gt;&gt;&gt; print(my_ba)
 bytearray(b'}\x0e\x15/ow4|-')
</code></pre>

<p>If I enter this in the console it works:</p>

<pre><code> &gt;&gt;&gt; print(base58.b58encode(b'}\x0e\x15/ow4|-'))
 2bKmhuGiGP7t8
</code></pre>

<p>But this gives an error, and I can't find out how to get the b'' string from the bytearray:</p>

<pre><code> &gt;&gt;&gt; print(base58.b58encode(my_ba)
 TypeError: a bytes-like object is required, not 'bytearray'
</code></pre>

<p>I'm sure it's obvious, but how do I convert the bytearray to a string with a b prefix?</p>
","150877","","","","","2017-09-05 22:23:54","Convert bytearray to bytes-like object?","<arrays><python-3.x>","1","3","1","","","CC BY-SA 3.0","0"
"47755442","1","","","2017-12-11 14:49:23","","25","28709","<p>What does it mean to vectorize for-loops in Python? Is there another way to write nested for-loops?</p>

<p>I am new to Python and on my research, I always come across the NumPy library. </p>
","6817397","","4230591","","2019-07-13 17:26:29","2019-07-13 17:26:29","What is vectorization?","<python><python-3.x><numpy><vectorization>","2","0","8","2017-12-12 02:41:41","","CC BY-SA 4.0","0"
"31758329","1","31758360","","2015-08-01 04:44:46","","11","28661","<p>I need to get the current date without time and then compare that date with other entered by user</p>

<pre><code>import datetime
now = datetime.datetime.now()
</code></pre>

<p>Example</p>

<pre><code>currentDate=""01/08/2015""
userDate=""01/07/2015""
if currentData &gt; userDate:
 .........
else:
  ...................
</code></pre>
","4840020","","","","","2020-08-12 10:09:05","create date in python without time","<python><python-2.7><python-3.x>","2","0","4","","","CC BY-SA 3.0","0"
"53376786","1","53379777","","2018-11-19 14:30:41","","24","28599","<p>You can convert a numpy array to bytes using <code>.tobytes()</code> function. </p>

<p>How do decode it back from this bytes array to numpy array?
I tried like this for array i of shape (28,28)</p>

<p><code>&gt;&gt;k=i.tobytes()</code></p>

<p><code>&gt;&gt;np.frombuffer(k)==i</code></p>

<p><code>False</code></p>

<p>also tried with uint8 as well.</p>
","6208328","","6208328","","2019-02-15 15:57:14","2020-06-12 20:38:19","Convert byte array back to numpy array","<python><python-3.x><numpy>","2","2","3","","","CC BY-SA 4.0","0"
"30647219","1","32243566","","2015-06-04 14:52:53","","28","28597","<p>I'm trying to make a HTTPS connection in Python3 and when I try to encode my username and password the <code>base64</code> <code>encodebytes</code> method returns the encoded value with a new line character at the end ""\n"" and because of this I'm getting an error when I try to connect. </p>

<p>Is there a way to tell the <code>base64</code> library not to append a new line character when encoding or what is the best way to remove this new line character? I tried using the <code>replace</code> method but I get the following error: </p>

<pre><code>Traceback (most recent call last):
  File ""data_consumer.py"", line 33, in &lt;module&gt;
    auth_base64 = auth_base64.replace('\n', '')
TypeError: expected bytes, bytearray or buffer compatible object
</code></pre>

<p>My code:</p>

<pre><code>auth = b'username@domain.com:passWORD'
auth_base64 = base64.encodebytes(auth)
auth_base64 = auth_base64.replace('\n', '')
</code></pre>

<p>Any ideas? Thanks</p>
","218183","","","","","2020-09-10 07:44:13","Remove the new line ""\n"" from base64 encoded strings in Python3?","<python><python-3.x><base64>","4","0","5","","","CC BY-SA 3.0","0"
"40897428","1","40909197","","2016-11-30 20:19:45","","33","28593","<p>Python 3.4.2</p>

<p>I am learning asyncio and I use it to continously listen IPC bus, while gbulb listens to the dbus.</p>

<h3>Some side notes:</h3>

<p><em>So I created a function <code>listen_to_ipc_channel_layer</code> that continously listens for incoming messages on the IPC channel and passes the message to a <code>message_handler</code>.</em></p>

<p><em>I am also listening to SIGTERM and SIGINT. So when I send a SIGTERM to the python process running the code you find at the bottom, the script should terminate gracefully.</em></p>

<h3>The problem</h3>

<p>… I am having is the following warning:</p>

<pre><code>got signal 15: exit
Task was destroyed but it is pending!
task: &lt;Task pending coro=&lt;listen_to_ipc_channel_layer() running at /opt/mainloop-test.py:23&gt; wait_for=&lt;Future cancelled&gt;&gt;

Process finished with exit code 0
</code></pre>

<p>… with the following code:</p>

<pre><code>import asyncio
import gbulb
import signal
import asgi_ipc as asgi

def main():
    asyncio.async(listen_to_ipc_channel_layer())
    loop = asyncio.get_event_loop()

    for sig in (signal.SIGINT, signal.SIGTERM):
        loop.add_signal_handler(sig, ask_exit)

    # Start listening on the Linux IPC bus for incoming messages
    loop.run_forever()
    loop.close()

@asyncio.coroutine
def listen_to_ipc_channel_layer():
    """"""Listens to the Linux IPC bus for messages""""""
    while True:
        message_handler(message=channel_layer.receive([""my_channel""]))
        try:
            yield from asyncio.sleep(0.1)
        except asyncio.CancelledError:
            break

def ask_exit():
    loop = asyncio.get_event_loop()
    for task in asyncio.Task.all_tasks():
        task.cancel()
    loop.stop()


if __name__ == ""__main__"":
    gbulb.install()
    # Connect to the IPC bus
    channel_layer = asgi.IPCChannelLayer(prefix=""my_channel"")
    main()
</code></pre>

<p>I still only understand very little of asyncio, but I think I know what is going on. While waiting for <code>yield from asyncio.sleep(0.1)</code> the signal handler caught the SIGTERM and in that process it calls <code>task.cancel()</code>.</p>

<p><strong>Question thrown in: Shouldn't this trigger the <code>CancelledError</code> within the <code>while True:</code> loop?</strong> (Because it is not, but that is how I understand <a href=""https://docs.python.org/3.4/library/asyncio-task.html#task"" rel=""noreferrer"">""Calling cancel() will throw a CancelledError to the wrapped coroutine""</a>).</p>

<p>Eventually <code>loop.stop()</code> is called which stops the loop without waiting for either <code>yield from asyncio.sleep(0.1)</code> to return a result or even the whole coroutine <code>listen_to_ipc_channel_layer</code>.</p>

<p>Please correct me if I am wrong.</p>

<p>I think the only thing I need to do is to make my program wait for the <code>yield from asyncio.sleep(0.1)</code> to return a result <em>and/or</em> coroutine to break out the while-loop and finish.</p>

<p>I believe I confuse a lot of things. Please help me get those things straight so that I can figure out how to gracefully close the event loop without warning.</p>
","2182044","","2182044","","2016-12-13 07:03:57","2020-06-22 17:40:21","Please explain ""Task was destroyed but it is pending!""","<python><python-3.x><python-3.4><python-asyncio>","4","0","3","","","CC BY-SA 3.0","0"
"35748239","1","35748269","","2016-03-02 13:13:37","","35","28574","<p>I have installed Python 3.5 on my Windows 8 Computer. I have also installed Pycharm Community Version 5.0.4. I am not able to install BeautifulSoup Module through Settings Option in Pycharm. I am getting the following error in Pycharm:</p>

<pre><code>Collecting BeautifulSoup
  Using cached BeautifulSoup-3.2.1.tar.gz
  Complete output from command python setup.py egg_info:
  Traceback (most recent call last):
    File ""&lt;string&gt;"", line 1, in &lt;module&gt;
    File ""C:\Users\Kashyap\AppData\Local\Temp\pycharm-packaging0.tmp\BeautifulSoup\setup.py"", line 22
      print ""Unit tests have failed!""
                                    ^
  SyntaxError: Missing parentheses in call to 'print'

  ----------------------------------------

Command ""python setup.py egg_info"" failed with error code 1 in C:\Users\Kashyap\AppData\Local\Temp\pycharm-packaging0.tmp\BeautifulSoup
</code></pre>

<p>Path of installed folder of Python is 3.5.1 (C:\Program Files (x86)\Python35-32\python.exe)</p>
","5757820","","100297","","2016-03-02 13:23:55","2018-10-07 14:05:45","Failed to install package Beautiful Soup. Error Message is ""SyntaxError: Missing parentheses in call to 'print'""","<python><python-3.x><beautifulsoup>","4","0","2","","","CC BY-SA 3.0","0"
"38697037","1","38751157","","2016-08-01 10:55:35","","16","28568","<p>I enabled the compatibility check in my Python IDE and now I realize that the inherited Python 2.7 code has a lot of calls to <code>unicode()</code> which are not allowed in Python 3.x.</p>

<p>I looked at the <a href=""https://docs.python.org/2/library/functions.html#unicode"" rel=""noreferrer"">docs</a> of Python2 and found no hint how to upgrade: </p>

<p>I don't want to switch to Python3 now, but maybe in the future.</p>

<p>The code contains about 500 calls to <code>unicode()</code></p>

<p>How to proceed?</p>

<p><strong>Update</strong></p>

<p>The comment of user vaultah to read the  <a href=""https://docs.python.org/3/howto/pyporting.html"" rel=""noreferrer"">pyporting</a> guide has received several upvotes.</p>

<p>My current solution is this (thanks to Peter Brittain):</p>

<pre><code>from builtins import str
</code></pre>

<p>... I could not find this hint in the pyporting docs.....</p>
","633961","","100297","","2017-10-01 11:05:44","2017-10-01 11:05:44","how to convert Python 2 unicode() function into correct Python 3.x syntax","<python><python-3.x><python-unicode>","4","17","3","","","CC BY-SA 3.0","0"
"52578270","1","52578838","","2018-09-30 13:24:02","","4","28541","<p>My question is if you can install python with powershell, cmd, vbs or any other language built into Windows already?
If this was already asked please redirect me to the answer.
""How to install Python using Windows Command Prompt"" explains how to install the python if you already have the exe installed, not how to install the exe.</p>

<hr>

<p><strong>EDIT:</strong> I am trying to install python with a file on a pc that does not have python installed, only thing restricted might be that the account does not an administrator and if possible in the background.</p>
","","user10122572","8306666","user10122572","2019-09-25 13:44:55","2020-02-18 09:45:00","Install Python with cmd or powershell","<python><python-3.x><powershell><cmd>","3","4","","","","CC BY-SA 4.0","0"
"35024118","1","35024600","","2016-01-26 21:16:28","","5","28518","<p>I've spent some time looking online and so far haven't found anything that answers this without using PIL, which i can't get to work is there another way to do this simply? </p>
","5530179","","","","","2020-08-27 11:45:48","How to load an image into a python 3.4 tkinter window?","<python><image><python-3.x><tkinter>","3","3","","","","CC BY-SA 3.0","0"
"36650583","1","36650955","","2016-04-15 14:59:42","","6","28502","<p>Here is the working code:</p>

<pre><code>def g(y=10):
    return y**2

def f(x,y=10):
    return x*g(y)

print(f(5)) #-&gt;500
</code></pre>

<p>However, let's suppose we don't want to remember and copy a default value of keyword parameter <code>y</code> to the definition of external function (especially if there are several layers of external functions). In the above example it means that we want to use parameter, already defined in <code>g</code>. </p>

<p>One way to do that:</p>

<pre><code>def f(x,y=None):
    if y==None: return x*g()
    else: return x*g(y)
</code></pre>

<p>But is there a cleaner way to do the same?
Something like:</p>

<pre><code>def f(x,y=empty()):
    return x*g(y)
</code></pre>
","4537166","","4537166","","2016-04-15 15:09:00","2017-12-06 20:25:31","How to pass an empty parameter to a python function?","<python><python-3.x>","5","0","","","","CC BY-SA 3.0","0"
"44617159","1","45771035","","2017-06-18 16:33:51","","6","28487","<p>I am trying to import folium into a Jupyter notebook I'm working on and I cannot seem to solve the import issues with the Folium library.  Has anyone else solved this problem?</p>

<pre><code>!pip install folium
import pandas as pd
import folium
</code></pre>

<p>Output from the above yields:</p>

<pre><code>`ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-7-a9938c267a0c&gt; in &lt;module&gt;()
      1 get_ipython().system('pip install folium')
      2 import pandas as pd
----&gt; 3 import folium

ModuleNotFoundError: No module named 'folium'`
</code></pre>
","7823253","","","","","2020-06-25 17:18:01","Python 3.6 Module cannot be found: Folium","<python><python-3.x><import><pip><folium>","12","8","","","","CC BY-SA 3.0","0"
"34246336","1","34246469","","2015-12-12 23:45:08","","16","28446","<p>I have trouble using RandomForest fit function</p>

<p>This is my training set</p>

<pre><code>         P1      Tp1           IrrPOA     Gz          Drz2
0        0.0     7.7           0.0       -1.4        -0.3
1        0.0     7.7           0.0       -1.4        -0.3
2        ...     ...           ...        ...         ...
3        49.4    7.5           0.0       -1.4        -0.3
4        47.4    7.5           0.0       -1.4        -0.3
... (10k rows)
</code></pre>

<p>I want to predict P1 thanks to all the other variables using sklearn.ensemble RandomForest</p>

<pre><code>colsRes = ['P1']
X_train = train.drop(colsRes, axis = 1)
Y_train = pd.DataFrame(train[colsRes])
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, Y_train)
</code></pre>

<p>Here is the error I get:</p>

<pre><code>ValueError: Unknown label type: array([[  0. ],
       [  0. ],
       [  0. ],
       ..., 
       [ 49.4],
       [ 47.4],
</code></pre>

<p>I did not find anything about this label error, I use Python 3.5.
Any advice would be a great help !</p>
","5673361","","5673361","","2015-12-12 23:59:11","2017-12-08 21:46:16","Python RandomForest - Unknown label Error","<python><python-3.x><scikit-learn><random-forest>","3","1","3","","","CC BY-SA 3.0","0"
"36423259","1","","","2016-04-05 10:06:40","","8","28426","<p>I'm relatively new to Python Programming, using Python 3.x, and am working on a Barbershop P.O.S system where the admin will have the privilege to add Services and their corresponding Prices.
I'm using the Pretty Table library to achieve printing out a table with serviceID, service and price.</p>

<p>Here's my code:</p>

<pre><code>from prettytable import PrettyTable
import random

serviceID = []
services = []
price = []
x = PrettyTable()

x.add_column(""ServiceID"",[serviceID])
x.add_column(""Service"", [services])
x.add_column(""Price"", [price])

while True:
try:

     ID = random.randint(1,90000) #range high to lower probability of non-uniqueness
     serviceID.append(ID) #Generates unique ID for each service
     prompt1 = input(""Please add a service name to the list\n"")
     services.append(prompt1)

     prompt2 = input(""Please enter a price for the service\n"")
     prompt2 == int(prompt2)
     price.append(prompt2)

     print(x)


except ValueError:
    print(""Please enter valid type"")
    continue
</code></pre>

<p>When I enter the first service and Price, the output is:</p>

<pre><code>+-----------+---------+--------+
| ServiceID | Service | Price  |
+-----------+---------+--------+
|   [9880]  | ['box'] | ['90'] |
+-----------+---------+--------+
</code></pre>

<p>When I enter the 2nd service and price, the output is this:</p>

<pre><code>+---------------+-----------------+--------------+
|   ServiceID   |     Service     |    Price     |
+---------------+-----------------+--------------+
| [9880, 47612] | ['box', 'trim'] | ['90', '80'] |
+---------------+-----------------+--------------+
</code></pre>

<p>I'd like the output to be this:</p>

<pre><code>+---------------+-----------------+--------------+
|   ServiceID   |     Service     |    Price     |
+---------------+-----------------+--------------+
|  9880         |      box        |       90     |
|  47612        |     trim        |       80     |
+---------------+-----------------+--------------+
</code></pre>

<p>Does anyone know how to achieve this?
Any help would be appreciated.</p>
","6160540","","","","","2020-01-06 13:24:07","How to use Pretty Table in Python to print out data from multiple lists?","<python><python-3.x><prettytable>","2","1","2","","","CC BY-SA 3.0","0"
"28142688","1","","","2015-01-25 23:11:25","","6","28364","<pre><code>print (""How much does your meal cost"")

meal = 0
tip = 0
tax = 0.0675

action = input( ""Type amount of meal "")

if action.isdigit():
    meal = (action)
    print (meal)

tips = input("" type the perentage of tip you want to give "")


if tips.isdigit():
    tip = tips 
    print(tip)
</code></pre>

<p>I have written this but I do not know how to get </p>

<pre><code>print(tip)
</code></pre>

<p>to be a percentage when someone types a number in.</p>
","4447353","","391161","","2015-01-25 23:15:24","2015-09-22 23:49:48","How to turn input number into a percentage in python","<python><python-3.x>","4","2","","","","CC BY-SA 3.0","0"
"44449972","1","44450267","","2017-06-09 05:34:33","","3","28317","<p>i have only 32bit system so i installed python 3.5 (64 bit ) error occurs .so i installed python 32bit successfully after that i followed by that document(<a href=""http://tensorflow.org/install/"" rel=""nofollow noreferrer"">http://tensorflow.org/install/</a>…) i tried this into command prompt </p>

<pre><code>C:\Users\mydoc&gt;pip3 install --upgrade tensorflow
</code></pre>

<p>but error occurs like this.</p>

<pre><code>C:\Users\mydoc&gt;pip install tensorflow

Collecting tensorflow
  Could not find a version that satisfies the requirement tensorflow (from versions: )
No matching distribution found for tensorflow

C:\Users\mydoc&gt;
</code></pre>

<p>please help me how to install tensorflow for my windows (32-bit)system.</p>
","7129860","","165216","","2017-06-09 05:50:39","2020-01-12 19:24:44","how to install tensorflow for windows 7 32bit system?i installed python 3.5(32 bit) into my system and also installed anaconda 3.4.4(32 bit)","<python-3.x><tensorflow><anaconda>","3","0","1","","","CC BY-SA 3.0","0"
"33135038","1","33135143","","2015-10-14 20:31:45","","22","28272","<p>Python 3.5's <code>os.scandir(path)</code> function returns lightweight DirEntry objects that are very helpful with information about files.  However, it only works for the immediate path handed to it.  Is there a way to wrap it in a recursive function so that it visits all subdirectories beneath the given path?</p>
","5446811","","68707","","2015-10-14 20:41:01","2020-07-14 12:51:30","How do I use os.scandir() to return DirEntry objects recursively on a directory tree?","<python><python-3.x>","1","1","7","","","CC BY-SA 3.0","0"
"35847084","1","35848313","","2016-03-07 15:06:57","","34","28270","<p>I have written the following sample code to demonstrate my issue.</p>

<pre><code>import argparse

parser = argparse.ArgumentParser()
parser.add_argument('-v', '--version', action='version',
                    version='%(prog)s 1.0')
parser.parse_args()
</code></pre>

<p>This produces the following help message.</p>

<pre><code>$ python foo.py --help
usage: foo.py [-h] [-v]

optional arguments:
  -h, --help     show this help message and exit
  -v, --version  show program's version number and exit
</code></pre>

<p>I want to customize this help output such that it capitalizes all phrases and sentences, and puts period after sentences. In other words, I want the help message to be generated like this.</p>

<pre><code>$ python foo.py --help
Usage: foo.py [-h] [-v]

Optional arguments:
  -h, --help     Show this help message and exit.
  -v, --version  Show program's version number and exit.
</code></pre>

<p>Is this something that I can control using the argparse API. If so, how? Could you please provide a small example that shows how this can be done?</p>
","1175080","","","","","2020-08-12 18:11:07","Customize argparse help message","<python><python-3.x><argparse>","3","2","12","","","CC BY-SA 3.0","0"
"38882721","1","38883015","","2016-08-10 20:15:13","","11","28259","<p>I'am trying to get running a very simple example on OSX with python 3.5.1 but I'm really stucked. Have read so many articles that deal with similar problems but I can not fix this by myself. Do you have any hints how to resolve this issue?</p>

<p>I would like to have the correct encoded latin-1 output as defined in mylist without any errors.</p>

<p><strong>My code:</strong></p>

<pre><code># coding=&lt;latin-1&gt;

mylist = [u'Glück', u'Spaß', u'Ähre',]
print(mylist)
</code></pre>

<p>The error:</p>

<pre><code>Traceback (most recent call last):
File ""/Users/abc/test.py"", line 4, in &lt;module&gt;
print(mylist)
UnicodeEncodeError: 'ascii' codec can't encode character '\xfc' in position 4: ordinal not in range(128)
</code></pre>

<p><strong>How I can fix the error but still get something wrong with stdout (print):</strong></p>

<pre><code>mylist = [u'Glück', u'Spaß', u'Ähre',]
    for w in mylist:
        print(w.encode(""latin-1""))
</code></pre>

<p><em>What I get as output:</em></p>

<pre><code>b'Gl\xfcck'
b'Spa\xdf'
b'\xc4hre'
</code></pre>

<p>What 'locale' shows me:</p>

<pre><code>LANG=""de_AT.UTF-8""
LC_COLLATE=""de_AT.UTF-8""
LC_CTYPE=""de_AT.UTF-8""
LC_MESSAGES=""de_AT.UTF-8""
LC_MONETARY=""de_AT.UTF-8""
LC_NUMERIC=""de_AT.UTF-8""
LC_TIME=""de_AT.UTF-8""
LC_ALL=
</code></pre>

<p>What 
-> 'python3' shows me:</p>

<pre><code>Python 3.5.1 (default, Jan 22 2016, 08:54:32) 
[GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import sys
&gt;&gt;&gt; sys.getdefaultencoding()
'utf-8'
</code></pre>
","6702020","","","","","2019-03-02 13:28:08","Python3: UnicodeEncodeError: 'ascii' codec can't encode character '\xfc'","<python-3.x><iso-8859-1><python-unicode>","3","1","1","","","CC BY-SA 3.0","0"
"36651680","1","","","2016-04-15 15:52:51","","39","28026","<p>I downloaded Quokka Python/Flask CMS to a CentOS7 server. Everything works fine with command</p>

<pre><code>sudo python3 manage.py runserver --host 0.0.0.0 --port 80
</code></pre>

<p>Then I create a file /etc/init.d/quokkacms. The file contains following code</p>

<pre><code>start() {
        echo -n ""Starting quokkacms: ""
        python3 /var/www/quokka/manage.py runserver --host 0.0.0.0 --port 80
        touch /var/lock/subsys/quokkacms
        return 0
}
stop() {
        echo -n ""Shutting down quokkacms: ""
        rm -f /var/lock/subsys/quokkacms
        return 0
}
case ""$1"" in
    start)
        start
        ;;
    stop)
        stop
        ;;
    status)

        ;;
    restart)
        stop
        start
        ;;

    *)
        echo ""Usage: quokkacms {start|stop|status|restart}""
        exit 1
        ;;
esac
exit $?
</code></pre>

<p>But I get error when running <code>sudo service quokkacms start</code></p>

<blockquote>
  <p>RuntimeError: Click will abort further execution because Python 3 was 
  configured to use ASCII as encoding for the environment. Either switch
  to Python 2 or consult <a href=""http://click.pocoo.org/python3/"" rel=""noreferrer"">http://click.pocoo.org/python3/</a> for<br>
  mitigation steps.</p>
</blockquote>

<p>It seems to me that it is the bash script. How come I get different results? Also I followed instructions in the link in the error message but still had no luck.</p>

<p>[update] I had already tried the solution provided by Click before I posted this question. Check the results below (i run in root):</p>

<pre><code>[root@webserver quokka]# python3
Python 3.4.3 (default, Jan 26 2016, 02:25:35)
[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import locale
&gt;&gt;&gt; import codecs
&gt;&gt;&gt; print(locale.getpreferredencoding())
UTF-8
&gt;&gt;&gt; print(codecs.lookup(locale.getpreferredencoding()).name)
utf-8
&gt;&gt;&gt; locale.getdefaultlocale()
('en_US', 'UTF-8')
&gt;&gt;&gt; locale.CODESET
14
&gt;&gt;&gt;
</code></pre>
","234118","","234118","","2016-04-15 18:59:02","2019-07-03 12:11:52","Click will abort further execution because Python 3 was configured to use ASCII as encoding for the environment","<python-3.x><centos><locale><redhat><python-click>","3","2","10","","","CC BY-SA 3.0","0"
"48116872","1","","","2018-01-05 15:43:57","","9","27883","<p>For some reason send_message isn't working properly on my Discord bot and I can't find anyway to fix it.</p>

<pre><code>import asyncio
import discord

client = discord.Client()

@client.async_event
async def on_message(message):
    author = message.author
   if message.content.startswith('!test'):
        print('on_message !test')
        await test(author, message)
async def test(author, message):
    print('in test function')
    await client.send_message(message.channel, 'Hi %s, i heard you.' % author)
client.run(""key"")
</code></pre>



<pre><code>on_message !test
in test function
Ignoring exception in on_message
Traceback (most recent call last):
  File ""C:\Users\indit\AppData\Roaming\Python\Python36\site-packages\discord\client.py"", line 223, in _run_event
    yield from coro(*args, **kwargs)
  File ""bot.py"", line 15, in on_message
    await test(author, message)
  File ""bot.py"", line 21, in test
    await client.send_message(message.channel, 'Hi %s, i heard you.' % author)
AttributeError: 'Client' object has no attribute 'send_message'
</code></pre>
","9178237","","5951320","","2018-01-05 18:56:08","2018-09-23 15:25:38","AttributeError: 'Client' object has no attribute 'send_message' (Discord Bot)","<python><python-3.x><discord><discord.py>","1","2","2","","","CC BY-SA 3.0","0"
"34753872","1","34754025","","2016-01-12 21:11:35","","0","27830","<p>I have a homework assignment. I've got the following code</p>

<pre><code>hey = [""lol"", ""hey"",""water"",""pepsi"",""jam""]

for item in hey:
    print(item)
</code></pre>

<p>Do I display the position in the list before the item, like this:</p>

<pre><code>1 lol
2 hey
3 water
4 pepsi
5 jam
</code></pre>
","5781089","","344286","","2016-01-12 21:40:21","2020-05-05 17:33:02","How do I display the the index of a list element in Python?","<python><list><python-3.x><position>","3","3","1","2016-01-12 22:10:40","","CC BY-SA 3.0","0"
"37649060","1","","","2016-06-06 03:14:23","","38","27791","<p>I think it would be immensely helpful to the Tensorflow community if there was a well-documented solution to the crucial task of testing a single new image against the model created by the <a href=""https://www.tensorflow.org/versions/r0.8/tutorials/deep_cnn/index.html"" rel=""noreferrer"">convnet in the CIFAR-10 tutorial</a>. </p>

<p>I may be wrong, but this critical step that makes the trained model usable in practice seems to be lacking. There is a ""missing link"" in that tutorial—a script that would directly load a single image (as array or binary), compare it against the trained model, and return a classification.</p>

<p>Prior answers give partial solutions that explain the overall approach, but none of which I've been able to implement successfully. Other bits and pieces can be found here and there, but unfortunately haven't added up to a working solution. Kindly consider the research I've done, before tagging this as duplicate or already answered.</p>

<p><a href=""https://stackoverflow.com/questions/33759623/tensorflow-how-to-restore-a-previously-saved-model-python"">Tensorflow: how to save/restore a model?</a></p>

<p><a href=""https://stackoverflow.com/questions/34982492/restoring-tensorflow-model"">Restoring TensorFlow model</a></p>

<p><a href=""https://stackoverflow.com/questions/37187597/unable-to-restore-models-in-tensorflow-v0-8"">Unable to restore models in tensorflow v0.8</a></p>

<p><a href=""https://gist.github.com/nikitakit/6ef3b72be67b86cb7868"" rel=""noreferrer"">https://gist.github.com/nikitakit/6ef3b72be67b86cb7868</a></p>

<p>The most popular answer is the first, in which @RyanSepassi and @YaroslavBulatov describe the problem and an approach: one needs to ""manually construct a graph with identical node names, and use Saver to load the weights into it"". Although both answers are helpful, it is not apparent how one would go about plugging this into the CIFAR-10 project.</p>

<p>A fully functional solution would be highly desirable so we could port it to other single image classification problems. There are several questions on SO in this regard that ask for this, but still no full answer (for example <a href=""https://stackoverflow.com/questions/37058236/load-checkpoint-and-evaluate-single-image-with-tensorflow-dnn"">Load checkpoint and evaluate single image with tensorflow DNN</a>).</p>

<p>I hope we can converge on a working script that everyone could use.</p>

<p>The below script is not yet functional, and I'd be happy to hear from you on how this can be improved to provide a solution for single-image classification using the CIFAR-10 TF tutorial trained model.</p>

<p>Assume all variables, file names etc. are untouched from the original tutorial.</p>

<p>New file: <strong>cifar10_eval_single.py</strong></p>

<pre><code>import cv2
import tensorflow as tf

FLAGS = tf.app.flags.FLAGS

tf.app.flags.DEFINE_string('eval_dir', './input/eval',
                           """"""Directory where to write event logs."""""")
tf.app.flags.DEFINE_string('checkpoint_dir', './input/train',
                           """"""Directory where to read model checkpoints."""""")

def get_single_img():
    file_path = './input/data/single/test_image.tif'
    pixels = cv2.imread(file_path, 0)
    return pixels

def eval_single_img():

    # below code adapted from @RyanSepassi, however not functional
    # among other errors, saver throws an error that there are no
    # variables to save
    with tf.Graph().as_default():

        # Get image.
        image = get_single_img()

        # Build a Graph.
        # TODO

        # Create dummy variables.
        x = tf.placeholder(tf.float32)
        w = tf.Variable(tf.zeros([1, 1], dtype=tf.float32))
        b = tf.Variable(tf.ones([1, 1], dtype=tf.float32))
        y_hat = tf.add(b, tf.matmul(x, w))

        saver = tf.train.Saver()

        with tf.Session() as sess:
            sess.run(tf.initialize_all_variables())
            ckpt = tf.train.get_checkpoint_state(FLAGS.checkpoint_dir)

            if ckpt and ckpt.model_checkpoint_path:
                saver.restore(sess, ckpt.model_checkpoint_path)
                print('Checkpoint found')
            else:
                print('No checkpoint found')

            # Run the model to get predictions
            predictions = sess.run(y_hat, feed_dict={x: image})
            print(predictions)

def main(argv=None):
    if tf.gfile.Exists(FLAGS.eval_dir):
        tf.gfile.DeleteRecursively(FLAGS.eval_dir)
    tf.gfile.MakeDirs(FLAGS.eval_dir)
    eval_single_img()

if __name__ == '__main__':
    tf.app.run()
</code></pre>
","445142","","-1","","2017-05-23 10:31:28","2017-11-27 08:22:11","Tensorflow: restoring a graph and model then running evaluation on a single image","<python><python-3.x><machine-learning><tensorflow>","4","0","29","","","CC BY-SA 3.0","0"
"36780514","1","","","2016-04-21 21:07:55","","4","27785","<p>When I use <code>""\n""</code> in my <code>print</code> function it gives me a syntax error in the following code</p>

<pre><code>from itertools import combinations
a=[comb for comb in combinations(range(1,96+1),7) if sum(comb) == 42]
print (a ""\n"")
</code></pre>

<p>Is there any way to add new line in each combination?</p>
","6234753","","364696","","2016-04-21 21:54:09","2020-01-03 23:37:24","new line with variable in python","<python><python-3.x><newline><combinations><line-breaks>","4","10","","","","CC BY-SA 3.0","0"
"43466927","1","43467402","","2017-04-18 08:09:26","","17","27740","<p>I tried to work with <code>CURD</code> operation using <code>Flask</code> and <code>SQLAlchemy</code>.But getting <code>Error</code> while connecting to database.</p>

<p>Here is the <code>Error</code> log. </p>

<pre><code>/usr/local/lib/python3.4/dist-packages/flask_sqlalchemy/__init__.py:819: UserWarning: SQLALCHEMY_DATABASE_URI not set. Defaulting to ""sqlite:///:memory:"".
  'SQLALCHEMY_DATABASE_URI not set. Defaulting to '
/usr/local/lib/python3.4/dist-packages/flask_sqlalchemy/__init__.py:839: FSADeprecationWarning: SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and will be disabled by default in the future.  Set it to True or False to suppress this warning.
  'SQLALCHEMY_TRACK_MODIFICATIONS adds significant overhead and '
</code></pre>

<p>here is my <strong>code &amp; setup</strong></p>

<pre><code># database.py
from flask import Flask
from flask_sqlalchemy import SQLAlchemy

app = Flask(__name__)
sqldb = SQLAlchemy(app)
app.config['SQLALCHEMY_DATABASE_URI'] = ""mysql+pymysql://root:root@localhost/myDbName""


# create app
def create_app():
    sqlDB = SQLAlchemy(app)
    sqlDB.init_app(app)
    sqlDB.create_all()
    return app
</code></pre>

<p>here is models.py</p>

<pre><code>from ..database import create_app
sqldb = create_app()
#  Users Model
class Users(sqldb.Model):
    __tablename__ = 'users'
    id = sqldb.Column(sqldb.Integer, primary_key = True)
    db = sqldb.Column(sqldb.String(40))
    def __init__(self,email,db):
        self.email = email
        self.db = db
    def __repr__(self,db):
        return '&lt;USER %r&gt;' % self.db
</code></pre>

<p>here is routes.py</p>

<pre><code># Import __init__ file
from __init__ import app
import sys
# JSON 
from bson import dumps
# login
@app.route('/', methods = ['GET'])
def login():
    try:
        # import users model
        from Model.models import Users,sqldb
        sqldb.init_app(app)
        sqldb.create_all()
        getUser = Users.query.all()
        print(getUser)
        return 'dsf'
    except Exception as e:
        print(e)
        return ""error."" 
</code></pre>
","5729416","","5729416","","2018-07-24 04:27:21","2018-07-24 04:27:21","SQLALCHEMY_DATABASE_URI not set","<python-3.x><flask><sqlalchemy><flask-sqlalchemy>","2","0","2","","","CC BY-SA 4.0","0"
"40445605","1","40445706","","2016-11-06 03:06:32","","-1","27736","<pre><code>    Traceback (most recent call last):
  File ""&lt;pyshell#0&gt;"", line 1, in &lt;module&gt;
    get_odd_palindrome_at('racecar', 3)
  File ""C:\Users\musar\Documents\University\Courses\Python\Assignment 2\palindromes.py"", line 48, in get_odd_palindrome_at
    for i in range(string[index:]):
TypeError: 'str' object cannot be interpreted as an integer
</code></pre>

<hr>

<p>I want to use the value index refers to but how do I do that?</p>
","7121000","","2225682","","2016-11-06 03:07:38","2016-11-06 03:29:45","(Help) TypeError: 'str' object cannot be interpreted as an integer","<python><python-3.x>","1","2","","","","CC BY-SA 3.0","0"
"45738414","1","45738557","","2017-08-17 14:50:28","","6","27707","<p>I'm trying to install a library of collections, a module сounter and defaultdict.
I have python 3.6 and Win 7 64x installed. When installing the ""pip install collections"" library, an error appears. ""The version that satisfies the requirement sets could not be found"".
How to fix this problem?</p>
","8412364","","","","","2020-07-03 18:36:40","how to install collections in python 3.6","<python-3.x>","3","1","","","","CC BY-SA 3.0","0"
"34753401","1","34822100","","2016-01-12 20:43:00","","102","27676","<p>Let's say we have a dummy function:</p>
<pre><code>async def foo(arg):
    result = await some_remote_call(arg)
    return result.upper()
</code></pre>
<p>What's the difference between:</p>
<pre><code>import asyncio    

coros = []
for i in range(5):
    coros.append(foo(i))

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(coros))
</code></pre>
<p>And:</p>
<pre><code>import asyncio

futures = []
for i in range(5):
    futures.append(asyncio.ensure_future(foo(i)))

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(futures))
</code></pre>
<p><em>Note</em>: The example returns a result, but this isn't the focus of the question. When return value matters, use <code>gather()</code> instead of <code>wait()</code>.</p>
<p>Regardless of return value, I'm looking for clarity on <code>ensure_future()</code>. <code>wait(coros)</code> and <code>wait(futures)</code> both run the coroutines, so when and why should a coroutine be wrapped in <code>ensure_future</code>?</p>
<p>Basically, what's the Right Way (tm) to run a bunch of non-blocking operations using Python 3.5's <code>async</code>?</p>
<p>For extra credit, what if I want to batch the calls? For example, I need to call <code>some_remote_call(...)</code> 1000 times, but I don't want to crush the web server/database/etc with 1000 simultaneous connections. This is doable with a thread or process pool, but is there a way to do this with <code>asyncio</code>?</p>
<p><strong>2020 update (Python 3.7+)</strong>: Don't use these snippets. Instead use:</p>
<pre><code>import asyncio

async def do_something_async():
    tasks = []
    for i in range(5):
        tasks.append(asyncio.create_task(foo(i)))
    await asyncio.gather(*tasks)

def do_something():
    asyncio.run(do_something_async)
</code></pre>
<p>Also consider using <a href=""https://trio.readthedocs.io/"" rel=""nofollow noreferrer"">Trio</a>, a robust 3rd party alternative to asyncio.</p>
","649167","","649167","","2020-08-23 21:07:42","2020-08-23 21:07:42","Difference between coroutine and future/task in Python 3.5?","<python><python-3.x><python-asyncio>","4","0","36","","","CC BY-SA 4.0","0"
"31468117","1","","","2015-07-17 03:49:22","","60","27661","<p>Based on this <a href=""https://stackoverflow.com/a/29704623/1202808"">comment</a> and the referenced documentation, Pickle 4.0+ from Python 3.4+ should be able to pickle byte objects larger than 4 GB. </p>

<p>However, using python 3.4.3 or python 3.5.0b2 on Mac OS X 10.10.4, I get an error when I try to pickle a large byte array:</p>

<pre><code>&gt;&gt;&gt; import pickle
&gt;&gt;&gt; x = bytearray(8 * 1000 * 1000 * 1000)
&gt;&gt;&gt; fp = open(""x.dat"", ""wb"")
&gt;&gt;&gt; pickle.dump(x, fp, protocol = 4)
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
OSError: [Errno 22] Invalid argument
</code></pre>

<p>Is there a bug in my code or am I misunderstanding the documentation?</p>
","1202808","","202229","","2019-11-19 01:23:27","2019-11-19 01:23:27","Python 3 - Can pickle handle byte objects larger than 4GB?","<python><python-3.x><size><pickle>","7","15","12","","","CC BY-SA 3.0","0"
"38281563","1","","","2016-07-09 11:56:11","","4","27634","<p>Main OS I am using are Ubuntu and Windows.</p>

<p>I couldn't find the package in conda.I am using python 3.5.</p>
","582398","","125389","","2018-04-19 18:59:33","2018-12-28 12:42:38","How to install Django Rest Framework using Anaconda?","<python><python-3.x><django-rest-framework><anaconda><conda>","5","3","1","","","CC BY-SA 3.0","0"
"38454852","1","","","2016-07-19 09:41:49","","24","27614","<p>In python 3 getting into ImportError issues.
My project structure is like:</p>

<pre><code>cts_sap_polaris/                                                                     
|-- etc                                                                              
|   |-- clean_cts_sap_polaris.yaml                                                   
|   |-- clean_env_variables.tcl                                                      
|   |-- cts_sap_polaris_ha_combined.yaml                                             
|   |-- cts_sap_polaris.yaml                                                         
|   `-- TCL_TESTBED_CONFIGS                                                          
|-- __init__.py                                                                      
|-- jobs
|   |-- __init__.py
|   |-- __pycache__
|   |   `-- run_cts_sap_polaris.cpython-34.pyc
|   `-- run_cts_sap_polaris.py
|-- lib
|   |-- cli_check.py
|   |-- cts_sap_polaris_utils.py
|   |-- __init__.py
|   |-- router_show_cts_cmd.py
|   |-- router_show_etherchannel_cmd.py
|   |-- router_show.py
|   |-- utils.py
|   |-- validate_show_output.py
|   `-- wait_for.py
|-- scripts
|   |-- cts_sap_polaris_ha_combined.py
|   |-- cts_sap_polaris.py
|   |-- __init__.py
|   `-- __pycache__
|       `-- cts_sap_polaris.cpython-34.pyc
`-- test
    |-- code_snippets
    |-- cts_interface.json
    |-- cts_interface_summary.json
    |-- etherchannel_port_channel.json
    |-- etherchannel_port.json
    |-- __init__.py
    |-- test_cts_sap_cli.py
    `-- test_router_show.py
</code></pre>

<p>In <code>scripts/cts_sap_polaris.py</code> I am trying an import </p>

<pre><code>import cts_sap_polaris.lib.cli_check as cli_check
</code></pre>

<p>Which is throwing this error:</p>

<pre><code>ImportError: No module named 'cts_sap_polaris.lib'; 'cts_sap_polaris' is not a package.
</code></pre>
","3442163","","3375713","","2016-07-19 09:57:16","2020-02-07 12:49:42","ImportError: with error 'is not a package'","<python><python-3.x>","2","3","3","","","CC BY-SA 3.0","0"
"40553285","1","40553322","","2016-11-11 17:33:15","","13","27576","<p>I would like to check if a variable is of the <code>NoneType</code> type. For other types we can do stuff like:</p>

<pre><code>    type([])==list
</code></pre>

<p>But for <code>NoneType</code> this simple way is not possible. That is, we cannot say <code>type(None)==NoneType</code>. Is there an alternative way? And why is this possible for some types and not for others? Thank you.</p>
","6204900","","","","","2016-11-11 17:35:41","Determining a variable's type is NoneType in python","<python><python-3.x><types><nonetype>","2","3","1","2016-11-11 17:39:11","","CC BY-SA 3.0","0"
"44488349","1","44488455","","2017-06-11 20:16:01","","9","27555","<p>Tried to </p>

<p><code>conda install -c conda-forge requests-futures=0.9.7</code></p>

<p>but failed with</p>

<p><code>conda is not recognized as an internal or external command</code>,</p>

<p><code>C:\Users\user_name\Anaconda3\Scripts</code> has been set for <code>Path</code> in <code>environment variables</code> under both <code>user</code> and <code>System variables</code>.</p>

<p>I installed <code>Python 3.5</code> as well and it is on <code>Path</code>, I am using <code>Win10 X64</code>.</p>

<p>How to fix the issue?</p>
","766708","","1860929","","2018-04-10 07:28:28","2019-04-15 12:08:06","Windows 10 conda is not recognized as an internal or external command","<python><python-3.x><anaconda><conda>","3","1","2","","","CC BY-SA 3.0","0"
"30460929","1","30467159","","2015-05-26 14:06:20","","19","27553","<p>In order to prevent from context switching, I want to create a big loop to serve both the network connections and some routines.</p>

<p>Here's the implementation for normal functions:</p>

<pre><code>import asyncio
import time


def hello_world(loop):
    print('Hello World')
    loop.call_later(1, hello_world, loop)

def good_evening(loop):
    print('Good Evening')
    loop.call_later(1, good_evening, loop)

print('step: asyncio.get_event_loop()')
loop = asyncio.get_event_loop()

print('step: loop.call_soon(hello_world, loop)')
loop.call_soon(hello_world, loop)
print('step: loop.call_soon(good_evening, loop)')
loop.call_soon(good_evening, loop)

try:
    # Blocking call interrupted by loop.stop()
    print('step: loop.run_forever()')
    loop.run_forever()
except KeyboardInterrupt:
    pass
finally:
    print('step: loop.close()')
    loop.close()
</code></pre>

<p>Here's the implementation for coroutines:</p>

<pre><code>import asyncio


@asyncio.coroutine
def hello_world():
    while True:
        yield from asyncio.sleep(1)
        print('Hello World')

@asyncio.coroutine
def good_evening():
    while True:
        yield from asyncio.sleep(1)
        print('Good Evening')

print('step: asyncio.get_event_loop()')
loop = asyncio.get_event_loop()
try:
    print('step: loop.run_until_complete()')
    loop.run_until_complete(asyncio.wait([
        hello_world(),
        good_evening()
    ]))
except KeyboardInterrupt:
    pass
finally:
    print('step: loop.close()')
    loop.close()
</code></pre>

<p>And the mixed one:</p>

<pre><code>import asyncio
import time


def hello_world(loop):
    print('Hello World')
    loop.call_later(1, hello_world, loop)

def good_evening(loop):
    print('Good Evening')
    loop.call_later(1, good_evening, loop)

@asyncio.coroutine
def hello_world_coroutine():
    while True:
        yield from asyncio.sleep(1)
        print('Hello World Coroutine')

@asyncio.coroutine
def good_evening_coroutine():
    while True:
        yield from asyncio.sleep(1)
        print('Good Evening Coroutine')

print('step: asyncio.get_event_loop()')
loop = asyncio.get_event_loop()

print('step: loop.call_soon(hello_world, loop)')
loop.call_soon(hello_world, loop)
print('step: loop.call_soon(good_evening, loop)')
loop.call_soon(good_evening, loop)
print('step: asyncio.async(hello_world_coroutine)')
asyncio.async(hello_world_coroutine())
print('step: asyncio.async(good_evening_coroutine)')
asyncio.async(good_evening_coroutine())

try:
    loop.run_forever()
except KeyboardInterrupt:
    pass
finally:
    print('step: loop.close()')
    loop.close()
</code></pre>

<p>As you see, each coroutine function has a while loop surrounded. How can I make it like the normal one? I.e. when it is done, call itself after the given delay time, but not just put a loop there.</p>
","1592410","","1592410","","2017-12-23 01:35:27","2017-12-23 01:35:27","How to create an event loop with rolling coroutines running on it forever?","<python><python-3.x><asynchronous><coroutine><python-asyncio>","4","1","9","","","CC BY-SA 3.0","0"
"28957573","1","28957588","","2015-03-10 06:39:09","","8","27548","<p>can some one tell me how can i check whether a line starts with string or space or tab? I tried this, but not working.. </p>

<pre><code>if line.startswith(\s):
    outFile.write(line);
</code></pre>

<p>below is the samp data.. </p>

<pre><code>female 752.9
    external 752.40
        specified type NEC 752.49
    internal NEC 752.9
male (external and internal) 752.9
    epispadias 752.62""
    hidden penis 752.65
    hydrocele, congenital 778.6
    hypospadias 752.61""*
</code></pre>
","3784963","","770830","","2015-03-10 06:41:54","2018-03-24 01:51:30","How to check whether a line starts with a word or tab or white space in python?","<python><python-3.x>","5","0","","","","CC BY-SA 3.0","0"
"43080259","1","43080772","","2017-03-28 21:26:54","","30","27545","<p>While doing some practice problems using seaborn and a Jupyter notebook, I realized that the distplot() graphs did not have the darker outlines on the individual bins that all of the sample graphs in the documentation have.  I tried creating the graphs using Pycharm and noticed the same thing.  Thinking it was a seaborn problem, I tried some hist() charts using matplotlib, only to get the same results.</p>

<pre><code>import matplotlib.pyplot as plt
import seaborn as sns
titanic = sns.load_dataset('titanic')
plt.hist(titanic['fare'], bins=30)
</code></pre>

<p>yielded the following graph: </p>

<p><a href=""https://i.stack.imgur.com/Jzzzq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Jzzzq.png"" alt=""enter image description here""></a></p>

<p>Finally I stumbled across the 'edgecolor' parameter on the plt.hist() function, and setting it to black did the trick.  Unfortunately I haven't found a similar parameter to use on the seaborn distplot() function, so I am still unable to get a chart that looks like it should.  </p>

<p>I looked into changing the rcParams in matplotlib, but I have no experience with that and the following script I ran seemed to do nothing:</p>

<pre><code>import matplotlib as mpl

mpl.rcParams['lines.linewidth'] = 1
mpl.rcParams['lines.color'] = 'black'
mpl.rcParams['patch.linewidth'] = 1
mpl.rcParams['patch.edgecolor'] = 'black'
mpl.rcParams['axes.linewidth'] = 1
mpl.rcParams['axes.edgecolor'] = 'black'
</code></pre>

<p>I was just kind of guessing at the value I was supposed to change, but running my graphs again showed no changes.</p>

<p>I then attempted to go back to the default settings using mpl.rcdefaults()
but once again, no change.</p>

<p>I reinstalled matplotlib using conda but still the graphs look the same.  I am running out of ideas on how to change the default edge color for these charts.  I am running the latest versions of Python, matplotlib, and seaborn using the Conda build.</p>
","7782333","","4124317","","2017-03-28 21:43:13","2020-05-14 11:28:55","No outlines on bins of Matplotlib histograms or Seaborn distplots","<python><python-3.x><matplotlib><seaborn>","1","0","16","","","CC BY-SA 3.0","0"
"48365252","1","48365300","","2018-01-21 09:07:02","","6","27525","<p>I have the below html code.</p>

<pre><code>&lt;div align=""center""&gt;
    &lt;input type=""file"" name=""filePath""&gt;&lt;br&gt;
    &lt;input type=""Submit"" value=""Upload File""&gt;&lt;br&gt;
&lt;/div&gt;
</code></pre>

<p>I am trying to find the two elements ""file"" and ""submit"" using Selenium with Python. Below is the code I have tried to use.</p>

<pre><code>from selenium import webdriver
from selenium.webdriver.common.keys import Keys

# create a new Firefox session
driver = webdriver.Chrome()

# Maximize the browser window
driver.maximize_window()

# Enter the url to load
driver.get(""&lt;&lt;MY PAGE TO LOAD&gt;&gt;"")

# Wait for the page to load
driver.implicitly_wait(5)

# find the upload file type and pass a test value
upload_field = driver.find_element_by_partial_link_text('file')
upload_field.clear()
upload_field.send_keys(""test"")
</code></pre>

<p>When I run this code, I am able to load the page successfully in the Chrome browser but I get the below exception.</p>

<pre><code># Exception when trying to get element by type
Traceback (most recent call last):
  File ""C:\Users\TEST\Desktop\Test.py"", line 33, in &lt;module&gt;
    upload_field = driver.find_element_by_partial_link_text('file')
  File ""C:\Python\Python36\lib\site-packages\selenium\webdriver\remote\webdriver.py"", line 453, in find_element_by_partial_link_text
    return self.find_element(by=By.PARTIAL_LINK_TEXT, value=link_text)
  File ""C:\Python\Python36\lib\site-packages\selenium\webdriver\remote\webdriver.py"", line 955, in find_element
    'value': value})['value']
  File ""C:\Python\Python36\lib\site-packages\selenium\webdriver\remote\webdriver.py"", line 312, in execute
    self.error_handler.check_response(response)
  File ""C:\Python\Python36\lib\site-packages\selenium\webdriver\remote\errorhandler.py"", line 237, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {""method"":""partial link text"",""selector"":""file""}
  (Session info: chrome=63.0.3239.132)
  (Driver info: chromedriver=2.35.528161 (5b82f2d2aae0ca24b877009200ced9065a772e73),platform=Windows NT 10.0.14393 x86_64)
</code></pre>

<p>I looked at the solution provided <a href=""https://sqa.stackexchange.com/questions/15509/selenium-how-to-identify-an-object-by-its-type"">here</a> but this too is throwing an error. I am currently using Python 3.6.4 x64 with Selenium 3.8.1. My OS is Windows 7 x64 bit. How can I get elements with 'type' in html?</p>
","8741108","","","","","2019-02-19 15:43:46","How to find element using type in Selenium and Python","<python><python-3.x><selenium><selenium-webdriver><selenium-chromedriver>","3","2","2","","","CC BY-SA 3.0","0"
"54915381","1","54915827","","2019-02-27 22:16:58","","15","27496","<p>Python 2.7 is reaching its end of life on 1st Jan, 2020 as mentioned by </p>

<p><a href=""https://legacy.python.org/dev/peps/pep-0373/"" rel=""noreferrer"">https://legacy.python.org/dev/peps/pep-0373/</a></p>

<p><a href=""https://pythonclock.org/"" rel=""noreferrer"">https://pythonclock.org/</a></p>

<p>Will current <code>pip</code> keep on working for python 2.7 after that date? It is already showing the msg for deprecation of python 2.7. Will we be able to run
<code>pip install abc==1.2.3</code> after that date?</p>

<p>We do understand that after EOL, no new fixes/support will be done for 2.7, so that is not the concern here. </p>

<p>The question stems from the desire to keep running on python 2.7 even after EOL.</p>
","1487645","","","","","2019-08-27 03:06:21","Will PIP work for python 2.7 after its End of Life on 1st Jan 2020","<python><python-3.x><python-2.7><pip>","2","0","3","","","CC BY-SA 4.0","0"
"28205805","1","","","2015-01-29 01:58:19","","2","27421","<p>I have a 2D list containing these values:</p>

<pre><code>text = [[4, 3, 8, 9, 5, 1, 2, 7, 6], [8, 3, 4, 1, 5, 9, 6, 7, 2], 
[6, 1, 8, 7, 5, 3, 2, 9, 4], [6, 9, 8, 7, 5, 3, 2, 1, 4], 
[6, 1, 8, 7, 5, 3, 2, 1, 4], [6, 1, 3, 2, 9, 4, 8, 7, 5]]
</code></pre>

<p>For instance, text[i] should be printed like this:</p>

<pre><code>4 3 8
9 5 1
2 7 6
</code></pre>

<p>But my matrix prints this:</p>

<pre><code>   r = 6
   m = []
    for i in range(r):
        m.append([int(x) for x in text[i]])
    for i in m:
        print (i) 
&gt;&gt;
    4 3 8 9 5 1 2 7 6 
    8 3 4 1 5 9 6 7 2
    6 1 8 7 5 3 2 9 4 
    6 9 8 7 5 3 2 1 4 
    6 1 8 7 5 3 2 1 4 
    6 1 3 2 9 4 8 7 5 
</code></pre>
","4505474","","","","","2015-01-29 03:43:18","How do I create 3x3 matrices?","<python><python-3.x>","5","2","","","","CC BY-SA 3.0","0"
"42145656","1","","","2017-02-09 19:53:22","","27","27398","<p>I'm trying to import:</p>

<pre><code>from django.db import models
</code></pre>

<p>PyCharm underlines <code>django.db</code> and complains: <code>Unresolved reference 'django'</code>.</p>

<p>How do I get PyCharm to recognize Django?</p>
","7476650","","3474146","","2018-11-19 18:01:19","2020-09-03 15:55:38","Unresolved reference: 'django' error in PyCharm","<python><django><python-3.x><pycharm>","12","2","8","","","CC BY-SA 4.0","0"
"45661139","1","45661289","","2017-08-13 13:59:18","","20","27391","<p>I'm trying to do:</p>

<pre><code>from sklearn.model_selection import cross_validate
</code></pre>

<p>as mentioned <a href=""http://scikit-learn.org/stable/modules/cross_validation.html#the-cross-validate-function-and-multiple-metric-evaluation"" rel=""noreferrer"">here</a>.
But get the error:</p>

<pre><code>ImportError: cannot import name 'cross_validate'
</code></pre>

<p>Everything else in Sklearn seems to work fine, it's just this bit. Error even occurs when I run this one line and nothing else.</p>
","1761806","","","","","2019-03-08 10:21:56","ImportError: cannot import name 'cross_validate'","<python-3.x><scikit-learn>","3","0","2","","","CC BY-SA 3.0","0"
"54619868","1","54619932","","2019-02-10 18:56:03","","18","27382","<p>In my code below, I use <code>requests.post</code>. What are the possibilities to simply continue if the site is down?</p>

<p>I have the following code:</p>

<pre><code>def post_test():

    import requests

    url = 'http://example.com:8000/submit'
    payload = {'data1': 1, 'data2': 2}
    try:
        r = requests.post(url, data=payload)
    except:
        return   # if the requests.post fails (eg. the site is down) I want simly to return from the post_test(). Currenly it hangs up in the requests.post without raising an error.
    if (r.text == 'stop'):
        sys.exit()  # I want to terminate the whole program if r.text = 'stop' - this works fine.
</code></pre>

<p>How could I make the requests.post timeout, or return from post_test() if example.com, or its /submit app is down?</p>
","5195626","","562769","","2019-07-08 07:27:55","2019-07-08 07:27:55","HTTP requests.post timeout","<python><python-3.x><python-requests>","2","0","1","","","CC BY-SA 4.0","0"
"52092810","1","52093523","","2018-08-30 08:49:59","","11","27364","<p>I tried to install tensorflow cpu using pip in my windows8.1 64bit python3.6.0
using <code>pip install tensorflow</code>
but it gives me this error:</p>

<pre><code>Traceback (most recent call last):   File ""C:\Users\Laitooo
San\Desktop\tf.py"", line 1, in &lt;module&gt;
     import tensorflow as tf   File ""C:\Users\Laitooo San\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"",
 line 24, in &lt;module&gt;
     from tensorflow.python import *   File ""C:\Users\Laitooo San\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"",
 line 52, in &lt;module&gt;
     from tensorflow.core.framework.graph_pb2 import *   File ""C:\Users\Laitooo
 San\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\core\framework\graph_pb2.py"",
 line 6, in &lt;module&gt;
     from google.protobuf import descriptor as _descriptor   File ""C:\Users\Laitooo
 San\AppData\Local\Programs\Python\Python36\lib\site-packages\google\protobuf\descriptor.py"",
 line 47, in &lt;module&gt;
     from google.protobuf.pyext import _message ImportError: DLL load failed: The specified procedure could not be found.
</code></pre>

<p>I downloaded python36.dll and made sure all other .dll is there and install Microsoft visual c++ 2015</p>

<p>I also uninstalled tensorflow and installed another version several times but without any result.</p>
","7515149","","1839439","","2019-08-25 10:54:34","2020-04-10 03:57:35","Tensorflow error : DLL load failed: The specified procedure could not be found","<python><python-3.x><tensorflow>","8","0","7","","","CC BY-SA 4.0","0"
"41454511","1","41454722","","2017-01-04 00:40:13","","15","27322","<p>I am trying to learn TensorFlow and studying the example at: <a href=""https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/autoencoder.ipynb"" rel=""noreferrer"">https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/3_NeuralNetworks/autoencoder.ipynb</a></p>

<p>I then have some questions in the code below:</p>

<pre><code>for epoch in range(training_epochs):
    # Loop over all batches
    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        # Run optimization op (backprop) and cost op (to get loss value)
        _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})
    # Display logs per epoch step
    if epoch % display_step == 0:
        print(""Epoch:"", '%04d' % (epoch+1),
              ""cost="", ""{:.9f}"".format(c))
</code></pre>

<p>Since mnist is just a dataset, what exactly does <code>mnist.train.next_batch</code> mean? How was the <code>dataset.train.next_batch</code> defined?</p>

<p>Thanks!</p>
","3993270","","","","","2017-01-04 01:07:10","TensorFlow: how is dataset.train.next_batch defined?","<python-3.x><tensorflow><neural-network><autoencoder>","1","0","7","","","CC BY-SA 3.0","0"
"42346984","1","","","2017-02-20 14:19:29","","6","27288","<p>EDIT to format:</p>

<p>This is the original code</p>

<pre><code>from __future__ import print_function
import socket
import sys

def socket_accept():
    conn, address = s.accept()
    print(""Connection has been established | "" + ""IP "" + address[0] + ""| Port "" + str(address[1]))
    send_commands(conn)
    conn.close()

def send_commands(conn):
    while True:
        cmd = raw_input()
        if cmd == 'quit':
            conn.close()
            s.close()
            sys.exit()
        if len(str.encode(cmd)) &gt; 0:
            conn.send(str.encode(cmd))
            client_response = str(conn.recv(1024), ""utf-8"")
            print(client_response, end ="""")

def main():
    socket_accept()
    main()
</code></pre>

<p>I am getting this error “TypeError: str() takes at most 1 argument (2 given)” at “client_response” variable</p>
","4551950","","4345659","","2017-02-20 16:04:25","2018-08-01 06:11:55","I am getting this error ""TypeError: str() takes at most 1 argument (2 given)"" at ""client_response"" variable","<python><python-2.7><python-3.x><sockets><websocket>","3","2","0","","","CC BY-SA 3.0","0"
"35609592","1","","","2016-02-24 17:59:28","","6","27267","<p>I am unable to find a pygame download for Python 3.5 and the ones I have downloaded don't seem to work when I import to the shell. Help?</p>

<p>This is the message I receive on the shell:</p>

<blockquote>
  <blockquote>
    <blockquote>
      <p>import pygame
      Traceback (most recent call last):
        File """", line 1, in 
          import pygame
      ImportError: No module named 'pygame'</p>
    </blockquote>
  </blockquote>
</blockquote>
","5869838","","","","","2017-04-12 03:58:27","How do I download Pygame for Python 3.5.1?","<python><python-3.x><pygame>","6","6","","","","CC BY-SA 3.0","0"
"42718547","1","42763361","","2017-03-10 12:34:39","","17","27262","<p>When I use MongoChef to connect remote mongo database, I use next parameters:</p>

<hr>

<p><strong>Server</strong></p>

<ul>
<li><em>Server:</em> localhost</li>
<li><em>Port:</em> 27017</li>
</ul>

<p><strong>SSH Tunnel</strong></p>

<ul>
<li><p><em>SSH address:</em> 10.1.0.90</p></li>
<li><p><em>Port:</em> 25</p></li>
<li><p><em>SSH Username:</em> username</p></li>
<li><p><em>SSH Password:</em> password</p></li>
</ul>

<hr>

<p>When I connect with Pymongo, I have the next code:</p>

<pre><code>import pymongo

MONGO_HOST = ""10.1.0.90""
MONGO_PORT = 25
MONGO_DB = ""db_name""
MONGO_USER = ""username""
MONGO_PASS = ""password""

con = pymongo.MongoClient(MONGO_HOST, MONGO_PORT)
db = con[MONGO_DB]
db.authenticate(MONGO_USER, MONGO_PASS)

print(db)
</code></pre>

<p>But I have the next error:</p>

<pre><code>pymongo.errors.ServerSelectionTimeoutError: 10.1.2.84:27017: [Errno 111] Connection refused
</code></pre>

<p>Please, could you help me with this problem? What did I do wrong?</p>
","6013311","","98044","","2019-08-30 20:44:18","2020-10-26 22:59:37","How to connect remote mongodb with pymongo","<python><mongodb><python-3.x><ssh><pymongo>","3","4","4","","","CC BY-SA 4.0","0"
"41649916","1","","","2017-01-14 12:20:33","","17","27251","<p>As mentioned, is there a way to send global <code>ESC</code> key to close popup(CSS MODAL Window)? I tried following but did not work:</p>

<pre><code>driver.find_element_by_tag_name('body').send_keys(Keys.ESCAPE)
</code></pre>

<p>I know I can use xPath etc but issue is the site has dynamic elementIds and classnames.</p>
","275002","","608639","","2019-12-22 19:39:35","2019-12-22 19:39:35","How to send ESC key to close pop up window using Python and Selenium?","<python><python-3.x><selenium><selenium-webdriver>","3","3","5","","","CC BY-SA 4.0","0"
"44145784","1","44145932","","2017-05-23 22:04:41","","6","27184","<p>I'm getting the JSON via:</p>

<pre><code>with open(""config.json"") as data_file:
    global data
    data = json.load(data_file)
</code></pre>

<p>And I want to check if the <code>data[""example""]</code> is empty or not.</p>
","8056128","","2778474","","2019-11-08 07:27:17","2019-11-08 07:27:17","Check if key is missing after loading json from file in python","<python><python-3.x>","3","4","1","","","CC BY-SA 4.0","0"
"38507426","1","38509168","","2016-07-21 14:47:13","","5","27068","<p>I am using python 3.x and using the following code to convert image into text:</p>

<pre><code>from PIL import Image
from pytesseract import image_to_string

image = Image.open('image.png', mode='r')
print(image_to_string(image))
</code></pre>

<p>I am getting the following error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/hp/Desktop/GII/Image_to_text.py"", line 12, in &lt;module&gt;
    print(image_to_string(image))
  File ""C:\Users\hp\Downloads\WinPython-64bit-3.5.1.2\python-3.5.1.amd64\lib\site-packages\pytesseract\pytesseract.py"", line 161, in image_to_string
    config=config)
  File ""C:\Users\hp\Downloads\WinPython-64bit-3.5.1.2\python-3.5.1.amd64\lib\site-packages\pytesseract\pytesseract.py"", line 94, in run_tesseract
    stderr=subprocess.PIPE)
  File ""C:\Users\hp\Downloads\WinPython-64bit-3.5.1.2\python-3.5.1.amd64\lib\subprocess.py"", line 950, in __init__
    restore_signals, start_new_session)
  File ""C:\Users\hp\Downloads\WinPython-64bit-3.5.1.2\python-3.5.1.amd64\lib\subprocess.py"", line 1220, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] The system cannot find the file specified
</code></pre>

<p>Please note that I have put the image in the same directory where my python is present. Also It does not raise error on  <code>image = Image.open('image.png', mode='r')</code> but it raises on the line <code>print(image_to_string(image))</code>. </p>

<p>Any idea what might be wrong here? Thanks</p>
","5453723","","","","","2019-12-19 09:21:25","Image to text python","<python><python-3.x><pytesser>","5","3","6","","","CC BY-SA 3.0","0"
"51832593","1","51832671","","2018-08-14 01:36:51","","10","27034","<p>I wrote the following code which extracts the info. of a file and orders it alphabetically based on its second column objects:</p>

<pre><code>import csv
import operator
import sys

def re_sort(in_file='books.csv', out_file='books_sort.csv'):
    data = csv.reader(open('books.csv', newline=''), delimiter=',')
    header = next(data)
    sortedlist = sorted(data, key=operator.itemgetter(1))
    with open(""books_sorted.csv"", ""w"", newline='') as csvfile:
        cvsWriter = csv.writer(csvfile, delimiter=',')
        cvsWriter.writerow(header)
        cvsWriter.writerows(sortedlist)
</code></pre>

<p>Whenever I try to run this code on the command line, it gives me the error TypeError: 'newline' is an invalid keyword argument for this function. Do you guys see reasons why this may be happening. The following if a structured version of the contents in the file:</p>

<pre><code>Title,            Author,        Publisher,  Year,  ISBN-10,   ISBN-13
Automate the...,  Al Sweigart,   No Sta...,  2015,  15932...,  978-15932...
Dive into Py...,  Mark Pilgr..., Apress,     2009,  14302...,  978-14302...
""Python Cook...,  ""David Bea..., O'Reil...,  2013,  14493...,  978-14493...
Think Python...,  Allen B. D..., O'Reil...,  2015,  14919...,  978-14919...
""Fluent Pyth...,  Luciano Ra..., O'Reil...,  2015,  14919...,  978-14919...
</code></pre>
","10200421","","","","","2018-08-14 15:23:43","TypeError: 'newline' is an invalid keyword argument for this function","<python><python-3.x>","2","3","","2018-08-14 03:02:48","","CC BY-SA 4.0","0"
"37710848","1","37713583","","2016-06-08 19:04:54","","16","27013","<p>I am using python 3.5 and I am doing Algorithms specialization courses on Coursera. Professor teaching this course posted a program which can help us to know the time and memory associated with running a program. It has <code>import resource</code> command at the top. I tried to run this program along with the programs I have written in python and every time I received <code>ImportError: No module named 'resource'</code> </p>

<p>I used the same code in ubuntu and have no errors at all. </p>

<p>I followed suggestions in stackoverflow answers and I have tried adding PYTHONPATH PYTHONHOME and edited the PATH environment variable.</p>

<p>I have no idea of what else I can do here.</p>

<p>Is there any file that I can download and install it in the Lib or site-packages folder of  my python installation ?</p>
","6065030","","","","","2018-12-18 22:45:53","ImportError: No module named 'resource'","<python><python-3.x><resources>","2","0","3","","","CC BY-SA 3.0","0"
"33069476","1","","","2015-10-11 19:56:22","","2","27002","<p>I have been asked to simulate rolling two fair dice with sides 1-6. So the possible outcomes are 2-12.</p>

<p>my code is as follows:</p>

<pre><code>def dice(n):
    x=random.randint(1,6)
    y=random.randint(1,6)
    for i in range(n):
        z=x+y
    return z
</code></pre>

<p>My problem is that this is only returning the outcome of rolling the dice 1 time, so the outcome is always 2-12. I want it to return the sum of rolling the dice (n) times. </p>

<p>Does anyone have any suggestions for me?</p>
","5434295","","100297","","2015-10-11 19:57:45","2016-11-12 10:48:36","simulating rolling 2 dice in Python","<python><python-3.x><random>","1","0","1","","","CC BY-SA 3.0","0"
"41066582","1","","","2016-12-09 18:20:54","","16","26998","<p>Is it possible to save a pandas data frame directly to a parquet file?
If not, what would be the suggested process? </p>

<p>The aim is to be able to send the parquet file to another team, which they can use scala code to read/open it. Thanks!</p>
","3993270","","741164","","2019-01-29 06:56:49","2020-10-28 17:57:51","Python: save pandas data frame to parquet file","<python-3.x><hdfs><parquet>","6","2","6","","","CC BY-SA 3.0","0"
"34497323","1","34497639","","2015-12-28 16:43:53","","11","26990","<p>Right now I'm trying to make a small code with a raspberry pi and and a makey makey. The makey makey is a small board that acts as a usb keyboard when certain contacts are powered. My question is what is the easiest way to detect those keypresses inside a python script. I understand using the GPIO pins would be easier, but right now I'm looking for this. I have seen examples such as using using getch() from msvcrt (which from what I understand is windows only,) using pygame.key, and using getKey. Which of theses is easiest to use? Are there any that can detect a key being pressed and a key being released?</p>

<p>Pseudocode Code (... is that what it's called?)</p>

<pre><code>import whatever needs importing    

if the ""W"" key is pressed:
   print (""You pressed W"")

elif the ""S"" is pressed:
    print (""You pressed S"")
</code></pre>

<p>and so on. Thanks.</p>
","5280147","","","","","2019-03-26 18:06:57","What is the easiest way to detect key presses in python 3 on a linux machine?","<python><linux><python-3.x><raspberry-pi>","2","1","4","","","CC BY-SA 3.0","0"
"51337167","1","51337267","","2018-07-14 09:35:10","","1","26964","<p>I found the following code on a python tutorial website: </p>

<pre><code>from nltk.tag import StanfordNERTagger
from nltk.tokenize import word_tokenize


stanford_classifier = open(r""C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\stanford-ner-2018-02-27\classifiers\english.all.3class.distsim.crf.ser.gz"")
stanford_ner_path = open(r""C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\stanford-ner-2018-02-27\stanford-ner.jar"")


st = StanfordNERTagger(stanford_classifier, stanford_ner_path)

text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'

tokenized_text = word_tokenize(text)
classified_text = st.tag(tokenized_text)

print (classified_text)
</code></pre>

<p>The error is as follows: </p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/DELL7810/AppData/Local/Programs/Python/Python37/stanpar.py"", line 9, in &lt;module&gt;
st = StanfordNERTagger(stanford_classifier, stanford_ner_path)
 File ""C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\tag\stanford.py"", line 180, in __init__
super(StanfordNERTagger, self).__init__(*args, **kwargs)
File ""C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\tag\stanford.py"", line 63, in __init__
verbose=verbose)
File ""C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\internals.py"", line 721, in find_jar
searchpath, url, verbose, is_regex))
File ""C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\lib\site-packages\nltk\internals.py"", line 632, in find_jar_iter
if os.path.isfile(path_to_jar):
File ""C:\Users\DELL7810\AppData\Local\Programs\Python\Python37\lib\genericpath.py"", line 30, in isfile
st = os.stat(path)
TypeError: stat: path should be string, bytes, os.PathLike or integer, not _io.TextIOWrapper
</code></pre>
","7051183","","6749601","","2018-07-14 09:37:59","2018-07-14 09:47:17","TypeError: stat: path should be string, bytes, os.PathLike or integer, not _io.TextIOWrapper","<python><python-3.x><stanford-nlp>","1","1","","","","CC BY-SA 4.0","0"
"42175190","1","","","2017-02-11 11:14:17","","7","26944","<p>I have a number of facebook groups that I would like to get the count of the members of. An example would be this group: <a href=""https://www.facebook.com/groups/347805588637627/"" rel=""noreferrer"">https://www.facebook.com/groups/347805588637627/</a>
I have looked at inspect element on the page and it is stored like so: </p>

<pre><code>&lt;span id=""count_text""&gt;9,413 members&lt;/span&gt;
</code></pre>

<p>I am trying to get ""9,413 members"" out of the page. I have tried using BeautifulSoup but cannot work it out.</p>

<p>Thanks</p>

<p>Edit:</p>

<pre><code>from bs4 import BeautifulSoup
import requests

url = ""https://www.facebook.com/groups/347805588637627/""
r  = requests.get(url)
data = r.text
soup = BeautifulSoup(data, ""html.parser"")
span = soup.find(""span"", id=""count_text"")
print(span.text)
</code></pre>
","7549832","","7549832","","2017-02-11 11:55:33","2019-12-20 05:47:42","Get value of span tag using BeautifulSoup","<python><html><python-3.x><parsing><beautifulsoup>","4","1","1","","","CC BY-SA 3.0","0"
"30323439","1","30323523","","2015-05-19 10:45:59","","11","26909","<p>How can I raise the numbers in list to a certain power?</p>
","4908068","","4908068","","2015-05-19 11:01:15","2020-06-03 19:02:58","Raising elements of a list to a power","<python><list><python-3.x>","8","2","2","","","CC BY-SA 3.0","0"
"51712693","1","","","2018-08-06 17:18:32","","22","26828","<p><strong>conda update conda</strong> >> successful</p>

<p><strong>conda update anaconda</strong> >> gives me error saying package is not installed in prefix.</p>

<p>I have single installation of Python distribution on my system. How do I solve this issue?</p>

<pre><code>(base) C:\Users\asukumari&gt;conda info
</code></pre>

<blockquote>
<pre><code> active environment : base
active env location : C:\Users\asukumari\AppData\Local\Continuum\anaconda3
        shell level : 1
   user config file : C:\Users\asukumari\.condarc  populated config files : C:\Users\asukumari\.condarc
      conda version : 4.5.9
conda-build version : 3.4.1
     python version : 3.6.4.final.0
   base environment : C:\Users\asukumari\AppData\Local\Continuum\anaconda3  (writable)
       channel URLs : https://repo.anaconda.com/pkgs/main/win-64
                      https://repo.anaconda.com/pkgs/main/noarch
                      https://repo.anaconda.com/pkgs/free/win-64
                      https://repo.anaconda.com/pkgs/free/noarch
                      https://repo.anaconda.com/pkgs/r/win-64
                      https://repo.anaconda.com/pkgs/r/noarch
                      https://repo.anaconda.com/pkgs/pro/win-64
                      https://repo.anaconda.com/pkgs/pro/noarch
                      https://repo.anaconda.com/pkgs/msys2/win-64
                      https://repo.anaconda.com/pkgs/msys2/noarch
      package cache : C:\Users\asukumari\AppData\Local\Continuum\anaconda3\pkgs
                      C:\Users\asukumari\AppData\Local\conda\conda\pkgs
   envs directories : C:\Users\asukumari\AppData\Local\Continuum\anaconda3\envs
                      C:\Users\asukumari\AppData\Local\conda\conda\envs
                      C:\Users\asukumari\.conda\envs
           platform : win-64
         user-agent : conda/4.5.9 requests/2.18.4 CPython/3.6.4 Windows/10 Windows/10.0.16299
      administrator : False
         netrc file : None
       offline mode : False
</code></pre>
</blockquote>
","2252819","","6573902","","2020-03-02 09:40:36","2020-05-26 07:36:24","PackageNotInstalledError: Package is not installed in prefix","<python><python-3.x><windows><anaconda><conda>","7","3","7","","","CC BY-SA 4.0","0"
"47515361","1","47515721","","2017-11-27 16:17:37","","8","26813","<p>I am wondering how I can make my bot upload an embedded image to a Discord channel. I know how to make it send embeds, but how do I upload an embedded image? Is it even possible with discord.py?
Keep in mind I am not referring to the thumbnail image you can have in an embed image, I am wondering if you even can upload an embed image using Python. Thanks!</p>
","8217911","","4762738","","2020-06-21 18:48:22","2020-06-21 18:48:22","Upload an embed image?","<python-3.x><discord.py>","1","0","2","","","CC BY-SA 4.0","0"
"49218302","1","","","2018-03-11 08:56:24","","20","26795","<p>I have seen both ways but I do not understand what the difference is and what I should use as ""best practice"":</p>

<pre><code>def custom_function(**kwargs):
    foo = kwargs.pop('foo')
    bar = kwargs.pop('bar')
    ...

def custom_function2(**kwargs):
    foo = kwargs.get('foo')
    bar = kwargs.get('bar')
    ...
</code></pre>
","5005715","","5658350","","2019-07-29 12:19:37","2019-07-29 12:19:37","Python: Difference between kwargs.pop() and kwargs.get()","<python><python-3.x><python-2.7><dictionary><keyword-argument>","4","5","6","","","CC BY-SA 3.0","0"
"50846431","1","50846530","","2018-06-13 21:19:13","","17","26788","<p>How can I load a YAML file and convert it to a Python JSON object?</p>

<p>My YAML file looks like this:</p>

<pre><code>Section:
    heading: Heading 1
    font: 
        name: Times New Roman
        size: 22
        color_theme: ACCENT_2

SubSection:
    heading: Heading 3
    font:
        name: Times New Roman
        size: 15
        color_theme: ACCENT_2
Paragraph:
    font:
        name: Times New Roman
        size: 11
        color_theme: ACCENT_2
Table:
    style: MediumGrid3-Accent2
</code></pre>
","9064794","","1307905","","2018-06-14 07:23:19","2020-01-09 16:10:01","Converting a YAML file to Python JSON object","<python><json><python-3.x><yaml>","4","0","5","","","CC BY-SA 4.0","0"
"47500266","1","47502733","","2017-11-26 19:43:36","","3","26771","<p>I want to fill my entries when I click in a name of my Combobox without buttons like 'check' to show the values. How can i do that?</p>

<pre><code>import tkinter as tk
from tkinter import ttk
import csv

root = tk.Tk()
cb = ttk.Combobox(root,state='readonly')
labName = ttk.Label(root,text='Names: ')
labTel = ttk.Label(root,text='TelNum:')
labCity = ttk.Label(root,text='City: ')
entTel = ttk.Entry(root,state='readonly')
entCity = ttk.Entry(root,state='readonly')

with open('file.csv','r',newline='') as file:
    reader = csv.reader(file,delimiter='\t')    


cb.grid(row=0,column=1)
labName.grid(row=0,column=0)
labTel.grid(row=1,column=0)
entTel.grid(row=1,column=1)
labCity.grid(row=2,column=0)
entCity.grid(row=2,column=1)
</code></pre>
","8826798","","","","","2018-11-05 04:12:22","Python tkinter Combobox","<python-3.x><tkinter><combobox><ttk>","1","0","1","","","CC BY-SA 3.0","0"
"51345024","1","","","2018-07-15 03:35:06","","4","26712","<p>I have a text file(.txt) just looks like below:</p>

<hr>

<p>Date, Day, Sect, 1, 2, 3</p>

<p>1, Sun, 1-1, 123, 345, 678</p>

<p>2, Mon, 2-2, 234, 585, 282</p>

<p>3, Tue, 2-2, 231, 232, 686</p>

<hr>

<p>With this data I want to do the followings:</p>

<p>1) Read the text file by line as a separate element in the list</p>

<ul>
<li><p>Split elements by comma</p></li>
<li><p>Delete non-necessary elements('\n') in the list</p></li>
</ul>

<p>For the two, I did these.</p>

<pre><code>file = open('abc.txt', mode = 'r', encoding = 'utf-8-sig')
lines = file.readlines()
file.close()
my_dict = {}
my_list = []
for line in lines:
    line = line.split(',')
    line = [i.strip() for i in line]
</code></pre>

<p>2) Set the first row(Date, Day, Sect, 1, 2, 3) as key and set the other rows as values in the dictionary.</p>

<pre><code>    my_dict['Date'] = line[0]
    my_dict['Day'] = line[1]
    my_dict['Sect'] = line[2]
    my_dict['1'] = line[3]
    my_dict['2'] = line[4]
    my_dict['3'] = line[5]
</code></pre>

<p>The above code has two issues: 1) Set the first row as dictionary, too. 2) If I add this to the list as the below, it only keeps the last row as all elements in the list.</p>

<p>3) Create a list including the dictionary as elements.</p>

<pre><code>    my_list.append(my_dict)    
</code></pre>

<p>4) Subset the elements that I want to.</p>

<p>I couldn't write any code from here. But What I want to do is subset elements meeting the condition: For example, choosing the element in the dictionary where the Sect is 2-2. Then the wanted results could be as the follows:</p>

<pre><code>&gt;&gt; [{'Date': '2', 'Day': 'Mon', 'Sect': '2-2', '1': '234', '2': '585', '3': '282'}, {'Date': '3', 'Day': 'Tue', 'Sect': '2-2', '1': '231', '2':'232', '3':'686'}]
</code></pre>

<p>Thanks,</p>
","8618146","","","","","2018-07-15 04:27:51","Read text file and parse in python","<python><python-3.x><list><parsing>","4","2","2","","","CC BY-SA 4.0","0"
"50019588","1","50019778","","2018-04-25 10:11:14","","6","26702","<p>Code:</p>

<pre><code>for i in range(1000):
    print(i) if i%10==0 else pass
</code></pre>

<p>Error:</p>

<pre><code>File ""&lt;ipython-input-117-6f18883a9539&gt;"", line 2
    print(i) if i%10==0 else pass
                            ^
SyntaxError: invalid syntax
</code></pre>

<p>Why isn't 'pass' working here?</p>
","6403976","","6403976","","2018-04-25 10:26:20","2018-04-25 10:27:42","doing ""nothing"" in else command of if-else clause","<python><python-3.x>","2","12","2","2018-04-25 17:28:35","","CC BY-SA 3.0","0"
"28253102","1","28253152","","2015-01-31 15:11:59","","10","26687","<p>I'm fairly new to Python and trying to create a function to multiply a vector by a matrix (of any column size).
e.g.:</p>

<pre><code>multiply([1,0,0,1,0,0], [[0,1],[1,1],[1,0],[1,0],[1,1],[0,1]])

[1, 1]
</code></pre>

<p>Here is my code:</p>

<pre><code>def multiply(v, G):
    result = []
    total = 0
    for i in range(len(G)):
        r = G[i]
        for j in range(len(v)):
            total += r[j] * v[j]
        result.append(total)
    return result  
</code></pre>

<p>The problem is that when I try to select the first row of each column in the matrix (r[j]) the error 'list index out of range' is shown. Is there any other way of completing the multiplication without using NumPy?</p>
","4514769","","2867928","","2018-03-26 05:56:14","2020-05-04 01:46:55","Python 3: Multiply a vector by a matrix without NumPy","<python><python-3.x><numpy><matrix><vector>","6","0","1","","","CC BY-SA 3.0","0"
"36329269","1","36330330","","2016-03-31 09:23:34","","14","26687","<p>Why am I getting an error here that relates to the plt.plot label?</p>

<pre><code>fig = plt.figure()
ax = plt.gca()
barplt = plt.bar(bins,frq,align='center',label='Dgr')
normplt = plt.plot(bins_n,frq_n,'--r', label='Norm');
ax.set_xlim([min(bins)-1, max(bins)+1])
ax.set_ylim([0, max(frq)])
plt.xlabel('Dgr')
plt.ylabel('Frequency')
plt.show()
plt.legend(handles=[barplt,normplt])
</code></pre>

<p>This is the error that I get:
<em>'list' object has no attribute 'get_label'</em></p>
","2812237","","2812237","","2016-03-31 09:31:59","2019-05-21 10:15:23","Python legend attribute error","<python-3.x><matplotlib>","2","1","2","","","CC BY-SA 3.0","0"
"42265119","1","","","2017-02-16 04:40:51","","1","26684","<p>I am learning programming with python (referring think <code>python 2</code>) and am struck at a program. Problem statement: <code>Python program to draw a symmetric flower after seeking the size of and number of petals from user</code>.</p>

<p>The code i came up with is below, except i am unable to get the angle between each petal mathematically right (part where the code near the end states <code>bob.lt(360/petal)</code>). Can someone help here?</p>

<pre><code>import math
radius=int(input(""What is the radius of the flower? ""))
petals=int(input(""How many petals do you want? ""))
#radius=100
#petals=4


def draw_arc(b,r):  #bob the turtle,corner-to-corner length (radius) of petal (assume 60 degree central angle of sector for simplicity)
    c=2*math.pi*r #Circumference of circle
    ca=c/(360/60)  #Circumference of arc (assume 60 degree central angle of sector as above)
    n=int(ca/3)+1  #number of segments
    l=ca/n  #length of segment
    for i in range(n):
        b.fd(l)
        b.lt(360/(n*6))

def draw_petal(b,r):
    draw_arc(b,r)
    b.lt(180-60)
    draw_arc(b,r)

import turtle
bob=turtle.Turtle()

#draw_petal(bob,radius)

for i in range(petals):
    draw_petal(bob,radius)
    bob.lt(360/petals)

turtle.mainloop()
</code></pre>

<p><img src=""https://i.stack.imgur.com/HFraN.png"" alt=""Expected Flower"">
Correct (Symmetric)
<img src=""https://i.stack.imgur.com/lvF6L.png"" alt=""Incorrect Flower"">
Incorrect (Asymmetric)</p>
","7572584","","7572584","","2017-02-16 07:58:55","2020-02-14 13:20:16","drawing flower with python turtle","<python><python-3.x><turtle-graphics>","2","2","","","","CC BY-SA 3.0","0"
"41494191","1","41495181","","2017-01-05 20:34:32","","-2","26647","<p>I have a list in python which is produced from a number of functions. It is produced like this: </p>

<pre><code>['01', '1.0', '[0.2]']
</code></pre>

<p>I would like to strip all of the unnecessary characters after it is produced.</p>

<p>Ideal output:</p>

<pre><code>['01', '1.0', '0.2']
</code></pre>

<p>I basically need to remove the [ and ] from the final string in the list.</p>

<p><strong>Update:</strong></p>

<pre><code>list_test = ['01', '1.0', '[0.2]']
[i.strip('[]') if type(i) == str else str(i) for i in list_test]
print(list_test)
</code></pre>

<p>This doesn't work as both with and without produce the same result:</p>

<pre><code>['01', '1.0', '[0.2]']
</code></pre>

<p>The required result is:</p>

<pre><code>['01', '1.0', '0.2']
</code></pre>

<hr>

<p><strong>Provided solution:</strong></p>

<pre><code>l = ['01', '1.0', '[0.2]']
[i.strip('[0.2]') if type(i) == str else str(i) for i in l]
print(l)
</code></pre>

<p>output:</p>

<pre><code>['01', '1.0', '0.2']

Process finished with exit code 0
</code></pre>
","7240325","","2063361","","2017-01-08 14:16:50","2017-01-11 19:09:35","Strip a list in Python","<python><list><python-3.x>","3","13","2","2017-01-08 14:18:38","","CC BY-SA 3.0","0"
"54191677","1","54191712","","2019-01-15 01:49:05","","3","26644","<p>I am working on an Image Convolution code using numpy:</p>

<pre><code>def CG(A, b, x, imax=10, epsilon = 0.01):
    steps=np.asarray(x)
    i = 0
    r = b - A * x
    d = r.copy()
    delta_new = r.T * r
    delta_0 = delta_new
    while i &lt; imax and delta_new &gt; epsilon**2 * delta_0:
        q = A * d
        alpha = float(delta_new / (d.T * q))
        x = x + alpha * d
        if i%50 == 0:
            r = b - A * x
        else:
            r = r - alpha * q
        delta_old = delta_new
        delta_new = r.T * r
        beta = float(delta_new / delta_old)
        d = r + beta * d
        i = i + 1
        steps = np.append(steps, np.asarray(x), axis=1)
    return steps
</code></pre>

<p>I get the below error:</p>

<pre><code>ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
</code></pre>

<p>on line <code>while i &lt; imax and delta_new &gt; epsilon**2 * delta_0:</code></p>

<p>Could anyone please tell me what am I doing wrong ? </p>
","1774508","","","","","2019-01-15 01:54:48","NumPy Error: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()","<python><python-3.x><numpy>","3","1","","","","CC BY-SA 4.0","0"
"40001892","1","40002420","","2016-10-12 14:58:44","","51","26575","<p>Can I use <code>argparse</code> to read named command line arguments that do not need to be in a specific order? I browsed through the <a href=""https://docs.python.org/3/library/argparse.html"" rel=""noreferrer"">documentation</a> but most of it focused on displaying content based on the arguments provided (such as <code>--h</code>).</p>

<p>Right now, my script reads ordered, unnamed arguments:</p>

<blockquote>
  <p>myscript.py foo-val bar-val</p>
</blockquote>

<p>using <code>sys.argv</code>:</p>

<pre><code>foo = sys.argv[1]
bar = sys.argv[2]
</code></pre>

<p>But I would like to change the input so that it is order agnostic by naming arguments:</p>

<blockquote>
  <p>myscript.py --bar=bar-val --foo=foo-val </p>
</blockquote>
","1312080","","1312080","","2016-10-12 15:01:45","2020-09-17 05:48:45","Reading named command arguments","<python><python-3.x><arguments>","3","1","12","","","CC BY-SA 3.0","0"
"48612603","1","48613050","","2018-02-04 20:18:57","","9","26557","<p>I am in the process of making a discord bot using discord.py and asyncio. The bot has commands like <code>kick</code> and <code>ban</code> which obviously should not be available to normal users.</p>

<p>I want to make a simple system which will detect what permissions the user's role has using <code>ctx.message.author</code> to get the user who sent the command.</p>

<p>I do not want the bot to detect a specific role name as these vary across servers. I also prefer not to have multiple files for the bot to keep it simple.</p>

<p>I have seen the discord.py documentation and various other sources but none contain examples of how to implement the various methods they talk about.</p>

<p>As an example, here is a single command from my bot:</p>

<pre><code>async def kick(ctx, userName: discord.User):
    if True: #ctx.message.author.Permissions.administrator
        await BSL.kick(userName)
    else:
        permission_error = str('Sorry ' + ctx.message.author + ' you do not have permissions to do that!')
        await BSL.send_message(ctx.message.channel, permission_error)
</code></pre>

<p>Where the <code>if else</code> statement is my attempt of doing this on my own. The <code>#ctx.message.author.Permissions.administrator</code> is commented out as it does not work and replaced with <code>True</code> for testing purposes.</p>

<p>Thank you for any help and suggestions in advance.</p>
","8623347","","8623347","","2018-03-08 09:10:08","2019-08-10 21:08:20","Permission System for Discord.py Bot","<python><python-3.x><python-asyncio><discord><discord.py>","3","0","1","","","CC BY-SA 3.0","0"
"33205895","1","","","2015-10-19 02:53:18","","4","26552","<p>My question is, can Sublime 3 be setup to run the code that has been written with in Sublime.   I've have searched on here and the internet and have tried numerous different suggestions.   If the answer has already been posted and someone could point me in the proper direction / URL I'd appreciate it.   If it cannot be done and you have other suggestion's I'll give it a try.</p>
","3085292","","1385931","","2015-10-19 07:54:10","2017-07-08 16:41:04","Running Code with Python 3.5.0 + Sublime 3.0 on Mac","<python><macos><python-3.x><sublimetext3>","4","0","2","","","CC BY-SA 3.0","0"
"37887624","1","37889713","","2016-06-17 17:58:48","","6","26526","<p>I have noticed that on any python 3 program no matter how basic it is if you press CTRL c it will crash the program for example:</p>

<pre><code>test=input(""Say hello"")
if test==""hello"":
    print(""Hello!"")
else:
    print(""I don't know what to reply I am a basic program without meaning :("")
</code></pre>

<p>If you press CTRL c the error will be KeyboardInterrupt is there anyway of stopping this from crashing the program?</p>

<p>The reason I want to do this is because I like to make my programs error proof, and whenever I want to paste something into the input and I accidentally press CTRL c I have to go through my program again..Which is just very annoying.  </p>
","6338079","","","","","2019-09-08 05:30:32","Python 3 KeyboardInterrupt error","<python-3.x><ctrl><keyboardinterrupt>","1","4","","","","CC BY-SA 3.0","0"
"50046158","1","50051708","","2018-04-26 14:55:07","","12","26494","<p>I am trying to connect to MS SQL Server using <code>pyodbc</code> from a remote machine running Ubuntu 16.04.</p>

<pre><code>import pyodbc 

conn = pyodbc.connect(r'DRIVER=ODBC Driver 17 for SQL Server; SERVER=xxxTest-SRV; PORT=51333; DATABASE=TestDB; UID=xxxx; PWD=xxxx;')
</code></pre>

<p>I'm getting following error:</p>

<blockquote>
  <p>pyodbc.OperationalError: ('HYT00', '[HYT00] [unixODBC][Microsoft][ODBC
  Driver 17 for SQL Server]Login timeout expired (0)
  (SQLDriverConnect)')</p>
</blockquote>

<p>I tried using the server <code>IP</code> in the connection string but still no luck.</p>

<p>However, I am able to connect to using <code>sqlcmd</code> from the terminal<br>
Following works:</p>

<pre><code>sqlcmd -S xxxTest-SRV, 51333 -d TestDB -U xxxx -P xxxx
</code></pre>

<p>I didn't find any issue that gave an answer to my problem. </p>

<p><strong><em>odbcinst.ini</em></strong>  </p>

<pre><code>[ODBC Driver 17 for SQL Server]
Description=Microsoft ODBC Driver 17 for SQL Server
Driver=/opt/microsoft/msodbcsql/lib64/libmsodbcsql-17.1.so.1.1
UsageCount=1
</code></pre>

<p>There's always seems to be an issue connecting to MS SQL Server using <code>pyodbc</code> from a linux machine. Is there a way to connect to SQL Server from Python. I'll appreciate your help in solving this error. Thank you. </p>

<p><strong>[UPDATE]</strong></p>

<p>As per the below answer, I updated the connection string. But, now I get following error:</p>

<blockquote>
  <p>pyodbc.Error: ('01000', ""[01000] [unixODBC][Driver Manager]Can't open
  lib '/opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.0.so.1.1' : file
  not found (0) (SQLDriverConnect)"")</p>
</blockquote>

<p>My <strong><em>odbcinst.ini</em></strong> file driver definition:</p>

<pre><code>[ODBC Driver 17 for SQL Server]
Description=Microsoft ODBC Driver 17 for SQL Server
Driver=/opt/microsoft/msodbcsql17/lib64/libmsodbcsql-17.0.so.1.1
UsageCount=1
</code></pre>

<p>It has always been a nightmare to connect to MS SQL Server from a Linux machine. Can you please tell which <code>pyodbc</code>, <code>unixODBC</code> and <code>Driver</code> version is the most stable?</p>

<p>I have installed the driver following <a href=""https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-2017"" rel=""noreferrer"">this</a> Microsoft instructions. My <code>pyodbc</code> version is <code>4.0.23</code></p>
","6395618","","6395618","","2018-04-28 18:12:41","2020-08-18 15:54:30","Pyodbc: Login Timeout Error","<sql-server><python-3.x><ubuntu-16.04><pyodbc>","2","0","","","","CC BY-SA 3.0","0"
"37092909","1","37097353","","2016-05-07 19:38:42","","10","26489","<p>I am trying to send a colored text message to a user as reply, using <code>sendMessage</code> with HTML parsing.</p>

<pre><code>bot.sendMessage(update.message.chat_id, ""&lt;span style=\""color:blue\""&gt;foo&lt;/span&gt;"", telegram.ParseMode.HTML)
</code></pre>

<p>Sending <code>&lt;span style=""color:blue""&gt;foo&lt;/span&gt;</code> doesn't work, as <code>span</code> is not supported: </p>

<blockquote>
  <p>Bad Request: Can't parse message text: Unsupported start tag ""span"" at byte offset 0 (400)</p>
</blockquote>

<p>I am using the <code>python-telegram-bot</code> v. 4.0.3.</p>

<p>Is there another way?</p>
","3980929","","3980929","","2017-07-25 19:24:55","2020-07-19 16:49:50","How to send a colored text message?","<python><python-3.x><python-telegram-bot>","2","3","3","","","CC BY-SA 3.0","0"
"31623194","1","","","2015-07-25 05:12:20","","31","26471","<p>I am using Python3 Asyncio module to create a load balancing application. I have two heavy IO tasks:</p>

<ul>
<li>A SNMP polling module, which determines the best possible server</li>
<li>A ""proxy-like"" module, which balances the petitions to the selected server.</li>
</ul>

<p>Both processes are going to run forever, are independent from eachother and should not be blocked by the other one.</p>

<p>I cant use 1 event loop because they would block eachother, is there any way to have 2 event loops or do I have to use multithreading/processing? </p>

<p>I tried using asyncio.new_event_loop() but havent managed to make it work.</p>
","5154590","","","","","2020-07-20 11:54:27","Asyncio two loops for different I/O tasks?","<python><python-3.x><python-asyncio>","5","6","11","","","CC BY-SA 3.0","0"
"47460085","1","47475201","","2017-11-23 16:41:36","","43","26460","<p>I'd like to install openCV to vectorize image, but there's a series error message regarding Xcode and Ruby.</p>

<p>First, I use terminal to install openCV, <code>brew install opencv</code>. </p>

<p>Then, I got error message indicating that the system doesn't like my ruby version.</p>

<pre><code>/usr/local/Homebrew/Library/Homebrew/brew.rb:12:in `&lt;main&gt;': 
Homebrew must be run under Ruby 2.3! You're running 2.0.0. (RuntimeError)
</code></pre>

<p>So, I want to upgrade my ruby. I followed several update strategy from <a href=""https://stackoverflow.com/questions/38194032/how-to-update-ruby-version-2-0-0-to-the-latest-version-in-mac-osx-yosemite"">this</a> post. First ruby upgrade trial: <code>brew link --overwrite ruby</code> &amp; <code>brew unlink ruby &amp;&amp; brew link ruby</code> and get</p>

<pre><code>Error: No such keg: /usr/local/Cellar/ruby
</code></pre>

<p>Then second ruby upgrade trial: <code>brew upgrade ruby</code> and see the following error message. </p>

<pre><code>Error: Xcode alone is not sufficient on Sierra.
Install the Command Line Tools:
xcode-select --install 
</code></pre>

<p>This error message means I need to install Xcode which I already install. So, I check my Xcode status with <code>code-select -p</code> and get <code>/Applications/Xcode.app/Contents/Developer</code> which means I am fine. </p>

<p>I saw a <a href=""https://www.pyimagesearch.com/2016/12/19/install-opencv-3-on-macos-with-homebrew-the-easy-way/"" rel=""noreferrer"">comment</a> regarding where you install python could be a big issue. Quote from the source:</p>

<blockquote>
  <p>If you see <code>/usr/local/bin/python3</code> then you are correctly using the Homebrew version of Python. If the output is instead <code>/usr/bin/python3</code> then you are incorrectly using the system version of Python.</p>
</blockquote>

<p>I check <code>which python3</code> and get</p>

<pre><code>/Users/******/anaconda3/bin/python3
</code></pre>

<p>Could this be the problem? How can I change system version to local?</p>
","6765415","","6765415","","2017-11-24 14:22:33","2020-01-18 00:03:40","Error Message ""Xcode alone is not sufficient on Sierra""","<ruby><xcode><python-3.x><homebrew><command-line-tool>","5","5","6","","","CC BY-SA 3.0","0"
"51268991","1","51273292","","2018-07-10 15:23:10","","5","26438","<p>I have found some other questions that have a similar error to what I am getting, but have not been able to figure out how to resolve this based on the answers. I am trying to import an excel file into SQL Server with the help of python. This is the code I wrote:</p>

<pre><code>import pandas as pd
import numpy as np
import pandas.io.sql
import pyodbc
import xlrd

server = ""won't disclose private info""
db = 'private info'
conn = pyodbc.connect('DRIVER={SQL Server};SERVER=' + Server + ';DATABASE=' + 
db + ';Trusted_Connection=yes')

cursor = conn.cursor()
book = xlrd.open_workbook(""Daily Flash.xlsx"")
sheet = book.sheet_by_name(""Sheet1"")

query1 = """"""CREATE TABLE [LEAF].[MK] ([LEAF][Lease_Number] varchar(255), 
[LEAF][Start_Date] varchar(255), [LEAF][Report_Status] varchar(255), [LEAF] 
[Status_Date] varchar(255), [LEAF][Current_Status] varchar(255), [LEAF] 
[Sales_Rep] varchar(255), [LEAF][Customer_Name] varchar(255),[LEAF] 
[Total_Finance] varchar(255),
[LEAF][Rate_Class] varchar(255) ,[LEAF][Supplier_Name] varchar(255) ,[LEAF] 
[DecisionStatus] varchar(255))""""""


query = """"""INSERT INTO [LEAF].[MK] (Lease_Number, Start_Date, Report_Status, 
Status_Date, Current_Status, Sales_Rep, Customer_Name,Total_Finance,
Rate_Class,Supplier_Name,DecisionStatus) VALUES (%s, %s, %s, %s, %s, %s, %s, 
%s, %s, %s, %s)""""""

for r in range(1, sheet.nrows):
    Lease_Number  = sheet.cell(r,0).value
    Start_Date    = sheet.cell(r,1).value
    Report_Status = sheet.cell(r,2).value
    Status_Date   = sheet.cell(r,3).value
    Current_Status= sheet.cell(r,4).value
    Sales_Rep     = sheet.cell(r,5).value
    Customer_Name = sheet.cell(r,6).value
    Total_Financed= sheet.cell(r,7).value
    Rate_Class    = sheet.cell(r,8).value
    Supplier_Name = sheet.cell(r,9).value
    DecisionStatus= sheet.cell(r,10).value


    values = (Lease_Number, Start_Date, Report_Status, Status_Date, 
    Current_Status, Sales_Rep, Customer_Name, Total_Financed, Rate_Class, 
    Supplier_Name, DecisionStatus)

    cursor.execute(query1)

    cursor.execute(query, values)


database.commit()


database.close()


database.commit()
</code></pre>

<p>The error message I get is:</p>

<pre><code>ProgrammingError                          Traceback (most recent call last)
&lt;ipython-input-24-c525ebf0af73&gt; in &lt;module&gt;()
 16 
 17     # Execute sql Query
 ---&gt; 18     cursor.execute(query, values)
 19 
 20 # Commit the transaction

 ProgrammingError: ('The SQL contains 0 parameter markers, but 11 parameters 
 were supplied', 'HY000')
</code></pre>

<p>Can someone please explain the problem to me and how I can fix it? Thank you!</p>

<p>Update:</p>

<p>I have gotten that error message to go away based on the comments below. I modified my query also because the table into which I am trying to insert values into was not previously created, so I updated my code in an attempt to create it. </p>

<p>However, now I am getting the error message:</p>

<pre><code>ProgrammingError: ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL 
Server]The specified schema name ""dbo"" either does not exist or you do not 
have permission to use it. (2760) (SQLExecDirectW)')
</code></pre>

<p>I tried changing that slightly by writing CREATE [HELLO][MK] instead of just CREATE MK but that tells me that MK is already in the database... What steps should I take next? </p>
","7620499","","7620499","","2018-07-10 17:35:22","2018-07-11 11:37:36","Importing data from an excel file using python into SQL Server","<python><python-3.x><pyodbc><xlrd>","1","20","4","","","CC BY-SA 4.0","0"
"48934100","1","48934223","","2018-02-22 17:50:27","","7","26376","<p>I try to get JSON data from client using this line (requests library) POST request:</p>

<pre><code>request.data
</code></pre>

<p>How to convert this to dict?</p>

<p>It works:</p>

<pre><code>response_data = request.get_json()
</code></pre>

<p>But how convert this to dict?</p>
","","user9377278","","user9377278","2018-02-22 18:24:50","2018-02-22 18:30:02","How to convert request.data to dict?","<python><json><python-3.x><python-requests>","2","4","","2018-10-29 20:23:57","","CC BY-SA 3.0","0"
"53522052","1","53535763","","2018-11-28 14:45:12","","20","26332","<p>I try to create a simple flask app:</p>

<pre><code>from flask import Flask

app = Flask(__name__)

if __name__ == '__main__':
  app.run()
</code></pre>

<p>but when I add the debug: </p>

<pre><code>FLASK_APP = run.py
FLASK_ENV = development
FLASK_DEBUG = 1
</code></pre>

<p>I got the following error:</p>

<p><strong>ValueError: signal only works in main thread</strong></p>

<p>here the full stacktrace</p>

<pre><code>FLASK_APP = run.py
FLASK_ENV = development
FLASK_DEBUG = 1
In folder c:/MyProjectPath/api
c:\MyProjectPath\api\venv\Scripts\python.exe -m flask run
 * Serving Flask-SocketIO app ""run.py""
 * Forcing debug mode on
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 283-122-745
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""c:\appdata\local\programs\python\python37\Lib\threading.py"", line 917, in _bootstrap_inner
    self.run()
  File ""c:\appdata\local\programs\python\python37\Lib\threading.py"", line 865, in run
    self._target(*self._args, **self._kwargs)
  File ""c:\MyProjectPath\api\venv\lib\site-packages\flask_socketio\cli.py"", line 59, in run_server
    return run_command()
  File ""c:\MyProjectPath\api\venv\lib\site-packages\click\core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""c:\MyProjectPath\api\venv\lib\site-packages\click\core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""c:\MyProjectPath\api\venv\lib\site-packages\click\core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""c:\MyProjectPath\api\venv\lib\site-packages\click\core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""c:\MyProjectPath\api\venv\lib\site-packages\click\decorators.py"", line 64, in new_func
    return ctx.invoke(f, obj, *args, **kwargs)
  File ""c:\MyProjectPath\api\venv\lib\site-packages\click\core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""c:\MyProjectPath\api\venv\lib\site-packages\flask\cli.py"", line 771, in run_command
    threaded=with_threads, ssl_context=cert)
  File ""c:\MyProjectPath\api\venv\lib\site-packages\werkzeug\serving.py"", line 812, in run_simple
    reloader_type)
  File ""c:\MyProjectPath\api\venv\lib\site-packages\werkzeug\_reloader.py"", line 267, in run_with_reloader
    signal.signal(signal.SIGTERM, lambda *args: sys.exit(0))
  File ""c:\appdata\local\programs\python\python37\Lib\signal.py"", line 47, in signal
    handler = _signal.signal(_enum_to_int(signalnum), _enum_to_int(handler))
ValueError: signal only works in main thread
</code></pre>
","5917759","","5917759","","2018-11-29 09:14:34","2019-08-19 23:04:47","Flask APP - ValueError: signal only works in main thread","<python><python-3.x><web><flask>","2","3","9","","","CC BY-SA 4.0","0"
"49677949","1","","","2018-04-05 17:13:02","","3","26319","<p>Why can't while loop be used on a range function in python ?</p>

<p>The code:</p>

<pre><code>def main():
  x=1;

  while x in range(1,11):
     print (str(x)+"" cm"");


if __name__==""__main__"":
    main();
</code></pre>

<p>executes as an infinite loop repeatedly printing 1 cm</p>
","1336200","","8195528","","2018-04-05 18:05:02","2018-09-07 12:57:23","python while loop range function","<python-3.x><while-loop><range>","2","4","0","","","CC BY-SA 3.0","0"
"35582959","1","35582986","","2016-02-23 16:32:18","","10","26318","<p>I'm wondering how to take user input and make a list of every character in it.</p>

<pre><code>magicInput = input('Type here: ')
</code></pre>

<p>And say you entered ""python rocks""
I want a to make it a list something like this</p>

<pre><code>magicList = [p,y,t,h,o,n, ,r,o,c,k,s]
</code></pre>

<p>But if I do this:</p>

<pre><code>magicInput = input('Type here: ')
magicList = [magicInput]
</code></pre>

<p>The magicList is just</p>

<pre><code>['python rocks']
</code></pre>
","5768299","","476496","","2016-02-23 16:35:01","2020-07-25 12:55:12","How do I convert user input into a list?","<python><list><python-3.x>","5","0","","","","CC BY-SA 3.0","0"
"28988072","1","29034299","","2015-03-11 13:35:51","","9","26307","<p>Two part question:</p>

<ol>
<li><p>What is the ""best"" way to query Microsoft AD with Python 3.x? With ""best"" defined as multi-OS support and use of core Python libraries preferable.</p></li>
<li><p>Examples of querying AD structure for members of a specific AD group would be extremely appreciated. </p></li>
</ol>

<p>I've looked at a few different libraries, but had issues loading them under OS X, Python 3, etc. Hoping someone's already looked at this issue. </p>
","4467461","","1268895","","2015-05-11 12:27:05","2015-05-11 12:27:05","Best way to Query Microsoft AD with Python 3","<python><python-3.x><active-directory>","1","0","2","","","CC BY-SA 3.0","0"
"53026985","1","","","2018-10-27 22:56:44","","21","26301","<p>I have just installed Anaconda 5.3 64-bit (Anaconda3-5.3.0-Windows-x86_64) on Windows 10 and am getting this error when trying to run Spyder.</p>
<blockquote>
<p>pythonw.exe - Ordinal Not Found</p>
<p>The ordinal could not be located in the dynamic link library C:\Users\username\Anaconda3\Library\bin\mkl_intel_thread.dll.</p>
</blockquote>
<p>I used <a href=""http://www.dependencywalker.com/"" rel=""noreferrer"">Dependency Walker</a> to view the functions in the DLL and see that ordinal 242 is there.  The function associated with ordinal 242 is mkl_blas_zherk.</p>
<p>Could anyone help me fix this or direct me to a resource to help me figure it out myself?</p>
","8663622","","-1","","2020-06-20 09:12:55","2019-09-09 11:40:49","The ordinal 242 could not be located in the dynamic link library Anaconda3\Library\bin\mkl_intel_thread.dll","<python><python-3.x><dll><anaconda><conda>","8","1","9","","","CC BY-SA 4.0","0"
"40918922","1","40919020","","2016-12-01 19:32:45","","3","26161","<p>I'm trying to make some manipulations with tick labels in matplotlib. But it seems that code does not do what I want, and I have no idea why? There is no labels.</p>

<pre><code>In[1]: import matplotlib.pyplot as plt
       import numpy as np

In[2]: x = np.arange(0,10)

In[3]: plt.plot(x,x)

       locs, labs = plt.xticks()
       plt.xticks(locs[1:], labs[1:])

       plt.show()
</code></pre>

<p><a href=""https://i.stack.imgur.com/wuZjb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wuZjb.png"" alt=""enter image description here""></a></p>

<p>Any help please! What I want is to delete first label on x axis:<br>
<a href=""https://i.stack.imgur.com/tlosU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tlosU.png"" alt=""enter image description here""></a><br>
I'm using:<br>
python 3.5.2<br>
matplotlib 1.5.3<br>
win 10  </p>
","6793085","","6793085","","2016-12-01 19:43:55","2016-12-01 20:19:37","Set tick labels in matplotlib","<python><python-3.x><matplotlib>","3","0","","","","CC BY-SA 3.0","0"
"33566843","1","33566923","","2015-11-06 12:30:36","","2","26160","<p>For example the web page is the link:</p>

<blockquote>
  <p><a href=""https://www.architecture.com/FindAnArchitect/FAAPractices.aspx?display=50"" rel=""nofollow"">https://www.architecture.com/FindAnArchitect/FAAPractices.aspx?display=50</a></p>
</blockquote>

<p>I must have the name of the firms and their address and website. I have tried the following to convert the html to text:</p>

<pre><code>import nltk   
from urllib import urlopen

url = ""https://www.architecture.com/FindAnArchitect/FAAPractices.aspx display=50""    
html = urlopen(url).read()    
raw = nltk.clean_html(html)  
print(raw)
</code></pre>

<p>But it returns the error:</p>

<pre><code>ImportError: cannot import name 'urlopen
</code></pre>
","4810579","","5299236","","2015-11-06 12:44:41","2018-10-31 06:30:21","How to extract text from html page?","<python><html><python-3.x><text>","1","2","2","","","CC BY-SA 3.0","0"
"43485569","1","43538799","","2017-04-19 02:59:48","","17","26155","<p>Forgive me but I'm new to python. I've installed a package (theano) using
<code>conda install theano</code>, and when I type <code>conda list</code>, the package exists</p>

<p>However, when I enter the python interpreter by running <code>python</code>, and try to import it with <code>import theano</code>, I get an error: ""no module named theano"", and when I list all python modules, theano doesn't exist.</p>

<p>What am I missing?</p>
","3105067","","","","","2020-02-04 13:39:53","Installed a package with Anaconda, can't import in Python","<python><python-3.x><package><installation><anaconda>","6","1","2","","","CC BY-SA 3.0","0"
"47378715","1","47378838","","2017-11-19 15:45:52","","7","26056","<p>I'm trying to offer the user the possibility to calculate his profit of his projected sales if the margin has a certain value (.23). The user should be able to enter any value as projected sales:</p>

<pre><code>from tkinter import *

root = Tk()

margin = 0.23
projectedSales = #value of entry
profit = margin * int(projectedSales)

#My function that is linked to the event of my button
def profit_calculator(event):
    print(profit)


#the structure of the window
label_pan = Label(root, text=""Projected annual sales:"")
label_profit = Label(root, text=""Projected profit"")
label_result = Label(root, text=(profit), fg=""red"")

entry = Entry(root)

button_calc = Button(root, text= ""Calculate"", command=profit_calculator)
button_calc.bind(""&lt;Button-1&gt;"", profit_calculator)

#position of the elements on the window
label_pan.grid(row=0)
entry.grid(row=0, column=1)
button_calc.grid(row=1)              
label_profit.grid(row=2)
label_result.grid(row=2, column=1)

root.mainloop()
</code></pre>
","8753004","","3329664","","2017-11-19 17:02:06","2017-11-19 17:02:06","tkinter, how to get the value of an Entry widget?","<python-3.x><user-interface><tkinter>","1","1","1","2017-11-19 21:30:50","","CC BY-SA 3.0","0"
"33141595","1","","","2015-10-15 06:42:58","","10","26034","<p>Say I have a string and I want to remove the rest of the string before or after certain characters are seen</p>

<p>For example, all my strings have 'egg' in them:</p>

<pre><code>""have an egg please""
""my eggs are good""
</code></pre>

<p>I want to get:</p>

<pre><code>""egg please""
""eggs are good""
</code></pre>

<p>and also the same question but how can I delete all but the string in front of the characters?</p>
","5382288","","2867928","","2019-05-09 06:13:46","2020-01-24 20:25:38","How can I remove everything in a string until a character(s) are seen in Python","<python><python-3.x><string><python-2.7>","5","0","2","","","CC BY-SA 3.0","0"
"27946030","1","27946097","","2015-01-14 15:06:28","","16","25909","<p>I've looked around a bit, but I can't find an answer to my error. Here is the code:</p>

<pre><code>import tkinter as tk

root=tk.Tk()

class Page(tk.Frame):
    '''Enables switching between pages of a window.'''
    def __init__(self):
        self.widgets={}
        self.grid(column=0,row=0)

page=Page()

tk.mainloop()
</code></pre>

<p>Here is the error:</p>

<pre><code>Traceback (most recent call last):  
  File ""C:\Documents and Settings\Desktop\Python Scripts\Tkinter.py"", line 11, in &lt;module&gt;  
    page=Page()  
  File ""C:\Documents and Settings\Desktop\Python Scripts\Tkinter.py"", line , in __init__  
    self.grid(column=0,row=0)  
  File ""C:\Python34\lib\tkinter\__init__.py"", line 2055, in grid_configure  
    self.tk.call(  
AttributeError: 'Page' object has no attribute 'tk'
</code></pre>

<p>I'm fairly new to tkinter, and this error has me stumped. I'd really appreciate any help, thank you!</p>
","3546190","","","","","2015-01-14 15:09:31","Tkinter AttributeError: object has no attribute 'tk'","<python><python-3.x><tkinter>","1","0","5","","","CC BY-SA 3.0","0"
"28369736","1","28370058","","2015-02-06 15:59:11","","1","25898","<p>I want to write a small script to tell me if the bass level is OK or not from user input.</p>

<p>I am just learning user input, and this is what I have so far:</p>

<pre><code>def crisp():
    bass = input(""Enter bass level on a scale of 1 to 5&gt;&gt;"")
    print (""Bass level is at"") + bass
    if bass &gt;=4:
       print (""Bass is crisp"")    
    elif bass &lt; 4:
       print (""Bass is not so crisp"")
</code></pre>
","4465487","","4367435","","2015-02-06 17:50:06","2015-02-06 17:56:49","Python 3.4 User Input","<python><python-3.x><evaluation>","3","2","","2018-09-09 11:09:50","","CC BY-SA 3.0","0"
"39851566","1","39852126","","2016-10-04 11:51:11","","12","25863","<p>I am using Windows 10. Currently, I have Python 2.7 installed. I would like to install Python 3.5 as well. However, if I have both 2.7 and 3.5 installed, when I run <code>pip</code>, how do I get the direct the package to be installed to the desired Python version?</p>
","1709088","","3906640","","2020-03-26 10:06:10","2020-06-30 08:59:40","Using pip on Windows installed with both python 2.7 and 3.5","<python><python-2.7><python-3.x><pip>","8","2","5","","","CC BY-SA 4.0","0"
"33679930","1","33680021","","2015-11-12 19:42:07","","53","25846","<p>What is best practice for extending <code>Enum</code> type in Python 3.4 and is there even a possibility for do this?</p>

<p>For example:</p>

<pre><code>from enum import Enum

class EventStatus(Enum):
   success = 0
   failure = 1

class BookingStatus(EventStatus):
   duplicate = 2
   unknown = 3

Traceback (most recent call last):
...
TypeError: Cannot extend enumerations
</code></pre>

<p>Currently there is no possible way to create a base enum class with members and use it in other enum classes (like in the example above). Is there any other way to implement inheritance for Python enums?</p>
","611982","","208880","","2015-11-28 20:00:34","2020-09-24 11:46:53","How to extend Python Enum?","<python><python-3.x><enums>","7","4","6","","","CC BY-SA 3.0","0"
"56283294","1","56284155","","2019-05-23 21:42:57","","19","25845","<p>I want to use a logit model and trying to import statsmodels library. 
My Version: Python 3.6.8</p>

<p>The best suggestion I got is to downgrade scipy but unclear how to and to what version should I downgrade. Please help how to resolve. 
<a href=""https://github.com/statsmodels/statsmodels/issues/5747"" rel=""noreferrer"">https://github.com/statsmodels/statsmodels/issues/5747</a></p>

<p><code>import statsmodels.formula.api as smf</code></p>

<p></p>

<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-52-f897a2d817de&gt; in &lt;module&gt;
----&gt; 1 import statsmodels.formula.api as smf

~/anaconda3/envs/py36/lib/python3.6/site-packages/statsmodels/formula/api.py in &lt;module&gt;
     13 from statsmodels.robust.robust_linear_model import RLM
     14 rlm = RLM.from_formula
---&gt; 15 from statsmodels.discrete.discrete_model import MNLogit
     16 mnlogit = MNLogit.from_formula
     17 from statsmodels.discrete.discrete_model import Logit

~/anaconda3/envs/py36/lib/python3.6/site-packages/statsmodels/discrete/discrete_model.py in &lt;module&gt;
     43 
     44 from statsmodels.base.l1_slsqp import fit_l1_slsqp
---&gt; 45 from statsmodels.distributions import genpoisson_p
     46 
     47 try:

~/anaconda3/envs/py36/lib/python3.6/site-packages/statsmodels/distributions/__init__.py in &lt;module&gt;
      1 from .empirical_distribution import ECDF, monotone_fn_inverter, StepFunction
----&gt; 2 from .edgeworth import ExpandedNormal
      3 from .discrete import genpoisson_p, zipoisson, zigenpoisson, zinegbin

~/anaconda3/envs/py36/lib/python3.6/site-packages/statsmodels/distributions/edgeworth.py in &lt;module&gt;
      5 import numpy as np
      6 from numpy.polynomial.hermite_e import HermiteE
----&gt; 7 from scipy.misc import factorial
      8 from scipy.stats import rv_continuous
      9 import scipy.special as special

ImportError: cannot import name 'factorial'```
</code></pre>
","4727412","","7131757","","2020-04-01 12:04:35","2020-05-24 04:58:20","ImportError: cannot import name 'factorial'","<python><python-3.x><import><statsmodels>","5","6","3","","","CC BY-SA 4.0","0"
"30674644","1","30675871","","2015-06-05 19:49:24","","15","25825","<p>Could anyone give me a clear set of instructions for installing mod_wsgi on Ubuntu for Python 3?</p>

<p>I did get Flask &amp; mod_wsgi successfully using Python3, and for a brief moment felt happy.</p>

<p>...until I looked at Apache's log and realised that I've run into this problem: <a href=""https://askubuntu.com/questions/569550/assertionerror-using-apache2-and-libapache2-mod-wsgi-py3-on-ubuntu-14-04-python"">https://askubuntu.com/questions/569550/assertionerror-using-apache2-and-libapache2-mod-wsgi-py3-on-ubuntu-14-04-python</a></p>

<p><code>apt-get</code> is installing an out of date version of <code>libapache2-mod-wsgi-py3</code> and this is causing errors in Apache's log.  <em>(Should I report this and if so where?)</em></p>

<p>In the link, the engineer is using <code>pip</code> to install a more up-to-date version.</p>

<p>Interestingly, he appears to be installing it into a virtual Python3 environment. (Would this be any different than using the system <code>pip3</code>?)</p>

<p>Also he uses <code>pip3 install mod_wsgi</code>, but <code>pip3 search mod_wsgi</code> returns:</p>

<p><em>(EDIT: no he doesn't, he uses pip not pip3. Can that be right? Has he got his wires crossed? Isn't pip going to ignore the fact that he is in his py3venv and simply use the system's py2 installation?  But anyway that doesn't resolve the confusion...)</em></p>

<pre><code>mod_wsgi-metrics          - Metrics package for Apache/mod_wsgi.
cykooz.recipe.pastewsgi   - Buildout recipe to create paste.deploy entry points for mod_wsgi or uwsgi
mod_wsgi-httpd            - Installer for Apache httpd web server.
apachemiddleware          - Useful Python middleware for use with mod_wsgi deployments
tranchitella.recipe.wsgi  - Buildout recipe to create paste.deploy entry points for mod_wsgi
mod_wsgi                  - Installer for Apache/mod_wsgi.
</code></pre>

<p>So what is <code>mod_wsgi-httpd</code>? And is it certain this isn't the one I want?</p>

<p>Finally, can anyone provide a link to installing mod_wsgi from source?</p>

<p>EDIT: I don't get why the engineer is using <code>pip install mod_wsgi</code> rather than <code>pip3 ...</code>. Surely that can't be right?  But if I do use pip3, I get:</p>

<pre><code>pi@PiDroplet:~$ cd web/piFlask

pi@PiDroplet:~/web/piFlask$ source ./venv3/bin/activate
(venv3)
pi@PiDroplet:~/web/piFlask$ pip3 install mod_wsgi
Downloading/unpacking mod-wsgi
  Downloading mod_wsgi-4.4.12.tar.gz (991kB): 991kB downloaded
  Running setup.py (path:/home/pi/web/piFlask/venv3/build/mod-wsgi/setup.py) egg_info for package mod-wsgi
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 17, in &lt;module&gt;
      File ""/home/pi/web/piFlask/venv3/build/mod-wsgi/setup.py"", line 141, in &lt;module&gt;
        'missing Apache httpd server packages.' % APXS)
    RuntimeError: The 'apxs' command appears not to be installed or is not executable. Please check the list of prerequisites in the documentation for this package and install any missing Apache httpd server packages.
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):

  File ""&lt;string&gt;"", line 17, in &lt;module&gt;

  File ""/home/pi/web/piFlask/venv3/build/mod-wsgi/setup.py"", line 141, in &lt;module&gt;

    'missing Apache httpd server packages.' % APXS)

RuntimeError: The 'apxs' command appears not to be installed or is not executable. Please check the list of prerequisites in the documentation for this package and install any missing Apache httpd server packages.

----------------------------------------
Cleaning up...
Command python setup.py egg_info failed with error code 1 in /home/pi/web/piFlask/venv3/build/mod-wsgi
Storing debug log for failure in /home/pi/.pip/pip.log
</code></pre>

<p>And now I try <code>pip3 install mod_wsgi-httpd</code>, it takes about five minutes to compile:</p>

<pre><code>(venv3)
pi@PiDroplet:~/web/piFlask$ pip3 install mod_wsgi-httpd
Downloading/unpacking mod-wsgi-httpd
  Downloading mod_wsgi-httpd-2.4.12.5.tar.gz
  Running setup.py (path:/home/pi/web/piFlask/venv3/build/mod-wsgi-httpd/setup.py) egg_info for package mod-wsgi-httpd
    apr-1.5.2/
    apr-1.5.2/config.layout
    apr-1.5.2/build.conf
    apr-1.5.2/emacs-mode
    :
</code></pre>

<p>So now I'm worried I've got a second Apache sitting in my Py3 virtualenv.</p>

<p>However that does get rid of the error; <code>pip3 install mod_wsgi</code> now completes successfully.</p>

<p>EDIT: but now I have come completely unstuck trying to follow his instructions: I don't have a <code>/etc/apache2/mods-available/wsgi_express.load</code>, and if he is suggesting making it, then for a start this seems arbitrary, and secondly the text he suggests putting in it, i.e. <code>LoadModule wsgi_module /usr/lib/apache2/modules/mod_wsgi-py34.cpython-34m.so</code> -- this file doesn't exist on my system.</p>

<p>I do have <code>/etc/apache2/mods-available/wsgi.load</code></p>

<p>Drowning in technology again, can someone throw me a line?</p>
","435129","","435129","","2015-06-05 23:14:44","2019-01-24 09:04:45","Installing mod_wsgi for Python3 on Ubuntu","<ubuntu><python-3.x><pip><virtualenv><mod-wsgi>","2","0","8","","","CC BY-SA 3.0","0"
"28395503","1","28395583","","2015-02-08 15:09:16","","22","25756","<p>When I try to or/and two sets using <code>&amp;=</code> and <code>|=</code> operator, I got some weird result.</p>

<pre><code>s1 = {1,2,3}
s2 = {2,3,4}
tmp = s1
tmp &amp;= s2 
</code></pre>

<p>As expected, tmp will be {2,3}, but I don't know why <code>s1</code> also changed it value to {2,3}. </p>

<p>However, if I do:</p>

<pre><code>tmp = tmp &amp; s2
</code></pre>

<p>Then, <code>s1</code> will be unchanged! Can anyone explain for me what happens underneath <code>&amp;=</code> operator?</p>
","2384196","","2225682","","2015-02-08 15:20:25","2019-12-23 17:48:46","Python: &= operator","<python><python-2.7><python-3.x><set>","4","1","4","","","CC BY-SA 3.0","0"
"33726361","1","33726420","","2015-11-15 23:26:14","","3","25754","<p>Using the following code from <a href=""https://stackoverflow.com/a/11899925"">https://stackoverflow.com/a/11899925</a>, I am able to find if a word is unique or not (by comparing if it was used once or greater than once):</p>

<pre><code>helloString = ['hello', 'world', 'world']
count = {}
for word in helloString :
   if word in count :
      count[word] += 1
   else:
      count[word] = 1
</code></pre>

<p>But, if I were to have a string with hundreds of words, how would I be able to count the number of unique words within that string?</p>

<p>For example, my code has:</p>

<pre><code>uniqueWordCount = 0
helloString = ['hello', 'world', 'world', 'how', 'are', 'you', 'doing', 'today']
count = {}
for word in words :
   if word in count :
      count[word] += 1
   else:
      count[word] = 1
</code></pre>

<p>How would I be able to set <code>uniqueWordCount</code> to <code>6</code>? Usually, I am really good at solving these types of algorithmic puzzles, but I have been unsuccessful with figuring this one out. I feel as if it is right beneath my nose.</p>
","1234120","","-1","","2017-05-23 12:03:56","2018-01-22 16:37:17","Counting the number of unique words in a list","<python><python-3.x>","6","2","2","","","CC BY-SA 3.0","0"
"41635052","1","41635468","","2017-01-13 12:49:43","","14","25730","<p><strong>ALL packages installed!!</strong>
Hi, I'm using python 3.5 Django==1.10 I installed Python-social-auth executed the command Python manage.py transfer error received!</p>

<pre><code>Traceback (most recent call last):
File ""manage.py"", line 22, in 
execute_from_command_line(sys.argv)
File ""/home/berluskuni/web_project/exprender/.exprender/lib64/python3.5/site-packages/django
/core/management/init.py"", line 367, in execute_from_command_line
utility.execute()
File ""/home/berluskuni/web_project/exprender/.exprender/lib64/python3.5/site-packages/django/core/management/init.py"", line 341, in execute
django.setup()
File ""/home/berluskuni/web_project/exprender/.exprender/lib64/python3.5/site-packages/django/init.py"", line 27, in setup
apps.populate(settings.INSTALLED_APPS)
File ""/home/berluskuni/web_project/exprender/.exprender/lib64/python3.5/site-packages/django/apps/registry.py"", line 108, in populate
app_config.import_models(all_models)
File ""/home/berluskuni/web_project/exprender/.exprender/lib64/python3.5/site-packages/django/apps/config.py"", line 199, in import_models
self.models_module = import_module(models_module_name)
File ""/usr/lib64/python3.5/importlib/init.py"", line 126, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
File """", line 986, in _gcd_import
File """", line 969, in _find_and_load
File """", line 958, in _find_and_load_unlocked
File """", line 673, in _load_unlocked
File """", line 665, in exec_module
File """", line 222, in _call_with_frames_removed
File ""/home/berluskuni/web_project/exprender/.exprender/lib64/python3.5/site-packages/social/apps/django_app/default/models.py"", line 1, in 
from social_django.models import AbstractUserSocialAuth, UserSocialAuth, Nonce, Association, Code, DjangoStorage
ImportError: No module named 'social_django'
</code></pre>
","6538069","","","","","2017-03-09 13:58:10","python-social-auth with Django: ImportError: No module named 'social_django'","<django><python-3.x><python-social-auth>","3","0","1","","","CC BY-SA 3.0","0"
"50438182","1","","","2018-05-20 18:35:55","","8","25681","<p>Is there any library available in python for the graphical user entry input. I know about <code>tk</code> but I believe it takes some line of codes to do that. I am looking for the shortest solution.</p>

<pre><code>a = input('Enter your string here:') 
</code></pre>

<p>In place of this, I want to get a dialogue box so that user can input there.</p>

<p>This did not serve the purpose. This only shows the dialogue box and you can't provide an input entry.</p>

<pre><code>import ctypes  # An included library with Python install.   
ctypes.windll.user32.MessageBoxW(0, ""Your text"", ""Your title"", 1)
</code></pre>
","6157515","","9445557","","2018-05-20 23:20:58","2020-04-11 15:43:32","User input in dialog box","<python><python-3.x><user-interface><tkinter>","3","9","2","","","CC BY-SA 4.0","0"
"30245397","1","30245465","","2015-05-14 19:09:24","","52","25666","<p>I was wondering why list comprehension is so much faster than appending to a list. I thought the difference is just expressive, but it's not.</p>

<pre><code>&gt;&gt;&gt; import timeit 
&gt;&gt;&gt; timeit.timeit(stmt='''\
t = []
for i in range(10000):
    t.append(i)''', number=10000)
9.467898777974142

&gt;&gt;&gt; timeit.timeit(stmt='t= [i for i in range(10000)]', number=10000)
4.1138417314859
</code></pre>

<p>The list comprehension is 50% faster. Why?</p>
","2535611","","68587","","2018-06-01 12:44:48","2019-08-15 18:47:21","Why is a list comprehension so much faster than appending to a list?","<python><list><python-2.7><python-3.x><list-comprehension>","3","3","14","","","CC BY-SA 4.0","0"
"40676085","1","40676282","","2016-11-18 11:34:20","","36","25662","<p>My code </p>

<pre><code>$ python
Python 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:53:06) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; a = (1, 2)
&gt;&gt;&gt; '%d %d %d' % (0, *a)
'0 1 2'
&gt;&gt;&gt; '%d %d %d' % (*a, 3)
'1 2 3'
&gt;&gt;&gt; '%d %d' % (*a)
  File ""&lt;stdin&gt;"", line 1
SyntaxError: can't use starred expression here
&gt;&gt;&gt; 
</code></pre>

<p>My question, why?</p>

<p>In a more serious tone: I'd like an answer, or a reference, that details all the ins and outs of using a starred expression, as it happens that I am sometimes surprised from its behaviours...</p>

<h3>Addendum</h3>

<p>To reflect some of the enlightening comments that
immediately followed my question I add the following code</p>

<pre><code>&gt;&gt;&gt; '%d %d' % (, *a)
  File ""&lt;stdin&gt;"", line 1
    '%d %d' % (, *a)
               ^
SyntaxError: invalid syntax
&gt;&gt;&gt; '%d %d' % (*a,)
'1 2'
&gt;&gt;&gt; 
</code></pre>

<p>(I had tried the <code>(, a)</code> part before posting the original question but I've omitted it 'cause the error was not related to the starring.)</p>

<p>There is a syntax, in python ≥ 3.5, that ""just works"" but nevertheless I would like some understanding.</p>
","2749397","","2146491","","2020-06-19 15:03:59","2020-08-19 13:45:50","Why can't I use a starred expression?","<python><python-3.x><python-2.x>","3","7","13","","","CC BY-SA 3.0","0"
"40683123","1","40683731","","2016-11-18 17:47:39","","24","25657","<p>I'm using the jupyter notebook installed with Anaconda (I'm on Mac). Few days ago, I wanted to change the theme to have a dark background, and I followed the <a href=""http://sherifsoliman.com/2016/01/11/theming-ipython-jupyter-notebook/"" rel=""noreferrer"">instructions here</a>. Namely, I've downloaded the theme <code>custom.css</code> and placed it in <code>~/.jupyter/custom/</code>. It worked very well. </p>

<p>I liked the theme, but I would like to go back to the default one (this one does not show the main toolbar, among other things). I tried to remove the <code>custom.css</code> from its folder, I reset my terminal, but nothing changes! I'm guessing that jupyter keeps a copy of the themes somewhere that I should delete, but I can't find it.</p>

<p>I have also tried uninstalling jupyter and reinstalling, following the commands:</p>

<pre><code>conda update conda
conda uninstall ipython
conda install jupyter
</code></pre>

<p>Again, no change. I'm stuck with my black background theme with no toolbar.</p>
","6550349","","434217","","2016-11-19 15:43:19","2020-05-26 14:15:15","How to reset jupyter notebook theme to default?","<python><python-3.x><jupyter-notebook><jupyter>","9","1","6","","","CC BY-SA 3.0","0"
"51124516","1","51124572","","2018-07-01 15:11:58","","0","25620","<p>I have a post request which I am trying to send using <code>requests</code> in python. But I get an invalid 403 error. The requests works fine through the browser.</p>

<pre><code>POST /ajax-load-system HTTP/1.1
Host: xyz.website.com
Accept: application/json, text/javascript, */*; q=0.01
Accept-Language: en-GB,en;q=0.5
User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0
Referer: http://xyz.website.com/help-me/ZYc5Yn
Content-Type: application/x-www-form-urlencoded; charset=UTF-8
X-Requested-With: XMLHttpRequest
Content-Length: 56
Cookie: csrf_cookie_name=a3f8adecbf11e29c006d9817be96e8d4; ci_session=ba92hlh6o0ns7f20t4bsgjt0uqfdmdtl; _ga=GA1.2.1535910352.1530452604; _gid=GA1.2.1416631165.1530452604; _gat_gtag_UA_21820217_30=1
Connection: close

csrf_test_name=a3f8adecbf11e29c006d9817be96e8d4&amp;vID=9999
</code></pre>

<p>What I am trying in python is:</p>

<pre><code>import requests
import json

url = 'http://xyz.website.com/ajax-load-system'

payload = {
'Host': 'xyz.website.com',
'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0',
'Accept': 'application/json, text/javascript, */*; q=0.01',
'Accept-Language': 'en-GB,en;q=0.5',
'Referer': 'http://xyz.website.com/help-me/ZYc5Yn',
'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
'X-Requested-With': 'XMLHttpRequest',
'Content-Length': '56',
'Cookie': 'csrf_cookie_name=a3f8adecbf11e29c006d9817be96e8d4; ci_session=ba92hlh6o0ns7f20t4bsgjt0uqfdmdtl; _ga=GA1.2.1535910352.1530452604; _gid=GA1.2.1416631165.1530452604; _gat_gtag_UA_21820217_30=1',
'Connection': 'close',
'csrf_test_name': 'a3f8adecbf11e29c006d9817be96e8d4',
'vID': '9999',
}    

headers = {}

r = requests.post(url, headers=headers, data=json.dumps(payload))
print(r.status_code)  
</code></pre>

<p>But this is printing a <code>403</code> error code. What am I doing wrong here? </p>

<p>I am expecting a return response as json:</p>

<p><code>{""status_message"":""Thanks for help."",""help_count"":""141"",""status"":true}</code></p>
","10017881","","","","","2020-03-06 15:02:40","python requests POST with header and parameters","<python><python-3.x><post><request><python-requests>","1","2","2","","","CC BY-SA 4.0","0"
"51007476","1","","","2018-06-24 06:24:04","","2","25579","<h2>Actually i am trying to run the code from <strong><a href=""https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/"" rel=""nofollow noreferrer"">tutorial</a></strong> .I have placed the code and dataset in the same directory but still i am getting  the following error.</h2>

<pre><code>FileNotFoundError                         Traceback (most recent call last)
&lt;ipython-input-6-5f5284db0527&gt; in &lt;module&gt;()
     39 # extract features from all images
     40 directory = 'Flicker8k'
---&gt; 41 features = extract_features(directory)
     42 print('Extracted Features: %d' % len(features))
     43 # save to file

&lt;ipython-input-6-5f5284db0527&gt; in extract_features(directory)
     18         # extract features from each photo
     19         features = dict()
---&gt; 20         for name in listdir(directory):
     21                 # load an image from file
     22                 filename = directory + '/' + name

**FileNotFoundError: [WinError 3] The system cannot find the path specified: 'Flicker8k'**
</code></pre>
","8229673","","","","","2020-03-10 05:44:51","FileNotFoundError: [WinError 3] The system cannot find the path specified:","<python-3.x><machine-learning><anaconda><jupyter-notebook>","2","0","2","","","CC BY-SA 4.0","0"
"31669945","1","31670157","","2015-07-28 07:27:32","","3","25533","<p>I'm trying to use tkinter with python3 to open an image, see here a piece of code :</p>

<pre><code>#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# --- Python 3.4

from PIL import Image
import Tkinter as tk
from Tkinter import filedialog
import numpy as np
import os
var = 'n'

# Importing the image to correct

while var != 'o' :
    var = raw_input(""Press \""o\"" to open the image to correct\n"")
    var = var.lower()
root = tk.Tk()
root.withdraw()
path = filedialog.askopenfilename()
image_test = Image.open(path)
</code></pre>

<p>I have installed python3-tk, and I have the demo window when I write</p>

<pre><code>python3 -m tkinter 
</code></pre>

<p>in the terminal. I tried several combinations that did not work :</p>

<pre><code>import tkinter as tk
from tkinter import filedialog
</code></pre>

<p>gives </p>

<pre><code>ImportError : No module named tkinter
</code></pre>

<p>,</p>

<pre><code>import Tkinter as tk
from Tkinter import filedialog    
</code></pre>

<p>gives </p>

<pre><code>ImportError : cannot import name filedialog
</code></pre>

<p>I tried with _tinker , FileDialog, file_dialog, but I always have ""ImportError : cannot import name filedialog"". Any clue ?</p>
","5138201","","","","","2018-11-09 10:17:59","Tkinter import filedialog error","<python><python-3.x><tkinter>","3","0","1","","","CC BY-SA 3.0","0"
"59317249","1","59317314","","2019-12-13 06:29:33","","7","25508","<p>I have written code using matmul, but I am getting the following error: </p>

<pre><code>   ""ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 1 is different from 3)""
</code></pre>

<p>Code:</p>

<pre><code>    R = [[0.40348195], [0.38658295], [0.82931052]]
    V = [0.33452744, 0.33823673, 0.32723583]
    print(""Rt_p: "", R)
    B = np.matmul(V,np.transpose(R))/pow(LA.norm(R), 2)
    print(""B"", B)
</code></pre>
","8998072","","10239789","","2019-12-13 06:30:28","2019-12-13 06:36:27","ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 3)","<python><python-3.x><numpy>","1","0","","","","CC BY-SA 4.0","0"
"41405728","1","41408261","","2016-12-31 05:22:44","","61","25508","<p>I'm trying to compile Python 3.6 on an arm based Linux machine, 
<code>./configure</code> outputs this:</p>

<blockquote>
  <p>If you want a release build with all optimizations active (LTO, PGO, etc),
  please run <code>./configure --enable-optimizations</code>.</p>
</blockquote>

<p>what does <code>--enable-optimizations</code> do?</p>
","3625865","","4952130","","2017-01-11 09:35:06","2020-08-23 17:22:19","what does --enable-optimizations do while compiling python?","<python><linux><python-3.x><configure><python-3.6>","1","2","11","","","CC BY-SA 3.0","0"
"50327906","1","50328215","","2018-05-14 10:14:46","","11","25492","<p>I've already gone through all the similar questions in this regard and tried the solutions proposed there. But I'm unable to get this error sorted out though my <code>python3-tk</code> package is installed in the proper virtualenv that I'm using for my project.</p>

<p>Though in my project, I don't use tkinter, when i try to run the file, I'm getting the following error related to the <code>_tkinter</code> module.</p>

<blockquote>
  <p>Traceback (most recent call last):<br>
    File ""/usr/lib/python3.5/tkinter/<strong>init</strong>.py"", line 36, in 
      import _tkinter<br>
  ImportError: No module named '_tkinter'</p>
  
  <p>During handling of the above exception, another exception occurred:</p>
  
  <p>Traceback (most recent call last):<br>
    File ""/home/manuelanayantarajeyaraj/PycharmProjects/ChatbotWord2Vec/main.py"", line 2, in 
      from matplotlib import pyplot as plt<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/matplotlib/pyplot.py"", line 115, in 
      _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup()<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/matplotlib/backends/<strong>init</strong>.py"", line 62, in pylab_setup
      [backend_name], 0)<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/matplotlib/backends/backend_tkagg.py"", line 4, in 
      from . import tkagg  # Paint image to Tk photo blitter extension.<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/matplotlib/backends/tkagg.py"", line 5, in 
      from six.moves import tkinter as Tk<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/six.py"", line 92, in <strong>get</strong>
      result = self._resolve()<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/six.py"", line 115, in _resolve
      return _import_module(self.mod)<br>
    File ""/home/manuelanayantarajeyaraj/usr/myProject/my_project/lib/python3.5/site-packages/six.py"", line 82, in _import_module
      <strong>import</strong>(name)<br>
    File ""/usr/lib/python3.5/tkinter/<strong>init</strong>.py"", line 38, in 
      raise ImportError(str(msg) + ', please install the python3-tk package')<br>
  ImportError: No module named '_tkinter', please install the python3-tk package</p>
</blockquote>

<p>Hence, I navigated to the location of my interpreter and created a virtualenv and installed the <code>python3-tk</code> package using the following</p>

<pre><code>sudo apt-get install python3-tk
</code></pre>

<p>When I checked, all the packages seem to be up to date</p>

<pre><code>Reading package lists... Done
Building dependency tree       
Reading state information... Done
python3-tk is already the newest version (3.6.5-3~16.04.york0.2).
The following packages were automatically installed and are no longer required:
  libappindicator1 libindicator7 libllvm4.0 linux-headers-4.10.0-28
  linux-headers-4.10.0-28-generic linux-headers-4.13.0-36
  linux-headers-4.13.0-36-generic linux-headers-4.13.0-37
  linux-headers-4.13.0-37-generic linux-image-4.10.0-28-generic
  linux-image-4.13.0-36-generic linux-image-4.13.0-37-generic
  linux-image-extra-4.10.0-28-generic linux-image-extra-4.13.0-36-generic
  linux-image-extra-4.13.0-37-generic linux-signed-image-4.10.0-28-generic
  linux-signed-image-4.13.0-36-generic linux-signed-image-4.13.0-37-generic
Use 'sudo apt autoremove' to remove them.
0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.
</code></pre>

<p>But I'm still getting the same import error <code>ImportError: No module named '_tkinter', please install the python3-tk package</code>.</p>

<p>Any suggestions in this regard will be highly appreciated.</p>
","6339494","","6339494","","2018-12-17 05:10:58","2018-12-17 05:10:58","ImportError: No module named '_tkinter', please install the python3-tk package","<python-3.x><matplotlib><tkinter><ubuntu-16.04><importerror>","2","1","3","","","CC BY-SA 4.0","0"
"46341329","1","46341647","","2017-09-21 10:18:16","","4","25491","<p>I was running a basic hex2dec converter, and wanted to transform this from console to GUI.</p>

<p>Now the program works fine in console, but after my conversion to GUI, it seems to throw out the <strong><em>int() can't convert non-string with explicit base</em></strong> error.</p>

<p>Here is the GUI code</p>

<pre><code>from tkinter import *

root = Tk()
root.geometry(""400x400+250+250"")
root.title(""Hex Converter"")

heading = Label(root, text=""Simple Hex to Decimal Converter"", font=('arial 15 bold'), fg=""steelblue"").pack()

entr_hex_val = Label(root, text=""Enter Hex Value to Convert"", font=('arial 13 bold')).place(x=10, y=50)

my_num = IntVar()
ent_box = Entry(root, width=50, textvariable=my_num).place(x=10, y=90)

def converter():
    hexdec = my_num.get()
    dec = int(hexdec, 16)
    lab = Label(root, text=(""decimal value = ""+ str(dec)), font=('arial 25 bold'), fg=""red"").place(x=10, y=200)

conv = Button(root, text=""Convert"", width=12, height=2, bg=""lightgreen"", command=converter).place(x=10, y=130)

root.mainloop()
</code></pre>

<p>and the console code</p>

<pre><code>import os

def hexconverter:
    os.system('cls')
    hexdec = input(""Enter number in Hexadecimal Format: "")
    dec = int(hexdec, 16)
    print(str(dec))

hexconverter()
</code></pre>

<p>I'm struggling to see why the same code works in console but not in the GUI.</p>
","6311015","","3134251","","2017-09-21 10:54:41","2017-09-21 10:54:41","int() can't convert non-string with explicit base when converting to GUI","<python><python-3.x><tkinter>","2","3","","","","CC BY-SA 3.0","0"
"53604339","1","","","2018-12-04 01:21:23","","2","25446","<p>I'm a beginner in Python and I have recently started making a discord bot for some friends and I. The idea is to type !startq and have the bot join the channel, play an mp3 file that is locally stored in the same folder that the bot.py is in also. </p>

<pre><code>import discord, chalk
from discord.ext import commands
import time
import asyncio

bot = commands.Bot(command_prefix = ""!"")

@bot.event
async def on_ready():
    print(""Bot is ready!"")

@bot.command()
async def q5(ctx):
    await ctx.send(""@here QUEUE STARTING IN 5 MINUTES"")

@bot.command()
async def q3(ctx):
    await ctx.send(""@here QUEUE STARTING IN 3 MINUTES"")

@bot.command()
async def q1(ctx):
    await ctx.send(""@here QUEUE STARTING IN 1 MINUTES"")

@bot.command()
async def ping(ctx):
    ping_ = bot.latency
    ping =  round(ping_ * 1000)
    await ctx.send(f""my ping is {ping}ms"")

@bot.command()
async def startq(ctx):
    voicechannel = discord.utils.get(ctx.guild.channels, name='queue')
    vc = await voicechannel.connect()
    vc.play(discord.FFmpegPCMAudio(""countdown.mp3""), after=lambda e: print('done', e))
    bot.run('TOKEN')
</code></pre>

<p>So far my bot joins the channel fine, but it doesn't actually play the mp3. I've asked countless people in the ""Unofficial Discord API Discord"" and a few other programming Discords, but I haven't gotten an answer yet.</p>
","10712440","","","","","2020-05-21 07:59:39","how do i make my discord.py bot play mp3 in voice channel?","<python-3.x><ffmpeg><discord><discord.py>","3","5","2","","","CC BY-SA 4.0","0"
"38485373","1","38485534","","2016-07-20 15:58:58","","18","25363","<p>I am writing a shell script, and before the script runs I want to verify that the user has Python 3 installed. Does anyone know or have any ideas of how I could check that, and the output be a boolean value?</p>
","5910749","","","","","2020-01-19 14:46:43","From the terminal verify if python 3 is installed","<bash><python-3.x><terminal>","3","3","3","","","CC BY-SA 3.0","0"
"37676623","1","46414982","","2016-06-07 10:19:18","","11","25342","<p>I need to install <a href=""https://pypi.python.org/pypi/python-Levenshtein/0.10.2"" rel=""noreferrer"">python Levenshtein distance package</a> in order to use <a href=""https://github.com/udibr/headlines"" rel=""noreferrer"">this library</a>.
Unfortunately, I am not able to install it succesfully. I usually install libraries with pip. However, this time I am getting <code>error: [WinError 2] The system cannot find the file specified</code> which had never happened to me before (when installing libraries). I have tried to install it using the <code>python setup.py install</code> but I get exactly the same error. This the output I get from the console.</p>

<pre><code>C:\Users\my_user\Anaconda3\Lib\site-packages\python-Levenshtein-0.10.2&gt;python setup.py install
running install
running bdist_egg
running egg_info
writing dependency_links to python_Levenshtein.egg-info\dependency_links.txt
writing namespace_packages to python_Levenshtein.egg-info\namespace_packages.txt
writing entry points to python_Levenshtein.egg-info\entry_points.txt
writing python_Levenshtein.egg-info\PKG-INFO
writing top-level names to python_Levenshtein.egg-info\top_level.txt
writing requirements to python_Levenshtein.egg-info\requires.txt
reading manifest file 'python_Levenshtein.egg-info\SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no files found matching '*' under directory 'docs'
warning: no previously-included files matching '*pyc' found anywhere in distribution
warning: no previously-included files matching '.project' found anywhere in distribution
warning: no previously-included files matching '.pydevproject' found anywhere in distribution
writing manifest file 'python_Levenshtein.egg-info\SOURCES.txt'
installing library code to build\bdist.win-amd64\egg
running install_lib
running build_ext
building 'Levenshtein' extension
error: [WinError 2] The system cannot find the file specified
</code></pre>

<p>On the other hand, this is what I get when running <code>pip install</code>:</p>

<pre><code>C:\Users\my_user\Anaconda3\Lib\site-packages\python-Levenshtein-0.10.2&gt;pip install python-Levenshtein
Collecting python-Levenshtein
Using cached python-Levenshtein-0.12.0.tar.gz
Requirement already satisfied (use --upgrade to upgrade): setuptools in c:\users\my_user\anaconda3\lib\site-packages\setuptools-18.4-py3.5.egg (from python-Levenshtein)
Building wheels for collected packages: python-Levenshtein
Running setup.py bdist_wheel for python-Levenshtein ... error
Complete output from command c:\users\my_user\anaconda3\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\my_user\\AppData\\Local\\Temp\\pip-build-99lnpr1w\\python-Levenshtein\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" bdist_wheel -d C:\Users\my_user\AppData\Local\Temp\tmpvw371ebspip-wheel- --python-tag cp35:
running bdist_wheel
running build
running build_py
creating build
creating build\lib.win-amd64-3.5
creating build\lib.win-amd64-3.5\Levenshtein
copying Levenshtein\StringMatcher.py -&gt; build\lib.win-amd64-3.5\Levenshtein
copying Levenshtein\__init__.py -&gt; build\lib.win-amd64-3.5\Levenshtein
running egg_info
writing python_Levenshtein.egg-info\PKG-INFO
writing dependency_links to python_Levenshtein.egg-info\dependency_links.txt
writing entry points to python_Levenshtein.egg-info\entry_points.txt
writing top-level names to python_Levenshtein.egg-info\top_level.txt
writing namespace_packages to python_Levenshtein.egg-info\namespace_packages.txt
writing requirements to python_Levenshtein.egg-info\requires.txt
warning: manifest_maker: standard file '-c' not found

reading manifest file 'python_Levenshtein.egg-info\SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no previously-included files matching '*pyc' found anywhere in distribution
warning: no previously-included files matching '*so' found anywhere in distribution
warning: no previously-included files matching '.project' found anywhere in distribution
warning: no previously-included files matching '.pydevproject' found anywhere in distribution
writing manifest file 'python_Levenshtein.egg-info\SOURCES.txt'
copying Levenshtein\_levenshtein.c -&gt; build\lib.win-amd64-3.5\Levenshtein
copying Levenshtein\_levenshtein.h -&gt; build\lib.win-amd64-3.5\Levenshtein
running build_ext
building 'Levenshtein._levenshtein' extension
error: [WinError 2] The system cannot find the file specified

----------------------------------------
Failed building wheel for python-Levenshtein
Running setup.py clean for python-Levenshtein
Failed to build python-Levenshtein
Installing collected packages: python-Levenshtein
Running setup.py install for python-Levenshtein ... error
Complete output from command c:\users\my_user\anaconda3\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\my_user\\AppData\\Local\\Temp\\pip-build-99lnpr1w\\python-Levenshtein\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record C:\Users\my_user\AppData\Local\Temp\pip-wjhuwi7v-record\install-record.txt --single-version-externally-managed --compile:
running install
running build
running build_py
creating build
creating build\lib.win-amd64-3.5
creating build\lib.win-amd64-3.5\Levenshtein
copying Levenshtein\StringMatcher.py -&gt; build\lib.win-amd64-3.5\Levenshtein
copying Levenshtein\__init__.py -&gt; build\lib.win-amd64-3.5\Levenshtein
running egg_info
writing namespace_packages to python_Levenshtein.egg-info\namespace_packages.txt
writing top-level names to python_Levenshtein.egg-info\top_level.txt
writing python_Levenshtein.egg-info\PKG-INFO
writing requirements to python_Levenshtein.egg-info\requires.txt
writing dependency_links to python_Levenshtein.egg-info\dependency_links.txt
writing entry points to python_Levenshtein.egg-info\entry_points.txt
warning: manifest_maker: standard file '-c' not found

reading manifest file 'python_Levenshtein.egg-info\SOURCES.txt'
reading manifest template 'MANIFEST.in'
warning: no previously-included files matching '*pyc' found anywhere in distribution
warning: no previously-included files matching '*so' found anywhere in distribution
warning: no previously-included files matching '.project' found anywhere in distribution
warning: no previously-included files matching '.pydevproject' found anywhere in distribution
writing manifest file 'python_Levenshtein.egg-info\SOURCES.txt'
copying Levenshtein\_levenshtein.c -&gt; build\lib.win-amd64-3.5\Levenshtein
copying Levenshtein\_levenshtein.h -&gt; build\lib.win-amd64-3.5\Levenshtein
running build_ext
building 'Levenshtein._levenshtein' extension
error: [WinError 2] The system cannot find the file specified

----------------------------------------
Command ""c:\users\my_user\anaconda3\python.exe -u -c ""import setuptools, tokenize;__file__='C:\\Users\\my_user\\AppData\\Local\\Temp\\pip-build-99lnpr1w\\python-Levenshtein\\setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record C:\Users\my_user\AppData\Local\Temp\pip-wjhuwi7v-record\install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in C:\Users\my_user\AppData\Local\Temp\pip-build-99lnpr1w\python-Levenshtein\
</code></pre>

<p>I am using Windows 10 with Python 3.5. I have read other issues like <a href=""https://stackoverflow.com/questions/13200330/how-to-install-python-levenshtein-on-windows"">this</a> and <a href=""https://stackoverflow.com/questions/14635547/how-to-install-python-levenshtein-windows"">this</a> but I found that those are different problems. By the way I have also tried the following the problem persists.</p>

<ul>
<li>Running console as Administrator</li>
<li>(Re)installing <a href=""https://download.microsoft.com/download/1/1/1/1116b75a-9ec3-481a-a3c8-1777b5381140/vcredist_x86.exe"" rel=""noreferrer"">VC++ 2008</a></li>
</ul>

<p>Thanks in advance</p>
","1309231","","-1","","2017-05-23 12:03:01","2020-03-25 13:03:56","Can't install Levenshtein distance package on Windows Python 3.5","<python-3.x><pip><levenshtein-distance>","5","2","2","","","CC BY-SA 3.0","0"
"54781243","1","54782540","","2019-02-20 07:47:30","","15","25314","<p>I would like to hide the Seaborn pairplot legend. The official docs don't mention a keyword legend. Everything I tried using <code>plt.legend</code> didn't work. Please suggest the best way forward. Thanks!</p>

<pre><code>import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

test = pd.DataFrame({
    'id': ['1','2','1','2','2','6','7','7','6','6'],
    'x': [123,22,356,412,54,634,72,812,129,110],
    'y':[120,12,35,41,45,63,17,91,112,151]})
sns.pairplot(x_vars='x', y_vars=""y"", 
                 data=test,
                 hue = 'id', 
                 height = 3)
</code></pre>
","5967886","","","","","2020-08-21 08:11:05","Hide legend from seaborn pairplot","<python-3.x><matplotlib><seaborn>","2","1","","","","CC BY-SA 4.0","0"
"47296969","1","47297097","","2017-11-14 23:26:26","","25","25260","<p>Any ideas how can I solve problem shown below? With the information that I found on the web it is associated with problem of reusing tensorflow scope however nothing works. </p>



<pre class=""lang-python prettyprint-override""><code>ValueError: Variable rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:

  File ""/code/backend/management/commands/RNN.py"", line 370, in predict
    states_series, current_state = tf.nn.dynamic_rnn(cell=cell, inputs=batchX_placeholder, dtype=tf.float32)
  File ""/code/backend/management/commands/RNN.py"", line 499, in Command
    predict(""string"")
  File ""/code/backend/management/commands/RNN.py"", line 12, in &lt;module&gt;
    class Command(BaseCommand):
</code></pre>

<p>I tried for instance something like this</p>

<pre class=""lang-python prettyprint-override""><code>with tf.variable_scope('scope'):
 states_series, current_state = tf.nn.dynamic_rnn(cell=cell, inputs=batchX_placeholder, dtype=tf.float32)
</code></pre>

<p>and this</p>

<pre class=""lang-python prettyprint-override""><code>with tf.variable_scope('scope', reuse = True ):
 states_series, current_state = tf.nn.dynamic_rnn(cell=cell, inputs=batchX_placeholder, dtype=tf.float32)
</code></pre>

<p>and this</p>

<pre class=""lang-python prettyprint-override""><code>with tf.variable_scope('scope', reuse = tf.AUTO_REUSE ):
 states_series, current_state = tf.nn.dynamic_rnn(cell=cell, inputs=batchX_placeholder, dtype=tf.float32)
</code></pre>

<p>Any ideas?</p>
","","user7304253","","","","2017-11-14 23:38:44","ValueError: Variable rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope?","<python><python-3.x><machine-learning><tensorflow><neural-network>","1","0","6","","","CC BY-SA 3.0","0"
"42519329","1","42519613","","2017-02-28 21:45:07","","12","25227","<p>I'm new to flask and python. For example, if you have the following code...</p>

<pre><code>@app.route('/')
def index():
 return ""Hello world!""

@app.route('/accounts')
def accounts():
    return some_data

@app.route('/login')
def login():
    return render_template(login.html)

if __name__ == ""__main__"":
    app.run()
</code></pre>

<p>In what order will these execute? From what I understand if you are on the /accounts page of the website, that function will run. If you are on the /login page, that function will run. What role does the parameter for the route method play? Secondly, how many times will the last two lines of code be run (will the app.run() be called once or every time the app.route() is called)? Additionally, what folder should an app be put in within a filestructure for a website?</p>
","4595005","","","","","2017-02-28 22:13:30","How do python flask app.route() sections execute?","<python-3.x><web><flask>","1","0","4","","","CC BY-SA 3.0","0"
"45131890","1","45132014","","2017-07-16 18:12:50","","16","25201","<p>The software fails to install. Any help in resolving this would be appreciated. </p>

<p>I believe that the error is probably a dependency error. </p>

<pre><code>             Running setup.py (path:/tmp/pip-build-9rlb94_r/hashlib/setup.py) egg_info for package hashlib
            Traceback (most recent call last):
              File ""&lt;string&gt;"", line 3, in &lt;module&gt;
              File ""/usr/local/lib/python3.4/dist-packages/setuptools/__init__.py"", line 10, in &lt;module&gt;
                from setuptools.extern.six.moves import filter, map
              File ""/usr/local/lib/python3.4/dist-packages/setuptools/extern/__init__.py"", line 1, in &lt;module&gt;
                from pkg_resources.extern import VendorImporter
              File ""/usr/local/lib/python3.4/dist-packages/pkg_resources/__init__.py"", line 36, in &lt;module&gt;
                import email.parser
              File ""/usr/lib/python3.4/email/parser.py"", line 12, in &lt;module&gt;
                from email.feedparser import FeedParser, BytesFeedParser
              File ""/usr/lib/python3.4/email/feedparser.py"", line 27, in &lt;module&gt;
                from email import message
              File ""/usr/lib/python3.4/email/message.py"", line 16, in &lt;module&gt;
                from email import utils
              File ""/usr/lib/python3.4/email/utils.py"", line 28, in &lt;module&gt;
                import random
              File ""/usr/lib/python3.4/random.py"", line 45, in &lt;module&gt;
                from hashlib import sha512 as _sha512
              File ""/tmp/pip-build-9rlb94_r/hashlib/hashlib.py"", line 80
                raise ValueError, ""unsupported hash type""
                                ^
            SyntaxError: invalid syntax
            Complete output from command python setup.py egg_info:
            Traceback (most recent call last):

          File ""&lt;string&gt;"", line 3, in &lt;module&gt;

          File ""/usr/local/lib/python3.4/dist-packages/setuptools/__init__.py"", line 10, in &lt;module&gt;

            from setuptools.extern.six.moves import filter, map

          File ""/usr/local/lib/python3.4/dist-packages/setuptools/extern/__init__.py"", line 1, in &lt;module&gt;

            from pkg_resources.extern import VendorImporter

          File ""/usr/local/lib/python3.4/dist-packages/pkg_resources/__init__.py"", line 36, in &lt;module&gt;

            import email.parser

          File ""/usr/lib/python3.4/email/parser.py"", line 12, in &lt;module&gt;

            from email.feedparser import FeedParser, BytesFeedParser

          File ""/usr/lib/python3.4/email/feedparser.py"", line 27, in &lt;module&gt;

            from email import message

          File ""/usr/lib/python3.4/email/message.py"", line 16, in &lt;module&gt;

            from email import utils

          File ""/usr/lib/python3.4/email/utils.py"", line 28, in &lt;module&gt;

            import random

          File ""/usr/lib/python3.4/random.py"", line 45, in &lt;module&gt;

            from hashlib import sha512 as _sha512

          File ""/tmp/pip-build-9rlb94_r/hashlib/hashlib.py"", line 80

            raise ValueError, ""unsupported hash type""

                            ^

        SyntaxError: invalid syntax

        ----------------------------------------
</code></pre>

<p>I am using this library to generate hashes for files and so alternative solutions would also be welcome.</p>
","3353752","","","","","2017-07-16 18:26:27","Failed to install hashlib, python 3, debian","<python><python-3.x><hashlib>","1","3","2","","","CC BY-SA 3.0","0"
"48938978","1","","","2018-02-22 23:35:45","","0","25177","<p>I have python 3.6. I want to execute python file named 'operation.py' from another python file named 'run.py'.</p>

<p>In <code>operation.py</code> I do <code>from cStringIO import StringIO</code>. PyCharm shows me a warning that there is no module named StringIO. I know that since python3 I have to import StringIO module from io. However, when I use this importation, the functions of this module are no longer work. </p>

<p>Although there is a warning in <code>from cStringIO import StringIO</code>, the code still works (I know this import really works because I tried to make it a comment and it couldn't run). The problem is that when I try to run this file by the 'run.py' file, it can't run and prints the following message: <code>ModuleNotFoundError: No module named 'cStringIO'</code>.</p>

<p>I tried to use this <a href=""https://stackoverflow.com/questions/21236824/unresolved-reference-issue-in-pycharm"">Unresolved reference issue in PyCharm</a> but it didn't help.</p>

<p>Why does 'operation.py' run though the warning, but 'run.py' does not? How can I solve this? </p>

<p>operation.py:</p>

<pre><code>    from cStringIO import StringIO


    str_io = StringIO()
    g = Generator(str_io, False)
    # There is a full code here...
</code></pre>

<p>run.py:</p>

<pre><code>    import operation


    def main():
        operation
</code></pre>

<p>The <code>operation.py</code> has a warning but runs well, run.py has a fail. </p>
","9399101","","9399101","","2018-02-22 23:52:15","2018-02-22 23:52:15","no module named StringIO","<python><python-3.x><stringio><cstringio>","1","3","","","","CC BY-SA 3.0","0"
"56119490","1","","","2019-05-13 20:29:24","","10","25176","<p>I installed the library and when trying to access SQL in jupyter notebook with my credentials the following error appears:</p>

<p>DatabaseError: DPI-1047: Cannot locate a 64-bit Oracle Client library: ""The specified module could not be found"". See <a href=""https://oracle.github.io/odpi/doc/installation.html#windows"" rel=""noreferrer"">https://oracle.github.io/odpi/doc/installation.html#windows</a> for help</p>
","10906075","","","","","2020-10-21 15:35:46","cx_Oracle error. DPI-1047: Cannot locate a 64-bit Oracle Client library","<python><python-3.x><windows-10><cx-oracle>","4","3","3","","","CC BY-SA 4.0","0"
"51187314","1","","","2018-07-05 09:04:24","","1","25118","<p>This is my code, it outputs a multiplication table but it's not what I wanted!</p>

<pre><code> num = int(input(""Multiplication using value? : ""))

while num &lt;= 10:
    i = 1
    while i &lt;= num:
        product = num*i
        print(num, "" * "", i, "" = "", product, ""\n"")
        i = i + 1
    print(""\n"")
    num = num + 1
</code></pre>

<p>I am basically creating a multiplication table from the user's input from 1-9. </p>

<p>Ex. If the user inputs ""3""</p>

<p>I should get this output:</p>

<pre><code>1*1=1
1*2=2
1*3=3

2*1=2
2*2=4
2*3=6

3*1=3
3*2=6
3*3=9
</code></pre>

<p>This is my first time learning Python, I could find any help online, Pls Help</p>
","8449817","","","","","2020-10-24 12:34:56","How do I use while loops to create a multiplication table in python?","<python><python-3.x><while-loop>","9","3","1","","","CC BY-SA 4.0","0"
"40834656","1","","","2016-11-27 22:27:05","","7","25107","<p>I am trying to install a package called ""simpleguitk"" via pip. (On Ubuntu 16.04 with Python 3.5)<br>
After running </p>

<pre><code>sudo -H pip3 install simpleguitk
</code></pre>

<p>it says installation is completed successfully. (Except for the pygame dependecy which is actually optional)</p>

<pre><code>Collecting simpleguitk
Using cached SimpleGUITk-1.1.3.tar.gz
Collecting Pillow&gt;=2.0.0 (from simpleguitk)
Using cached Pillow-3.4.2-cp35-cp35m-manylinux1_x86_64.whl
Collecting pygame&gt;=1.9.0 (from simpleguitk)
Could not find a version that satisfies the requirement pygame&gt;=1.9.0 (from simpleguitk) (from versions: 1.9.2.dev1, 1.9.2b7, 1.9.2b8)
No matching distribution found for pygame&gt;=1.9.0 (from simpleguitk)
</code></pre>

<p>I cannot find the package at /usr/local/lib/python3.5/dist-packages
or /usr/lib/python3.5 or /usr/lib/python3</p>

<p>When I try to import the module it says:</p>

<pre><code>Traceback (most recent call last):
File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: No module named 'simpleguitk'
</code></pre>

<p>I tried to reinstall it, but running:</p>

<pre><code>sudo -H pip3 uninstall simpleguitk
</code></pre>

<p>returns: ""Cannot uninstall requirement simpleguitk, not installed
""</p>

<p>I have tried this on both pip 8.1.2 and pip 9.0.1 with the same results.
I have even reinstalled Ubuntu, but still the same.</p>

<p>I think Python Path is wrong as it does not have python 3.5 but I do not know how to fix it</p>

<pre><code>['', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/gtk-2.0']
</code></pre>
","6316916","","","","","2019-03-08 08:51:27","Installed module using pip, not found","<python><python-3.x><ubuntu><pip><pythonpath>","5","1","2","","","CC BY-SA 3.0","0"
"50686243","1","50717003","","2018-06-04 17:55:14","","8","25105","<p>I'm trying to install cx_Freeze and scipy but I get a </p>

<blockquote>
  <p>compile failed with error code 1</p>
</blockquote>

<p>error every time. Here's what I see when I try to do it with cx_Freeze:</p>

<pre><code> error: file 'C:\Users\myAccount\AppData\Local\Temp\pip-install-nabp1tpo\cx-fre
eze\cxfreeze-postinstall' does not exist

    ----------------------------------------
Command ""c:\users\myAccount\appdata\local\programs\python\python37\python.exe -u -
c ""import setuptools, tokenize;__file__='C:\\Users\\myAccount\\AppData\\Local\\Tem
p\\pip-install-nabp1tpo\\cx-freeze\\setup.py';f=getattr(tokenize, 'open', open)(
__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __fil
e__, 'exec'))"" install --record C:\Users\myAccount\AppData\Local\Temp\pip-record-3
6fbtmht\install-record.txt --single-version-externally-managed --compile"" failed
 with error code 1 in C:\Users\myAccount\AppData\Local\Temp\pip-install-nabp1tpo\c
x-freeze\
</code></pre>

<p>How can I fix this?</p>
","7973972","","8516269","","2019-06-05 12:45:27","2019-07-08 15:04:32","Can't install cx_Freeze or scipy for Python 3.7 64-bit","<python-3.x><cx-freeze>","5","0","4","","","CC BY-SA 4.0","0"
"29795488","1","29795561","","2015-04-22 11:02:25","","22","25072","<p>Using Python 3.4 I want to test whether an Enum class contains a member with a certain name.</p>

<p>Example:</p>

<pre><code>class Constants(Enum):
    One = 1
    Two = 2
    Three = 3

print(Constants['One'])
print(Constants['Four'])
</code></pre>

<p>gives:</p>

<pre><code>Constants.One
  File ""C:\Python34\lib\enum.py"", line 258, in __getitem__
    return cls._member_map_[name]
KeyError: 'Four'
</code></pre>

<p>I could catch the <code>KeyError</code> and take the exception as indication of existence but maybe there is a more elegant way?</p>
","1536976","","","","","2020-06-30 16:42:33","How to test if an Enum member with a certain name exists?","<python><python-3.x><enums>","4","0","4","","","CC BY-SA 3.0","0"
"45499191","1","","","2017-08-04 05:56:12","","2","25063","<pre><code>x = input()
y = 1 
print (x)
while 1 == y:
if x == 1:
    y == y + 1
elif x % 2 == 0: #even
    x = x // 2
    print (x)
else:
    x = 3 * x + 1
    print (x)
</code></pre>

<p>If you know what the Collatz conjecture is, I'm trying to make a calculator for that. I want to have x as my input so I don't have to change x's number and save every time I want to try out a new number. </p>

<p>I get below error </p>

<blockquote>
  <p>TypeError: not all arguments converted during string formatting' 
  at line 7. </p>
</blockquote>

<p>Please help a noobie out.</p>
","8361219","","5014455","","2017-08-04 10:10:13","2017-08-04 10:10:13","not all arguments converted during string formatting.. NO % variables","<python><python-3.x><typeerror>","1","1","2","","","CC BY-SA 3.0","0"
"35999344","1","35999835","","2016-03-14 22:30:07","","19","24991","<p>Hi have been scavenging the web for answers on how to do this but there was no direct answer. Does anyone know how I can find the version number of tkinter?</p>
","6036220","","","","","2020-08-24 18:24:30","How to determine what version of python3 tkinter is installed on my linux machine?","<python-3.x><tkinter>","5","0","2","","","CC BY-SA 3.0","0"
"33906539","1","33926651","","2015-11-25 00:36:08","","18","24923","<p>Yes I want to create a run configuration in PyCharm to run Pyinstaller and get my executable. According to the <a href=""https://pythonhosted.org/PyInstaller/#installed-commands"" rel=""noreferrer"">Pyinstaller documentation</a> you should be able to locate an python script called <code>pyinstaller-folder/pyinstaller.py</code> after the installation, but it wasn't there. Then I look carefully and found this other one named <code>pyinstaller-folder/__main__.py</code> which should be the same &lt;--(me wild guessing), so I set up my running configuration like this:</p>

<p><a href=""https://i.stack.imgur.com/A2ULF.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/A2ULF.png"" alt=""enter image description here""></a></p>

<p>After running it, is giving me this error:</p>

<pre><code>/usr/local/Cellar/python3/3.4.3/bin/python3.4 /usr/local/lib/python3.4/sit

e-packages/PyInstaller/__main__.py --onefile --nowindow --osx-bundle-identifier=jg.optimizer -F --name=genoptimizer optimizer/manage.py
Traceback (most recent call last):
  File ""/usr/local/lib/python3.4/site-packages/PyInstaller/__main__.py"", line 26, in &lt;module&gt;
    from . import __version__
SystemError: Parent module '' not loaded, cannot perform relative import

Process finished with exit code 1
</code></pre>

<p>It seems to require a parent module to run but, how would that look like? </p>
","1903784","","327064","","2016-03-03 19:05:15","2020-09-02 12:43:37","Configuring Pycharm to run Pyinstaller","<python><python-3.x><pycharm><osx-yosemite><pyinstaller>","7","0","10","","","CC BY-SA 3.0","0"
"37465816","1","37466076","","2016-05-26 15:59:34","","24","24908","<p>The Getting Started docs for aiohttp give the following client example:</p>

<pre><code>import asyncio
import aiohttp

async def fetch_page(session, url):
    with aiohttp.Timeout(10):
        async with session.get(url) as response:
            assert response.status == 200
            return await response.read()

loop = asyncio.get_event_loop()
with aiohttp.ClientSession(loop=loop) as session:
    content = loop.run_until_complete(
        fetch_page(session, 'http://python.org'))
    print(content)
</code></pre>

<p>And they give the following note for Python 3.4 users:</p>

<blockquote>
  <p>If you are using Python 3.4, please replace await with yield from and
  async def with a @coroutine decorator.</p>
</blockquote>

<p>If I follow these instructions I get:</p>

<pre><code>import aiohttp
import asyncio

@asyncio.coroutine
def fetch(session, url):
    with aiohttp.Timeout(10):
        async with session.get(url) as response:
            return (yield from response.text())

if __name__ == '__main__':
    loop = asyncio.get_event_loop()
    with aiohttp.ClientSession(loop=loop) as session:
        html = loop.run_until_complete(
            fetch(session, 'http://python.org'))
        print(html)
</code></pre>

<p>However, this will not run, because <code>async with</code> is not supported in Python 3.4:</p>

<pre><code>$ python3 client.py 
  File ""client.py"", line 7
    async with session.get(url) as response:
             ^
SyntaxError: invalid syntax
</code></pre>

<p>How can I translate the <code>async with</code> statement to work with Python 3.4?</p>
","58866","","100297","","2016-05-26 20:53:27","2017-07-31 22:05:52","""async with"" in Python 3.4","<python><python-3.x><async-await><python-asyncio><aiohttp>","2","0","5","","","CC BY-SA 3.0","0"
"39233077","1","39233152","","2016-08-30 16:52:51","","14","24873","<p>I am trying to separate my script into several files with functions, so I moved some functions into separate files and want to import them into one main file. The structure is:</p>

<pre><code>core/
  main.py
  posts_run.py
</code></pre>

<p><code>posts_run.py</code> has two functions, <code>get_all_posts</code> and <code>retrieve_posts</code>, so I try import <code>get_all_posts</code> with:</p>

<pre><code>from posts_run import get_all_posts
</code></pre>

<p>Python 3.5 gives the error:</p>

<pre><code>ImportError: cannot import name 'get_all_posts'
</code></pre>

<p>Main.py contains following rows of code:</p>

<pre><code>import vk
from configs import client_id, login, password
session = vk.AuthSession(scope='wall,friends,photos,status,groups,offline,messages',   app_id=client_id, user_login=login,
                     user_password=password)
api = vk.API(session)
</code></pre>

<p>Then i need to import api to functions, so I have ability to get API calls to vk.</p>

<p>Full stack trace</p>

<pre><code>Traceback (most recent call last):
  File ""E:/gited/vkscrap/core/main.py"", line 26, in &lt;module&gt;
    from posts_run import get_all_posts
  File ""E:\gited\vkscrap\core\posts_run.py"", line 7, in &lt;module&gt;
    from main import api, absolute_url, fullname
  File ""E:\gited\vkscrap\core\main.py"", line 26, in &lt;module&gt;
    from posts_run import get_all_posts
ImportError: cannot import name 'get_all_posts'
</code></pre>

<p>api - is a <code>api = vk.API(session)</code> in main.py.
absolute_url and fullname are also stored in main.py.
I am using PyCharm 2016.1 on Windows 7, Python 3.5 x64 in virtualenv.
How can I import this function?</p>
","5219341","","3627387","","2018-05-04 07:15:12","2018-05-04 07:15:12","How to import a function from a module in the same folder?","<python><python-3.x>","4","12","1","","","CC BY-SA 4.0","0"
"28254807","1","28254828","","2015-01-31 18:05:01","","0","24863","<p>I've written a BMI calculator in python 3.4 and at the end I'd like to ask whether the user would like to use the calculator again and if yes go back to the beginning of the code. I've got this so far. Any help is very welcome :-)</p>

<pre><code>#Asks if the user would like to use the calculator again
again =input(""Thank you again for using this calculator, would you like to try again? Please type y for yes or n for no-"")

while(again != ""n"") and (again != ""y""):
    again =input(""Please type a valid response. Would you like to try again? Please type y for yes or n for no-"")

if again == ""n"" :
    print(""Thank you, bye!"")

elif again == ""y"" :
</code></pre>

<p>....</p>
","4512897","","","","","2015-01-31 18:16:21","How to loop back to the beginning of a programme - Python","<python><loops><python-3.x>","2","0","","2015-01-31 18:10:36","","CC BY-SA 3.0","0"
"50189403","1","50189430","","2018-05-05 12:24:37","","6","24861","<p>I'm trying using opencv python codes for my mini project that is Basic paint application.</p>

<p>I want to write a text on my image in run-time application. Not before running the code or hard coded text. How can I do that?</p>

<p>I need help for that, Thanks.</p>
","8107458","","8107458","","2018-05-07 04:48:26","2018-05-07 08:14:22","How to write text on image using opencv python in run-time application","<python-3.x><opencv>","1","1","2","2018-05-05 12:30:53","","CC BY-SA 4.0","0"
"36666225","1","36673656","","2016-04-16 15:42:27","","10","24860","<p>I recently installed the PyQt5 module setup for <strong>32-Bit</strong> computers on <strong>Windows</strong>. But when I am trying to run their examples <strong>none of then</strong> would run. All of the examples provided were having the <strong>similar</strong> type of error as in the following image. And when I tried to import PyQt5 in the Python shell it just imported correctly.</p>

<p><a href=""https://i.stack.imgur.com/fWzRf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fWzRf.jpg"" alt=""Enter image description here""></a></p>

<p>I think this shows PyQt is installed successfully.</p>

<p>But when i try to run the examples it shows like:</p>

<p><a href=""https://i.stack.imgur.com/AQtP4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AQtP4.jpg"" alt=""enter image description here""></a></p>

<p>This is of one example and other examples have different 'no founds' </p>

<blockquote>
  <p><strong>Facts</strong> - Running Windows 7, PyQt5 latest version, Python 3.5, Installed PyQt5 from original site with setup</p>
</blockquote>

<p>I know there are several questions of such type, but none helped me because most of them were for Linux.</p>
","5986816","","63550","","2017-12-02 01:40:32","2018-07-15 08:15:19","PyQt5: The DLL load failed: the specified module could not be found","<python><python-3.x><module><pyqt><pyqt5>","4","5","0","","","CC BY-SA 3.0","0"
"43062047","1","43062407","","2017-03-28 06:25:17","","9","24845","<p>In below code, i want to store URL in a variable to check error on which URL error occured.</p>
<pre><code>@app.route('/flights', methods=['GET'])
def get_flight():
    flight_data= mongo.db.flight_details
    info = []
    for index in flight_data.find():
        info.append({'flight_name': index['flight_name'], 'flight_no': index['flight_no'], 'total_seat': index['total_seat'] })
    if request.headers['Accept'] == 'application/xml':
        template = render_template('data.xml', info=info)
        xml_response = make_response(template)
        xml_response.headers['Accept'] = 'application/xml'
        logger.info('sucessful got data')
        return xml_response
    elif request.headers['Accept'] == 'application/json':
        logger.info('sucessful got data')
        return jsonify(info)
</code></pre>
<p>Output:</p>
<pre><code>* Restarting with stat
 * Debugger is active!
 * Debugger PIN: 165-678-508
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
127.0.0.1 - - [28/Mar/2017 10:44:53] &quot;GET /flights HTTP/1.1&quot; 200 -
</code></pre>
<p>I want this message</p>
<pre><code>&quot;127.0.0.1 - - [28/Mar/2017 10:44:53] &quot;GET /flights HTTP/1.1&quot; 200 -&quot;
</code></pre>
<p>should be stored in a variable or how can I get current URL that is executing?</p>
","7690443","","5446749","","2020-07-23 14:43:54","2020-07-23 14:43:54","How can I get current base URI in flask?","<python><python-3.x><url><flask>","2","0","2","2017-03-28 14:49:41","","CC BY-SA 4.0","0"
"50168647","1","52230415","","2018-05-04 06:36:05","","78","24790","<p>I am relatively new to Python and trying to implement a Multiprocessing module for my for loop.</p>

<p>I have an array of Image url's stored in img_urls which I need to download and apply some Google vision.</p>

<pre><code>if __name__ == '__main__':

    img_urls = [ALL_MY_Image_URLS]
    runAll(img_urls)
    print(""--- %s seconds ---"" % (time.time() - start_time)) 
</code></pre>

<p>This is my runAll() method</p>

<pre><code>def runAll(img_urls):
    num_cores = multiprocessing.cpu_count()

    print(""Image URLS  {}"",len(img_urls))
    if len(img_urls) &gt; 2:
        numberOfImages = 0
    else:
        numberOfImages = 1

    start_timeProcess = time.time()

    pool = multiprocessing.Pool()
    pool.map(annotate,img_urls)
    end_timeProcess = time.time()
    print('\n Time to complete ', end_timeProcess-start_timeProcess)

    print(full_matching_pages)


def annotate(img_path):
    file =  requests.get(img_path).content
    print(""file is"",file)
    """"""Returns web annotations given the path to an image.""""""
    print('Process Working under ',os.getpid())
    image = types.Image(content=file)
    web_detection = vision_client.web_detection(image=image).web_detection
    report(web_detection)
</code></pre>

<p>I am getting this as the warning when I run it and python crashes</p>

<pre><code>objc[67570]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67570]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67567]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67567]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67568]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67568]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67569]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67569]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67571]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67571]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
objc[67572]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called.
objc[67572]: +[__NSPlaceholderDate initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
</code></pre>
","5608734","","","","","2020-07-02 07:14:21","Multiprocessing causes Python to crash and gives an error may have been in progress in another thread when fork() was called","<python><multithreading><python-3.x>","1","8","21","","","CC BY-SA 4.0","0"
"51783852","1","","","2018-08-10 09:55:22","","3","24770","<h1>Code:</h1>
<pre><code>import tkinter as tk
a = &quot;hi&quot;
print(a)
a1 = tk.StringVar()
a1.set(&quot;Hi&quot;)
print(a1)
</code></pre>
<h1>Output:</h1>
<pre><code>hi ##(Output from first print function) 

AttributeError: 'NoneType' object has no attribute '_root' (Output from second print function) 
</code></pre>
<h1>My question:</h1>
<p>What is the difference between <code>a</code> and <code>a1</code> in above code and their use-cases. Why <code>a1</code> is giving error?</p>
","10101450","","-1","","2020-06-20 09:12:55","2020-09-16 16:02:53","what is the difference between a variable and StringVar() of tkinter","<python-3.x><tkinter>","4","0","1","","","CC BY-SA 4.0","0"
"41394749","1","","","2016-12-30 10:35:02","","1","24767","<p>I have a simple for loop to calculate RMS(root mean square) which is defined in sigma summation form:</p>

<pre><code>for i in range(int(N-(n*periyot/delta)), N+1):
    sum = np.sqrt((1 / N) * (sum((Cl[i]**2))))
</code></pre>

<p>Then I got this error:</p>

<pre><code>TypeError: 'numpy.float64' object is not iterable
</code></pre>

<p>Here are some information about my definitons:</p>

<pre><code>N=40000, n=10.0, periyot=6.451290, delta=0.005  

Cl=[-21.91969   -12.452671   -7.928303  ...,  -0.0833991  -0.0579686
  -0.0823822]
</code></pre>
","","user4179448","1478537","","2016-12-30 11:26:20","2018-10-20 16:01:05","For loop and 'numpy.float64' object is not iterable error","<python><python-3.x><numpy><for-loop>","3","8","","","","CC BY-SA 3.0","0"
"28247374","1","28247482","","2015-01-31 01:57:00","","7","24749","<p>I am trying to run web application using mongodb and pymongo to serve data from database.  </p>

<p>The error I am getting is ImportError: No module named parse. Please see below error.log from apache2 web server:</p>

<pre><code>mod_wsgi (pid=18824): Target WSGI script '/var/www/FlaskApp/flaskapp.wsgi' cannot be loaded as Python module.
[:error] [pid 18824:tid 139967053518592] mod_wsgi (pid=18824): Exception occurred processing WSGI script '/var/www/FlaskApp/flaskapp.wsgi'.
[:error] [pid 18824:tid 139967053518592] Traceback (most recent call last):
File ""/var/www/FlaskApp/flaskapp.wsgi"", line 12, in &lt;module&gt;
[:error] [pid 18824:tid 139967053518592]      from ABC import app as application
[:error] [pid 18824:tid 139967053518592]    File ""var/www/FlaskApp/ABC/__init__.py"", line 1, in &lt;module&gt;
[:error] [pid 18824:tid 139967053518592]     from pymongo import MongoClient
[:error] [pid 18824:tid 139967053518592]   File ""/var/www/FlaskApp/ABC/venv/lib/python3.4/site-packages/pymongo/__init__.py"", line 92, in &lt;module&gt;
[:error] [pid 18824:tid 139967053518592]     from pymongo.connection import Connection
[:error] [pid 18824:tid 139967053518592]    File ""/var/www/FlaskApp/ABC/venv/lib/python3.4/site-packages/pymongo/connection.py"", line 39, in &lt;module&gt;
[:error] [pid 18824:tid 139967053518592]      from pymongo.mongo_client import MongoClient
[:error] [pid 18824:tid 139967053518592]    File ""/var/www/FlaskApp/ABC/venv/lib/python3.4/site-packages/pymongo/mongo_client.py"", line 46, in &lt;module&gt;
[:error] [pid 18824:tid 139967053518592]      from pymongo import (auth,
[:error] [pid 18824:tid 139967053518592]    File ""/var/www/FlaskApp/ABC/venv/lib/python3.4/site-packages/pymongo/uri_parser.py"", line 18, in &lt;module&gt;
[:error] [pid 18824:tid 139967053518592]     from urllib.parse import unquote_plus
[:error] [pid 18824:tid 139967053518592]  ImportError: No module named parse
</code></pre>

<p>I have virtual environment for Python 3.4, Flask, and pymongo. I am using mongodb 2.6.7. </p>

<p>Any ideas what causes issue?</p>
","3151858","","","","","2018-03-31 10:04:06","ImportError: No module named parse","<python><mongodb><python-3.x><flask><pymongo>","2","0","2","","","CC BY-SA 3.0","0"
"46000191","1","46000253","","2017-09-01 12:19:53","","7","24740","<p>I am trying to read in a dataset called df1, but it does not work </p>

<pre><code>import pandas as pd
df1=pd.read_csv(""https://raw.githubusercontent.com/tuyenhavan/Statistics/Dataset/World_Life_Expectancy.csv"",sep="";"")

df1.head()
</code></pre>

<p>Here are huge errors from the above code, but this is the most relevant </p>

<pre><code>UnicodeDecodeError: 'utf-8' codec can't decode byte 0x92 in position 18: invalid start byte
</code></pre>
","7265114","","7265114","","2017-09-01 12:30:06","2019-02-26 13:16:32","'utf-8' codec can't decode byte 0x92 in position 18: invalid start byte","<python-3.x><data-import>","2","3","3","","","CC BY-SA 3.0","0"
"28061223","1","28061267","","2015-01-21 06:52:33","","13","24728","<p>I try to find common list of values for three different lists:</p>

<pre><code>a = [1,2,3,4]
b = [2,3,4,5]
c = [3,4,5,6]
</code></pre>

<p>of course naturally I try to use the <code>and</code> operator however that way I just get the value of last <code>list</code> in expression:</p>

<pre><code>&gt;&gt; a and b and c
out: [3,4,5,6]
</code></pre>

<p>Is any short way to find the common values list:</p>

<pre><code>[3,4]
</code></pre>

<p>Br</p>
","1407731","","","","","2018-03-03 19:42:08","Python: how to find common values in three lists","<python><python-2.7><python-3.x>","3","0","5","","","CC BY-SA 3.0","0"
"35974056","1","35974160","","2016-03-13 18:27:58","","11","24706","<p>I'm trying to do something like this:</p>

<pre><code>subs = 'world'
""Hello {[subs]}""
</code></pre>

<p>in Python 3.</p>

<p>I can't quite figure out the syntax (having come from Ruby/PHP). Am I missing something? The % operator in the docs I think is Python 2.x deprecated.</p>
","222151","","","","","2019-06-24 13:32:10","String substitution in Python 3?","<python-3.x>","4","1","5","","","CC BY-SA 3.0","0"
"47688338","1","47688420","","2017-12-07 05:40:51","","15","24679","<p>I would like to concatenate a bytearray to another bytearray. I thought this might work:</p>

<pre><code>byt1 = bytearray(10)
byt2 = bytearray(10)
byt1.join(byt2)
print(repr(byt1))
</code></pre>

<blockquote>
  <p>byt1.join(byt2)</p>
  
  <p>TypeError: sequence item 0: expected a bytes-like object, int found</p>
</blockquote>

<p>What is the most efficient way to achieve this?</p>
","145574","","","","","2020-09-10 18:34:58","python: join two bytearray objects","<python-3.x>","2","4","","","","CC BY-SA 3.0","0"
"59013109","1","59013131","","2019-11-23 23:08:32","","32","24632","<p>I am trying to train the following CNN as follows, but I keep getting the same error regarding .cuda() and I am not sure how to fix it. Here is a chunk of my code so far.</p>

<pre><code>import matplotlib.pyplot as plt
import numpy as np
import torch
from torch import nn
from torch import optim
import torch.nn.functional as F
import torchvision
from torchvision import datasets, transforms, models
from torch.utils.data.sampler import SubsetRandomSampler


data_dir = ""/home/ubuntu/ML2/ExamII/train2/""
valid_size = .2

# Normalize the test and train sets with torchvision
train_transforms = transforms.Compose([transforms.Resize(224),
                                           transforms.ToTensor(),
                                           ])

test_transforms = transforms.Compose([transforms.Resize(224),
                                          transforms.ToTensor(),
                                          ])

# ImageFolder class to load the train and test images
train_data = datasets.ImageFolder(data_dir, transform=train_transforms)
test_data = datasets.ImageFolder(data_dir, transform=test_transforms)


# Number of train images
num_train = len(train_data)
indices = list(range(num_train))
# Split = 20% of train images
split = int(np.floor(valid_size * num_train))
# Shuffle indices of train images
np.random.shuffle(indices)
# Subset indices for test and train
train_idx, test_idx = indices[split:], indices[:split]
# Samples elements randomly from a given list of indices
train_sampler = SubsetRandomSampler(train_idx)
test_sampler = SubsetRandomSampler(test_idx)
# Batch and load the images
trainloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=1)
testloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=1)


#print(trainloader.dataset.classes)

device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
model = models.resnet50(pretrained=True)

model.fc = nn.Sequential(nn.Linear(2048, 512),
                                 nn.ReLU(),
                                 nn.Dropout(0.2),
                                 nn.Linear(512, 10),
                                 nn.LogSigmoid())
                                 # nn.LogSoftmax(dim=1))
# criterion = nn.NLLLoss()
criterion = nn.BCELoss()
optimizer = optim.Adam(model.fc.parameters(), lr=0.003)
model.to(device)

#Train the network
for epoch in range(2):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')
</code></pre>

<p>However, I keep getting this error in the console: </p>

<blockquote>
  <p>RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same`</p>
</blockquote>

<p>Any thoughts on how to fix it? I read that maybe the model hasn't been pushed into my GPU, but not sure how to fix it. Thanks!</p>
","10168730","","10908375","","2020-09-30 18:19:46","2020-10-23 09:38:29","RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same","<python><python-3.x><machine-learning><deep-learning><pytorch>","5","0","4","","","CC BY-SA 4.0","0"
"30377620","1","30379057","","2015-05-21 15:06:56","","5","24603","<h1>QUESTION</h1>

<p>I am returning an ImportError: No module named 'cStringIO'.  Unfortunately cStringIO doesn't exist anymore and I need to use StringIO as a replacement.  How can I do this?</p>

<pre><code>import edgar
import ftplib
from io import StringIO 

ftp = ftplib.FTP(edgar.FTP_ADDR)
ftp.login()
try:
   edgar.download_all(ftp, ""/tmp"")
except Exception as e:
   print(e)
finally:
   ftp.close()
</code></pre>

<h1>OUTPUT</h1>

<pre><code>Traceback (most recent call last):
File ""/usr/local/lib/ana/lib/python3.4/site-         packages/edgar/downloader.py"", line 5, in &lt;module&gt;
from cStringIO import StringIO
ImportError: No module named 'cStringIO'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""/home/aranjohn/PycharmProjects/edgar/secEd.py"", line 1, in     &lt;module&gt;
import edgar
File ""/usr/local/lib/ana/lib/python3.4/site-  packages/edgar/__init__.py"", line 1, in &lt;module&gt;
from .downloader import FTP_ADDR, file_list, download, download_all
File ""/usr/local/lib/ana/lib/python3.4/site-  packages/edgar/downloader.py"", line 7, in &lt;module&gt;
from StringIO import StringIO
ImportError: No module named 'StringIO'

Process finished with exit code 1
</code></pre>
","4857686","","","","","2015-05-21 16:10:02","Python 3.4 : cStringIO vs. StringIO","<python-3.x><import>","1","0","1","","","CC BY-SA 3.0","0"
"40694470","1","40694623","","2016-11-19 15:16:53","","31","24589","<p>I need a faster way to store and access around 3GB of <code>k:v</code> pairs. Where <code>k</code> is a <code>string</code> or an <code>integer</code> and <code>v</code> is an <code>np.array()</code> that can be of different shapes.
Is there any object, that is faster than the standard python dict in storing and accessing such a table? For example, <code>a pandas.DataFrame</code>?</p>

<p>As far I have understood python dict is a quite fast implementation of a hashtable, is there anything better than that for my specific case?</p>
","3190076","","2867928","","2018-03-24 18:00:24","2020-09-09 21:59:20","Is there anything faster than dict()?","<python><python-3.x><numpy><dictionary><python-internals>","4","14","12","","","CC BY-SA 3.0","0"
"51573309","1","51573336","","2018-07-28 16:42:11","","-3","24581","<p>How to print words starting from a particular letter in python without using functions, but using methods or loops.</p>
<p>1 ) I got a string and wants to print words starting with  'm'</p>
<pre><code>St= &quot;where is my mobile&quot;
</code></pre>
<blockquote>
<p>result =  &quot;my&quot;, &quot;mobile&quot;</p>
</blockquote>
<p>2 ) For the below list, how to output list starting with &quot;p&quot;, which  can be either lower or upper.</p>
<pre><code>List = ['mobile',&quot;pencil&quot;,&quot;Pen&quot;,&quot;eraser&quot;,&quot;Book&quot;]
 
 RESULT= &quot;pencil&quot;,&quot;pen&quot;.
</code></pre>
<p>Thanks</p>
<p>Nb: This is not a homework, only a python newbie</p>
","9775079","","13917632","","2020-07-14 07:58:20","2020-07-14 07:58:20","Print words starts from a particular letter in python?","<python><string><python-3.x>","2","3","2","2018-07-29 04:22:04","","CC BY-SA 4.0","0"
"32035823","1","","","2015-08-16 13:53:03","","-2","24541","<p>I have been writing programs in python, but then it comes up with can't assign to literal, What does it mean? and What causes it? I've searched to try and find this but I can't find it.</p>
","5232437","","5232437","","2015-08-17 14:40:36","2015-08-17 14:40:36","what does can't assign to literal mean in python?","<python><python-3.x>","1","1","","2015-08-16 14:27:24","","CC BY-SA 3.0","0"
"31965205","1","31998000","","2015-08-12 12:26:29","","4","24539","<p>I know that there have been multiple posts on numpy and pyserial installation in python on stack overflow, but somehow none of them seem to work for me.</p>

<p>Possibly, i am not able to clearly understand what i have done and am stuck</p>

<p>I will list down the things i did that i think are relevant to solving the question.</p>

<p>A request- Please help me reach a solution before marking my question as a repeat.</p>

<p>Things you need to know- 64bit, Windows10, Python3.4, Python2.6.1, Python 2.7</p>

<p>Things i tried- </p>

<p>1) used the Official git repository for NUMPY -> tried to install it using the command prompt as:-</p>

<p>C:\Desktop\numpy-1.9.2\numpy-1.9.2> python setup.py install</p>

<p>[ At this point Python 3.4 was used because the statement ""python"" simply gave version 3.4 as a response ]</p>

<p>2) then tried using the unofficial site</p>

<p>3) tried using ""pip""</p>

<p>4) tried a .whl file and .tar.gz file</p>

<p>5) Downloaded Visual C++ and tried reinstalling each</p>

<p>6) messed up with environment variables a couple of times</p>

<p>7) for pyserial, i was trying to get it in python 3.4 and i failed to do so consistently</p>

<p>8) uninstalled and installed python 2.7 and 3.4 multiple times while trying out various methods</p>

<p>9) finally i downloaded <strong>""conda""</strong> and tried using that. Set up more environment variables and now it is working in command prompt window but doesnt let me open the IDLE
Can someone suggest a way to understand what i have done and possibly do it in a better way?</p>

<p>EDIT-
Now when i type ""python"", it shows the following:-</p>

<p><em>Python 2.7.9 |Continuum Analytics, Inc.| (default, Dec 18 2014, 16:57:52) [MSC v.1500 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: <a href=""http://continuum.io/thanks"">http://continuum.io/thanks</a> and <a href=""https://binstar.org_"">https://binstar.org</em></a></p>

<p>I can import numpy in command prompt but cant open IDLE</p>
","5115992","","5115992","","2015-08-12 15:23:25","2017-10-18 08:25:00","Python Numpy Installation Windows 10 64-bit","<python-2.7><python-3.x><numpy><windows-10><pyserial>","3","5","3","","","CC BY-SA 3.0","0"
"32448414","1","32448477","","2015-09-08 02:36:27","","21","24511","<p>I came accross the following code:</p>

<pre><code># O(n) space       
def rotate(self, nums, k):
    deque = collections.deque(nums)
    k %= len(nums)
    for _ in xrange(k):
        deque.appendleft(deque.pop())
    nums[:] = list(deque) # &lt;- Code in question
</code></pre>

<p>What does <code>nums[:] =</code> do that <code>nums =</code> does not? For that matter, what does <code>nums[:]</code> do that <code>nums</code> does not?</p>
","1218699","","2202071","","2017-12-06 22:20:27","2020-01-07 06:13:41","What does colon at assignment for list[:] = [...] do in Python","<python><python-3.x>","2","3","16","2015-09-08 03:14:03","","CC BY-SA 3.0","0"
"39899088","1","","","2016-10-06 14:46:28","","5","24510","<p>I am having trouble uploading a CSV file into a table in MS SQL Server, The CSV file has 25 columns and the header has the same name as table in SQL which also has 25 columns. When I run the script it throws an error </p>

<pre><code>params arg (&lt;class 'list'&gt;) can be only a tuple or a dictionary
</code></pre>

<p>What is the best way to import this data into MS SQL? Both the CSV and SQL table have the exact same column names. </p>

<p>Here is the code:</p>

<pre><code>import csv
import pymssql

conn = pymssql.connect(
    server=""xx.xxx.xx.90"",
    port = 2433,
    user='SQLAdmin',
    password='xxxxxxxx',
    database='NasrWeb'
)

cursor = conn.cursor()
customer_data = csv.reader('cleanNVG.csv') #25 columns with same header as SQL

for row in customer_data:
    cursor.execute('INSERT INTO zzzOracle_Extract([Customer Name]\
      ,[Customer #]\
      ,[Account Name]\
      ,[Identifying Address Flag]\
      ,[Address1]\
      ,[Address2]\
      ,[Address3]\
      ,[Address4]\
      ,[City]\
      ,[County]\
      ,[State]\
      ,[Postal Code]\
      ,[Country]\
      ,[Category ]\
      ,[Class]\
      ,[Reference]\
      ,[Party Status]\
      ,[Address Status]\
      ,[Site Status]\
      ,[Ship To or Bill To]\
      ,[Default Warehouse]\
      ,[Default Order Type]\
      ,[Default Shipping Method]\
      ,[Optifacts Customer Number]\
      ,[Salesperson])''VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,)',row)

conn.commit()
cursor.close()
print(""Done"")
conn.close()
</code></pre>

<p>This is what the first rows of the CSV file looks like</p>

<p><a href=""https://i.stack.imgur.com/NA0je.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NA0je.png"" alt=""enter image description here""></a></p>
","5869177","","2144390","","2016-10-06 20:02:53","2019-01-16 21:02:20","Import CSV file into SQL Server using Python","<python><python-3.x><pymssql>","3","3","3","","","CC BY-SA 3.0","0"
"46267705","1","46267969","","2017-09-17 18:51:02","","9","24439","<p>I'm trying to make the status for a test discord bot change between two messages every ten seconds. I need the rest of the script to execute while the status message changes, but an error keeps popping up whenever I try to make it work. There's threading in my script, but I'm not entirely sure how to use it in this circumstance.</p>

<pre><code>@test_bot.event
async def on_ready():
    print('Logged in as')
    print(test_bot.user.name)
    print(test_bot.user.id)
    print('------')
    await change_playing()


@test_bot.event
async def change_playing():
    threading.Timer(10, change_playing).start()
    await test_bot.change_presence(game=discord.Game(name='Currently on ' + str(len(test_bot.servers)) +
                                                          ' servers'))
    threading.Timer(10, change_playing).start()
    await test_bot.change_presence(game=discord.Game(name='Say test.help'))
</code></pre>

<p>The error message reads:</p>

<pre><code>C:\Python\Python36-32\lib\threading.py:1182: RuntimeWarning: coroutine 'change_playing' was never awaited
  self.function(*self.args, **self.kwargs)
</code></pre>
","8606148","","6622587","","2018-11-01 04:48:55","2019-04-30 00:58:57","Making a discord bot change playing status every 10 seconds","<python><python-3.x><python-3.6><discord.py>","3","2","1","","","CC BY-SA 4.0","0"
"55558605","1","55570825","","2019-04-07 11:38:59","","17","24438","<p>I've written a simple web scraper for a comic website. I'm running it on Ubuntu (<code>Linux ubuntu 4.18.0-16-generic #17~18.04.1-Ubuntu</code>) but when I execute the script (permissions set to <code>chmod ug+x</code>) I keep getting a series of errors with imported system libraries along with a confusing syntax error:</p>

<pre><code>import-im6.q16: not authorized `time' @ error/constitute.c/WriteImage/1037.
import-im6.q16: not authorized `os' @ error/constitute.c/WriteImage/1037.
import-im6.q16: not authorized `sys' @ error/constitute.c/WriteImage/1037.
import-im6.q16: not authorized `re' @ error/constitute.c/WriteImage/1037.
import-im6.q16: not authorized `requests' @ error/constitute.c/WriteImage/1037.
from: can't read /var/mail/bs4
./poorlywrittenscraper.py: line 15: DEFAULT_DIR_NAME: command not found
./poorlywrittenscraper.py: line 16: syntax error near unexpected token `('
./poorlywrittenscraper.py: line 16: `COMICS_DIRECTORY = os.path.join(os.getcwd(), DEFAULT_DIR_NAME)'
</code></pre>

<p>Interestingly enough, when I run the same script via <code>python3</code> it fires up, creates the folder, fetches the images but... does not save them. o.O</p>

<p>Any idea what am I missing here or how to fix this?</p>

<p>Here's the full code of the script:</p>

<pre><code>""""""
A simple image downloader for poorlydrawnlines.com/archive
""""""
import time
import os
import sys
import re
import concurrent.futures


import requests
from bs4 import BeautifulSoup as bs


DEFAULT_DIR_NAME = ""poorly_created_folder""
COMICS_DIRECTORY = os.path.join(os.getcwd(), DEFAULT_DIR_NAME)


LOGO = """"""
a Python comic(al) scraper for poorlydwarnlines.com
                         __
.-----.-----.-----.----.|  |.--.--.
|  _  |  _  |  _  |   _||  ||  |  |
|   __|_____|_____|__|  |__||___  |
|__|                        |_____|
                __ __   __
.--.--.--.----.|__|  |_|  |_.-----.-----.
|  |  |  |   _||  |   _|   _|  -__|     |
|________|__|  |__|____|____|_____|__|__|
.-----.----.----.---.-.-----.-----.----.
|__ --|  __|   _|  _  |  _  |  -__|   _|
|_____|____|__| |___._|   __|_____|__|
                      |__|
version: 0.4 | author: baduker | https://github.com/baduker
""""""


ARCHIVE_URL = ""http://www.poorlydrawnlines.com/archive/""
COMIC_PATTERN = re.compile(r'http://www.poorlydrawnlines.com/comic/.+')

def download_comics_menu(comics_found):
    """"""
    Main download menu, takes number of available comics for download
    """"""
    print(""\nThe scraper has found {} comics."".format(len(comics_found)))
    print(""How many comics do you want to download?"")
    print(""Type 0 to exit."")

    while True:
        try:
            comics_to_download = int(input(""&gt;&gt; ""))
        except ValueError:
            print(""Error: expected a number. Try again."")
            continue
        if comics_to_download &gt; len(comics_found) or comics_to_download &lt; 0:
            print(""Error: incorrect number of comics to download. Try again."")
            continue
        elif comics_to_download == 0:
            sys.exit()
        return comics_to_download


def grab_image_src_url(session, url):
    """"""
    Fetches urls with the comic image source
    """"""
    response = session.get(url)
    soup = bs(response.text, 'html.parser')
    for i in soup.find_all('p'):
        for img in i.find_all('img', src=True):
            return img['src']


def download_and_save_comic(session, url):
    """"""
    Downloads and saves the comic image
    """"""
    file_name = url.split('/')[-1]
    with open(os.path.join(COMICS_DIRECTORY, file_name), ""wb"") as file:
        response = session.get(url)
        file.write(response.content)


def fetch_comics_from_archive(session):
    """"""
    Grabs all urls from the poorlydrawnlines.com/archive and parses for only those that link to published comics
    """"""
    response = session.get(ARCHIVE_URL)
    soup = bs(response.text, 'html.parser')
    comics = [url.get(""href"") for url in soup.find_all(""a"")]
    return [url for url in comics if COMIC_PATTERN.match(url)]


def download_comic(session, url):
    """"""
    Download progress information
    """"""
    print(""Downloading: {}"".format(url))
    url = grab_image_src_url(session, url)
    download_and_save_comic(session, url)


def main():
    """"""
    Encapsulates and executes all methods in the main function
    """"""
    print(LOGO)

    session = requests.Session()

    comics = fetch_comics_from_archive(session)
    comics_to_download = download_comics_menu(comics)

    try:
        os.mkdir(DEFAULT_DIR_NAME)
    except OSError as exc:
        sys.exit(""Failed to create directory (error_no {})"".format(exc.error_no))

    start = time.time()
    with concurrent.futures.ThreadPoolExecutor() as executor:
        executor.map(lambda url: download_comic(session, url), comics[:comics_to_download])
    executor.shutdown()
    end = time.time()
    print(""Finished downloading {} comics in {:.2f} sec."".format(comics_to_download, end - start))

if __name__ in ""__main__"":
    main()
</code></pre>
","6106791","","1000551","","2019-04-07 11:46:23","2019-04-08 10:00:42","import-im6.q16: not authorized error 'os' @ error/constitue.c/WriteImage/1037 for a Python web scraper","<python><python-3.x><ubuntu><beautifulsoup>","1","0","2","","","CC BY-SA 4.0","0"
"38141404","1","38141574","","2016-07-01 09:07:49","","6","24428","<p>I have an issue in printing a float number.
I tried:</p>

<pre><code>a = 2
c = 4
print (str(c/a).format(1.6))
</code></pre>

<p>Output:</p>

<pre><code>2.0
</code></pre>

<p>Required Output:</p>

<pre><code>2.000000
</code></pre>

<p>How can I print up to 6 decimal places?</p>
","6517513","","4686625","","2016-07-01 09:09:54","2019-12-24 11:50:37","Printing float up to six decimal places","<python><python-3.x><floating-point>","1","2","2","2016-07-01 09:15:21","","CC BY-SA 3.0","0"
"38738548","1","38738652","","2016-08-03 08:38:41","","20","24403","<p>I am trying to reverse the index given by <code>enumerate</code> whilst retaining the original order of the list being enumerated.</p>

<p>Assume I have the following:</p>

<pre><code>&gt;&gt; range(5)
[0, 1, 2, 3, 4]
</code></pre>

<p>If I enumerate this I would get the following:</p>

<pre><code>&gt;&gt; list(enumerate(range(5)))
[(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]
</code></pre>

<p>However I want to reverse the index provided by enumerate so that I get:</p>

<pre><code>[(4, 0), (3, 1), (2, 2), (1, 3), (0, 4)]
</code></pre>

<p>So far I have the following code:</p>

<pre><code>reversed(list(enumerate(reversed(range(5)))))
</code></pre>

<p>I was just wondering if there was a neater way to do this?</p>
","1792328","","","","","2019-08-23 06:27:26","Python enumerate reverse index only","<python><python-3.x><reverse><enumerate>","10","0","4","","","CC BY-SA 3.0","0"
"46046752","1","46046966","","2017-09-05 03:51:20","","8","24399","<p>I'm implementing this <a href=""https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson5.ipynb"" rel=""noreferrer"">notebook</a> on Windows with Python 3.5.3 and got the follow error on load_vectors() call. I've tried different solutions posted but none worked.</p>

<pre><code>&lt;ipython-input-86-dd4c123b0494&gt; in load_vectors(loc)
      1 def load_vectors(loc):
      2     return (load_array(loc+'.dat'),
----&gt; 3         pickle.load(open(loc+'_words.pkl','rb')),
      4         pickle.load(open(loc+'_idx.pkl','rb')))

UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)
</code></pre>
","1569341","","149069","","2017-09-05 06:44:59","2019-09-18 19:00:49","Python 3 UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 0: ordinal not in range(128)","<python><python-3.x><pickle>","3","0","1","","","CC BY-SA 3.0","0"
"49510049","1","49510257","","2018-03-27 10:07:25","","11","24336","<p>I have this JSON file, <code>currencies.json</code>:</p>

<pre><code>{
    ""AUD"": 1.5978,
    ""BGN"": 1.9558,
    ""BRL"": 4.0726,
    ""CAD"": 1.5868,
    ""CHF"": 1.1703,
    ""CNY"": 7.7975,
    ""CZK"": 25.405,
    ""DKK"": 7.4478,
    ""GBP"": 0.87285,
    ""HKD"": 9.6889,
    ""HRK"": 7.4398,
    ""HUF"": 312.9,
    ""IDR"": 16993.0,
    ""ILS"": 4.2984,
    ""INR"": 80.255,
    ""ISK"": 122.1,
    ""JPY"": 129.74,
    ""KRW"": 1330.3,
    ""MXN"": 22.88,
    ""MYR"": 4.8365,
    ""NOK"": 9.5715,
    ""NZD"": 1.7024,
    ""PHP"": 64.64,
    ""PLN"": 4.2262,
    ""RON"": 4.663,
    ""RUB"": 70.539,
    ""SEK"": 10.194,
    ""SGD"": 1.6216,
    ""THB"": 38.495,
    ""TRY"": 4.888,
    ""USD"": 1.2346,
    ""ZAR"": 14.52
}
</code></pre>

<p>And this connection in Python:</p>

<pre><code>from pymongo import MongoClient
client = MongoClient('localhost', 27017)
db = client['countries_db']
collection_currency = db['currency']
</code></pre>

<p>My db name is <code>countries_db</code> with the <code>currency</code> collection.
Is there a way to import the file to the db using <code>python</code>?<br>
Thanks for your help.</p>
","9216678","","1717069","","2020-06-10 09:58:50","2020-06-10 09:58:50","How to import JSON file to MongoDB using Python","<python><json><python-3.x><mongodb>","1","0","3","","","CC BY-SA 4.0","0"
"43752560","1","","","2017-05-03 06:24:23","","9","24300","<p>Quite new to Python. I would like to install multiprocessing module of python. I am using python 3.6 and pip version 9.1. </p>

<p>I am getting an error which lead me to believe that since there isn't a multiprocessing module compatible with python 3 the below error can happen.</p>

<pre><code>$ pip3 install multiprocessing
Collecting multiprocessing
  Using cached multiprocessing-2.6.2.1.tar.gz
    Complete output from command python setup.py egg_info:
    Traceback (most recent call last):
      File ""&lt;string&gt;"", line 1, in &lt;module&gt;
      File ""/private/var/folders/8m/2fkldrg12lg0qzlhpm8yvyq00000gn/T/pip-build-dqdczlx9/multiprocessing/setup.py"", line 94
</code></pre>

<p>So, i installed the module using pip install multiprocessing which installed the module. I have written a lot of code in python 3 so i would like to use it and i am  using pycharm editor which i have configured to use python3. Now if i am executing the code in the editor it throws error like </p>

<pre><code>/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6 /Users/kkk/Desktop/testing/multiprocessing.py
Traceback (most recent call last):
  File ""/Users/testing/multiprocessing.py"", line 11, in &lt;module&gt;
    p = multiprocessing.Process(target=worker)
AttributeError: module 'multiprocessing' has no attribute 'Process'

Process finished with exit code 1
</code></pre>

<p>for the code</p>

<pre><code>    import multiprocessing

def worker():
    """"""worker function""""""
    print ('Worker')
    return

if __name__ == '__main__':
    jobs = []
    for i in range(5):
        p = multiprocessing.Process(target=worker)
        jobs.append(p)
        p.start()
</code></pre>

<p>What can i do to resolve this?</p>

<p>Thanks</p>
","2605718","","","","","2019-12-27 00:50:06","Install Multiprocessing python3","<python><python-3.x><multiprocessing>","3","4","1","","","CC BY-SA 3.0","0"
"58423367","1","58423646","","2019-10-17 00:40:21","","1","24290","<p>I am revisiting the python language and experiencing difficulty setting up my environment. </p>

<p>I am using 
- Mac Mojave (10.14)
- python 2.7.10 (packaged with the system)
- python 3.7.4 (installed using homebrew)
- homebrew 2.1.14
- pip 19.2.3</p>

<p>I encounter an error message when attempting to install watchdog via pip. I believe the error is caused by pip attempting to install in python 2.7 folders (without sufficient permissions) instead of the python 3 folder</p>

<p>I have tried uninstalling, reinstalling and upgrading python 3</p>

<p>I encounter the following error message when attempting to install watchdog via pip </p>

<pre><code> 1 error generated.
    Error compiling module, falling back to pure Python
    running install_lib
    creating /Library/Python/2.7/site-packages/yaml
    error: could not create '/Library/Python/2.7/site-packages/yaml': Permission denied
    ----------------------------------------
ERROR: Command errored out with exit status 1: /usr/bin/python -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/private/var/folders/4d/spq3r5t92654252ql994_l540000gr/T/pip-install-nqmq6O/PyYAML/setup.py'""'""'; __file__='""'""'/private/var/folders/4d/spq3r5t92654252ql994_l540000gr/T/pip-install-nqmq6O/PyYAML/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /private/var/folders/4d/spq3r5t92654252ql994_l540000gr/T/pip-record-g8Qjzh/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.
</code></pre>
","12213302","","","","","2020-09-20 19:44:35","How to fix ‘“ERROR: Command errored out with exit status 1:” when trying to install watchdog using pip","<python><python-3.x><pip><python-watchdog>","2","2","","","","CC BY-SA 4.0","0"
"41212273","1","41213232","","2016-12-18 19:55:10","","11","24288","<p>I want to fill empty cells with with previous row value if they start with number. For example, I have </p>

<pre><code>    Text    Text    
    30      Text    Text    
            Text    Text    
            Text    Text    
    31      Text    Text
    Text    Text    
    31      Text    Text    
            Text    Text    
            Text    Text    
    32      Text    Text
    Text    Text    
            Text    Text    
            Text    Text    
            Text    Text    
            Text    Text
</code></pre>

<p>I however, want to have </p>

<pre><code>Text    Text    
30      Text    Text    
30      Text    Text    
30      Text    Text    
31      Text    Text
Text    Text    
31      Text    Text    
31      Text    Text    
31      Text    Text    
32      Text    Text
Text    Text    
        Text    Text    
        Text    Text    
        Text    Text    
        Text    Text
</code></pre>

<p>I tried to reach this by using this code:</p>

<pre><code>data = pd.read_csv('DATA.csv',sep='\t', dtype=object, error_bad_lines=False)
data = data.fillna(method='ffill', inplace=True)
print(data)
</code></pre>

<p>but it did not work. </p>

<p>Is there anyway to do this?</p>
","6437260","","","","","2016-12-18 21:47:26","Pandas(Python) : Fill empty cells with with previous row value?","<python><python-3.x><pandas>","2","2","6","","","CC BY-SA 3.0","1"
"53975234","1","","","2018-12-30 04:15:30","","26","24276","<p>I'm currently trying to implement steam login into website. But I'm unable to get pass this error within the code. I've created the database object but it keeps showing the error I mentioned earlier. I'm not sure whether SQLAlchemy has changed or what since I used it.</p>
<pre><code>from flask import Flask
from flask_sqlalchemy import SQLAlchemy

app = Flask(__name__)
db = SQLAlchemy(app)

class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
</code></pre>
<p>The error given by <code>pylint</code> is</p>
<pre><code>E1101: Instance of 'SQLAlchemy' has no 'Column' member (no-member)
</code></pre>
","9176166","","3015186","","2020-07-26 20:09:11","2020-08-13 08:42:41","Instance of 'SQLAlchemy' has no 'Column' member (no-member)","<python><python-3.x><flask><sqlalchemy>","6","2","14","","","CC BY-SA 4.0","0"
"32491161","1","32491264","","2015-09-10 00:27:26","","12","24273","<p>I am attempting to install Python 3 from within Windows XP Professional; however I receive the following screen (there is no install button):</p>

<p><a href=""https://i.stack.imgur.com/h2yKR.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/h2yKR.png"" alt=""python install""></a></p>

<p>In my opinion this is obviously an unreported error, or a rendering problem. How is this problem alleviated? If this problem can be overcome, is it a sign of additional related issues?</p>
","397281","","208880","","2015-12-15 12:28:36","2020-04-02 21:34:35","Unable to install Python 3.5 within Windows XP Professional","<python><windows><python-3.x><windows-xp><python-3.5>","7","6","2","","","CC BY-SA 3.0","0"
"44858120","1","","","2017-07-01 06:43:15","","2","24228","<p>This code is working but I'm only able to switch between the voices which came preInstalled in Microsoft Windows. These voices are ""Microsoft David Mobile"" and ""Microsoft Zira Mobile"".  </p>

<p>Later I installed ""Microsoft Kalpana Mobile"" and set it as the default Windows voice. But still I'm not able to switch to ""Microsoft Kalpana Mobile"". The code is-  </p>

<pre><code>import pyttsx3
engine = pyttsx3.init()
voices = engine.getProperty('voices')
engine.setProperty('voice', voices[0].id) #changing index changes voices but ony 0 and 1 are working here
engine.say('Hello World')
engine.runAndWait()
</code></pre>

<p>Only 0 and 1 are working as indices inside <em>voices[]</em>.</p>

<p>I want the ""Microsoft Kalpana Mobile"" to speak. I'm working on this project for past 2 months. If this doesn't work, all my efforts will go in vein. Please Help:(</p>

<p>Thanks in advance.</p>
","7337965","","","","","2020-07-23 06:23:53","How to change the voice in pyttsx3?","<python><python-3.x><text-to-speech><speech-synthesis><pyttsx>","2","0","4","","","CC BY-SA 3.0","0"
"28694380","1","28694569","","2015-02-24 11:17:34","","24","24227","<p>After my <a href=""https://codereview.stackexchange.com/questions/61798/mysql-class-to-add-user-database"">first CodeReview Q</a> - I got tip in answer:</p>

<blockquote>
  <p>Your code appears to be for Python 2.x. To be a bit more ready for a possible future migration to Python 3.x, I recommend to start writing your print ... statements as print(...)</p>
</blockquote>

<p>Thus, in my following code (I'm using Python 2.6 and 2.7 on my boxes) I always us <code>()</code> for <code>print</code>:</p>

<pre><code>print('Hello')
</code></pre>

<p>Today I first time test my code with PyLint, and it says:</p>

<blockquote>
  <p>C: 43, 0: Unnecessary parens after 'print' keyword (superfluous-parens)</p>
</blockquote>

<p>Which explained <a href=""http://pylint-messages.wikidot.com/messages:c0325"" rel=""noreferrer"">here</a>.</p>

<p>So - does <code>print(str)</code> is really incorrect, or I can disregard this PyLint messages?</p>
","2720802","","-1","","2017-04-13 12:40:33","2015-02-24 11:31:46","pylint says ""Unnecessary parens after %r keyword""","<python><python-2.7><python-3.x>","2","0","4","","","CC BY-SA 3.0","0"
"36730372","1","36730457","","2016-04-19 22:08:54","","7","24222","<p>I am very new to web-scraping with Python, and I am really having a hard time with extracting nested text from within HTML (<code>p</code> within <code>div</code>, to be exact). Here is what I got so far:</p>

<pre><code>from bs4 import BeautifulSoup
import urllib

url = urllib.urlopen('http://meinparlament.diepresse.com/')
content = url.read()
soup = BeautifulSoup(content, 'lxml')
</code></pre>

<p>This works fine:</p>

<pre><code>links=soup.findAll('a',{'title':'zur Antwort'})
for link in links:
    print(link['href'])
</code></pre>

<p>This extraction works fine:</p>

<pre><code>table = soup.findAll('div',attrs={""class"":""content-question""})
for x in table:
    print(x)
</code></pre>

<p>This is the output:</p>

<pre><code>&lt;div class=""content-question""&gt;
&lt;p&gt;[...] Die Verhandlungen über die mögliche Visabefreiung für    
türkische Staatsbürger per Ende Ju...
&lt;a href=""http://meinparlament.diepresse.com/frage/10144/"" title=""zur 
Antwort""&gt;mehr »&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
</code></pre>

<p>Now, I want to extract the text within <code>p</code> and <code>/p</code>. This is the code I use:</p>

<pre><code>table = soup.findAll('div',attrs={""class"":""content-question""})
for x in table:
    print(x['p'])
</code></pre>

<p>However, Python raises a <code>KeyError</code>.</p>
","4587552","","5299236","","2016-04-19 23:47:42","2017-04-18 21:27:47","Extract the text from `p` within `div` with BeautifulSoup","<python><python-3.x><web-scraping><beautifulsoup>","1","0","2","","","CC BY-SA 3.0","0"
"34618149","1","34627164","","2016-01-05 17:50:59","","8","24207","<p>I'm trying to post a snippet of text containing fancy unicode symbols to a web service using the <a href=""http://docs.python-requests.org/en/latest/"" rel=""noreferrer"">requests</a> library. I'm using Python 3.5.</p>

<pre><code>text = ""Två dagar kvar🎉🎉""
r = requests.post(""http://json-tagger.herokuapp.com/tag"", data=text)
print(r.json()
</code></pre>

<p>I get an UnicodeEncodeError, but I can't figure out what I'm doing wrong on my side, the docs for requests only talk about unicode in GET requests from what I see.</p>

<pre><code>    UnicodeEncodeError                        Traceback (most recent call last)
&lt;ipython-input-125-3ebcae3d7918&gt; in &lt;module&gt;()
     19         print(""cleaned : "" + line)
     20 
---&gt; 21         r = requests.post(""http://json-tagger.herokuapp.com/tag"", data=line)
     22         sentences = r.json()['sentences']
     23         for sentence in sentences:

//anaconda/lib/python3.4/site-packages/requests/api.py in post(url, data, json, **kwargs)
    105     """"""
    106 
--&gt; 107     return request('post', url, data=data, json=json, **kwargs)
    108 
    109 

//anaconda/lib/python3.4/site-packages/requests/api.py in request(method, url, **kwargs)
     51     # cases, and look like a memory leak in others.
     52     with sessions.Session() as session:
---&gt; 53         return session.request(method=method, url=url, **kwargs)
     54 
     55 

//anaconda/lib/python3.4/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth,     timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    466         }
    467         send_kwargs.update(settings)
--&gt; 468         resp = self.send(prep, **send_kwargs)
    469 
    470         return resp

//anaconda/lib/python3.4/site-packages/requests/sessions.py in send(self, request, **kwargs)
    574 
    575         # Send the request
--&gt; 576         r = adapter.send(request, **kwargs)
    577 
    578         # Total elapsed time of the request (approximately)

//anaconda/lib/python3.4/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    374                     decode_content=False,
    375                     retries=self.max_retries,
--&gt; 376                     timeout=timeout
    377                 )
    378 

//anaconda/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries,     redirect, assert_same_host, timeout, pool_timeout, release_conn, **response_kw)
    557             httplib_response = self._make_request(conn, method, url,
    558                                                   timeout=timeout_obj,
--&gt; 559                                                   body=body, headers=headers)
    560 
    561             # If we're going to release the connection in ``finally:``, then

//anaconda/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout,     **httplib_request_kw)
    351         # conn.request() calls httplib.*.request, not the method in
    352         # urllib3.request. It also calls makefile (recv) on the socket.
--&gt; 353         conn.request(method, url, **httplib_request_kw)
    354 
    355         # Reset the timeout for the recv() on the socket

//anaconda/lib/python3.4/http/client.py in request(self, method, url, body, headers)
   1086     def request(self, method, url, body=None, headers={}):
   1087         """"""Send a complete request to the server.""""""
-&gt; 1088         self._send_request(method, url, body, headers)
   1089 
   1090     def _set_content_length(self, body):

//anaconda/lib/python3.4/http/client.py in _send_request(self, method, url, body, headers)
   1123             # RFC 2616 Section 3.7.1 says that text default has a
   1124             # default charset of iso-8859-1.
-&gt; 1125             body = body.encode('iso-8859-1')
   1126         self.endheaders(body)
   1127 

UnicodeEncodeError: 'latin-1' codec can't encode characters in position 14-15: ordinal not in range(256)
</code></pre>

<p><strong>WORKAROUND:</strong> I remove all unicode characters from the text from the ""emoticon"" block, U+1F600 - U+1F64F and Symbols And Pictographs"" block, U+1F300 - U+1F5FF according to <a href=""https://stackoverflow.com/questions/12013341/removing-characters-of-a-specific-unicode-range-from-a-string"">this</a> answer with the following code, since I don't need emoticons and pictures for the analysis:</p>

<pre><code>text = re.sub(r'[^\u1F600-\u1F64F ]|[^\u1F300-\u1F5FF ]',"""",text)
</code></pre>

<p><strong>UPDATE</strong> The creator of the web service has fixed this now and updated the documentation. All you have to do is to send an encoded string, in Python 3:</p>

<pre><code>""""Två dagar kvar🎉🎉"".encode(""utf-8"")
</code></pre>
","1842091","","-1","","2017-05-23 12:33:57","2016-01-07 11:32:06","Post unicode string to web service using Python Requests library","<python-3.x><post><unicode><utf-8><httprequest>","1","1","1","","","CC BY-SA 3.0","0"
"28431350","1","","","2015-02-10 12:11:09","","27","24179","<p>I'm searching for the most appropriate tool for python3.x on Windows to create a Bayesian Network, learn its parameters from data and perform the inference.</p>

<p>The network structure I want to define myself as follows:
<img src=""https://i.stack.imgur.com/iscKX.jpg"" alt=""enter image description here""></p>

<p>It is taken from <a href=""http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6697180&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6697180"" rel=""noreferrer"">this</a> paper. </p>

<p>All the variables are discrete (and can take only 2 possible states) except ""Size"" and ""GraspPose"", which are continuous and should be modeled as Mixture of Gaussians.</p>

<p>Authors use <em>Expectation-Maximization algorithm</em> to learn the parameters for conditional probability tables and <em>Junction-Tree algorithm</em> to compute the exact inference.</p>

<p>As I understand all is realised in MatLab with Bayes Net Toolbox by Murphy.</p>

<p>I tried to search something similar in python and here are my results:</p>

<ol>
<li>Python Bayesian Network Toolbox <a href=""http://sourceforge.net/projects/pbnt.berlios/"" rel=""noreferrer"">http://sourceforge.net/projects/pbnt.berlios/</a> (<a href=""http://pbnt.berlios.de/"" rel=""noreferrer"">http://pbnt.berlios.de/</a>). Web-site doesn't work, project doesn't seem to be supported. </li>
<li>BayesPy <a href=""https://github.com/bayespy/bayespy"" rel=""noreferrer"">https://github.com/bayespy/bayespy</a>
I think this is what I actually need, but I fail to find some examples similar to my case, to understand how to approach construction of the network structure.</li>
<li><p>PyMC seems to be a powerful module, but I have problems with importing it on Windows 64, python 3.3. I get error when I install development version </p>

<p>WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.</p></li>
</ol>

<p>UPDATE:</p>

<ol start=""4"">
<li>libpgm (<a href=""http://pythonhosted.org/libpgm/"" rel=""noreferrer"">http://pythonhosted.org/libpgm/</a>). Exactly what I need, unfortunately not supported by python 3.x</li>
<li>Very interesting actively developing library: PGMPY. Unfortunately continuous variables and learning from data is not supported yet. <a href=""https://github.com/pgmpy/pgmpy/"" rel=""noreferrer"">https://github.com/pgmpy/pgmpy/</a> </li>
</ol>

<p>Any advices and concrete examples will be highly appreciated.</p>
","4202221","","4202221","","2015-02-11 11:56:36","2020-08-15 21:06:57","Create Bayesian Network and learn parameters with Python3.x","<python-3.x><machine-learning><scikit-learn><probability><bayesian-networks>","5","4","18","2020-08-15 22:43:04","","CC BY-SA 3.0","0"
"43123408","1","43123579","","2017-03-30 16:25:20","","44","24132","<p>I'm using the <code>.format()</code> a lot in my Python 3.5 projects, but I'm afraid that it will be deprecated during the next Python versions because of f-strings, the new kind of string literal.</p>
<pre><code>&gt;&gt;&gt; name = &quot;Test&quot;
&gt;&gt;&gt; f&quot;My app name is {name}.&quot;
'My app name is Test.'
</code></pre>
<p>Does the formatted string feature come to fully replace the old <code>.format()</code>? And from now on, would it be better to use the new style in all cases?</p>
<p>I understand that it's based on the idea that &quot;Simple is better than complex.&quot; However, what about performance issues; is there any difference between them? Or is it just a simple look of the same feature?</p>
","6550767","","4518341","","2020-07-19 21:34:22","2020-07-19 21:50:15","f-strings vs str.format()","<python><python-3.x><string-formatting><string-interpolation><f-string>","4","5","9","","","CC BY-SA 4.0","0"
"45252305","1","45252415","","2017-07-22 08:17:16","","10","24117","<p>My original code is this.</p>

<pre><code>#py3.6, windows10   
import time
from selenium import webdriver
import codecs
import sys

reload(sys)
sys.setdefaultencoding('utf-8')
</code></pre>

<p>Reload is not supported. It was fixed.</p>

<pre><code>Import importlib
Importlib.reload (sys)
</code></pre>

<p>But there was also an error.</p>

<blockquote>
  <p>AttributeError: module 'sys' has no attribute 'setdefaultencoding'</p>
</blockquote>

<p>How should I fix this? I would appreciate your help.</p>

<p>I also attach my entire code.</p>

<pre><code>import time
from selenium import webdriver
import codecs
import sys

reload(sys)
sys.setdefaultencoding('utf-8')

browser = webdriver.PhantomJS('C:\phantomjs-2.1.1-windows/bin/phantomjs')
url = u'https://twitter.com/search?f=tweets&amp;vertical=default&amp;q=%EB%B0%B0%EA%B3%A0%ED%8C%8C%20since%3A2017-07-19%20until%3A2017-07-20&amp;l=ko&amp;src=typd&amp;lang=ko'

browser.get(url)
time.sleep(1)

body = browser.find_element_by_tag_name('body')
browser.execute_script(""window.scrollTo(0,document.body.scrollHeight);"")

start = time.time()
for _ in range(500):
    now = time.time()
    browser.execute_script(""window.scrollTo(0, 
    document.body.scrollHeight);"")
    print str(_) + ""    seconds: "" + str(now - start) 
    time.sleep(0.2)

tweets=browser.find_elements_by_class_name('tweet-text')

with codecs.open(""test.txt"", ""w"",""utf-8"") as f:
    i = 1
    for i, tweet in enumerate(tweets):
        data = tweet.text
        data = data.encode('utf-8')
        print i, "":"", data
        msg = (str(data) +'\n')
        f.write(msg)
        i += 1

end = time.time()
print(end - start)
browser.quit()
</code></pre>
","8313366","","436560","","2017-07-22 12:00:21","2018-03-16 07:21:57","AttributeError: module 'sys' has no attribute 'setdefaultencoding'","<python><python-3.x><encoding><web-scraping>","1","3","","","","CC BY-SA 3.0","0"
"37088512","1","37088604","","2016-05-07 12:25:01","","9","24114","<p>I'm new to Python and I'm wanting to print only the first 10 lines of a huge csv file. </p>

<p>Here's my code so far that prints all of the lines in the csv file</p>

<pre><code>import csv
with open('titanic.csv') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        print(row['survived'], row['pclass'], row['name'], row['sex'], row['age'])
</code></pre>
","6303866","","1252759","","2016-05-07 13:10:58","2019-08-04 05:54:45","How do I print only the first 10 lines from a csv file using Python?","<python><python-3.x><csv>","4","0","4","","","CC BY-SA 3.0","0"
"39155953","1","39156006","","2016-08-25 22:54:27","","36","24093","<p>I've installed jupyter on local network LAN but im unable to access <code>http://&lt;IP&gt;:8888</code> from another macine on LAN. I've opened ports 8888 and port range 49152 to 65535 with iptables (this range is specified at <a href=""http://jupyter-notebook.readthedocs.io/en/latest/public_server.html"" rel=""noreferrer"">http://jupyter-notebook.readthedocs.io/en/latest/public_server.html</a>) </p>

<p>This guide <a href=""http://jupyter-notebook.readthedocs.io/en/latest/public_server.html"" rel=""noreferrer"">http://jupyter-notebook.readthedocs.io/en/latest/public_server.html</a> describes exposing a notebook publicly but I'm just attempting to share over LAN.</p>

<p>Have I missed a step ? </p>
","470184","","470184","","2016-08-25 23:04:42","2020-02-29 13:29:35","Exposing python jupyter on LAN","<python><linux><python-3.x><ubuntu-14.04><jupyter>","3","0","12","","","CC BY-SA 3.0","0"
"34603628","1","","","2016-01-05 03:14:34","","44","24075","<p>I am trying to do a &quot;Hello World&quot; program in Cython, following this tutorial <a href=""http://docs.cython.org/src/tutorial/cython_tutorial.html#cython-hello-world"" rel=""nofollow noreferrer"">http://docs.cython.org/src/tutorial/cython_tutorial.html#cython-hello-world</a></p>
<p>I created <code>helloworld.pyx</code></p>
<pre><code>print(&quot;Hello World&quot;)
</code></pre>
<p>and <code>setup.py</code>:</p>
<pre><code>from distutils.core import setup
from Cython.Build import cythonize

setup(
    ext_modules = cythonize(&quot;helloworld.pyx&quot;)
)
</code></pre>
<p>How can I change setup.py to specify that my source is Python 3, rather than Python 2 like in the tutorial? If I invoke &quot;cython&quot; command from the command line, it accepts <code>-3</code> option. But if I compile with <code>python setup.py build_ext --inplace</code> like shown in the tutorial, how do I specify Python 3 source? It may not matter much for a Hello World program, but will matter as I start using Cython for real projects.</p>
","5742060","","5446749","","2020-10-28 20:20:23","2020-10-28 20:20:23","How to specify Python 3 source in Cython's setup.py?","<python><python-3.x><cython>","3","0","8","","","CC BY-SA 4.0","0"
"51235428","1","","","2018-07-08 19:40:38","","21","24075","<p>I'm hesitating whether to downgrade to Python 3.6 or install a new version of TensorFlow. </p>

<p>Does TensorFlow 1.9 support Python 3.7?</p>
","10050507","","3924118","","2018-07-08 20:34:01","2019-05-02 19:14:09","Does TensorFlow 1.9 support Python 3.7","<python-3.x><tensorflow>","10","3","4","","","CC BY-SA 4.0","0"
"40226729","1","40226757","","2016-10-24 20:11:36","","1","23977","<p>How would I go about defining a fuctions in counting the number of rows and columns in a list of lists? For example group1 would be 1 row with 6 columns.</p>

<pre><code>group1  =   [['.',  'A',    'A',    '.',    '.',    '.']]

def num_rows(group):

def num_columns(group):
</code></pre>
","6927980","","6927980","","2016-10-24 20:16:40","2018-01-14 04:52:20","Count rows and columns","<python><python-3.x>","3","2","1","","","CC BY-SA 3.0","0"
"32054066","1","32056300","","2015-08-17 15:21:54","","29","23935","<p>I'm using the <a href=""https://github.com/aaugustin/websockets""><code>websockets</code></a> library to create a websocket server in Python 3.4. Here's a simple echo server:</p>

<pre class=""lang-python prettyprint-override""><code>import asyncio
import websockets

@asyncio.coroutine
def connection_handler(websocket, path):
    while True:
        msg = yield from websocket.recv()
        if msg is None:  # connection lost
            break
        yield from websocket.send(msg)

start_server = websockets.serve(connection_handler, 'localhost', 8000)
asyncio.get_event_loop().run_until_complete(start_server)
asyncio.get_event_loop().run_forever()
</code></pre>

<p>Let's say we – additionally – wanted to send a message to the client whenever some event happens. For simplicity, let's send a message periodically every 60 seconds. How would we do that? I mean, because <code>connection_handler</code> is constantly waiting for incoming messages, the server can only take action <em>after</em> it has received a message from the client, right? What am I missing here?</p>

<p>Maybe this scenario requires a framework based on events/callbacks rather than one based on coroutines? <a href=""http://www.tornadoweb.org/en/stable/websocket.html"">Tornado</a>?</p>
","1668646","","1668646","","2015-08-18 07:32:04","2019-09-10 19:42:27","Python - how to run multiple coroutines concurrently using asyncio?","<python><python-3.x><websocket><python-asyncio>","3","0","9","","","CC BY-SA 3.0","0"
"28795859","1","","","2015-03-01 16:17:47","","4","23902","<p>I am building a program for Windows PCs that contains a lot of buttons and seems very plain. So I was wondering, can I make it so when you push a button (using tkinter), can I play a sound to liven up the program a bit? Please keep in mind I am learning so please dumb it down a bit. </p>
","4375841","","355230","","2018-04-01 18:46:55","2020-05-31 11:07:16","How can I play a sound when a tkinter button is pushed?","<windows><python-3.x><button><audio><tkinter>","3","1","0","","","CC BY-SA 3.0","0"
"37526026","1","40303128","","2016-05-30 12:45:02","","18","23898","<p><code>python3</code> is my local Anaconda version of python, while <code>python3.4</code> is the system one. I can import <code>gi</code> module with <code>python3.4</code> (probably because i installed it with <code>sudo apt-get install python3-gi</code>) but <code>python3</code> doesn't see it:</p>

<pre><code>$ python3 -c 'import gi'
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 1, in &lt;module&gt;
ImportError: No module named 'gi'
$ python3.4 -c 'import gi'                                       

$ python3 --version
Python 3.5.1 :: Anaconda 4.0.0 (64-bit)
$ python3.4 --version
Python 3.4.3
$ which python3
/home/kossak/anaconda3/bin/python3
$ which python3.4
/usr/bin/python3.4
$
</code></pre>

<p>How should i install <code>gi</code> for Anaconda python? Or maybe i can somehow import sysem-wide modules?</p>

<p>My os:</p>

<pre><code>System:    Kernel: 3.19.0-32-generic x86_64 (64 bit gcc: 4.8.2) Desktop: Cinnamon 2.8.8 (Gtk 2.24.23) dm: mdm
           Distro: Linux Mint 17.3 Rosa
</code></pre>
","3024945","","3650983","","2019-05-20 20:04:24","2019-05-20 20:04:24","How to install gi module for anaconda python3?","<python><python-3.x><anaconda><conda>","4","1","8","","","CC BY-SA 3.0","0"
"58688481","1","58688482","","2019-11-04 06:52:44","","18","23888","<p>Now, the official TensorFlow on Anaconda is 2.0. My question is how to force Anaconda to install an earlier version of TensorFlow instead. So, for example, I would like Anaconda to install TensorFlow 1.14 as plenty of my projects are depending on this version.</p>
","5612363","","","","","2019-11-04 06:52:44","Force Anaconda to install tensorflow 1.14","<python><python-3.x><tensorflow><anaconda><version>","1","0","7","","","CC BY-SA 4.0","0"
"31420351","1","31420723","","2015-07-15 02:10:26","","-1","23882","<p>I am newbie to Python.
Basically, I want to write a program to read column <code>D</code> &amp; <code>E</code> from an excel file, and calculate the total <code>Incoming</code> and <code>Outgoing</code> duration.</p>

<p>Which Python module is used to read excel files and how to process data inside it?</p>

<p><strong>Excel file:</strong></p>

<pre><code>D            E
Incoming    18
Outgoing    99
Incoming    20
Outgoing    59
Incoming    30
Incoming    40
</code></pre>
","5117515","","4895040","","2015-07-15 05:01:07","2018-03-01 08:29:41","How to read an excel file in Python?","<python><python-3.x>","3","2","","","","CC BY-SA 3.0","0"
"38856180","1","38856535","","2016-08-09 16:36:22","","7","23864","<p>I mean, i want to replace <code>str[9:11]</code> for another string.
If I do <code>str.replace(str[9:11], ""###"")</code> It doesn't work, because the sequence [9:11] can be more than one time.
If str is <code>""cdabcjkewabcef""</code> i would get <code>""cd###jkew###ef""</code> but I only want to replace the second. </p>
","4505998","","3059812","","2016-08-09 16:40:44","2020-05-02 17:51:48","Python: String replace index","<python><string><python-3.x>","6","1","1","","","CC BY-SA 3.0","0"
"39998330","1","","","2016-10-12 12:13:31","","4","23861","<p>I had a trouble when i use selenium to control my Chrome.
Here is my code:</p>

<pre><code>from selenium import webdriver
driver = webdriver.Chrome()
</code></pre>

<p>When i tried to operate it ,it runs successfully at first,the Chrome pop on the screen. However, it shut down at the few seconds.
<a href=""https://i.stack.imgur.com/EbQE7.jpg"" rel=""nofollow""><img src=""https://i.stack.imgur.com/EbQE7.jpg"" alt=""Here is the Traceback information""></a></p>

<pre><code>Traceback (most recent call last):
  File ""&lt;pyshell#3&gt;"", line 1, in &lt;module&gt;
    driver = webdriver.Chrome('C:\Program Files (x86)\Google\Chrome\chrome.exe')
  File ""C:\Users\35273\AppData\Local\Programs\Python\Python35\lib\site-packages\selenium\webdriver\chrome\webdriver.py"", line 62, in __init__
    self.service.start()
  File ""C:\Users\35273\AppData\Local\Programs\Python\Python35\lib\site-packages\selenium\webdriver\common\service.py"", line 86, in start
    self.assert_process_still_running()
  File ""C:\Users\35273\AppData\Local\Programs\Python\Python35\lib\site-packages\selenium\webdriver\common\service.py"", line 99, in assert_process_still_running
    % (self.path, return_code)
selenium.common.exceptions.WebDriverException: Message: Service C:\Program Files (x86)\Google\Chrome\chrome.exe unexpectedly exited. Status code was: 0
</code></pre>
","7007522","","4549554","","2016-10-12 13:02:11","2020-05-19 08:12:36","selenium.common.exceptions.WebDriverException: Message: Service","<google-chrome><python-3.x><selenium><selenium-webdriver><web-crawler>","2","0","","","","CC BY-SA 3.0","0"
"49944871","1","49944909","","2018-04-20 15:17:45","","23","23823","<p>How can I deactivate my pipenv environment? </p>

<p>With other tools I've been able to do something like <code>source deactivate</code>, but that has no affect here.</p>

<p>Create an environment:</p>

<pre><code>pipenv --three
</code></pre>

<p>Activate the environment:</p>

<pre><code>source $(pipenv --venv)/bin/activate
</code></pre>

<p>But how to deactivate?</p>
","4909076","","","","","2020-02-28 00:52:24","Deactivate a pipenv environment","<python-3.x><pipenv><virtual-environment>","5","1","7","","","CC BY-SA 3.0","0"
"39929258","1","39932885","","2016-10-08 06:07:46","","11","23785","<blockquote>
  <p>psycopg2.OperationalError: could not connect to server: Connection refused</p>
</blockquote>

<p>Is the server running on host ""45.32.1XX.2XX"" and accepting TCP/IP connections on port 5432?</p>

<p>Here,I've open my sockets.</p>

<pre><code>tcp        0      0 127.0.0.1:5432          0.0.0.0:*  LISTEN      11516/postgres                
tcp6       0      0 ::1:5432                :::*       LISTEN      11516/postgres
</code></pre>

<p>I googled that I should modify this <code>pg_hba.conf</code>，but in my <code>postgresql</code>root files, I didn't find this file at all.</p>

<p>Also I've succeed in connecting my another server.</p>

<p>Thanks.</p>

<p>Here,I've modified the <code>pg_hba.conf</code>,updated this<code>host all all 218.3.A.B trust</code> and reloaded.But it didn't work either.</p>
","6882419","","6882419","","2016-10-08 09:05:49","2018-06-19 02:50:50","Connection refused with postgresql using psycopg2","<python><postgresql><python-3.x>","2","2","1","","","CC BY-SA 3.0","0"
"49478573","1","","","2018-03-25 17:13:40","","21","23783","<p>When trying to install a package for Python 3 (in Ubuntu), using <code>pip3 install packageName</code> (or <code>sudo pip3 install packageName</code>), I get the following error message:</p>

<pre><code>Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/pip/_vendor/__init__.py"", line 33, in vendored
    __import__(vendored_name, globals(), locals(), level=0)
ImportError: No module named 'pip._vendor.pkg_resources'
</code></pre>

<p>I have been reading for days and have tried the following WITHOUT any success:</p>

<ol>
<li>Un-installing and re-installing pip3 using the following code: <code>sudo apt-get remove python3-pip</code> followed by <code>sudo apt-get install python3-pip</code>. This was suggested in several posts that say that sometimes <code>pip3</code> doesn't install properly for Ubuntu. However, it didn't work.</li>
<li>Other post suggested it was an <code>ssl</code> package problem and that if the <code>ssl</code> package doesn't load on Python3, that's the issue. However the following command does not raise any error: <code>python3 -c ""import ssl""</code>.</li>
<li>Some other post suggested the problem was with the <code>requests</code> package. I then tried <code>sudo apt-get remove python3-requests</code> followed by <code>sudo apt-get install python3-requests</code> also to no avail.</li>
</ol>

<p>Other information you may need:</p>

<ul>
<li><code>pip3 --version</code> gives me the same error reported above.</li>
<li><p><code>dpkg -L python3-pip</code> gives me the following information:</p>

<pre><code>/.
/usr
/usr/lib
/usr/lib/python3
/usr/lib/python3/dist-packages
/usr/lib/python3/dist-packages/pip-8.1.1.egg-info
/usr/lib/python3/dist-packages/pip-8.1.1.egg-info/PKG-INFO
/usr/lib/python3/dist-packages/pip-8.1.1.egg-info/dependency_links.txt
/usr/lib/python3/dist-packages/pip-8.1.1.egg-info/not-zip-safe
/usr/lib/python3/dist-packages/pip-8.1.1.egg-info/requires.txt
/usr/lib/python3/dist-packages/pip-8.1.1.egg-info/entry_points.txt
/usr/lib/python3/dist-packages/pip-8.1.1.egg-info/top_level.txt
/usr/lib/python3/dist-packages/pip
/usr/lib/python3/dist-packages/pip/baseparser.py
/usr/lib/python3/dist-packages/pip/__main__.py
/usr/lib/python3/dist-packages/pip/req
/usr/lib/python3/dist-packages/pip/req/req_set.py
/usr/lib/python3/dist-packages/pip/req/req_install.py
/usr/lib/python3/dist-packages/pip/req/__init__.py
/usr/lib/python3/dist-packages/pip/req/req_file.py
/usr/lib/python3/dist-packages/pip/req/req_uninstall.py
/usr/lib/python3/dist-packages/pip/index.py
/usr/lib/python3/dist-packages/pip/status_codes.py
/usr/lib/python3/dist-packages/pip/utils
/usr/lib/python3/dist-packages/pip/utils/setuptools_build.py
/usr/lib/python3/dist-packages/pip/utils/appdirs.py
/usr/lib/python3/dist-packages/pip/utils/outdated.py
/usr/lib/python3/dist-packages/pip/utils/ui.py
/usr/lib/python3/dist-packages/pip/utils/logging.py
/usr/lib/python3/dist-packages/pip/utils/encoding.py
/usr/lib/python3/dist-packages/pip/utils/deprecation.py
/usr/lib/python3/dist-packages/pip/utils/__init__.py
/usr/lib/python3/dist-packages/pip/utils/filesystem.py
/usr/lib/python3/dist-packages/pip/utils/hashes.py
/usr/lib/python3/dist-packages/pip/utils/build.py
/usr/lib/python3/dist-packages/pip/compat
/usr/lib/python3/dist-packages/pip/compat/dictconfig.py
/usr/lib/python3/dist-packages/pip/compat/__init__.py
/usr/lib/python3/dist-packages/pip/compat/ordereddict.py
/usr/lib/python3/dist-packages/pip/models
/usr/lib/python3/dist-packages/pip/models/index.py
/usr/lib/python3/dist-packages/pip/models/__init__.py
/usr/lib/python3/dist-packages/pip/vcs
/usr/lib/python3/dist-packages/pip/vcs/bazaar.py
/usr/lib/python3/dist-packages/pip/vcs/subversion.py
/usr/lib/python3/dist-packages/pip/vcs/mercurial.py
/usr/lib/python3/dist-packages/pip/vcs/__init__.py
/usr/lib/python3/dist-packages/pip/vcs/git.py
/usr/lib/python3/dist-packages/pip/cmdoptions.py
/usr/lib/python3/dist-packages/pip/basecommand.py
/usr/lib/python3/dist-packages/pip/commands
/usr/lib/python3/dist-packages/pip/commands/completion.py
/usr/lib/python3/dist-packages/pip/commands/install.py
/usr/lib/python3/dist-packages/pip/commands/hash.py
/usr/lib/python3/dist-packages/pip/commands/uninstall.py
/usr/lib/python3/dist-packages/pip/commands/__init__.py
/usr/lib/python3/dist-packages/pip/commands/list.py
/usr/lib/python3/dist-packages/pip/commands/search.py
/usr/lib/python3/dist-packages/pip/commands/show.py
/usr/lib/python3/dist-packages/pip/commands/download.py
/usr/lib/python3/dist-packages/pip/commands/wheel.py
/usr/lib/python3/dist-packages/pip/commands/freeze.py
/usr/lib/python3/dist-packages/pip/commands/help.py
/usr/lib/python3/dist-packages/pip/_vendor
/usr/lib/python3/dist-packages/pip/_vendor/__init__.py
/usr/lib/python3/dist-packages/pip/operations
/usr/lib/python3/dist-packages/pip/operations/__init__.py
/usr/lib/python3/dist-packages/pip/operations/freeze.py
/usr/lib/python3/dist-packages/pip/__init__.py
/usr/lib/python3/dist-packages/pip/locations.py
/usr/lib/python3/dist-packages/pip/pep425tags.py
/usr/lib/python3/dist-packages/pip/exceptions.py
/usr/lib/python3/dist-packages/pip/download.py
/usr/lib/python3/dist-packages/pip/wheel.py
/usr/share
/usr/share/man
/usr/share/man/man1
/usr/share/man/man1/pip3.1.gz
/usr/share/doc
/usr/share/doc/python3-pip
/usr/share/doc/python3-pip/copyright
/usr/bin
/usr/bin/pip3
/usr/share/doc/python3-pip/changelog.Debian.gz
</code></pre></li>
</ul>

<p>How can I make <code>pip3</code> work?</p>

<p>NOTE: <code>pip</code> for Python2 works just fine.</p>

<p>================================================</p>

<p>EDIT:</p>

<ol>
<li>When trying to <code>import setuptools</code> in Python3 I get the following error:</li>
</ol>

<pre class=""lang-none prettyprint-override""><code>    Traceback (most recent call last):
      File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
      File ""/usr/lib/python3/dist-packages/setuptools/__init__.py"", line 11, in &lt;module&gt;
        from setuptools.extern.six.moves import filterfalse, map
      File ""/usr/lib/python3/dist-packages/setuptools/extern/__init__.py"", line 1, in &lt;module&gt;
        from pkg_resources.extern import VendorImporter
      File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2927, in &lt;module&gt;
        @_call_aside
      File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2913, in _call_aside
        f(*args, **kwargs)
      File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2952, in _initialize_master_working_set
        add_activation_listener(lambda dist: dist.activate())
      File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 956, in subscribe
        callback(dist)
      File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2952, in &lt;lambda&gt;
        add_activation_listener(lambda dist: dist.activate())
      File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2515, in activate
        declare_namespace(pkg)
      File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2097, in declare_namespace
        _handle_ns(packageName, path_item)
      File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2047, in _handle_ns
        _rebuild_mod_path(path, packageName, module)
      File ""/usr/lib/python3/dist-packages/pkg_resources/__init__.py"", line 2066, in _rebuild_mod_path
        orig_path.sort(key=position_in_sys_path)
    AttributeError: '_NamespacePath' object has no attribute 'sort'
</code></pre>

<ol start=""2"">
<li><p>When running the following command <code>sed -n '31,37p' &lt; /usr/lib/python3/dist-packages/pip/_vendor/__init__.py</code> in the terminal, I get the following:</p>

<pre><code>try:
    __import__(vendored_name, globals(), locals(), level=0)
except ImportError:
    try:
        __import__(modulename, globals(), locals(), level=0)
    except ImportError:
</code></pre></li>
</ol>

<p>===========================================</p>

<p>EDIT2:</p>

<p>My <code>python3 --version</code> is <code>Python 3.5.2</code>.</p>

<p>List of installed packages obtained running the code <code>ls /usr/lib/python3/dist-packages</code>.</p>

<pre><code>apport
apport_python_hook.py
apt
aptdaemon
apt_inst.cpython-35m-x86_64-linux-gnu.so
apt_pkg.cpython-35m-x86_64-linux-gnu.so
aptsources
AptUrl
apturl-0.5.2.egg-info
beautifulsoup4-4.4.1.egg-info
blinker
blinker-1.3.egg-info
Brlapi-0.6.4.egg-info
brlapi.cpython-35m-x86_64-linux-gnu.so
bs4
cairo
_cffi_backend.cpython-35m-x86_64-linux-gnu.so
chardet
chardet-2.3.0.egg-info
checkbox_support
checkbox_support-0.22.egg-info
CommandNotFound
command_not_found-0.3.egg-info
cryptography
cryptography-1.2.3.egg-info
cups.cpython-35m-x86_64-linux-gnu.so
cupsext.cpython-35m-x86_64-linux-gnu.so
curl
cycler-0.9.0.egg-info
cycler.py
dateutil
dbus
_dbus_bindings.cpython-35m-x86_64-linux-gnu.so
_dbus_glib_bindings.cpython-35m-x86_64-linux-gnu.so
deb822.py
debconf.py
debian
debian_bundle
decorator-4.0.6.egg-info
decorator.py
defer
defer-1.0.6.egg-info
DistUpgrade
easy_install.py
feedparser-5.1.3.egg-info
feedparser.py
feedparser_sgmllib3.py
gi
guacamole
guacamole-0.9.2.egg-info
hpmudext.cpython-35m-x86_64-linux-gnu.so
html5lib
html5lib-0.999.egg-info
httplib2
httplib2-0.9.1.egg-info
idna
idna-2.0.egg-info
janitor
jinja2
Jinja2-2.8.egg-info
jwt
LanguageSelector
language_selector-0.1.egg-info
language_support_pkgs.py
louis
louis-2.6.4.egg-info
lsb_release.py
lxml
lxml-3.5.0.egg-info
mako
Mako-1.0.3.egg-info
markupsafe
MarkupSafe-0.23.egg-info
matplotlib
matplotlib-1.5.1.egg-info
matplotlib-1.5.1-nspkg.pth
mpl_toolkits
networkx
networkx-1.11.egg-info
numexpr
numexpr-2.4.3.egg-info
numpy
numpy-1.11.0.egg-info
NvidiaDetector
oauthlib
oauthlib-1.0.3.egg-info
Onboard
onboard-1.2.0.egg-info
orca
padme
padme-1.1.1.egg-info
pandas
pandas-0.17.1.egg-info
pcardext.cpython-35m-x86_64-linux-gnu.so
pexpect
pexpect-4.0.1.egg-info
PIL
Pillow-3.1.2.egg-info
pip
pip-8.1.1.egg-info
pkg_resources
plotly
plotly-1.9.5.egg-info
problem_report.py
ptyprocess
ptyprocess-0.5.egg-info
pyasn1
pyasn1-0.1.9.egg-info
pyatspi
__pycache__
pycups-1.9.73.egg-info
pycurl-7.43.0.egg-info
pycurl.cpython-35m-x86_64-linux-gnu.so
pygobject-3.20.0.egg-info
pygtkcompat
PyJWT-1.3.0.egg-info
pylab.py
pyparsing-2.0.3.egg-info
pyparsing.py
python_apt-1.1.0.b1_ubuntu0.16.04.1.egg-info
python_dateutil-2.4.2.egg-info
python_debian-0.1.27.egg-info
python_systemd-231.egg-info
pytz
pytz-2014.10.egg-info
pyxdg-0.25.egg-info
PyYAML-3.11.egg-info
Quirks
reportlab
reportlab-3.3.0.egg-info
requests
requests-2.9.1.egg-info
scanext.cpython-35m-x86_64-linux-gnu.so
scipy
scipy-0.17.0.egg-info
sessioninstaller
sessioninstaller-0.0.0.egg-info
setuptools
setuptools-20.7.0.egg-info
six-1.10.0.egg-info
six.py
softwareproperties
speechd
speechd_config
systemd
system_service-0.3.egg-info
tables
tables-3.2.2.egg-info
UbuntuDrivers
ubuntu_drivers_common-0.0.0.egg-info
UbuntuSystemService
ufw
ufw-0.35.egg-info
unattended_upgrades-0.1.egg-info
unity_scope_calculator-0.1.egg-info
unity_scope_chromiumbookmarks-0.1.egg-info
unity_scope_colourlovers-0.1.egg-info
unity_scope_devhelp-0.1.egg-info
unity_scope_firefoxbookmarks-0.1.egg-info
unity_scope_gdrive-0.7.egg-info
unity_scope_manpages-0.1.egg-info
unity_scope_openclipart-0.1.egg-info
unity_scope_texdoc-0.1.egg-info
unity_scope_tomboy-0.1.egg-info
unity_scope_virtualbox-0.1.egg-info
unity_scope_yelp-0.1.egg-info
unity_scope_zotero-0.1.egg-info
unohelper.py
uno.py
UpdateManager
urllib3
urllib3-1.13.1.egg-info
usbcreator
usb_creator-0.3.0.egg-info
wheel
wheel-0.29.0.egg-info
xdg
xdiagnose
xdiagnose-3.8.4.1.egg-info
xkit
xkit-0.0.0.egg-info
xlsxwriter
XlsxWriter-0.7.3.egg-info
yaml
_yaml.cpython-35m-x86_64-linux-gnu.so
</code></pre>
","6838716","","6838716","","2018-03-26 18:50:47","2019-11-26 10:20:38","pip3 install not working - No module named 'pip._vendor.pkg_resources'","<python-3.x><ubuntu><pip>","4","8","6","","","CC BY-SA 3.0","0"
"41317248","1","41317279","","2016-12-24 22:22:18","","3","23773","<p>So all day, I have been trying to install pip. I've executed <code>curl https://bootstrap.pypa.io/get-pip.py &gt; get-pip.py</code>. Then <code>sudo python get-pip.py</code>. Next I went to install numpy with <code>sudo pip install numpy</code>. Finally, I opened up the python 3.6 IDLE and I tried to <code>import numpy as np</code> and got an error saying that the module didn't exist. </p>

<p>I found out that the pip was installed in <code>/Library/Python/2.7/site-packages</code> so I deleted 2.7 from the directory. I redid the whole process again and got the same error :|</p>

<p>I'm wondering if I need to change the default <code>python</code> which I've been told is not wise. What do I do? I' sure I've deleted everything with python 2.7 in it?</p>

<p>MacBookAir OSX - Sierra</p>

<p>Or is there an alternate way that I can install numpy in python 3.6????</p>
","7338578","","","","","2019-05-23 18:46:10","How to install pip on python 3.6, not the default python 2.7?","<python><macos><python-3.x><numpy><pip>","1","4","","","","CC BY-SA 3.0","0"
"37993624","1","37993830","","2016-06-23 13:51:45","","12","23765","<p>I have a Rails 4 application which uses token based authentication for APIs and need to be able to update records through Python 3 script.</p>

<p>My current script looks like this</p>

<pre><code>import requests
import json

url = 'http://0.0.0.0:3000/api/v1/update_experiment.json'
payload = {'expt_name' : 'A60E001', 'status' : 'done' }

r = requests.patch(url, payload)
</code></pre>

<p>which works OK if I disable API authentication. </p>

<p>I can't figure out how to add headers to it, <code>requests.patch</code> only takes two parameters according to docs. </p>

<p>I would need to get to the point where the following header info would added</p>

<pre><code>'Authorization:Token token=""xxxxxxxxxxxxxxxxxxxxxx""'
</code></pre>

<p>This type of header works OK in curl. How can I do this in Python 3 and requests?</p>
","3862092","","3001761","","2016-06-23 14:00:54","2019-09-05 13:07:23","How to use requests to send a PATCH request with headers","<python><python-3.x><python-requests>","1","2","1","","","CC BY-SA 3.0","0"
"36163900","1","43019462","","2016-03-22 19:52:18","","0","23747","<p>I'm in the process of making a python GUI calculator. I've been coding for no more than 3-4 week so my knowledge is limited. Anyway i want to make a pop up window that takes input from the user(Enter number, press a button to save that number in a variable).
That should be done twice(in order to add, subtract,... 2 numbers). Then i'll make another pop-up window saying: ""The result is:(result)""
I know how to make an entry widget so my question is how do i make a button to save the user's input to a variable?</p>
","6094381","","","","","2018-10-26 17:48:08","Taking User Input. Python GUI","<python-3.x>","1","2","2","","","CC BY-SA 3.0","0"
"47665760","1","47666357","","2017-12-06 02:12:48","","5","23733","<p>I'm a fairly new to Python and I am trying to enter the following code:</p>

<pre><code>from websocket import create_connection as cc
import json, time
</code></pre>

<p>I want to look at BTC and LTC stocks in a live feed type-fashion but I keep getting this error:</p>

<blockquote>
  <p>ModuleNotFoundError: No module named 'websocket'</p>
</blockquote>

<p>I've been looking around and it seems that a common solution is:</p>

<pre><code>pip install websocket
</code></pre>

<p>that just didn't connect with me. Most sites say install this and just have the above code but I have no idea where to install it or what to do with it.</p>
","9059395","","100297","","2020-04-03 15:50:05","2020-04-03 15:50:05","No module named 'websocket'","<python><python-3.x><websocket>","1","1","","","","CC BY-SA 4.0","0"
"36826570","1","42283059","","2016-04-24 17:30:28","","13","23713","<p>I'm working on a simple program in Python 3.5 that contains turtle graphics
and I have a problem: after the turtle work is finished the user has to close the window manually.</p>

<p>Is there any way to program the window to close after the turtle work is done?</p>

<p>Any help is appreciated.</p>
","6248185","","6243352","","2019-10-25 21:40:38","2019-11-27 18:01:54","How to close the Python turtle window after it does its code?","<python><python-3.x><turtle-graphics>","3","0","2","","","CC BY-SA 4.0","0"
"51540404","1","51849384","","2018-07-26 13:45:50","","37","23698","<p>I am using <a href=""https://pipenv.kennethreitz.org/"" rel=""noreferrer""><code>pipenv</code></a> to handle a Python package dependencies.</p>

<p>The Python package is using two packages (named <code>pckg1</code> and <code>pckg2</code>) that relies on the same package named <code>pckg3</code>, <strong>but from two different versions</strong>. Showing the dependency tree :</p>

<pre><code>$ pipenv graph
  pckg1==3.0.0
    - pckg3 [required: &gt;=4.1.0]
  pckg2==1.0.2
    - pckg3 [required: ==4.0.11]
</code></pre>

<p>An attempt to install dependencies :</p>

<pre><code>$ pipenv install

Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies.
You can use $ pipenv install --skip-lock to bypass this mechanism, then run $ pipenv graph to inspect the situation.
Hint: try $ pipenv lock --pre if it is a pre-release dependency.
Could not find a version that matches pckg3==4.0.11,==4.1.0,&gt;=4.1.0 (from -r C:\Users\user\AppData\Local\Temp\pipenv-o7uxm080-requirements\pipenv-hwekv7dc-constraints.txt (line 2))
Tried: 3.3.1, 3.3.2, 3.3.3, 3.4.0, 3.4.2, 4.0.0, 4.0.0, 4.0.1, 4.0.1, 4.0.2, 4.0.2, 4.0.3, 4.0.3, 4.0.4, 4.0.4, 4.0.6, 4.0.6, 4.0.8, 4.0.8, 4.0.9, 4.0.9, 4.0.10, 4.0.10, 4.0.11, 4.0.11, 4.1.0, 4.1.0, 4.1.1, 4.1.1, 4.1.2, 4.1.2, 4.2.1, 4.2.1, 4.3.0, 4.3.0
There are incompatible versions in the resolved dependencies.
</code></pre>

<p>As suggested, <code>pip install --skip-lock</code> does the trick, but the dependency tree is still unresolved.</p>

<p>I would love to tell <code>Pipenv</code> to override <code>pckg2</code>'s requirement, and specify <code>pckg3&gt;=4.1.0</code>.</p>

<p>How can it be resolved ?</p>
","6290463","","6290463","","2020-05-16 11:16:20","2020-08-02 15:45:05","How to resolve Python package depencencies with pipenv?","<python><python-3.x><pip><dependencies><pipenv>","11","0","11","","","CC BY-SA 4.0","0"
"50757497","1","53420574","","2018-06-08 09:28:03","","53","23695","<p>I've read many examples, blog posts, questions/answers about <code>asyncio</code> / <code>async</code> / <code>await</code> in Python 3.5+, many were complex, the simplest I found was probably <a href=""https://stackoverflow.com/a/37345564/1422096"">this one</a>. Still it uses <code>ensure_future</code>, and for learning purposes about asynchronous programming in Python, I would like to see if an even more minimal example is possible (i.e. what are the <em>minimum tools necessary</em> to do a basic async / await example).</p>

<p>Question: for learning purposes about asynchronous programming in Python, is it possible to give a <strong>simple example showing how <code>async</code> / <code>await</code> works</strong>, by using only these two keywords + <code>asyncio.get_event_loop()</code> + <code>run_until_complete</code> + other Python code but no other <code>asyncio</code> functions?</p>

<p>Example: something like this:</p>

<pre><code>import asyncio

async def async_foo():
    print(""async_foo started"")
    await asyncio.sleep(5)
    print(""async_foo done"")

async def main():
    asyncio.ensure_future(async_foo())  # fire and forget async_foo()
    print('Do some actions 1')
    await asyncio.sleep(5)
    print('Do some actions 2')

loop = asyncio.get_event_loop()
loop.run_until_complete(main())
</code></pre>

<p>but without <code>ensure_future</code>, and still demonstrates how await / async works.</p>
","1422096","","1422096","","2018-06-08 11:02:49","2020-10-23 23:31:17","Simplest async/await example possible in Python","<python><python-3.x><asynchronous><async-await><python-asyncio>","4","0","24","","","CC BY-SA 4.0","0"
"43767289","1","43789641","","2017-05-03 18:14:36","","7","23669","<p>I'm trying to add tesseract to be able to install pytesseract. 
I use Windows 7. </p>

<p>I add this path to my PATH environmental variable
<code>C:\Program Files (x86)\Tesseract-OCR\tesseract.exe</code></p>

<p>From the command line if I run</p>

<p><code>tesseract DMTX_screenshot.png out</code> 
        OR
<code>tesseract</code></p>

<p>I'm getting </p>

<p><code>tesseract is not recognized as an internal or external command.</code></p>

<p>Here is a copy-paste of the a portion of my environmental variable:</p>

<p><code>C:\Program Files (x86)\Tesseract-OCR\tesseract.exe;C:\Users\Moondra\Anaconda_related\Anaconda\geckodriver.exe;</code></p>

<p>Any ideas as to what I may be doing wrong? </p>

<p>Thank you.</p>
","6802252","","","","","2020-08-14 06:12:00","Can't seem to run tesseract from command line despite adding PATH","<windows><python-3.x><tesseract>","8","0","1","","","CC BY-SA 3.0","0"
"30521975","1","30522343","","2015-05-29 05:45:46","","6","23628","<p><code>A = [[1, 2, 3], [2, 3, 4], [4, 5, 6]]</code></p>

<p>I am trying my best to print <code>A</code> of the form:</p>

<pre><code>1 2 3
2 3 4
4 5 6
</code></pre>

<p>That is in different lines, but I am unable to do so without all the elements in different lines. This is my code so far:</p>

<pre><code>for r in A:
   for t in r:
       print(t,)
    print
</code></pre>

<p>This is my output:</p>

<pre><code>1
2
3
2
3
4
4
5
6
</code></pre>

<p>It seems really simple, and I think a minor change would do it. Thanks!</p>
","4947103","","","","","2019-12-13 18:24:08","Print a nested list line by line - Python","<python><python-3.x><for-loop><printing><nested-lists>","7","0","5","","","CC BY-SA 3.0","0"
"52681747","1","52681933","","2018-10-06 17:45:14","","2","23607","<p>I am trying to iterate through a column in a text file, where each entry has only three choices  <code>A, B, and C</code>. </p>

<p>I want to identify the number of different types of choices <code>(another text file has A, B, C, and D)</code>, but if I iterate through each element in the column with a <code>100 entries</code> and add it to a list, I'll have multiple repetitions of each type. For example, if I do this, the list might read <code>[A,A,A,B,C,C,D,D,D,B,B...]</code>, but I want to remove the extraneous entries and just have my list show the distinguishable types <code>[A,B,C,D]</code>, regardless of how many entries there were. </p>

<p>Any ideas how I might reduce a list with many common elements to a list with only the different distinguishable elements displayed? Thanks!</p>

<p>Desired Output:</p>

<p><code>[A, B, C, D]</code></p>
","5289913","","7505395","","2018-10-06 17:52:22","2020-01-30 09:56:26","How to get only distinct values from a list?","<python><python-3.x><list><for-loop>","3","2","1","2018-10-07 08:25:35","","CC BY-SA 4.0","0"
"32886927","1","32887994","","2015-10-01 11:50:53","","26","23599","<p>I have a page whose source code is not available, but there is a input box where cursor is blinking.    </p>

<p>Can i write something into the text box without finding the element. I mean, some way where send key can automatically look for focussed inputbox and type input to it.</p>

<p>My code does not work obviuosly</p>

<pre><code>driver.send_keys(""testdata"")
</code></pre>
","5167841","","","","","2020-08-26 17:12:51","Send keys without specifying element in python selenium webdriver","<python><python-2.7><python-3.x><selenium><selenium-webdriver>","3","1","6","","","CC BY-SA 3.0","0"
"49472108","1","49472183","","2018-03-25 02:58:57","","1","23560","<p>I'm working on a library for calculating certain values in a game. I have this code:</p>

<pre><code>million = [1000000, ""M""]
billion = [million * 1000, ""B""]
trillion = [billion * 1000, ""T""]
quadrillion = [trillion * 1000, ""Qd""]
quintillion = [quadrillion * 1000, ""Qn""]
sx = [quintillion * 1000, ""Sx""]
septillion = [sx * 1000, ""Sp""]

suffixes = [million, billion, trillion, quadrillion, quintillion, sx, septillion]

def getSetupResult(orevalue, furnacemultiplier, *upgrades, **kwargs):
    for i in upgrades:
        orevalue *= i
    orevalue *= furnacemultiplier
    for suffix in suffixes:
        if orevalue &gt; suffix[0] - 1 and orevalue &lt; suffix[0] * 1000:
            print(""$""+str(orevalue)+suffix[1])

getSetupResult(quintillion,700,5,4,10,100)
</code></pre>

<p>When I try to run it, it raises this error:</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/???/Desktop/MinersHavenCalculator.py"", line 19, in &lt;module&gt;
    getSetupResult(quintillion,700,5,4,10,100)
  File ""C:/Users/???/Desktop/MinersHavenCalculator.py"", line 16, in getSetupResult
    if orevalue &gt; suffix[0] - 1 and orevalue &lt; suffix[0] * 1000:
TypeError: '&gt;' not supported between instances of 'list' and 'int'
</code></pre>

<p>What's causing this error?</p>
","8328157","","568785","","2018-03-25 03:04:24","2018-03-25 03:14:02","TypeError: '>' not supported between instances of 'list' and 'int'","<python><python-3.x><if-statement><math><calculator>","3","7","3","","","CC BY-SA 3.0","0"
"32288466","1","32288554","","2015-08-29 16:50:38","","1","23558","<p>I have got this error when I try to run my program:</p>

<blockquote>
  <p>C:\Users\Goldsmitd\chess\Scripts\python.exe
  C:/Users/Goldsmitd/PycharmProjects/CHESS/chess_ver0.07.py Traceback
  (most recent call last):   File
  ""C:/Users/Goldsmitd/PycharmProjects/CHESS/chess_ver0.07.py"", line 138,
  in 
      a.display()   File ""C:/Users/Goldsmitd/PycharmProjects/CHESS/chess_ver0.07.py"", line 80,
  in display
      if self.board[i][j].sl=='R': AttributeError: 'str' object has no attribute 'sl'</p>
</blockquote>

<p>Someone know what I do wrong?</p>

<pre><code>__author__ = 'Goldsmitd'

class Rook:
    def __init__(self,x,y,sl,team):
        self.name = 'Rook'
        self.x = x
        self.y = y
        self.sl = sl
        self.team = team


class Knight:
    def __init__(self,x,y,sl,team):
        self.name = 'Knight'
        self.x = x
        self.y = y
        self.sl = sl
        self.team = team


class Bishop:
    def __init__(self,x,y,sl,team):
        self.name = 'Bishop'
        self.x = x
        self.y = y
        self.sl = sl
        self.team = team


class Queen:
    def __init__(self,x,y,sl,team):
        self.name = 'Queen'
        self.x = x
        self.y = y
        self.sl = sl
        self.team = team


class King:
    def __init__(self,x,y,sl,team):
        self.name = 'King'
        self.x = x
        self.y = y
        self.sl = sl
        self.team = team


class Pawn:
    def __init__(self,x,y,sl,team):
        self.name = 'Pawn'
        self.x = x
        self.y = y
        self.sl = sl
        self.team = team


class Chess_Board:
    def __init__(self):
        self.board = [['.']*8 for _ in range(8)]
        self.board[7][0] = Rook(x=7,y=0,sl='R',team='white')
        self.board[7][1] = Knight(x=7,y=1,sl='N',team='white')
        self.board[7][2] = Bishop(x=7,y=2,sl='B',team='white')
        self.board[7][3] = Queen(x=7,y=3,sl='Q',team='white')
        self.board[7][4] = King(x=7,y=4,sl='K',team='white')
        self.board[7][5] = Bishop(x=7,y=5,sl='B',team='white')
        self.board[7][6] = Knight(x=7,y=6,sl='N',team='white')
        self.board[7][7] = Rook(x=7,y=7,sl='R',team='white')
        self.board[6][0] = Pawn(x=6,y=0,sl='P',team='white')
        self.board[6][0] = Pawn(x=6,y=1,sl='P',team='white')
        self.board[6][0] = Pawn(x=6,y=2,sl='P',team='white')
        self.board[6][0] = Pawn(x=6,y=3,sl='P',team='white')
        self.board[6][0] = Pawn(x=6,y=4,sl='P',team='white')
        self.board[6][0] = Pawn(x=6,y=5,sl='P',team='white')
        self.board[6][0] = Pawn(x=6,y=6,sl='P',team='white')
        self.board[6][0] = Pawn(x=6,y=7,sl='P',team='white')

    def display(self):
        for i in range(8):
            for j in range(8):
                if self.board[i][j].sl=='R':
                    print('R',end=' ')
                elif self.board[i][j].sl=='N':
                    print('N',end=' ')
                elif self.board[i][j].sl=='B':
                    print('B',end=' ')
                elif self.board[i][j].sl=='Q':
                    print('Q',end=' ')
                elif self.board[i][j].sl=='K':
                    print('K',end=' ')
                elif self.board[i][j].sl=='P':
                    print('P',end=' ')
                else:
                    print(self.board[i][j],end=' ')
            print()

    def figure_choice(self):
        while True:
            try:
                print('please give a position of figure which you chose')
                sx=int(input())
                sy=int(input())
                return sx,sy
            except:
                print('ERROR. Your choice is valid. Please choose only integers')

    def move_king(self):

        while True:
            try:
                print('please give a position of figure which you chose')
                sx=int(input())
                sy=int(input())
                return sx,sy
            except:
                print('ERROR. Your choice is valid. Please choose only integers')
            try:
                print('please give a position of king')
                sx=int(input())
                sy=int(input())
            except:
                print('ERROR. Your choice is valid. Please choose only integers')
            try:
                print('please choose a destination for king')
                dx=int(input())
                dy=int(input())
            except:
                print('ERROR. Your choice is valid. Please choose only integers')
            if self.board[dx][dy] == '.' :
                    if ( abs(sx-dx) &lt;2 and abs(sx-dy) &lt; 2 ):
                        self.board[dx][dy]=King(x=dx,y=dy,sl='K',team='white')
                        self.board[sx][sy] = '.'
                        return self.board
                        break


a=Chess_Board()

a.display()
print(a.board[7][0].sl)
</code></pre>
","5224988","","","","","2015-08-29 17:13:34","Python. AttributeError: 'str' object has no attribute","<string><python-3.x><attributes>","1","0","1","","","CC BY-SA 3.0","0"
"39678984","1","39679114","","2016-09-24 17:18:07","","10","23525","<p>What is the most efficient (""pythonic"") way to test/check if two numbers are co-primes (relatively prime) in Python.</p>

<p>For the moment I have this code:</p>

<pre><code>def gcd(a, b):
    while b != 0:
        a, b = b, a % b
    return a

def coprime(a, b):
    return gcd(a, b) == 1

print(coprime(14,15)) #Should be true
print(coprime(14,28)) #Should be false
</code></pre>

<p>Can the code for checking/testing if two numbers are relatively prime be considered ""Pythonic"" or there is some better way?</p>
","4786305","","1324","","2019-09-17 16:12:46","2019-09-17 16:12:46","Efficiently check if two numbers are co-primes (relatively primes)?","<python><python-3.x><algorithm><primes>","1","4","3","","","CC BY-SA 4.0","0"
"56204985","1","","","2019-05-19 05:21:27","","9","23462","<p>I have some code from my friend. 
He run it smoothly but I encounter </p>

<p><code>module **scipy.misc** has no attribute *imresize*</code> </p>

<p>I'm searching, installed Pillow (PIL), scipy, scikit,.. but dont work</p>

<p>I asked my friend but he forgot what he has installed.</p>
","11300658","","8472377","","2019-05-19 05:22:31","2020-02-18 10:39:26","How to fix ""-scipy.misc has no attribute ""imresize""""","<python><python-3.x><scikit-learn><scipy>","2","0","","","","CC BY-SA 4.0","0"
"40710811","1","40710862","","2016-11-20 23:47:25","","17","23436","<p>I have the Yelp dataset and I want to count all reviews which have greater than 3 stars. I get the count of reviews by doing this:</p>

<pre><code>reviews.groupby('business_id')['stars'].count()
</code></pre>

<p>Now I want to get the count of reviews which had more than 3 stars, so I tried this by taking inspiration from <a href=""https://stackoverflow.com/questions/22751498/pandas-groupby-apply-function-to-count-values-greater-than-zero"">here</a>:</p>

<pre><code>reviews.groupby('business_id')['stars'].agg({'greater':lambda val: (val &gt; 3).count()})
</code></pre>

<p>But this just gives me the count of all stars like before. I am not sure if this is the right way to do it? What am I doing incorrectly here. Does the lambda expression not go through each value of the stars column?</p>

<p>EDIT:
Okay I feel stupid. I should have used the sum function instead of count to get the value of elements greater than 3, like this:</p>

<pre><code>reviews.groupby('business_id')['stars'].agg({'greater':lambda val: (val &gt; 3).sum()})
</code></pre>
","4928920","","-1","","2017-05-23 11:59:55","2020-05-28 13:20:20","Count items greater than a value in pandas groupby","<python><python-3.x><pandas>","4","0","","","","CC BY-SA 3.0","1"
"45201628","1","45381146","","2017-07-19 21:49:17","","10","23388","<p>I am using the following filters in Postman to make a POST request in a Web API but I am unable to make a simple POST request in Python with the requests library. </p>

<p><strong>First</strong>, I am sending a POST request to this URL (<a href=""http://10.61.202.98:8081/T/a/api/rows/cat/ect/tickets"" rel=""noreferrer"">http://10.61.202.98:8081/T/a/api/rows/cat/ect/tickets</a>) with the following filters in Postman applied to the Body, with the raw and JSON(application/json) options selected. </p>

<pre><code>Filters in Postman

{
  ""filter"": {
    ""filters"": [
      {
        ""field"": ""RCA_Assigned_Date"",
        ""operator"": ""gte"",
        ""value"": ""2017-05-31 00:00:00""
      },
      {
        ""field"": ""RCA_Assigned_Date"",
        ""operator"": ""lte"",
        ""value"": ""2017-06-04 00:00:00""
      },
      {
        ""field"": ""T_Subcategory"",
        ""operator"": ""neq"",
        ""value"": ""Temporary Degradation""
      },
      {
        ""field"": ""Issue_Status"",
        ""operator"": ""neq"",
        ""value"": ""Queued""
      }],
     ""logic"": ""and""
    }
}
</code></pre>

<p>The database where the data is stored is Cassandra and according to the following links <a href=""https://stackoverflow.com/questions/21925525/cassandra-not-equal-operator"">Cassandra not equal operator</a>, <a href=""https://stackoverflow.com/questions/7255734/cassandra-between-order-by-operations?rq=1"">Cassandra OR operator</a>,
 <a href=""https://stackoverflow.com/questions/26309198/cassandra-cql-or-operator"">Cassandra Between order by operators</a>, Cassandra does not support the <strong>NOT EQUAL TO</strong>, <strong>OR</strong>, <strong>BETWEEN</strong> operators, so there is no way I can filter the URL with these operators except with <strong>AND</strong>. </p>

<p><strong>Second</strong>, I am using the following code to apply a simple filter with the requests library. </p>

<pre><code>import requests
payload = {'field':'T_Subcategory','operator':'neq','value':'Temporary Degradation'}
url = requests.post(""http://10.61.202.98:8081/T/a/api/rows/cat/ect/tickets"",data=payload)
</code></pre>

<p>But what I've got is the complete data of tickets instead of only those that are not temporary degradation. </p>

<p><strong>Third</strong>, the system is actually working but we are experiencing a delay of 2-3 mins to see the data. The logic goes as follows: <em>We have 8 users and we want to see all the tickets per user that are not temporary degradation, then we do</em>:</p>

<pre><code>def get_json():
    if user_name == ""user 001"":
        with urllib.request.urlopen(
    ""http://10.61.202.98:8081/T/a/api/rows/cat/ect/tickets?user_name=user&amp;001"",timeout=15) as url:
            complete_data = json.loads(url.read().decode())

    elif user_name == ""user 002"":
        with urllib.request.urlopen(             
    ""http://10.61.202.98:8081/T/a/api/rows/cat/ect/tickets?user_name=user&amp;002"",timeout=15) as url:
            complete_data = json.loads(url.read().decode())
    return complete_data

def get_tickets_not_temp_degradation(start_date,end_date,complete_):
    return Counter([k['user_name'] for k in complete_data if start_date &lt; dateutil.parser.parse(k.get('DateTime')) &lt; end_date and k['T_subcategory'] != 'Temporary Degradation'])
</code></pre>

<p>Basically, we get the whole set of tickets from the current and last year, then we let Python to filter the complete set by user and so far there are only 10 users which means that this process is repeated 10 times and makes me no surprise to discover why we get the delay... </p>

<p>My questions is how can I fix this problem of the requests library? I am using the following link <a href=""http://docs.python-requests.org/en/master/user/quickstart/#more-complicated-post-requests"" rel=""noreferrer"">Requests library documentation</a> as a tutorial to make it working but it just seems that my payload is not being read.</p>
","2514936","","2514936","","2017-07-20 20:01:57","2017-07-28 19:48:52","How to make a post request with the Python requests library?","<python><json><django><python-3.x><cassandra>","4","11","6","","","CC BY-SA 3.0","0"
"39623429","1","39626328","","2016-09-21 17:54:33","","3","23378","<p>Context: I would like to use <code>numpy ndarrays</code> with <code>float32</code> instead of <code>float64</code>.</p>

<p>Edit: Additional context -  I'm concerned about how <code>numpy</code> is executing these calls because they will be happening repeatedly as part of a backpropagation routine in a neural net. I'd like the net to carry out all addition/subtraction/multiplication/division in <code>float32</code> for validation purposes, as I want to compare results with another group's work. It seems like initialization for methods like <code>randn</code> will always go from <code>float64</code> -> <code>float32</code> with <code>.astype()</code> casting. Once my <code>ndarray</code> is of type <code>float32</code> if i use <code>np.dot</code> for example will those multiplications happen in <code>float32</code>? How can I verify?</p>

<p>The documentation is not clear to me - <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html"" rel=""nofollow"">http://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html</a></p>

<p>I figured out I can just add <code>.astype('float32')</code> to the end of a numpy call, for example, <code>np.random.randn(y, 1).astype('float32')</code>.</p>

<p>I also see that <code>dtype=np.float32</code> is an option, for example, <code>np.zeros(5, dtype=np.float32)</code>. However, trying <code>np.random.randn((y, 1), dtype=np.float32)</code> returns the following error: </p>

<pre><code>    b = np.random.randn((3,1), dtype=np.float32)
TypeError: randn() got an unexpected keyword argument 'dtype'
</code></pre>

<p>What is the difference between declaring the type as <code>float32</code> using <code>dtype</code> and using <code>.astype()</code>?</p>

<p>Both <code>b = np.zeros(5, dtype=np.float32)</code> and <code>b = np.zeros(5).astype('float32')</code> when evaluated with: </p>

<pre><code>print(type(b))
print(b[0])
print(type(b[0]))
</code></pre>

<p>prints:</p>

<pre><code>[ 0.  0.  0.  0.  0.]
&lt;class 'numpy.ndarray'&gt;
0.0
&lt;class 'numpy.float32'&gt;
</code></pre>
","3277902","","3277902","","2016-09-21 21:50:17","2020-06-27 01:25:13","What is the difference between dtype= and .astype() in numpy?","<python-3.x><numpy>","3","2","5","","","CC BY-SA 3.0","0"
"34699948","1","","","2016-01-09 23:01:50","","38","23334","<p>Does asyncio supports asynchronous I/O for file operations? If yes, how I can use this in Python 3.5 with async/await syntax code?</p>
","5258343","","","","","2018-01-27 19:29:34","Does asyncio supports asynchronous I/O for file operations?","<python><python-3.x><python-asyncio><python-3.5>","3","3","6","","","CC BY-SA 3.0","0"
"39736000","1","39823782","","2016-09-27 23:51:55","","18","23312","<p>A month ago I solved my applcation freezing issues for Python 2.7 as you can see <a href=""https://stackoverflow.com/questions/39135408/using-pyinstaller-on-parmap-causes-a-tkinter-matplotlib-import-error-why"">here</a>. I have since adapted my code to python 3.5 (using Anaconda) and it appears to be working.  couldn't get pyinstaller working with Anaconda so switched to trying to generate an .exe using the standard Python 3.5 compiler. I am using the same settings as in the link above (<code>pyinstaller --additional-hooks-dir=. --clean --win-private-assemblies pipegui.py</code>), except I get the following error message instead: </p>

<pre><code>`Exception: Cannot find PyQt5 plugin directories`
</code></pre>

<p><a href=""https://stackoverflow.com/questions/19207746/using-cx-freeze-in-pyqt5-cant-find-pyqt5"">This</a> may be related? Except I'm using Pyinstaller and I don't have a setup.py so don't know how I can make use of the solution there, if at all</p>

<p>I find this error message bizarre because I am not using PyQt5, but PyQt4. Here is the full output:</p>

<pre><code>C:\Users\Cornelis Dirk Haupt\PycharmProjects\Mesoscale-Brain-Explorer\src&gt;pyinstaller --additional-hooks-dir=. --clean --win-private-assemblies pipegui.py
62 INFO: PyInstaller: 3.2
62 INFO: Python: 3.5.0
62 INFO: Platform: Windows-10.0.14393
62 INFO: wrote C:\Users\Cornelis Dirk Haupt\PycharmProjects\Mesoscale-Brain-Explorer\src\pipegui.spec
62 INFO: UPX is not available.
62 INFO: Removing temporary files and cleaning cache in C:\Users\Cornelis Dirk Haupt\AppData\Roaming\pyinstaller
62 INFO: Extending PYTHONPATH with paths
['C:\\Users\\Cornelis Dirk Haupt\\PycharmProjects\\Mesoscale-Brain-Explorer',
 'C:\\Users\\Cornelis Dirk '
 'Haupt\\PycharmProjects\\Mesoscale-Brain-Explorer\\src']
62 INFO: checking Analysis
62 INFO: Building Analysis because out00-Analysis.toc is non existent
62 INFO: Initializing module dependency graph...
62 INFO: Initializing module graph hooks...
62 INFO: Analyzing base_library.zip ...
1430 INFO: running Analysis out00-Analysis.toc
1727 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-math-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1742 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-runtime-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1742 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-locale-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1758 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-stdio-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1758 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-heap-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1774 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-string-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1774 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-environment-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1774 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-time-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1789 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-filesystem-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1789 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-conio-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1789 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-process-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1805 WARNING: Can not get binary dependencies for file: C:\Anaconda3\api-ms-win-crt-convert-l1-1-0.dll
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 695, in getImports
    return _getImports_pe(pth)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\depend\bindepend.py"", line 122, in _getImports_pe
    dll, _ = sym.forwarder.split('.')
TypeError: a bytes-like object is required, not 'str'
1805 INFO: Caching module hooks...
1805 INFO: Analyzing C:\Users\Cornelis Dirk Haupt\PycharmProjects\Mesoscale-Brain-Explorer\src\pipegui.py
1992 INFO: Processing pre-find module path hook   distutils
2055 INFO: Processing pre-safe import module hook   six.moves
3181 INFO: Processing pre-find module path hook   site
3181 INFO: site: retargeting to fake-dir 'c:\\users\\cornelis dirk haupt\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\PyInstaller\\fake-modules'
4298 INFO: Processing pre-safe import module hook   win32com
9975 INFO: Loading module hooks...
9975 INFO: Loading module hook ""hook-_tkinter.py""...
10121 INFO: checking Tree
10121 INFO: Building Tree because out00-Tree.toc is non existent
10122 INFO: Building Tree out00-Tree.toc
10184 INFO: checking Tree
10184 INFO: Building Tree because out01-Tree.toc is non existent
10185 INFO: Building Tree out01-Tree.toc
10198 INFO: Loading module hook ""hook-matplotlib.py""...
10404 INFO: Loading module hook ""hook-pywintypes.py""...
10526 INFO: Loading module hook ""hook-xml.py""...
10526 INFO: Loading module hook ""hook-pydoc.py""...
10527 INFO: Loading module hook ""hook-scipy.linalg.py""...
10527 INFO: Loading module hook ""hook-scipy.sparse.csgraph.py""...
10529 INFO: Loading module hook ""hook-plugins.py""...
10721 INFO: Processing pre-find module path hook   PyQt4.uic.port_v3
10726 INFO: Processing pre-find module path hook   PyQt4.uic.port_v2
12402 INFO: Loading module hook ""hook-OpenGL.py""...
12583 INFO: Loading module hook ""hook-PyQt4.QtGui.py""...
12802 INFO: Loading module hook ""hook-encodings.py""...
12807 INFO: Loading module hook ""hook-PyQt4.uic.py""...
12812 INFO: Loading module hook ""hook-PyQt5.QtWidgets.py""...
12813 INFO: Loading module hook ""hook-xml.etree.cElementTree.py""...
12813 INFO: Loading module hook ""hook-setuptools.py""...
12814 INFO: Loading module hook ""hook-scipy.special._ufuncs.py""...
12814 INFO: Loading module hook ""hook-PyQt5.QtCore.py""...
Traceback (most recent call last):
  File ""&lt;string&gt;"", line 2, in &lt;module&gt;
ImportError: DLL load failed: The specified procedure could not be found.
Traceback (most recent call last):
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\runpy.py"", line 170, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Cornelis Dirk Haupt\AppData\Local\Programs\Python\Python35\Scripts\pyinstaller.exe\__main__.py"", line 9, in &lt;module&gt;
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\__main__.py"", line 90, in run
    run_build(pyi_config, spec_file, **vars(args))
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\__main__.py"", line 46, in run_build
    PyInstaller.building.build_main.main(pyi_config, spec_file, **kwargs)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\build_main.py"", line 788, in main
    build(specfile, kw.get('distpath'), kw.get('workpath'), kw.get('clean_build'))
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\build_main.py"", line 734, in build
    exec(text, spec_namespace)
  File ""&lt;string&gt;"", line 16, in &lt;module&gt;
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\build_main.py"", line 212, in __init__
    self.__postinit__()
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\datastruct.py"", line 178, in __postinit__
    self.assemble()
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\build_main.py"", line 470, in assemble
    module_hook.post_graph()
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\imphook.py"", line 409, in post_graph
    self._load_hook_module()
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\building\imphook.py"", line 376, in _load_hook_module
    self.hook_module_name, self.hook_filename)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\compat.py"", line 725, in importlib_load_source
    return mod_loader.load_module()
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 385, in _check_name_wrapper
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 806, in load_module
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 665, in load_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 268, in _load_module_shim
  File ""&lt;frozen importlib._bootstrap&gt;"", line 693, in _load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 673, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 662, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\hooks\hook-PyQt5.QtCore.py"", line 15, in &lt;module&gt;
    binaries = qt_plugins_binaries('codecs', namespace='PyQt5')
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\utils\hooks\qt.py"", line 64, in qt_plugins_binaries
    pdir = qt_plugins_dir(namespace=namespace)
  File ""c:\users\cornelis dirk haupt\appdata\local\programs\python\python35\lib\site-packages\PyInstaller\utils\hooks\qt.py"", line 38, in qt_plugins_dir
    raise Exception('Cannot find {0} plugin directories'.format(namespace))
Exception: Cannot find PyQt5 plugin directories
</code></pre>

<p>I will say I also have no clue what to make of the <code>TypeError: a bytes-like object is required, not 'str'</code><a href=""https://stackoverflow.com/questions/33054527/python-3-5-typeerror-a-bytes-like-object-is-required-not-str"">This</a> may be related? I only use binary mode with pickle a handful of times as far as I can tell this is my only usage:</p>

<pre><code>pickle.dump( roiState, open( fileName, ""wb"" ) )
roiState = pickle.load(open(fileName, ""rb""))
</code></pre>

<p>I don't have any errors when I run the application, only getting these errors when trying to generate an .exe using pyinstaller. Why?</p>

<p>Note also that Anaconda3 does pop up in the traceback above (why is it looking for binaries there?) but I:</p>

<ol>
<li>Uninstalled pyinstaller from Anaconda</li>
<li>Am using the standard Python 3.5 (64-bit) compiler</li>
</ol>

<p>Only thing I can think of that may be the culprit is that I'm no longer using the developer version of Pyinstaller (it just flat does not run in Python 3.5). I had to use the developer version to solve my freezing issue <a href=""https://stackoverflow.com/questions/39135408/using-pyinstaller-on-parmap-causes-a-tkinter-matplotlib-import-error-why"">here</a> when my code was written for python 2.7</p>
","2734863","","-1","","2017-05-23 10:34:06","2020-03-28 19:19:37","Exception: Cannot find PyQt5 plugin directories when using Pyinstaller despite PyQt5 not even being used","<python><python-2.7><python-3.x><pyinstaller>","7","0","7","","","CC BY-SA 3.0","0"
"36869258","1","36869259","","2016-04-26 15:23:37","","7","23297","<p>I am attempting to use Graphviz from Spyder (via an Anaconda install). I am having trouble understanding what is needed to do this and how to go about loading packages, setting variables, etc.</p>

<p>I straight forward approach for a new Python and Graphviz and Spyder user would be great!</p>

<p>Also, apart from just creating and running Graphviz, how can one run Graphviz from python with a pre-generated .gv file?</p>
","5639728","","5639728","","2017-11-02 00:31:20","2020-10-06 13:49:22","How to use Graphviz with Anaconda/Spyder?","<python-3.x><anaconda><graphviz><spyder>","3","0","4","","","CC BY-SA 3.0","0"
"39949845","1","","","2016-10-10 00:18:44","","4","23275","<p>I just downloaded Python and Visual Studio. I'm trying to test the debugging feature for a simple ""Hello World"" script and I'm receiving this error: </p>

<blockquote>
  <p>Failed to launch the Python Process, please validate the path 'python'</p>
</blockquote>

<p>followed by this in the debug console:</p>

<blockquote>
  <p>Error: spawn python ENOENT</p>
</blockquote>

<p>Could someone please help me out and tell me how to fix this?</p>

<p>I'm running on windows 10.</p>

<p>Thanks!</p>
","6946633","","6765643","","2016-10-10 04:29:06","2017-12-20 19:25:59","Visual Studio Python ""Failed to launch the Python Process, please validate the path 'python'' & Error: spawn python ENOENT","<python-3.x>","6","1","","","","CC BY-SA 3.0","0"
"37601804","1","","","2016-06-02 20:46:00","","5","23270","<p>I am working on converting an <a href=""https://github.com/kabniel/last2libre/blob/master/scrobble.py#L34"" rel=""noreferrer"">existing program</a> from Python2 to Python3. One of the methods in the program authenticates the user with a remote server. It will prompt the user to enter in a password.</p>

<pre><code>def _handshake(self):
    timestamp = int(time.time())
    token = (md5hash(md5hash((self.password).encode('utf-8')).hexdigest()
                + str(bytes('timestamp').encode('utf-8'))))
    auth_url = ""%s/?hs=true&amp;p=1.2&amp;u=%s&amp;t=%d&amp;a=%s&amp;c=%s"" % (self.name,
                                                          self.username,
                                                          timestamp,
                                                          token,
                                                          self.client_code)
    response = urlopen(auth_url).read()
    lines = response.split(""\n"")
    if lines[0] != ""OK"":
        raise ScrobbleException(""Server returned: %s"" % (response,))
    self.session_id = lines[1]
    self.submit_url = lines[3]
</code></pre>

<p>The problem with this method is that after the integer is converted to a string, it needs to be encoded. But as far as I can tell, it is already encoded? I found <a href=""https://stackoverflow.com/questions/31161243/python-string-argument-without-an-encoding"">this question</a> but I was having a hard time applying that to the context of this program.</p>

<p>This is the line giving me problems.</p>

<ul>
<li><code>+ str(bytes('timestamp').encode('utf-8'))))</code>

<ul>
<li><code>TypeError: string argument without an encoding</code></li>
</ul></li>
</ul>

<p>I have tried playing around with alternate ways of doing this, all with varying types of errors.</p>

<ul>
<li><code>+ str(bytes('timestamp', 'utf-8'))))</code>

<ul>
<li><code>TypeError: Unicode-objects must be encoded before hashing</code></li>
</ul></li>
<li><code>+ str('timestamp', 'utf-8')))</code>

<ul>
<li><code>TypeError: decoding str is not supported</code></li>
</ul></li>
</ul>

<p>I'm still getting started learning Python (but I have beginner to intermediate knowledge of Java), so I am not completely familiar with the language yet. Does anyone have any thoughts on what this issue might be?</p>

<p>Thanks!</p>
","2497452","","5827215","","2016-06-02 20:52:30","2018-02-04 17:52:05","""TypeError: string argument without an encoding"", but the string is encoded?","<python><string><python-3.x><encoding><utf-8>","1","5","2","","","CC BY-SA 3.0","0"
"30929363","1","30929497","","2015-06-19 03:17:38","","8","23188","<p>This is an example from the <em>O'Reilly Cookbook</em> (truncated dataset)</p>

<pre><code>headers = ['Symbol','Price','Date','Time','Change','Volume']
rows = [{'Symbol': 'AA', 'Volume': 181800, 'Change': -0.18,
         'Time': '9:36am', 'Date': '6/11/2007', 'Price': 39.48},
        {'Symbol': 'AIG', 'Volume': 195500, 'Change': -0.15,
         'Time': '9:36am', 'Date': '6/11/2007', 'Price': 71.38} ]

with open('stocks2.csv','w') as f:
    f_csv = csv.DictWriter(f, headers)
    f_csv.writeheader()
    f_csv.writerows(rows)
</code></pre>

<p>the output file has a <code>\n</code> at the end of each line, and apparently one more at the end.  When I bring it into Excel, I get blank lines between each row.   The same if I open it with Notepad++.</p>

<p>But, if I <code>more</code> if from a command line, the <code>\n</code> don't show up.</p>

<p>I saw another Q&amp;A about a <code>\n</code> at the end of a file - but this one is about a <code>\n</code> at the end of each line.   (And I don't see why <code>more</code> doesn't give the <code>\n</code>.)</p>

<p>I plan to bring the file into OpenOffice Calc.</p>
","4771362","","4530863","","2015-06-19 03:47:45","2018-12-02 03:56:22","csv.writerows() puts newline after each row","<python><csv><python-3.x>","3","2","","","","CC BY-SA 3.0","0"
"28396036","1","28396430","","2015-02-08 15:57:26","","23","23169","<p>I'm trying to open and parse a html page. In python 2.7.8 I have no problem:</p>

<pre><code>import urllib
url = ""https://ipdb.at/ip/66.196.116.112""
html = urllib.urlopen(url).read()
</code></pre>

<p>and everything is fine. However I want to move to python 3.4 and there I get HTTP error 403 (Forbidden). My code:</p>

<pre><code>import urllib.request
html = urllib.request.urlopen(url) # same URL as before

File ""C:\Python34\lib\urllib\request.py"", line 153, in urlopen
return opener.open(url, data, timeout)
File ""C:\Python34\lib\urllib\request.py"", line 461, in open
response = meth(req, response)
File ""C:\Python34\lib\urllib\request.py"", line 574, in http_response
'http', request, response, code, msg, hdrs)
File ""C:\Python34\lib\urllib\request.py"", line 499, in error
return self._call_chain(*args)
File ""C:\Python34\lib\urllib\request.py"", line 433, in _call_chain
result = func(*args)
File ""C:\Python34\lib\urllib\request.py"", line 582, in http_error_default
raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
</code></pre>

<p>It work for other URLs which don't use https.</p>

<pre><code>url = 'http://www.stopforumspam.com/ipcheck/212.91.188.166'
</code></pre>

<p>is ok.</p>
","3800773","","2225682","","2015-02-08 16:35:22","2017-06-13 15:41:15","Python 3.4 urllib.request error (http 403)","<python><python-3.x><urllib>","2","1","6","","","CC BY-SA 3.0","0"
"59476165","1","","","2019-12-25 07:29:00","","9","23169","<p>Spyder(python 3.7)</p>

<p>I am facing following errors here. I have already update all library from anaconda prompt. But can't findout the solution of the problem.</p>

<pre><code>from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X_1 = LabelEncoder()
X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])
labelencoder_X_2 = LabelEncoder()

X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])
onehotencoder = OneHotEncoder(categorical_features = [1])
X = onehotencoder.fit_transform(X).toarray()
Traceback (most recent call last):

File ""&lt;ipython-input-4-05deb1f02719&gt;"", line 2, in &lt;module&gt;
onehotencoder = OneHotEncoder(categorical_features = [1])

TypeError: __init__() got an unexpected keyword argument 'categorical_features'
</code></pre>
","9262402","","","","","2020-06-25 08:21:07","TypeError: __init__() got an unexpected keyword argument 'categorical_features'","<python-3.x><ipython><spyder>","10","0","3","","","CC BY-SA 4.0","0"
"58612306","1","61129517","","2019-10-29 17:08:14","","10","23155","<p>I'm setting up an autoclicker in Python 3.8 and I need win32api for GetAsyncKeyState but it always gives me this error:</p>

<pre><code>&gt;&gt;&gt; import win32api
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ImportError: DLL load failed while importing win32api: The specified module could not be found.
</code></pre>

<p>I'm on Windows 10 Home 64x. I've already tried</p>

<pre><code>pip install pypiwin32
</code></pre>

<p>and it successfully installs but nothing changes. I tried uninstalling and re-installing python as well. I also tried installing 'django' in the same way and it actually works when I <code>import django</code>, so i think it's a win32api issue only.</p>

<pre><code>&gt;&gt;&gt; import win32api
</code></pre>

<p>I expect the output to be none, but the actual output is always that error ^^</p>
","12292714","","2745495","","2020-02-28 04:29:34","2020-10-15 16:25:40","How to fix ""ImportError: DLL load failed"" while importing win32api","<python><python-3.x><winapi><pip><pywin32>","9","0","3","","","CC BY-SA 4.0","0"
"38007240","1","38082681","","2016-06-24 06:50:46","","4","23145","<p>I am new at using raspberry pi.
I have a python 3.4 program that connects to a database on hostinger server.
I want to install mysql connector in raspberry pi.I searched a lot but I was not able to find answers . any help would be appreciated</p>
","5990044","","","","","2018-08-08 13:22:46","installing mysql connector for python 3 in raspberry pi","<mysql><python-3.x><raspberry-pi2>","3","1","1","","","CC BY-SA 3.0","0"
"50821312","1","50821362","","2018-06-12 15:54:51","","41","23137","<p>What does -m in <code>python -m pip install &lt;package&gt;</code> mean ? 
or while upgrading pip using <code>python -m pip install --upgrade pip</code>. </p>
","9673915","","","","","2020-04-05 22:02:11","Meaning of python -m flag","<python><python-3.x><pip>","4","6","8","","","CC BY-SA 4.0","0"
"54802616","1","54806079","","2019-02-21 08:39:28","","29","23129","<p>I have a model class of which I want two fields to be a choice fields, so to populate those choices I am using an enum as listed below</p>

<pre><code>#models.py
class Transaction(models.Model):
    trasaction_status = models.CharField(max_length=255, choices=TransactionStatus.choices())
    transaction_type = models.CharField(max_length=255, choices=TransactionType.choices())

#enums.py
class TransactionType(Enum):

    IN = ""IN"",
    OUT = ""OUT""

    @classmethod
    def choices(cls):
        print(tuple((i.name, i.value) for i in cls))
        return tuple((i.name, i.value) for i in cls)

class TransactionStatus(Enum):

    INITIATED = ""INITIATED"",
    PENDING = ""PENDING"",
    COMPLETED = ""COMPLETED"",
    FAILED = ""FAILED""
    ERROR = ""ERROR""

    @classmethod
    def choices(cls):
        print(tuple((i.name, i.value) for i in cls))
        return tuple((i.name, i.value) for i in cls)
</code></pre>

<p>However, when I am trying to access this model through admin I am getting the following error :</p>

<pre><code>Django Version: 1.11
Exception Type: ValueError
Exception Value:    
too many values to unpack (expected 2)
</code></pre>

<p>I followed two articles that described how to use enums:</p>

<ul>
<li><a href=""https://hackernoon.com/using-enum-as-model-field-choice-in-django-92d8b97aaa63"" rel=""noreferrer"">https://hackernoon.com/using-enum-as-model-field-choice-in-django-92d8b97aaa63</a></li>
<li><a href=""https://blog.richard.do/2014/02/18/how-to-use-enums-for-django-field-choices/"" rel=""noreferrer"">https://blog.richard.do/2014/02/18/how-to-use-enums-for-django-field-choices/</a></li>
</ul>
","4755440","","237091","","2019-07-09 16:51:16","2020-07-28 12:32:15","How to use enums as a choice field in django model","<python><django><python-3.x><django-models><enums>","4","2","2","","","CC BY-SA 4.0","0"
"46030481","1","49946274","","2017-09-04 04:57:55","","10","23101","<p>I am trying to convert trained_checkpoint  to final frozen model from the export_inference_graph.py script provided in tensorflow/models,but the following error results.
And yes,I have already setup $PYTHONPATH to ""models/slim"" but still I get this error,can someone help me out?</p>

<pre><code>$ echo $PYTHONPATH
:/home/ishara/tensorflow_models/models:/home/ishara/tensorflow_models/models/slim
</code></pre>

<p>*****************************problem****************************************************************************</p>

<pre><code>$sudo python3 object_detection/export_inference_graph.py  --input_type image_tensor  --pipeline_config_path = ""ssd_inception_v2_pets.config""  --trained_checkpoint_prefix=""output/model.ckpt-78543""  --output_directory=""birds_inference_graph.pb""

Traceback (most recent call last):
  File ""object_detection/export_inference_graph.py"", line 74, in &lt;module&gt;
    from object_detection import exporter
  File ""/usr/local/lib/python3.5/dist-packages/object_detection-0.1-py3.5.egg/object_detection/exporter.py"", line 28, in &lt;module&gt;

  File ""/usr/local/lib/python3.5/dist-packages/object_detection-0.1-py3.5.egg/object_detection/builders/model_builder.py"", line 30, in &lt;module&gt;
  File ""/usr/local/lib/python3.5/dist-packages/object_detection-0.1-py3.5.egg/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py"", line 28, in &lt;module&gt;
ImportError: No module named 'nets'
</code></pre>

<hr>

<p>I have been struggling with this for days now,tried many solutions nothing work
I am using Ubuntu 16.04 with tensorflow-gpu version.</p>
","4112804","","","","","2020-05-22 15:04:17","ImportError: No module named 'nets'","<python><python-3.x><tensorflow><tensorflow-gpu>","9","3","4","","","CC BY-SA 3.0","0"
"45411924","1","45458473","","2017-07-31 09:20:07","","4","23072","<p>I am trying to establish a two-way communication via Python3. There is a laser range finder plugged into one of my USB ports and I'd like to send/receive commands to that. I have a sheet of commands which can be sent and what they would return, so this part is already there. </p>

<p>What I need is a convenient way to do it in real-time. So far I have the following code:</p>

<pre><code>import serial, time

SERIALPORT = ""/dev/ttyUSB0""
BAUDRATE = 115200

ser = serial.Serial(SERIALPORT, BAUDRATE)
ser.bytesize = serial.EIGHTBITS #number of bits per bytes
ser.parity = serial.PARITY_NONE #set parity check: no parity
ser.stopbits = serial.STOPBITS_ONE #number of stop bits
ser.timeout = None          #block read
ser.xonxoff = False     #disable software flow control
ser.rtscts = False     #disable hardware (RTS/CTS) flow control
ser.dsrdtr = False       #disable hardware (DSR/DTR) flow control
ser.writeTimeout = 0     #timeout for write

print (""Starting Up Serial Monitor"")

try:
    ser.open()
except Exception as e:
    print (""Exception: Opening serial port: "" + str(e))

if ser.isOpen():
    try:
        ser.flushInput()
        ser.flushOutput()
        ser.write(""1\r\n"".encode('ascii'))
        print(""write data: 1"")
        time.sleep(0.5)
        numberOfLine = 0
        while True:
            response = ser.readline().decode('ascii')
            print(""read data: "" + response)
            numberOfLine = numberOfLine + 1
            if (numberOfLine &gt;= 5):
                break
        ser.close()
    except Exception as e:
        print (""Error communicating...: "" + str(e))
else:
    print (""Cannot open serial port."")
</code></pre>

<p>So in the above code I am sending ""1"" which should trigger ""getDistance()"" function of the laser finder and return the distance in mm. I tried this on Putty and it works, returns distances up to 4 digits. However, when I launch the above Python script, my output is only the following:</p>

<pre><code>Starting Up Serial Monitor
Exception: Opening serial port: Port is already open.
write data: 1
read data: 
</code></pre>

<p>and it goes forever. There is no read data or whatsoever. </p>

<p>Where am I mistaken?</p>
","6533075","","","","","2019-11-16 21:00:11","Python3 Two-Way Serial Communication: Reading In Data","<python-3.x><serial-port><pyserial>","3","2","3","","","CC BY-SA 3.0","0"
"29409273","1","29409320","","2015-04-02 09:18:22","","8","23060","<p>I take a string of integers as input and there are no spaces or any kind of separator:</p>

<pre><code>12345
</code></pre>

<p>Now I want this string to converted into a list of individual digits</p>

<pre><code>[1,2,3,4,5]
</code></pre>

<p>I've tried both</p>

<pre><code>numlist = map(int,input().split(""""))
</code></pre>

<p>and</p>

<pre><code>numlist = map(int,input().split(""""))
</code></pre>

<p>Both of them give me Empty Separator error. Is there any other function to perform this task?</p>
","3234357","","","","","2015-04-02 09:27:58","How to split string without spaces into list of integers in Python?","<python><python-3.x>","3","0","2","2015-04-02 09:21:51","","CC BY-SA 3.0","0"
"30953615","1","30954122","","2015-06-20 11:46:20","","2","23004","<p>I'm using DJango 1.8 and Python 3.4</p>

<p>When the below view is being ran, Django throws Type Error - Object is not JSON Serializable</p>

<p><strong>Views.py</strong></p>

<pre><code>from django.http import HttpRequest,HttpResponse
from django.http import JsonResponse
from json import dumps

def get_stats(request):
    if request.method == ""POST"":
        srch_dropV = request.POST['srch_dropAJ']
    else:
        srch_dropV = ''
    if(srch_dropV == 'Green'):
        students = GreenBased.objects.all()
    if(srch_dropV == 'Yellow'):
        students = YellowBased.objects.all()
    response_data = {}
    try:
        response_data['result'] = 'Success'
        response_data['message'] = list(students)
    except:
        response_data['result'] = 'Ouch!'
        response_data['message'] = 'Script has not ran correctly'
    return HttpResponse(JsonResponse(response_data), content_type=""application/json"")
</code></pre>

<p>I'm trying to read couple of rows from mysql database and display it on the html file, I'm facing below error message when above view is being ran </p>

<pre><code>TypeError: YellowBased: YelloBased object is not JSON serializable
</code></pre>

<p>In HTML Page, I have a drop down list.. based on the option that is selected, my Ajax would return me the records that were fetched from mysql table.</p>

<p><strong>Models.py</strong></p>

<pre><code>class GreenBased(models.Model):
    NumOfStudents = models.CharField(max_length=300,blank=True)
    Green = models.CharField(max_length=300,blank=True)
    class Meta:
        managed = False
        db_table = ""GreenStats""

class YelloBased(models.Model):
    NumOfStudents = models.CharField(max_length=300,blank=True)
    Yellow = models.CharField(max_length=300,blank=True)
    class Meta:
        managed = False
        db_table = ""YellowStats""
</code></pre>

<p>GreenStats and YellowStats tables contains only 2*2 rows in mysql
Can someone please help me to identify this issue ?</p>
","3128771","","4724196","","2015-06-20 11:53:57","2019-01-08 07:58:25","TypeError: object is not JSON serializable in DJango 1.8 Python 3.4","<python><json><django><python-3.x><serialization>","1","4","3","","","CC BY-SA 3.0","0"
"54700726","1","","","2019-02-14 23:36:33","","9","22969","<p>Does anyone know how to get a command shortcut to work for block indenting and un-indenting in Jupyter notebooks?
 
In the Jupiter notebook command group there is a command “automatically indent selection”. When I put in a command mode control-/  for that command the notebook does block commenting. 
 
 
I don’t see any other command that refers to indenting.
 
I can’t seem to figure this </p>
","6182064","","","","","2019-10-08 22:31:05","Block indent jupyter notebook","<python-3.x><jupyter-notebook><jupyter>","1","3","3","","","CC BY-SA 4.0","0"
"49471867","1","49471944","","2018-03-25 02:11:37","","9","22931","<p>So I've had Python 3.6 on my Windows 10 computer for a while now, and today I just downloaded and installed the <code>graphviz 0.8.2</code> (<a href=""https://pypi.python.org/pypi/graphviz"" rel=""noreferrer"">https://pypi.python.org/pypi/graphviz</a>) package via the admin commandline with:</p>

<p><code>pip3 install graphviz</code></p>

<p>It was only after this point that I downloaded the Graphviz 2.38 MSI installer file and installed the program at:</p>

<p><code>C:\Program Files (x86)\Graphviz2.38</code></p>

<p>So then I tried to run this simple Python program:</p>

<pre><code>from graphviz import Digraph

dot = Digraph(comment=""The round table"")
dot.node('A', 'King Arthur')
dot.node('B', 'Sir Bedevere the Wise')
dot.node('L', 'Sir Lancelot the Brave')
dot.render('round-table.gv', view=True)
</code></pre>

<p>But unfortunately, I received the following error when I try to run my Python program from commandline:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\graphviz\backend.py"", line 124, in render
    subprocess.check_call(args, startupinfo=STARTUPINFO, stderr=stderr)
  File ""C:\Program Files\Python36\lib\subprocess.py"", line 286, in check_call
    retcode = call(*popenargs, **kwargs)
  File ""C:\Program Files\Python36\lib\subprocess.py"", line 267, in call
    with Popen(*popenargs, **kwargs) as p:
  File ""C:\Program Files\Python36\lib\subprocess.py"", line 709, in __init__
    restore_signals, start_new_session)
  File ""C:\Program Files\Python36\lib\subprocess.py"", line 997, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] The system cannot find the file specified

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\foldername\testing.py"", line 11, in &lt;module&gt;
    dot.render('round-table.gv', view=True)
  File ""C:\Program Files\Python36\lib\site-packages\graphviz\files.py"", line 176, in render
    rendered = backend.render(self._engine, self._format, filepath)
  File ""C:\Program Files\Python36\lib\site-packages\graphviz\backend.py"", line 127, in render
    raise ExecutableNotFound(args)
graphviz.backend.ExecutableNotFound: failed to execute ['dot', '-Tpdf', '-O', 'round-table.gv'], make sure the Graphviz executables are on your systems' PATH
</code></pre>

<p>Notice how what I've asked seems VERY similar to this question asked here:
<a href=""https://stackoverflow.com/questions/35064304/runtimeerror-make-sure-the-graphviz-executables-are-on-your-systems-path-aft"">&quot;RuntimeError: Make sure the Graphviz executables are on your system&#39;s path&quot; after installing Graphviz 2.38</a></p>

<p>But for some reason, adding those paths (suggested in the solutions at the link above) to the system variables isn't working, and I don't know why!  I tried restarting the computer after adding the paths as well, still to no success.  See the image below:</p>

<p><a href=""https://i.stack.imgur.com/VYRrK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/VYRrK.png"" alt=""enter image description here""></a></p>

<p>Although the other suggested solution, which was to add these few lines in front of my Python code, <em>did</em> work:</p>

<pre><code>import os
os.environ[""PATH""] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'
</code></pre>

<p>But here's the issue: I don't understand why adding to the environment variables didn't work, and this is my primary concern.  <strong>So my question is this: why did adding those lines of code in front of the Python script work but changing the environment variables didn't?</strong>  What do I need to do to get my script to run without adding those lines of code in front?</p>
","8948670","","","","","2018-03-25 02:27:39","Installing Graphviz for use with Python 3 on Windows 10","<python><python-3.x><installation><pip><graphviz>","1","0","1","","","CC BY-SA 3.0","0"
"40527769","1","40541336","","2016-11-10 12:34:37","","11","22927","<p>I have been trying to remove the black background from the grabcut output using python opencv. </p>

<pre><code>import numpy as np
import cv2

img = cv2.imread(r'myfile_1.png')
mask = np.zeros(img.shape[:2],np.uint8)

bgdModel = np.zeros((1,65),np.float64)
fgdModel = np.zeros((1,65),np.float64)

rect = (1,1,665,344)
cv2.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)

mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')
img = img*mask2[:,:,np.newaxis]

cv2.imshow('img',img)
cv2.imwrite('img.png',img)
cv2.waitKey(0)
cv2.destroyAllWindows()
</code></pre>

<p>Above code I had written to save the grabcut output. Please suggest, How I can remove the black background and make it transparent?</p>

<p><a href=""https://i.stack.imgur.com/M4rgG.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/M4rgG.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/2lAK2.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/2lAK2.png"" alt=""enter image description here""></a></p>
","3243040","","","","","2018-01-26 07:37:45","Removing black background and make transparent from grabcut output in python open cv","<python-3.x><opencv><opencv3.0>","2","2","10","","","CC BY-SA 3.0","0"
"33139020","1","33139049","","2015-10-15 02:52:25","","66","22911","<p>Python can multiply strings like so:</p>

<pre><code>Python 3.4.3 (default, Mar 26 2015, 22:03:40)
[GCC 4.9.2] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; x = 'my new text is this long'
&gt;&gt;&gt; y = '#' * len(x)
&gt;&gt;&gt; y
'########################'
&gt;&gt;&gt;
</code></pre>

<p>Can Golang do the equivalent somehow?</p>
","627492","","1705598","","2016-10-01 20:51:23","2020-08-19 13:04:18","Can Golang multiply strings like Python can?","<python><string><python-3.x><go>","3","0","16","","","CC BY-SA 3.0","0"
"50064646","1","50098044","","2018-04-27 14:32:36","","8","22844","<p>I installed apache-spark and pyspark on my machine (Ubuntu), and in Pycharm, I also updated the environment variables (e.g. spark_home, pyspark_python).
I'm trying to do:</p>

<pre><code>import os, sys
os.environ['SPARK_HOME'] = "".../spark-2.3.0-bin-hadoop2.7""
sys.path.append("".../spark-2.3.0-bin-hadoop2.7/bin/pyspark/"")
sys.path.append("".../spark-2.3.0-bin-hadoop2.7/python/lib/py4j-0.10.6-src.zip"")
from pyspark import SparkContext
from pyspark import SparkConf
sc = SparkContext('local[2]')
words = sc.parallelize([""scala"", ""java"", ""hadoop"", ""spark"", ""akka""])
print(words.count())
</code></pre>

<p>But, I receive some weird warnings:</p>

<pre><code>py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: java.lang.IllegalArgumentException
at org.apache.xbean.asm5.ClassReader.&lt;init&gt;(Unknown Source)
at org.apache.xbean.asm5.ClassReader.&lt;init&gt;(Unknown Source)
at org.apache.xbean.asm5.ClassReader.&lt;init&gt;(Unknown Source)
at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:46)
at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:449)
at org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:432)
at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
at org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:432)
at org.apache.xbean.asm5.ClassReader.a(Unknown Source)
at org.apache.xbean.asm5.ClassReader.b(Unknown Source)
at org.apache.xbean.asm5.ClassReader.accept(Unknown Source)
at org.apache.xbean.asm5.ClassReader.accept(Unknown Source)
at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:262)
at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:261)
at scala.collection.immutable.List.foreach(List.scala:381)
at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:261)
at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:159)
at org.apache.spark.SparkContext.clean(SparkContext.scala:2292)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2066)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)
at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
at org.apache.spark.rdd.RDD.collect(RDD.scala:938)
at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)
at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.base/java.lang.reflect.Method.invoke(Method.java:564)
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
at py4j.Gateway.invoke(Gateway.java:282)
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
at py4j.commands.CallCommand.execute(CallCommand.java:79)
at py4j.GatewayConnection.run(GatewayConnection.java:214)
at java.base/java.lang.Thread.run(Thread.java:844)
</code></pre>

<p>How can I solve this problem?</p>
","9692180","","-1","","2018-04-30 06:14:58","2020-01-28 02:03:28","py4j.protocol.Py4JJavaError occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe","<python-3.x><apache-spark><pyspark><pycharm><py4j>","7","0","5","","","CC BY-SA 3.0","0"
"49005651","1","","","2018-02-27 09:48:11","","138","22814","<p>This question is motivated by my another question: <a href=""https://stackoverflow.com/questions/48989065/how-to-await-in-cdef"">How to await in cdef?</a></p>

<p>There are tons of articles and blog posts on the web about <code>asyncio</code>, but they are all very superficial.  I couldn't find any information about how <code>asyncio</code> is actually implemented, and what makes I/O asynchronous.  I was trying to read the source code, but it's thousands of lines of not the highest grade C code, a lot of which deals with auxiliary objects, but most crucially, it is hard to connect between Python syntax and what C code it would translate into.</p>

<p>Asycnio's own documentation is even less helpful.  There's no information there about how it works, only some guidelines about how to use it, which are also sometimes misleading / very poorly written.</p>

<p>I'm familiar with Go's implementation of coroutines, and was kind of hoping that Python did the same thing.  If that was the case, the code I came up in the post linked above would have worked.  Since it didn't, I'm now trying to figure out why.  My best guess so far is as follows, please correct me where I'm wrong:</p>

<ol>
<li>Procedure definitions of the form <code>async def foo(): ...</code> are actually interpreted as methods of a class inheriting <code>coroutine</code>.</li>
<li>Perhaps, <code>async def</code> is actually split into multiple methods by <code>await</code> statements, where the object, on which these methods are called is able to keep track of the progress it made through the execution so far.</li>
<li>If the above is true, then, essentially, execution of a coroutine boils down to calling methods of coroutine object by some global manager (loop?).</li>
<li>The global manager is somehow (how?) aware of when I/O operations are performed by Python (only?) code and is able to choose one of the pending coroutine methods to execute after the current executing method relinquished control (hit on the <code>await</code> statement).</li>
</ol>

<p>In other words, here's my attempt at ""desugaring"" of some <code>asyncio</code> syntax into something more understandable:</p>

<pre><code>async def coro(name):
    print('before', name)
    await asyncio.sleep()
    print('after', name)

asyncio.gather(coro('first'), coro('second'))

# translated from async def coro(name)
class Coro(coroutine):
    def before(self, name):
        print('before', name)

    def after(self, name):
        print('after', name)

    def __init__(self, name):
        self.name = name
        self.parts = self.before, self.after
        self.pos = 0

    def __call__():
        self.parts[self.pos](self.name)
        self.pos += 1

    def done(self):
        return self.pos == len(self.parts)


# translated from asyncio.gather()
class AsyncIOManager:

    def gather(*coros):
        while not every(c.done() for c in coros):
            coro = random.choice(coros)
            coro()
</code></pre>

<p>Should my guess prove correct: then I have a problem.  How does I/O actually happen in this scenario? In a separate thread?  Is the whole interpreter suspended and I/O happens outside the interpreter?  What exactly is meant by I/O?  If my python procedure called C <code>open()</code> procedure, and it in turn sent interrupt to kernel, relinquishing control to it, how does Python interpreter know about this and is able to continue running some other code, while kernel code does the actual I/O and until it wakes up the Python procedure which sent the interrupt originally?  How can Python interpreter in principle, be aware of this happening?</p>
","5691066","","1658617","","2018-06-30 19:41:18","2020-08-17 02:59:25","How does asyncio actually work?","<python><python-3.x><python-asyncio>","4","6","118","","","CC BY-SA 3.0","0"
"28802417","1","28802496","","2015-03-02 03:48:48","","6","22799","<pre><code>Level =""""""
            aaaaaa
            awawa""""""
</code></pre>

<p>I'm wondering how do you count the lines of a multi lined string in python.</p>

<p>Also once you've counted those lines how do you count how many letters are in that line. I'd assume to do this part you'd do <code>len(line_of_string)</code>. </p>
","3578820","","2214462","","2015-03-02 04:25:12","2017-08-19 14:14:47","How to count lines in multi lined strings","<python-3.x>","3","0","1","","","CC BY-SA 3.0","0"
"43124340","1","43127152","","2017-03-30 17:14:16","","2","22790","<p>So when ever I run my program and connect to it with the echo client it gives me this error. </p>

<pre><code>Starting server
Serving on ('127.0.0.1', 8881)
Exception in callback UVTransport._call_connection_made
handle: &lt;Handle UVTransport._call_connection_made&gt;
Traceback (most recent call last):
File ""uvloop/cbhandles.pyx"", line 52, in uvloop.loop.Handle._run (uvloop/loop.c:48414)
File ""uvloop/handles/tcp.pyx"", line 141, in uvloop.loop.TCPTransport._call_connection_made (uvloop/loop.c:80488)
File ""uvloop/handles/basetransport.pyx"", line 140, in uvloop.loop.UVBaseTransport._call_connection_made (uvloop/loop.c:65774)
File ""uvloop/handles/basetransport.pyx"", line 137, in uvloop.loop.UVBaseTransport._call_connection_made (uvloop/loop.c:65671)
AttributeError: 'coroutine' object has no attribute 'connection_made'
/home/kenton/Programming/bridal/bridal-middle/middle/lib/server.py:16:RuntimeWarning: coroutine 'handle_request' was never awaited
loop.run_forever()
</code></pre>

<p>As far as I know I have everything that should be awaited awaited. 
Here is the code:</p>

<pre><code>class Server:

    def __init__(self, port):
        asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
    loop = asyncio.get_event_loop()
    server = loop.run_until_complete(self.init(loop))

    print(""Serving on {}"".format(server.sockets[0].getsockname()))
    try:
        loop.run_forever()
    except KeyboardInterrupt:
        print(""\rclosing the server"")
        pass

    server.close()
    loop.run_until_complete(server.wait_closed())
    loop.close()

    async def init(self, loop):
        server = await loop.create_server(self.handle_request, '127.0.0.1', 8881)
        return server

    async def handle_request(self):
        print(datetime.datetime.now())
        reader = asyncio.StreamReader()
        writer = asyncio.StreamWriter()
        data = await reader.read(100)
        message = data.decode()
        addr = writer.get_extra_info('peername')
        code = message.partition('-')
        if code[0].startswith(""1"") or code[0].startswith(""5""):
            accounts = lib.settings.database.accounts
            if code[0] == ""101"":
                result = await self.login_101(code, accounts, writer)
            if code[0] == ""501"":
                result = await accounts.find_one({""username"":code[2]})
                print(""looking up"", code[0])
            #code logic to keep asking for a valid username if one exists
                if result is None:
                    username = code[2]
                    print(username, "" does not exist. Creating"")
                    writer.write(b""0"")
                    await writer.drain()
                    data = await reader.read(100)
                    message = data.decode()
                    code = message.partition('-')
                    post = {""username"":username,""password"":code[0],""email"":code[2]}
                    post_id = await accounts.insert_one(post).inserted_id
                    writer.write(b(post_id))
                    await writer.drain()
        print(""Closed the client socket"")
        writer.close()
        print(datetime.datetime.now())
</code></pre>
","5504795","","513951","","2019-02-08 21:31:01","2019-02-08 21:31:01","python 3.6 coroutine was never awaited","<python><python-3.x><python-asyncio><python-3.6>","1","3","","","","CC BY-SA 4.0","0"
"48606334","1","48606526","","2018-02-04 08:51:08","","2","22781","<p>I'm new to programming and Python is my first language of choice to learn. I think it's generally very easy and logical and maybe that's why this minor understanding-issue is driving me nuts...</p>

<p>Why is ""i"" often used in learning material when illustrating the range function?
Using a random number just seems more logical to me when the range function is dealing with numbers..</p>

<p>Please release me from my pain. </p>
","9311819","","","","","2019-03-11 12:21:53","Why an ""i"" in ""for i in range""?","<python-3.x><range>","3","4","3","","","CC BY-SA 3.0","0"
"29923129","1","","","2015-04-28 15:09:53","","-1","22746","<pre><code>#Imports#
import sys
sys.path.append(""F:\A2\Computing\Comp 4\Python34\Lib\site-packages"")

from tkinter import *
import tkinter as tk
from PIL import Image, ImageTk

#Main Code#


class GUIImage(tk.Tk):
    def __init__(self, master, *pargs):
        tk.Tk.__init__(self, master, *pargs)

        self.image = Image.open(""F:\A2\Computing\Comp 4\Code\main.jpg"")
        self.img_copy= self.image.copy()

        self.background_image = ImageTk.PhotoImage(self.image)

        self.background = Label(self, image=self.background_image)
        self.background.pack(fill=BOTH, expand=YES)
        self.background.bind('&lt;Configure&gt;', self._resize_image)

        self.frames={}

        for F in(mainMenuGUI,addUserGUI,delUserGUI,visitorGUI,prechkGUI,userManual):
            frame = F(container, self)
            self.frames[Frames] = frame
            frame.grid(row=0,column=0,sticky=""nsew"")

    def show_frame(self, cont):
        frame = self.frames[cont]
        frame.tkraise()

    def _resize_image(self,event):

        new_width = event.width
        new_height = event.height

        self.image = self.img_copy.resize((new_width, new_height))

        self.background_image = ImageTk.PhotoImage(self.image)
        self.background.configure(image =  self.background_image)


class mainMenuGUI(tk.Frame):
    def __init__(self,parent,controller):
        tk.Frame.__init(self,parent)
        mainMenuGUI = GUIImage(App)
        mainMenuGUI.pack(fill=BOTH, expand=YES)
        MMText = Label(self, text =""Main Menu"",bg =""#FD7F17"", font = (""Arial Black"",18)).place(relx=.40, rely=.05)
        MMButton1= Button(self, text = ""Add User"", fg = ""white"",bg = ""dark grey"", command = lambda:controller.show_frame(addUserGUI),height = ""1"", width =""10"", font = (""Arial Black"",14)).place(relx =.38 , rely=.14)
        MMButton2= Button(self, text = ""Delete User"", fg = ""white"",bg = ""dark grey"", command = lambda:controller.show_frame(delUserGUI),height = ""1"", width =""10"", font = (""Arial Black"",14)).place(relx =.38 , rely=.28)
        MMButton3= Button(self, text = ""Add Visitor"", fg = ""white"",bg = ""dark grey"", command = lambda:controller.show_frame(VisitorGUI),height = ""1"", width =""10"", font = (""Arial Black"",14)).place(relx =.38 , rely=.42)
        MMButton4= Button(self, text = ""Premises Check"", fg = ""white"",bg = ""dark grey"", command =   lambda:controller.show_frame(prechkGUI) ,height = ""1"", width =""14"", font = (""Arial Black"",14)).place(relx =.32 , rely=.56)
        MMButton5= Button(self, text = ""Vehicle Check"", fg = ""white"",bg = ""dark grey"", command = ""navVehChe"",height = ""1"", width =""14"", font = (""Arial Black"",14)).place(relx =.32 , rely=.70)
        MMButton6= Button(self, text = ""User Manual"", fg = ""white"",bg = ""dark grey"", command =  lambda:controller.show_frame(userManual),height = ""1"", width =""10"", font = (""Arial Black"",14)).place(relx =.38 , rely=.84)

App = GUIImage()
App.mainloop()
</code></pre>

<p>Currently, I get the error</p>

<pre><code>__init__() missing 1 required positional argument: 'master'
</code></pre>

<p>when it is ran, I have a vague idea of why it doesn't work.</p>
","4842633","","100297","","2015-04-28 15:14:22","2015-04-28 15:27:20","Why am I getting TypeError: __init__() missing 1 required positional argument: 'master'?","<python><python-3.x><tkinter><python-imaging-library>","1","4","0","","","CC BY-SA 3.0","0"
"39791243","1","39791424","","2016-09-30 12:11:18","","18","22728","<p>The package manager in Project Interpreter doesn't appear to have any way for me to run a pure pip command so I'm unable to install the wheel as I normally would through command line.</p>

<p>Running through command line installs the wheel on my base python install and not the virtualenv. Help?</p>
","4121552","","","","","2018-11-21 15:29:51","How do I install a .whl file in a PyCharm virtualenv?","<python><python-2.7><python-3.x><pycharm>","4","2","4","","","CC BY-SA 3.0","0"
"38878741","1","38879520","","2016-08-10 16:18:23","","9","22703","<p>When I run this code:</p>

<pre><code>import getpass

p = getpass.getpass(prompt='digite a senha\n')
if p == '12345':
    print('YO Paul')
else:
    print('BRHHH')
print('O seu input foi:', p) # p = seu input
</code></pre>

<p>I got this warning:</p>

<pre><code>Warning (from warnings module):
   File ""/usr/lib/python3.4/getpass.py"", line 63
    passwd = fallback_getpass(prompt, stream)
GetPassWarning: Can not control echo on the terminal. Warning: Password input may be echoed.
</code></pre>
","6693417","","14122","","2016-08-10 17:04:32","2020-05-29 18:32:27","""GetPassWarning: Can not control echo on the terminal"" when running from IDLE","<python><python-3.x>","4","6","3","","","CC BY-SA 3.0","0"
"33770129","1","33770290","","2015-11-18 01:15:26","","18","22697","<p>I'm using urllib.request.urlretrieve to download a file to local.</p>

<pre><code>urllib.request.urlretrieve(url_string,file_name)
</code></pre>

<p>It throws error:</p>

<blockquote>
  <p>ssl.CertificateError was unhandled by user code
  Message: hostname 'foo.net' doesn't match either of 'a248.e.akamai.net', '<em>.akamaihd.net', '</em>.akamaihd-staging.net', '<em>.akamaized.net', '</em>.akamaized-staging.net'</p>
</blockquote>

<p>If you copy the url into Chrome, it will show you a notification and you need to say something like ""keep going to the url"". </p>
","5574453","","2225682","","2015-11-18 01:34:05","2019-11-26 20:41:45","How do I disable the ssl check in python 3.x?","<python><python-3.x><ssl>","2","1","3","","","CC BY-SA 3.0","0"
"51575931","1","53085935","","2018-07-28 23:08:40","","82","22682","<p>I'm currently trying my hands on the new dataclass constructions introduced in Python 3.7. I am currently stuck on trying to do some inheritance of a parent class. It looks like the order of the arguments are botched by my current approach such that the bool parameter in the child class is passed before the other parameters. This is causing a type error.</p>

<pre><code>from dataclasses import dataclass

@dataclass
class Parent:
    name: str
    age: int
    ugly: bool = False

    def print_name(self):
        print(self.name)

    def print_age(self):
        print(self.age)

    def print_id(self):
        print(f'The Name is {self.name} and {self.name} is {self.age} year old')

@dataclass
class Child(Parent):
    school: str
    ugly: bool = True


jack = Parent('jack snr', 32, ugly=True)
jack_son = Child('jack jnr', 12, school = 'havard', ugly=True)

jack.print_id()
jack_son.print_id()
</code></pre>

<p>When I run this code I get this <code>TypeError</code>:</p>

<pre><code>TypeError: non-default argument 'school' follows default argument
</code></pre>

<p>How do I fix this?</p>
","8194007","","100297","","2018-10-31 15:01:17","2020-08-31 19:20:07","Class inheritance in Python 3.7 dataclasses","<python><python-3.x><python-3.7><python-dataclasses>","8","0","23","","","CC BY-SA 4.0","0"
"53621696","1","53993117","","2018-12-04 21:29:04","","6","22644","<p>I have this for-loop. I want <code>i in range(nI)</code> to start from the second number in the <code>I</code> list. Could you guide me?</p>

<pre><code>I=[0,1,2,3,4,5,6]
nI=len(I)
for i in range(nI):
    sum=0
    for v in range(nV):
        for j in range(nJ):
            sum=sum+x1[i][j][v]
return sum
</code></pre>
","7721773","","6347629","","2019-01-01 04:26:59","2020-09-07 10:13:38","How to start from second index for for-loop","<python><python-3.x><numpy>","4","0","","","","CC BY-SA 4.0","0"
"53978542","1","53978543","","2018-12-30 14:42:22","","32","22573","<p>In Python 3.3 ""abstract base classes"" in <code>collections</code> (like <code>MutableMapping</code> or <code>MutableSequence</code>) were moved to second-level module <code>collections.abc</code>. So in Python 3.3+ the real type is <code>collections.abc.MutableMapping</code> and so on. <a href=""https://docs.python.org/3/library/collections.html"" rel=""noreferrer"">Documentation</a> states that the old alias names (e.g. <code>collections.MutableMapping</code>) will be available up to Python 3.7 (currently the latest version), however in 3.8 these aliases will be removed.</p>

<p>Current version of Python 3.7 even produces a warning when you use the alias names:</p>

<pre><code>./scripts/generateBoard.py:145: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  elif isinstance(value, (collections.MutableMapping, collections.MutableSequence)) == True:
</code></pre>

<p>In python 2.7 there is no <code>collections.abc</code>.</p>

<p>How can Python script handle this difference in the most convenient way, when it is meant to be used with (almost) any Python version? I'm looking for a solution which would ideally solve this mess in one central place, without having to use <code>try: ... except: ...</code> all over the script everywhere I need this type?</p>
","157344","","502381","","2018-12-30 16:18:50","2020-09-11 18:17:43","How to use collections.abc from both Python 3.8+ and Python 2.7","<python><python-3.x><python-2.7>","4","0","2","","","CC BY-SA 4.0","0"
"57505071","1","57505099","","2019-08-15 05:06:10","","40","22571","<p>I'm really unsure why this isn't working. Here is the important part of the code (it's from a leetcode challenge).
The first line throws the NameError.</p>

<pre class=""lang-py prettyprint-override""><code>def totalFruit(self, tree: List[int]) -&gt; int:
    pass
</code></pre>

<p>If I try importing <code>List</code> first I get an error <code>No module named 'List'</code>. I'm using Python 3.7.3 from Anaconda.</p>
","1715153","","4518341","","2019-11-27 02:15:24","2020-08-06 10:55:43","NameError: name 'List' is not defined","<python><python-3.x><python-typing>","3","0","3","","","CC BY-SA 4.0","0"
"37734470","1","37751832","","2016-06-09 19:18:38","","9","22565","<p>I am using <code>subprocess.run()</code> for some automated testing. Mostly to automate doing:</p>

<pre><code>dummy.exe &lt; file.txt &gt; foo.txt
diff file.txt foo.txt
</code></pre>

<p>If you execute the above redirection in a shell, the two files are always identical. But whenever <code>file.txt</code> is too long, the below Python code does not return the correct result.</p>

<p>This is the Python code:</p>

<pre class=""lang-python3 prettyprint-override""><code>import subprocess
import sys


def main(argv):

    exe_path = r'dummy.exe'
    file_path = r'file.txt'

    with open(file_path, 'r') as test_file:
        stdin = test_file.read().strip()
        p = subprocess.run([exe_path], input=stdin, stdout=subprocess.PIPE, universal_newlines=True)
        out = p.stdout.strip()
        err = p.stderr
        if stdin == out:
            print('OK')
        else:
            print('failed: ' + out)

if __name__ == ""__main__"":
    main(sys.argv[1:])
</code></pre>

<p>Here is the C++ code in <code>dummy.cc</code>:</p>

<pre class=""lang-c++ prettyprint-override""><code>#include &lt;iostream&gt;


int main()
{
    int size, count, a, b;
    std::cin &gt;&gt; size;
    std::cin &gt;&gt; count;

    std::cout &lt;&lt; size &lt;&lt; "" "" &lt;&lt; count &lt;&lt; std::endl;


    for (int i = 0; i &lt; count; ++i)
    {
        std::cin &gt;&gt; a &gt;&gt; b;
        std::cout &lt;&lt; a &lt;&lt; "" "" &lt;&lt; b &lt;&lt; std::endl;
    }
}
</code></pre>

<p><code>file.txt</code> can be anything like this:</p>

<pre class=""lang-none prettyprint-override""><code>1 100000
0 417
0 842
0 919
...
</code></pre>

<p>The second integer on the first line is the number of lines following, hence here <code>file.txt</code> will be 100,001 lines long. </p>

<p><strong>Question:</strong> Am I misusing subprocess.run() ?  </p>

<p><strong>Edit</strong></p>

<p>My exact Python code after comment (newlines,rb) is taken into account:</p>

<pre><code>import subprocess
import sys
import os


def main(argv):

    base_dir = os.path.dirname(__file__)
    exe_path = os.path.join(base_dir, 'dummy.exe')
    file_path = os.path.join(base_dir, 'infile.txt')
    out_path = os.path.join(base_dir, 'outfile.txt')

    with open(file_path, 'rb') as test_file:
        stdin = test_file.read().strip()
        p = subprocess.run([exe_path], input=stdin, stdout=subprocess.PIPE)
        out = p.stdout.strip()
        if stdin == out:
            print('OK')
        else:
            with open(out_path, ""wb"") as text_file:
                text_file.write(out)

if __name__ == ""__main__"":
    main(sys.argv[1:])
</code></pre>

<p>Here is the first diff:</p>

<p><a href=""https://i.stack.imgur.com/Fk2IW.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Fk2IW.jpg"" alt=""enter image description here""></a></p>

<p>Here is the input file: <a href=""https://drive.google.com/open?id=0B--mU_EsNUGTR3VKaktvQVNtLTQ"" rel=""noreferrer"">https://drive.google.com/open?id=0B--mU_EsNUGTR3VKaktvQVNtLTQ</a></p>
","3064877","","3064877","","2016-06-10 22:10:01","2016-06-13 14:48:49","Why is subprocess.run output different from shell output of same command?","<python><c++><python-3.x><subprocess><io-redirection>","2","8","2","","","CC BY-SA 3.0","0"
"31349438","1","","","2015-07-10 20:09:22","","5","22552","<p>I am trying to install 'os' module and 'os.path' module on a red hat machine. I tries following commands.</p>

<pre><code>pip install os
yum install os
</code></pre>

<p>But I keep gettin the following error</p>

<pre><code>Could not find a version that satisfies the requirement os.path (from versions: )
No matching distribution found for os.path
</code></pre>

<p>I am able to install other modules using aforementioned command but not able to install these.</p>

<p>I need to install both os and os.path.</p>

<p>Using version python 3.4.3</p>
","3664020","","","","","2020-05-19 07:40:20","Can't install modules 'os' and 'os.path'","<python><python-3.x><redhat>","2","2","","","","CC BY-SA 3.0","0"
"46517613","1","","","2017-10-01 22:49:01","","34","22551","<p>There seem to be different implementations of <a href=""https://en.wikipedia.org/wiki/Job_queue"" rel=""noreferrer"">task/job queues</a> for Python 3:</p>

<ol>
<li><a href=""https://github.com/celery/celery"" rel=""noreferrer"">Celery</a>, popular but apparently unmaintained and stale;</li>
<li><a href=""https://github.com/nvie/rq"" rel=""noreferrer"">RQ</a>, of which I have little information;</li>
<li><a href=""https://github.com/closeio/tasktiger"" rel=""noreferrer"">TaskTiger</a>, similarly to RQ I know little about it;</li>
<li><a href=""https://github.com/coleifer/huey"" rel=""noreferrer"">Huey</a> , similarly to RQ I know little about it;</li>
<li><a href=""https://github.com/millerdev/WorQ/"" rel=""noreferrer"">WorQ</a> had its last update in 2016.</li>
</ol>

<p>Then there are “cloud” based solutions like <a href=""https://cloud.google.com/appengine/docs/standard/python/taskqueue/"" rel=""noreferrer"">Google’s Task Queue API</a> or <a href=""http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html"" rel=""noreferrer"">AWS’s Cloud Watch Events</a>, but that’s more of a last resort.</p>

<p>For my project I am looking for a <em>stable</em> and <em>active</em> task queue implementation. I’ve used Celery for the past year, but the lack of support and non-attention to existing bugs is worrisome. </p>

<p>What alternatives exist? </p>
","356307","","356307","","2017-11-27 11:06:44","2018-07-25 07:38:53","Python task queue alternatives and frameworks","<python-3.x><celery>","3","5","15","2019-06-14 21:47:17","","CC BY-SA 3.0","0"
"39534496","1","39534936","","2016-09-16 14:55:18","","6","22542","<p><strong>python 3.5 and windows 10</strong></p>

<p>I installed open cv using this command :</p>

<pre><code>pip install opencv_python-3.1.0-cp35-cp35m-win_amd64.whl
</code></pre>

<p>This command in python works fine :</p>

<pre><code>import cv2
</code></pre>

<p>But when i want to import cv2.cv :</p>

<pre><code>import cv2.cv as cv
</code></pre>

<p>This error comes up :</p>

<pre><code>import cv2.cv as cv
ImportError: No module named 'cv2.cv'; 'cv2' is not a package
</code></pre>

<p>So what is the problem and how can i fix it?</p>
","6788370","","","","","2017-10-31 19:06:33","ImportError: No module named cv2.cv","<python><python-3.x><opencv><windows-10>","1","4","1","","","CC BY-SA 3.0","0"
"47926088","1","47926698","","2017-12-21 13:32:18","","3","22541","<p>C:\Users\vipul>pip install webbrowser
Collecting webbrowser
  Could not find a version that satisfies the requirement webbrowser (from versions: )
No matching distribution found for webbrowser</p>

<p>whenever I try to install I get this error</p>
","7362045","","","","","2017-12-21 14:07:43","how to get webbrowser module for python 3.6 using pip ?","<python-3.x><python-3.6><python-webbrowser>","1","1","","","","CC BY-SA 3.0","0"
"44864633","1","44864639","","2017-07-01 19:27:06","","9","22525","<p>The following list is given:</p>

<pre><code>lst = [3, 7, -10]
</code></pre>

<p>I want to find the maximum value of absolute value. For the above list it will be 10 (abs(-10) = 10).</p>

<p>I can do it as follows:</p>

<pre><code>max_abs_value = lst[0]
for num in lst:
    if abs(num) &gt; max_abs_value:
        max_abs_value = abs(num)
</code></pre>

<p>What are better ways of solving this problem?</p>
","3095195","","3095195","","2017-10-11 08:32:24","2017-10-11 08:32:24","Pythonic way to find maximum absolute value of list","<python><python-3.x>","4","0","2","","","CC BY-SA 3.0","0"
"44855603","1","47952913","","2017-06-30 22:31:01","","15","22466","<p>I'm having trouble using buckets in my Tensorflow model. When I run it with <code>buckets = [(100, 100)]</code>, it works fine. When I run it with <code>buckets = [(100, 100), (200, 200)]</code> it doesn't work at all (stacktrace at bottom).</p>

<p>Interestingly, running Tensorflow's Seq2Seq tutorial gives the same kind of issue with a nearly identical stacktrace. For testing purposes, the link to the repository is <a href=""https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate"" rel=""noreferrer"">here</a>. </p>

<p><strong>I'm not sure what the issue is, but having more than one bucket always seems to trigger it.</strong></p>

<p>This code won't work as a standalone, but this is the function where it is crashing - remember that changing <code>buckets</code> from <code>[(100, 100)]</code> to <code>[(100, 100), (200, 200)]</code> triggers the crash.</p>

<pre><code>class MySeq2Seq(object):
    def __init__(self, source_vocab_size, target_vocab_size, buckets, size, num_layers, batch_size, learning_rate):
        self.source_vocab_size = source_vocab_size
        self.target_vocab_size = target_vocab_size
        self.buckets = buckets
        self.batch_size = batch_size

        cell = single_cell = tf.nn.rnn_cell.GRUCell(size)
        if num_layers &gt; 1:
            cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)

        # The seq2seq function: we use embedding for the input and attention
        def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):
            return tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(
                encoder_inputs, decoder_inputs, cell,
                num_encoder_symbols=source_vocab_size,
                num_decoder_symbols=target_vocab_size,
                embedding_size=size,
                feed_previous=do_decode)

        # Feeds for inputs
        self.encoder_inputs = []
        self.decoder_inputs = []
        self.target_weights = []
        for i in range(buckets[-1][0]):
            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=""encoder{0}"".format(i)))
        for i in range(buckets[-1][1] + 1):
            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=""decoder{0}"".format(i)))
            self.target_weights.append(tf.placeholder(tf.float32, shape=[None], name=""weight{0}"".format(i)))

        # Our targets are decoder inputs shifted by one
        targets = [self.decoder_inputs[i + 1] for i in range(len(self.decoder_inputs) - 1)]
        self.outputs, self.losses = tf.contrib.legacy_seq2seq.model_with_buckets(
            self.encoder_inputs, self.decoder_inputs, targets,
            self.target_weights, [(100, 100)],
            lambda x, y: seq2seq_f(x, y, False))

        # Gradients update operation for training the model
        params = tf.trainable_variables()
        self.updates = []
        for b in range(len(buckets)):
            self.updates.append(tf.train.AdamOptimizer(learning_rate).minimize(self.losses[b]))

        self.saver = tf.train.Saver(tf.global_variables())
</code></pre>

<p>Stacktrace:</p>

<pre><code>    Traceback (most recent call last):
  File ""D:/Stuff/IdeaProjects/myproject/src/main.py"", line 38, in &lt;module&gt;
    model = predict.make_model(input_vocab_size, output_vocab_size, buckets, cell_size, model_layers, batch_size, learning_rate)
  File ""D:\Stuff\IdeaProjects\myproject\src\predictor.py"", line 88, in make_model
    size=cell_size, num_layers=model_layers, batch_size=batch_size, learning_rate=learning_rate)
  File ""D:\Stuff\IdeaProjects\myproject\src\predictor.py"", line 45, in __init__
    lambda x, y: seq2seq_f(x, y, False))
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\legacy_seq2seq\python\ops\seq2seq.py"", line 1206, in model_with_buckets
    decoder_inputs[:bucket[1]])
  File ""D:\Stuff\IdeaProjects\myproject\src\predictor.py"", line 45, in &lt;lambda&gt;
    lambda x, y: seq2seq_f(x, y, False))
  File ""D:\Stuff\IdeaProjects\myproject\src\predictor.py"", line 28, in seq2seq_f
    feed_previous=do_decode)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\contrib\legacy_seq2seq\python\ops\seq2seq.py"", line 848, in embedding_attention_seq2seq
    encoder_cell = copy.deepcopy(cell)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 161, in deepcopy
    y = copier(memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\layers\base.py"", line 476, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 215, in _deepcopy_list
    append(deepcopy(a, memo))
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 180, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 280, in _reconstruct
    state = deepcopy(state, memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 150, in deepcopy
    y = copier(x, memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 240, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python36\lib\copy.py"", line 169, in deepcopy
    rv = reductor(4)
TypeError: can't pickle _thread.lock objects
</code></pre>
","6772171","","712995","","2017-12-23 14:24:07","2019-06-10 14:17:13","TypeError: can't pickle _thread.lock objects in Seq2Seq","<python-3.x><tensorflow><nlp><lstm><sequence-to-sequence>","2","0","2","","","CC BY-SA 3.0","0"
"47694421","1","","","2017-12-07 11:46:32","","16","22463","<p>I'm attempting to write a program that places text onto an image, I'm trying to get my head round PIL and have run into the error: OSError: cannot open resource. This is my first python program so apologies if the error is obvious.</p>

<pre><code>from PIL import Image
from PIL import ImageDraw
from PIL import ImageFont


im = Image.open(""example.jpg"")
font_type = ImageFont.truetype(""Arial.ttf"", 18)
draw = ImageDraw.Draw(im)
draw.text(xy=(50, 50), text= ""Text One"", fill =(255,69,0), font = font_type)
im.show()
</code></pre>

<p>I get the error:</p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\laurence.maskell\Desktop\attempt.py"", line 7, in &lt;module&gt;
font_type = ImageFont.truetype(""Arial.ttf"", 18)
File ""C:\Python34\lib\site-packages\PIL\ImageFont.py"", line 259, in truetype
return FreeTypeFont(font, size, index, encoding, layout_engine)
File ""C:\Python34\lib\site-packages\PIL\ImageFont.py"", line 143, in __init__
self.font = core.getfont(font, size, index, encoding, 
layout_engine=layout_engine)
OSError: cannot open resource
</code></pre>
","9067100","","","","","2020-08-01 00:17:46","PIL Issue, OSError: cannot open resource","<image><python-3.x><fonts><python-imaging-library>","6","1","3","","","CC BY-SA 3.0","0"
"34692009","1","34695096","","2016-01-09 09:52:09","","13","22461","<p>I want to download image file from a url using python module ""urllib.request"", which works for some website (e.g. mangastream.com), but does not work for another (mangadoom.co) receiving error ""HTTP Error 403: Forbidden"". What could be the problem for the latter case and how to fix it?</p>

<p>I am using python3.4 on OSX. </p>

<pre><code>import urllib.request

# does not work
img_url = 'http://mangadoom.co/wp-content/manga/5170/886/005.png'
img_filename = 'my_img.png'
urllib.request.urlretrieve(img_url, img_filename)
</code></pre>

<p>At the end of error message it said:</p>

<pre><code>... 
HTTPError: HTTP Error 403: Forbidden
</code></pre>

<p>However, it works for another website</p>

<pre><code># work
img_url = 'http://img.mangastream.com/cdn/manga/51/3140/006.png'
img_filename = 'my_img.png'
urllib.request.urlretrieve(img_url, img_filename)
</code></pre>

<p>I have tried the solutions from the post below, but none of them works on mangadoom.co.</p>

<p><a href=""https://stackoverflow.com/questions/3042757/downloading-a-picture-via-urllib-and-python"">Downloading a picture via urllib and python</a></p>

<p><a href=""https://stackoverflow.com/questions/1394721/how-do-i-copy-a-remote-image-in-python"">How do I copy a remote image in python?</a></p>

<p>The solution here also does not fit because my case is to download image.
<a href=""https://stackoverflow.com/questions/13303449/urllib2-httperror-http-error-403-forbidden"">urllib2.HTTPError: HTTP Error 403: Forbidden</a></p>

<p>Non-python solution is also welcome. Your suggestion will be very appreciated. </p>
","1613297","","-1","","2017-05-23 11:54:22","2016-04-16 12:05:56","download image from url using python urllib but receiving HTTP Error 403: Forbidden","<image><python-3.x><url><download><urllib>","3","2","5","","","CC BY-SA 3.0","0"
"44299666","1","44300704","","2017-06-01 06:06:15","","24","22455","<pre><code>import tensorflow as tf
x = tf.constant(35, name='x')
y = tf.Variable(x + 5, name='y')
# model = tf.global_variables_initializer()
with tf.Session() as session:
        print(""x = "", session.run(x)) 
        # session.run(model)
        print(""y = "", session.run(y))
</code></pre>

<p>I was not able to understand when <code>global_variables_initializer()</code> is actually required. In the above code, if we uncomment lines 4 &amp; 7, I can execute the code and see the values. If I run as-is, I see a crash.</p>

<p>My question is which variables it is initializing. <code>x</code> is a constant which does not need initialization and <code>y</code> is variable which is not being initialized but is used as an arithmetic operation.</p>
","3890359","","5657159","","2018-05-15 12:38:25","2018-05-15 12:38:25","When global_variables_initializer() is actually required","<python><python-3.x><tensorflow><initializer>","4","0","5","","","CC BY-SA 4.0","0"
"45433995","1","45435335","","2017-08-01 09:27:34","","3","22453","<p>I've tried many code to access and read email content, for example, about Gmail i only can do authentication and with Outlook i have code that can read email but it's encrypted...but now it only access email and doesn't output with encrypted information. So i need help to solve this. Thanks</p>

<p><strong>Here's the code:</strong></p>

<pre><code>import imaplib
import base64 

email_user = input('Email: ')
email_pass = input('Password: ')

M = imaplib.IMAP4_SSL('imap-mail.outlook.com', 993)
M.login(email_user, email_pass)
M.select()
typ, data = M.search(None, 'ALL')
for num in data[0].split():
    typ, data = M.fetch(num, '(RFC822)')
    num1 = base64.b64decode(num1)
    data1 = base64.b64decode(data)
    print('Message %s\n%s\n' % (num, data[0][1]))
M.close()
M.logout()
</code></pre>
","8398589","","","","","2019-12-04 09:08:04","How to read email content in Python 3","<python><python-3.x><email><base64><imaplib>","2","2","2","","","CC BY-SA 3.0","0"
"39284842","1","39284955","","2016-09-02 05:25:37","","15","22434","<p>I have created a order dictionary and could not get the index out of it.
I have gone through the below url but not working.</p>
<blockquote>
<p><a href=""https://stackoverflow.com/questions/15114843/accessing-dictionary-value-by-index-in-python"">Accessing dictionary value by index in python</a></p>
</blockquote>
<p>Here is my code and output.</p>
<pre><code>line_1 = OrderedDict((('A1', &quot;Miyapur&quot;), ('A2', &quot;JNTU College&quot;), ('A3', &quot;KPHB Colony&quot;),
                ('A4', &quot;Kukatpally&quot;), ('A5', &quot;Balanagar&quot;), ('A6', &quot;Moosapet&quot;),
                ('A7', &quot;Bharat Nagar&quot;), ('A8', &quot;Erragadda&quot;), ('A9', &quot;ESI Hospital&quot;),
                ('A10', &quot;S R Nagar&quot;), ('X1', &quot;Ameerpet&quot;), ('A12', &quot;Punjagutta&quot;),
                ('A13', &quot;Irrum Manzil&quot;), ('A14', &quot;Khairatabad&quot;), ('A15', &quot;Lakdikapul&quot;),
                ('A16', &quot;('Assembly&quot;), ('A17', &quot;Nampally&quot;), ('A18', &quot;Gandhi Bhavan&quot;),
                ('A19', &quot;Osmania Medical College&quot;), ('X2', &quot;MG Bus station&quot;), ('A21', &quot;Malakpet&quot;),
                ('A22', &quot;New Market&quot;), ('A23', &quot;Musarambagh&quot;), ('A24', &quot;Dilsukhnagar&quot;),
                ('A25', &quot;Chaitanyapuri&quot;), ('A26', &quot;Victoria Memorial&quot;), ('A27', &quot;L B Nagar&quot;)))

print(line_1.values()[1])
print(line_1[1])
print(line_1.keys()[1])
</code></pre>
<p>All the above options are not working as mentioned in the referenced link.
Any guidance is highly appreciated.
Here is the output for each print statement in the given order.</p>
<blockquote>
<p>TypeError: 'odict_values' object does not support indexing</p>
<p>KeyError: 1</p>
<p>TypeError: 'odict_keys' object does not support indexing</p>
</blockquote>
","5060926","","-1","","2020-06-20 09:12:55","2016-09-02 12:35:57","Order dictionary index in python","<python><python-3.x><dictionary>","2","4","1","","","CC BY-SA 3.0","0"
"49326164","1","","","2018-03-16 17:10:10","","0","22426","<p>I am trying to work with jupyter notebook, but when I open a file I receive the following error: </p>

<p>The kernel has died, and the automatic restart has failed. It is possible the kernel cannot be restarted. If you are not able to restart the kernel, you will still be able to save the notebook, but running code will no longer work until the notebook is reopened.</p>

<p>In the CMD I see the following:</p>

<pre><code>(base) C:\Users\Dan Eran&gt;jupyter notebook
[W 19:05:33.006 NotebookApp] Error loading server extension jupyterlab
    Traceback (most recent call last):
      File ""C:\Users\Dan Eran\AppData\Roaming\Python\Python36\site-packages\notebook\notebookapp.py"", line 1451, in init_server_extensions
        mod = importlib.import_module(modulename)
      File ""C:\Anaconda3\lib\importlib\__init__.py"", line 126, in import_module
        return _bootstrap._gcd_import(name[level:], package, level)
      File ""&lt;frozen importlib._bootstrap&gt;"", line 994, in _gcd_import
      File ""&lt;frozen importlib._bootstrap&gt;"", line 971, in _find_and_load
      File ""&lt;frozen importlib._bootstrap&gt;"", line 953, in _find_and_load_unlocked
    ModuleNotFoundError: No module named 'jupyterlab'
[I 19:05:33.122 NotebookApp] Serving notebooks from local directory: C:\Users\Dan Eran
[I 19:05:33.122 NotebookApp] 0 active kernels
[I 19:05:33.122 NotebookApp] The Jupyter Notebook is running at:
[I 19:05:33.122 NotebookApp] http://localhost:8888/?token=99a355c23c6617857e387f53d0af607ae26b89c20598336e
[I 19:05:33.122 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 19:05:33.122 NotebookApp]

    Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://localhost:8888/?token=99a355c23c6617857e387f53d0af607ae26b89c20598336e
[I 19:05:33.247 NotebookApp] Accepting one-time-token-authenticated connection from ::1
[I 19:05:42.699 NotebookApp] Creating new notebook in
[I 19:05:43.563 NotebookApp] Kernel started: 1433cbbf-f4b9-4dd3-be19-e91d7ee3d82f
Traceback (most recent call last):
  File ""C:\Anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Anaconda3\lib\site-packages\ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""C:\Anaconda3\lib\site-packages\ipykernel\__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""C:\Anaconda3\lib\site-packages\ipykernel\connect.py"", line 13, in &lt;module&gt;
    from IPython.core.profiledir import ProfileDir
ModuleNotFoundError: No module named 'IPython'
[I 19:05:46.555 NotebookApp] KernelRestarter: restarting kernel (1/5), new random ports
Traceback (most recent call last):
  File ""C:\Anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Anaconda3\lib\site-packages\ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""C:\Anaconda3\lib\site-packages\ipykernel\__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""C:\Anaconda3\lib\site-packages\ipykernel\connect.py"", line 13, in &lt;module&gt;
    from IPython.core.profiledir import ProfileDir
ModuleNotFoundError: No module named 'IPython'
[I 19:05:49.591 NotebookApp] KernelRestarter: restarting kernel (2/5), new random ports
Traceback (most recent call last):
  File ""C:\Anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Anaconda3\lib\site-packages\ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""C:\Anaconda3\lib\site-packages\ipykernel\__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""C:\Anaconda3\lib\site-packages\ipykernel\connect.py"", line 13, in &lt;module&gt;
    from IPython.core.profiledir import ProfileDir
ModuleNotFoundError: No module named 'IPython'
[I 19:05:52.620 NotebookApp] KernelRestarter: restarting kernel (3/5), new random ports
Traceback (most recent call last):
  File ""C:\Anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Anaconda3\lib\site-packages\ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""C:\Anaconda3\lib\site-packages\ipykernel\__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""C:\Anaconda3\lib\site-packages\ipykernel\connect.py"", line 13, in &lt;module&gt;
    from IPython.core.profiledir import ProfileDir
ModuleNotFoundError: No module named 'IPython'
[W 19:05:53.595 NotebookApp] Timeout waiting for kernel_info reply from 1433cbbf-f4b9-4dd3-be19-e91d7ee3d82f
[I 19:05:55.632 NotebookApp] KernelRestarter: restarting kernel (4/5), new random ports
WARNING:root:kernel 1433cbbf-f4b9-4dd3-be19-e91d7ee3d82f restarted
Traceback (most recent call last):
  File ""C:\Anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Anaconda3\lib\site-packages\ipykernel_launcher.py"", line 15, in &lt;module&gt;
    from ipykernel import kernelapp as app
  File ""C:\Anaconda3\lib\site-packages\ipykernel\__init__.py"", line 2, in &lt;module&gt;
    from .connect import *
  File ""C:\Anaconda3\lib\site-packages\ipykernel\connect.py"", line 13, in &lt;module&gt;
    from IPython.core.profiledir import ProfileDir
ModuleNotFoundError: No module named 'IPython'
[W 19:05:58.671 NotebookApp] KernelRestarter: restart failed
[W 19:05:58.671 NotebookApp] Kernel 1433cbbf-f4b9-4dd3-be19-e91d7ee3d82f died, removing from map.
ERROR:root:kernel 1433cbbf-f4b9-4dd3-be19-e91d7ee3d82f restarted failed!
[W 19:05:58.705 NotebookApp] 410 DELETE /api/sessions/fd456273-adb3-48cd-92f8-d531c9b8f7a8 (::1): Kernel deleted before session
[W 19:05:58.709 NotebookApp] Kernel deleted before session
[W 19:05:58.709 NotebookApp] 410 DELETE /api/sessions/fd456273-adb3-48cd-92f8-d531c9b8f7a8 (::1) 4.00ms referer=http://localhost:8888/notebooks/Untitled11.ipynb?kernel_name=python3
</code></pre>

<p>I have tried to uninstall and then reinstall some modules. However, I was not able to solve the problem. any ideas? THANKS!! </p>
","9351684","","6763976","","2019-03-28 08:53:50","2019-12-20 19:12:08","Jupyter notebook - Dead Kernel","<python-3.x><jupyter-notebook>","2","0","","","","CC BY-SA 4.0","0"
"35470171","1","35470356","","2016-02-17 23:56:34","","13","22383","<p>Is it possible to click multiply buttons with the same text with <a href=""http://en.wikipedia.org/wiki/Selenium_%28software%29"" rel=""noreferrer"">Selenium</a>?</p>

<p><a href=""https://i.stack.imgur.com/KzAK7.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/KzAK7.png"" alt=""Text = Unlock this result here""></a></p>
","","user5936345","63550","","2016-02-18 01:32:41","2019-05-07 03:11:39","Click button by text using Python and Selenium","<python><python-2.7><python-3.x><selenium>","3","0","2","","","CC BY-SA 3.0","0"
"34417279","1","34418733","","2015-12-22 13:43:57","","4","22356","<p><strong>rocksteady's solution worked</strong></p>

<p>He did originally refer to dictionaries. But the following code to send the JSON string also worked wonders using requests:</p>

<pre><code>import requests

headers = {
  'Authorization': app_token
}
url = api_url + ""/b2api/v1/b2_get_upload_url""
content = json.dumps({'bucketId': bucket_id})

r = requests.post(url, data = content, headers = headers)
</code></pre>

<hr>

<p>I'm working with an API that requires me to send JSON as a POST request to get results. Problem is that Python 3 won't allow me to do this.</p>

<p>The following Python 2 code works fine, in fact it's the official sample:</p>

<pre><code>request = urllib2.Request(
    api_url +'/b2api/v1/b2_get_upload_url',
    json.dumps({ 'bucketId' : bucket_id }),
    headers = { 'Authorization': account_authorization_token }
)
response = urllib2.urlopen(request)
</code></pre>

<p>However, using this code in Python 3 only makes it complain about data being invalid:</p>

<pre><code>import json
from urllib.request import Request, urlopen
from urllib.parse import urlencode

# -! Irrelevant code has been cut out !-

headers = {
  'Authorization': app_token
}
url = api_url + ""/b2api/v1/b2_get_upload_url""

# Tested both with encode and without
content = json.dumps({'bucketId': bucket_id}).encode('utf-8')

request = Request(
  url=url,
  data=content,
  headers=headers
)

response = urlopen(req)
</code></pre>

<p>I've tried doing <code>urlencode()</code>, like you're supposed to. But this returns a 400 status code from the web server, because it's expecting pure JSON. Even if the pure JSON data is invalid, I need to somehow force Python into sending it.</p>

<p><strong>EDIT</strong>: As requested, here are the errors I get. Since this is a flask application, here's a screenshot of the debugger:</p>

<p><a href=""https://chitoge.quad.moe/storage/sharex/2015-12-22_15-10-13.png"" rel=""nofollow"">Screenshot</a></p>

<p>Adding <code>.encode('utf-8')</code> gives me an ""Expected string or buffer"" error</p>

<p><strong>EDIT 2</strong>: <a href=""https://chitoge.quad.moe/storage/sharex/2015-12-22_15-30-02.png"" rel=""nofollow"">Screenshot</a> of the debugger with <code>.encode('utf-8')</code> added</p>
","5449279","","5449279","","2015-12-22 15:28:13","2015-12-22 15:38:52","Sending a JSON string as a post request","<python><json><http><python-3.x>","1","6","2","","","CC BY-SA 3.0","0"
"46258499","1","","","2017-09-16 21:16:03","","10","22336","<p>I have a two requirements .</p>

<p><strong>First Requirement</strong>-I want to read the last line of a file and assign the last value to a variable in python.</p>

<p><strong>Second Requirement</strong>- </p>

<p>Here is my sample file.</p>

<pre><code>&lt;serviceNameame=""demo"" wsdlUrl=""demo.wsdl"" serviceName=""demo""/&gt;
&lt;context:property-placeholder location=""filename.txt""/&gt;
</code></pre>

<p></p>

<p>From this file I want to read the content i.e <strong>filename.txt</strong> which will be after <code>&lt;context:property-placeholder location= .</code>.And want to assign that value to a variable in python.</p>
","4464983","","2683116","","2017-09-18 06:16:53","2020-04-16 05:55:44","read the last line of a file in python","<python><python-2.7><python-3.x><python-requests>","6","1","9","","","CC BY-SA 3.0","0"
"36600583","1","58337431","","2016-04-13 13:53:03","","20","22330","<p>I have a server setup for testing, with a self-signed certificate, and want to be able to test towards it.</p>

<p><strong>How do you ignore SSL verification in the Python 3 version of <code>urlopen</code>?</strong></p>

<p>All information I found regarding this is regarding <code>urllib2</code> or Python 2 in general.</p>

<p><code>urllib</code> in python 3 has changed from <code>urllib2</code>:</p>

<p><strong>Python 2, urllib2</strong>: <code>urllib2.urlopen(url[, data[, timeout[, cafile[, capath[, cadefault[, context]]]]])</code></p>

<p><a href=""https://docs.python.org/2/library/urllib2.html#urllib2.urlopen"" rel=""noreferrer"">https://docs.python.org/2/library/urllib2.html#urllib2.urlopen</a></p>

<p><strong>Python 3</strong>: <code>urllib.request.urlopen(url[, data][, timeout])</code>
<a href=""https://docs.python.org/3.0/library/urllib.request.html?highlight=urllib#urllib.request.urlopen"" rel=""noreferrer"">https://docs.python.org/3.0/library/urllib.request.html?highlight=urllib#urllib.request.urlopen</a></p>

<p>So I know this can be done in Python 2 in the following way. However Python 3 <code>urlopen</code> is missing the context parameter.</p>

<pre><code>import urllib2
import ssl

ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

urllib2.urlopen(""https://your-test-server.local"", context=ctx)
</code></pre>

<p>And yes I know this is a bad idea. This is only meant for testing on a private server.</p>

<p>I could not find how this is supposed to be done in the Python 3 documentation, or in any other question. Even the ones explicitly mentioning Python 3, still had a solution for urllib2/Python 2.</p>
","16432","","","","","2019-10-11 08:44:13","Python 3 urllib ignore SSL certificate verification","<python><python-3.x><ssl><ssl-certificate>","2","1","4","","","CC BY-SA 3.0","0"
"46647744","1","46648010","","2017-10-09 13:32:34","","9","22324","<p>So I'm creating a program to show number systems, however I've run into issues at the first hurdle. The program will take a number from the user and then use that number throughout the program in order to explain several computer science concepts.</p>

<p>When explaining my first section, number systems, the program will say what type of number it is. I'm doing this by converting the string into a float number. If the float number only has '.0' after it then it converts it into a integer.</p>

<p>Currently I'm using this code</p>

<pre><code>while CorrectNumber == False:
try:
    Number = float(NumberString) - 0
    print (Number)
except:
    print (""Error! Not a number!"")
</code></pre>

<p>This is useful as it shows if the user has entered a number or not. However I am unsure how to now check the value after the decimal place to check if I should convert it into a integer or not. Any tips?</p>
","","user6470150","7954504","","2017-10-09 13:35:13","2020-05-14 05:33:56","Checking to see if a string is an integer or float","<python><python-3.x><number-systems>","5","1","3","","","CC BY-SA 3.0","0"
"48548622","1","48556690","","2018-01-31 18:00:01","","32","22291","<p>The problem is this:
I'm trying to replace the <strong>standard queryset</strong>:</p>

<pre><code>queryset: MyModel.objects.all()
</code></pre>

<p>on my:</p>

<pre><code>def get_queryset(self, username=None):
    if username is not None:
        user = UserModel.objects.get(username=username)
        queryset = MyModel.filter(author=user)
        return queryset
    else:
        queryset = MyModel.objects.all()
        return queryset
</code></pre>

<p>when I remove the ""queryset"", and leave only ""get_queryset"", an error appears:</p>

<blockquote>
  <p>AssertionError: <code>base_name</code> argument not specified, and could not automatically determine the name from the viewset, as it does not have a <code>.queryset</code> attribute.</p>
</blockquote>

<p>All together looks so:</p>

<pre><code>class MyModelView(viewsets.ModelViewSet):

permissions_classes = (permissions.IsAuthenticated,)
serializer_class = MyModelleSerializer

def get_queryset(self, username=None):
    if username is not None:
        user = UserModel.objects.get(username=username)
        queryset = MyModel.filter(author=user)
        return queryset
    else:
        queryset = MyModel.objects.all()
        return queryset

lookup_field = 'username'
lookup_value_regex = '[a-zA-Z0-9$&amp;(._)\-]+'
</code></pre>

<p><strong>so How to override method correctly?</strong></p>
","8544536","","4137194","","2018-02-03 11:02:42","2020-03-14 18:52:32","base_name argument not specified, and could not automatically determine the name from the viewset, as it does not have a .queryset attribute","<python-3.x><django-rest-framework><django-views>","2","1","3","","","CC BY-SA 3.0","0"
"32811992","1","32812020","","2015-09-27 19:31:10","","3","22291","<p><strong>What I'm trying to do:</strong> </p>

<p>I'm trying to change the formatting on a csv file from space delimited to comma delimited. </p>

<p><strong>What I've done:</strong></p>

<p>I can ingest the csv file just fine, and print the output row-by-row to the console. That code looks like this:</p>

<pre><code>with open(txtpath, mode='r', newline='') as f:
    fReader = csv.reader(f)
        for rows in fReader:
            print(rows)
</code></pre>

<p>This does exactly what it's supposed to, and spot checking the output confirms that the rows are being read correctly.</p>

<p><strong>The Problem:</strong></p>

<p>According to the official <a href=""https://docs.python.org/3/library/csv.html"" rel=""nofollow"">Python3 Documentation on csv.writer</a>, ""If csvfile is a file object, it should be opened with newline='' <a href=""https://docs.python.org/3/library/csv.html"" rel=""nofollow"">1</a>."" My code looks like this:</p>

<pre><code>with open(csvpath, 'w') as g:
        gWriter = csv.writer(g, newline='')
        gWriter.writerows(rows)
</code></pre>

<p>so all together, it looks like this:</p>

<pre><code>with open(txtpath, mode='r', newline='') as f:
    fReader = csv.reader(f)
    for rows in fReader:
        print(rows)
        with open(csvpath, 'w') as g:
            gWriter = csv.writer(g, newline='')
            gWriter.writerows(rows)
</code></pre>

<p>However, when I run the code with both Pycharm (Anacondas 3.4 selected as project interpreter) and from the console with python3 mycode.py, both results tell me that newline ""is an invalid keyword argument for this function"" and references line 42, which is where my writer object is instantiated. I ran it through the debugger and it craps out as soon as I try to create the writer object. If I don't add the newline argument it asks for a dialect specification, so that doesn't work, either.</p>

<p>I'm sure there's something blindingly obvious that I'm missing, but I can't see it.</p>
","4035775","","","","","2020-10-05 10:54:17","Python3 csv writer failing, exiting on error ""TypeError: 'newline' is an invalid keyword argument for this function","<csv><python-3.x>","4","0","","","","CC BY-SA 3.0","0"
"57117855","1","57235314","","2019-07-19 18:23:33","","15","22274","<p>conda update brakes everything.</p>

<p>conda env-solving took hours and forever </p>

<p>probably due to a conda optimization:
<a href=""https://www.anaconda.com/why-we-removed-the-free-channel-in-conda-4-7/"" rel=""noreferrer"">https://www.anaconda.com/why-we-removed-the-free-channel-in-conda-4-7/</a></p>

<p>but that breaks many systems (like mine):
<a href=""https://github.com/conda/conda/issues/8844"" rel=""noreferrer"">https://github.com/conda/conda/issues/8844</a></p>

<p>got errors like </p>

<pre><code>Collecting package metadata (current_repodata.json): - WARNING conda.models.version:get_matcher(531): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.*, but conda is ignoring the .* and treating it as 1
done
Solving environment: failed with current_repodata.json, will retry with next repodata source.
Initial quick solve with frozen env failed.  Unfreezing env and trying again.
Solving environment: failed with current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
</code></pre>

<p>so I downgraded to conda=4.6</p>

<p>but got </p>

<pre><code>Collecting package metadata: / WARNING conda.core.index:push_record(193): Skipping conda-forge/label/broken/linux-64::ipywidgets-5.2.3-py36_0 due to InvalidSpec: ==1.*
WARNING conda.core.index:push_record(193): Skipping conda-forge/label/broken/linux-64::ipywidgets-5.2.3-py27_0 due to InvalidSpec: ==1.*
WARNING conda.core.index:push_record(193): Skipping conda-forge/label/broken/linux-64::ipywidgets-5.2.3-py35_0 due to InvalidSpec: ==1.*
done
Solving environment: failed

InvalidVersionSpec: Invalid version '==1.*': invalid operator with '.*'
</code></pre>

<p>and </p>

<pre><code>conda install conda=4.7 
Collecting package metadata: failed

CondaUpgradeError: This environment has previously been operated on by a conda version that's newer
than the conda currently being used. A newer version of conda is required.
  target environment location: /home/aeug/conda
  current conda version: 4.6.0
  minimum conda version: 4.7
</code></pre>

<p>Is there anything to fix the system / envs again?</p>

<p>All I can see is to reinstall with an good old conda 4.6.11 <code>https://repo.anaconda.com/miniconda/</code></p>

<hr>

<pre><code>conda install conda=4.6
conda config --set pip_interop_enabled True
</code></pre>

<p>and I could rebuild the env with some errors but it took ~6 hours.</p>
","7084278","","7084278","","2020-05-10 06:25:00","2020-05-10 06:25:00","conda 4.7.7 ->4.6 - Collecting package metadata (current_repodata.json) - (channel conda-forge) ipywidgets-5.2.3-py36_0 due to InvalidSpec: ==1.*","<python><python-3.x><anaconda><conda>","3","1","5","","","CC BY-SA 4.0","0"
"50376990","1","","","2018-05-16 17:36:58","","8","22273","<p>I tried running the following code:</p>

<pre><code>from imblearn import under_sampling, over_sampling
from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=12, ratio = 1.0)
x_SMOTE, y_SMOTE = sm.fit_sample(X, y) 
</code></pre>

<p>which gives me the error message:</p>

<pre><code>ModuleNotFoundError: No module named 'imblearn'
</code></pre>

<p>I have tried installing the imblearn module in multiple ways, they all seem to work (there are no errors given during the installation but when I run the above code, I get an error message).</p>

<p>I tried istalling imblearn using the following suggested in other stackoverflow questions:</p>

<pre><code>pip install -U imbalanced-learn
pip install imblearn
!pip install imblearn
pip install -c glemaitre imbalanced-learn
pip install imblearn==0.0
</code></pre>

<p>None of these seem to help... Any ideas? Thank you!</p>
","7620499","","6573902","","2019-02-18 07:43:52","2020-06-20 20:44:04","ModuleNotFoundError: No module named 'imblearn'","<python-3.x><machine-learning><pip><imblearn>","6","4","1","","","CC BY-SA 4.0","0"
"43871604","1","43871678","","2017-05-09 13:45:56","","33","22272","<p>I am trying to start a webpage using the Django framework. This is my first web development project.</p>

<p>After creating the project, I tried to start an app that utilizes customized users and registration with email validation using django-registration.</p>

<p>This is what happened when I ran <code>manage runserver</code>:</p>

<p><a href=""https://i.stack.imgur.com/hig34.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hig34.png"" alt=""enter image description here""></a></p>

<p>This is what <strong><em>models.py</em></strong> file contains:</p>

<pre><code>from django.db import models
from django.contrib.auth.models import AbstractUser
from django.utils.translation import ugettext_lazy as _
class User(AbstractUser):
    username = models.CharField(max_length=255, unique=True, verbose_name=_(""Username"")) 
    email = models.EmailField(unique=True, verbose_name=_(""Email Address""))
    favorite_animal = models.CharField(max_length=255, verbose_name=_(""Favorite Animal""))
</code></pre>
","7986373","","5776616","","2019-07-03 00:15:12","2019-07-03 00:15:12","ValueError: Dependency on app with no migrations: customuser","<python><django><python-3.x><django-migrations><unhandled-exception>","2","1","6","","","CC BY-SA 4.0","0"
"54138898","1","","","2019-01-11 00:42:53","","10","22252","<p>I am trying to build an <code>.exe</code> file from <code>.py</code> file using <code>pysinstaller</code> and Python 3.7.2.</p>

<p>It worked with Python 3.6; then I re-installed the last version of Python (3.7.2) and tried to generate an exe file, but pyinstaller barfs.</p>

<p>Below is the error report I get.</p>

<pre><code>(venv) C:\Users\user\Desktop\untitled1&gt;pyinstaller test.py

53 INFO: PyInstaller: 3.4
53 INFO: Python: 3.7.2
54 INFO: Platform: Windows-10-10.0.17134-SP0
58 INFO: wrote C:\Users\user\Desktop\untitled1\test.spec
60 INFO: UPX is not available.
61 INFO: Extending PYTHONPATH with paths
['C:\\Users\\user\\Desktop\\untitled1', 'C:\\Users\\user\\Desktop\\untitled1']
61 INFO: checking Analysis
187 INFO: checking PYZ
236 INFO: checking PKG
237 INFO: Building PKG because PKG-00.toc is non existent
238 INFO: Building PKG (CArchive) PKG-00.pkg

Traceback (most recent call last):
  File ""C:\Users\user\Desktop\untitled1\venv\Scripts\pyinstaller-script.py"", line 11, in  &lt;module&gt;
    load_entry_point('PyInstaller==3.4', 'console_scripts', 'pyinstaller')()
  File ""C:\Users\user\Desktop\untitled1\venv\lib\site-packages\PyInstaller\__main__.py"", line 111, in run
    run_build(pyi_config, spec_file, **vars(args))
  File ""C:\Users\user\Desktop\untitled1\venv\lib\site-packages\PyInstaller\__main__.py"", line 63, in run_build
    PyInstaller.building.build_main.main(pyi_config, spec_file, **kwargs)
  File ""C:\Users\user\Desktop\untitled1\venv\lib\site- packages\PyInstaller\building\build_main.py"", line 838, in main
    build(specfile, kw.get('distpath'), kw.get('workpath'), kw.get('clean_build'))   
  File ""C:\Users\user\Desktop\untitled1\venv\lib\site- packages\PyInstaller\building\build_main.py"", line 784, in build
    exec(text, spec_namespace)
  File ""&lt;string&gt;"", line 29, in &lt;module&gt;   
  File ""C:\Users\user\Desktop\untitled1\venv\lib\site-packages\PyInstaller\building\api.py"", line 424, in __init__
    strip_binaries=self.strip, upx_binaries=self.upx,   
  File ""C:\Users\user\Desktop\untitled1\venv\lib\site-packages\PyInstaller\building\api.py"", line 196, in __init__
    self.__postinit__()
  File ""C:\Users\user\Desktop\untitled1\venv\lib\site-packages\PyInstaller\building\datastruct.py"", line 158, in __postinit__
    self.assemble()   
  File ""C:\Users\user\Desktop\untitled1\venv\lib\site-packages\PyInstaller\building\api.py"", line 273, in assemble
    pylib_name = os.path.basename(bindepend.get_python_library_path())   
  File ""C:\Users\user\AppData\Local\Programs\Python\Python37\lib\ntpath.py"", line 214, in basename
    return split(p)[1]
  File ""C:\Users\user\AppData\Local\Programs\Python\Python37\lib\ntpath.py"", line 183, in split
    p = os.fspath(p) TypeError: expected str, bytes or os.PathLike object, not NoneType
</code></pre>

<p>What could be the problem?</p>
","5876713","","705086","","2019-01-11 01:22:14","2020-04-15 15:10:07","An error for generating an exe file using pyinstaller - typeerror: expected str, bytes or os.PathLike object, not NoneType","<python><python-3.x><pyinstaller>","4","1","5","","","CC BY-SA 4.0","0"
"37265229","1","37265693","","2016-05-17 00:26:20","","0","22248","<p>I am creating a program in Python to automatically backup my flash drive to. The first backup, it works great, but when it repeats the process to backup again, I get this error message.  </p>

<pre><code>File ""D:\Programming\_Languages\Python\Automatic_Backup\Automatic_Backup.py"", line 51, in &lt;module&gt;
    shutil.copytree(src, dst)
  File ""C:\Users\carso\AppData\Local\Programs\Python\Python35-32\lib\shutil.py"", line 303, in copytree
    names = os.listdir(src)
FileNotFoundError: [WinError 3] The system cannot find the path specified: ''
</code></pre>

<p>The contents of path_dst.txt are ""C://Users//carso//Dropbox//Backup"" (No quotes). The contents of path_src.txt are ""D://Programming"" (Again, no quotes).</p>

<p>I have been searching to try and find a solution, but I don't know how to fix this. Here is the code to the program, I would appreciate any help.</p>

<pre><code>import shutil
import os
import time

with open('program_started.txt', 'r+') as program_started_file:
    # If program was previously started, runs instructions. If not, continues as normal
    program_started = program_started_file.read()
    if program_started == 'False':
        # Gets src path file and dst path file, write only
        src_file = open('path_src.txt', 'w')
        dst_file = open('path_dst.txt', 'w')
        # Gets src and dst paths
        src_path = input('Enter source path: ') 
        dst_path = input('Enter destination path: ')
        # Writes src and dst paths to txt file
        src_file.write(src_path)
        dst_file.write(dst_path)
        # Moves to beginning of document
        program_started_file.seek(0)
        # Writes 'True' in front of prevous 'False'
        program_started_file.write('True')
        # Removes 'False'
        program_started_file.truncate()
        # Displays 'Completed' message
        print(""Completed getting source and destination paths"")
    elif program_started == 'True':
        # Gets src path file and dst path file, read only
        src_file = open('path_src.txt', 'r')
        dst_file = open('path_dst.txt', 'r')
        # Checks if flash drive is plugged in
        while True:
            if os.system('cd D:') == 0:
                # Stores src path and dst path in string
                src = src_file.read()
                dst = dst_file.read()
                # If a 2nd backup has been made, removes first and renames 2nd
                if os.path.isdir(dst + ""_2"") == True:
                    os.rmdir(dst)
                    os.rename(dst + ""_2"", dst)
                    dst = dst + ""_2""
                 # If only a 1st backup was made, creates a 2nd
                elif os.path.isdir(dst) == True:
                    dst = dst + ""_2""
                # Copies data
                print('Backing up...', end='')
                shutil.copytree(src, dst)
                print('Completed')
                # Sleeps for 20 minutes
                for x in range(1,12):
                    print(""Second: "", x)
                    time.sleep(1)
            else:
                # If no flash drive is detected, tries again in 5 minutes. 
                time.sleep(600)
    else:
        # Error message if program_started.txt != true or false
        print(""Error: program_started.txt must only contain either 'True' or 'False'."")
</code></pre>
","5408062","","","","","2019-02-15 07:04:51","Python: FileNotFoundError: [WinError 3] The system cannot find the path specified: ''","<python><python-3.x><python-3.5>","3","2","","","","CC BY-SA 3.0","0"
"41540751","1","41550042","","2017-01-09 03:46:57","","14","22210","<p>Let's say I'm examining up to 10 clusters, with scipy I usually generate the 'elbow' plot as follows:</p>

<pre><code>from scipy import cluster
cluster_array = [cluster.vq.kmeans(my_matrix, i) for i in range(1,10)]

pyplot.plot([var for (cent,var) in cluster_array])
pyplot.show()
</code></pre>

<p>I have since became motivated to use sklearn for clustering, however I'm not sure how to create the array needed to plot as in the scipy case. My best guess was:</p>

<pre><code>from sklearn.cluster import KMeans

km = [KMeans(n_clusters=i) for i range(1,10)]
cluster_array = [km[i].fit(my_matrix)]
</code></pre>

<p>That unfortunately resulted in an invalid command error. What is the best way sklearn way to go about this?</p>

<p>Thank you</p>
","6505146","","","","","2019-02-11 20:00:34","Sklearn kmeans equivalent of elbow method","<python-3.x><scipy><scikit-learn>","3","0","3","","","CC BY-SA 3.0","0"
"37633550","1","37634202","","2016-06-04 18:16:43","","34","22198","<p>With the 3.5.0 release, <a href=""https://www.python.org/"" rel=""noreferrer"">Python.org</a> has  introduced a distribution billed as  <em>embeddable zip file</em>.</p>

<p>Unfortunately the zipped file comes without a help file  (not even a readme). The <a href=""https://www.python.org/downloads/windows"" rel=""noreferrer"">download page</a> on Python.org just lists it among the downloads. </p>

<p>Apparently this is a portable Python distribution.  It is anyway quite different in structure and size from the standard distribution using the installer. </p>

<p>I realised that it is possible to install pip with <a href=""https://bootstrap.pypa.io/get-pip.py"" rel=""noreferrer"">get-pip.py</a> and, thanks to pip, it is a breeze to add many other application packages, though I am still unable to add Tkinter (adjust slashes according to your shell):</p>

<pre><code>curl https://www.python.org/ftp/python/3.x.x/python-3.x.x-embed-amd64.zip &gt; epython.zip
unzip -o epython.zip -d env1
curl -L https://bootstrap.pypa.io/get-pip.py&gt;env1/get-pip.py
env1/python env1/get-pip.py
</code></pre>

<p>Add what you need, e.g <a href=""https://www.djangoproject.com"" rel=""noreferrer"">django</a>:</p>

<pre><code>env1/python -m pip install django  
</code></pre>

<p>Given the size (6.5 Mega for the 3.5.1-x64), I think that it can be convenient as a means to create isolated environments.</p>

<p>In fact the general Python <a href=""https://docs.python.org/3.5/using/windows.html#embedded-distribution"" rel=""noreferrer"">documentation</a> says that </p>

<blockquote>
  <p>the embedded distribution is (almost) fully isolated from the user’s system, including environment variables, system registry settings, and installed package</p>
</blockquote>

<p>Given this, in Windows there are now two isolated Python environments, the second being the standard
<a href=""https://pypi.python.org/pypi/virtualenv"" rel=""noreferrer"">Virtualenv</a>. The same process in     Virtualenv is like follows:</p>

<pre><code>virtualenv env2
</code></pre>

<p>and for  django  it would be: </p>

<pre><code>env2/Scripts/python -m pip install django  
</code></pre>

<p>Comparing the contents of <code>env1</code> and <code>env2</code>, they appear to have the same files. The only significant  difference is <a href=""https://wiki.python.org/moin/TkInter"" rel=""noreferrer"">Tkinter1</a>, which is anyway not much  significant for desktop apps.</p>

<p>Which is the difference between  Python Virtualenv and Python embeddable?</p>

<p>Specifically, which is the difference between the isolated  web app created with the embeddable zip (<code>env1</code>) and Virtualenv  (<code>env2</code>)?</p>
","1851270","","1851270","","2016-06-05 21:25:11","2020-04-30 17:45:39","Python embeddable zip","<python><python-3.x><pip>","1","1","6","","","CC BY-SA 3.0","0"
"43301841","1","","","2017-04-09 00:38:26","","0","22175","<p>I have a list which hold data that was scraped from a website online. The list is something like this </p>

<pre><code>list1 = ['\nJob Description\n\nDESCRIPTION: Interacts with users and technical team members to analyze requirements and develop
technical design specifications.  Troubleshoot complex issues and make recommendations to improve efficiency and accurac
y. Interpret complex data, analyze results using statistical techniques and provide ongoing reports. Identify, analyze,
and interpret trends or patterns in complex data sets. Filter and ""clean data, review reports, and performance indicator
s to locate and correct code problems. Work closely with management to prioritize business and information needs. Locate
 and define new process improvement opportunities. Employ excellent interpersonal and verbal communication skills necess
ary to effectively coordinate interrelated activities with coworkers, end-users, and management. Works autonomously with
 minimal supervision. Provides technical guidance and mentoring to other team members. Multi tasks and balances multiple
 assignments and priorities. Provides timely status updates.\nQUALIFICATIONS: Proven 5 years working experience as a dat
a analyst Technical expertise regarding data models, database design development, data mining and segmentation technique
s Knowledge of and experience with reporting packages (preferably Microsoft BI Stack), databases (SQL, DB2 etc.), and qu
ery language (SQL) Knowledge of statistics and experience using statistical packages for analyzing large datasets Strong
 analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information wi
th attention to detail and accuracy Adept at queries, report writing and presenting findings\nNTT DATA is a leading IT s.............]
</code></pre>

<blockquote>
  <p>How do I remove ""\n""</p>
</blockquote>

<p>Keeping in mind that it has to be done in a loop while scrapping so that the data is scraped, ""\n"" and the unwanted spaces are removed and the data is pushed into the csv.  </p>
","6287020","","995714","","2017-04-09 00:52:01","2017-10-25 06:27:51","Remove `\n` from a list","<python><python-3.x><web-scraping><beautifulsoup>","2","11","2","2017-04-09 01:16:43","","CC BY-SA 3.0","0"
"44914961","1","44915354","","2017-07-04 23:00:58","","25","22127","<p><strong>How do I get mod_wsgi for Apache2 that was compiled for Python 3.6.1?</strong> </p>

<p>(or any future Python version)</p>

<p>I am using a Python 3.6.1 virtual environment with Django 1.11 and Everything is working according to the Apache error log except that mod_wsgi for Apache 2.4 was compiled for Python/3.5.1+ and is using Python/3.5.2 so my Python 3.6.1 code is failing because I'm using new features not available in 3.5.2</p>

<p>All of the other configurations and installs involved in setting my system up seem to be fine (Running in daemon mode) though mod_wsgi doesn't seem to be using my Python 3.6.1 virtual environment (though it is trying to use it for Django according to the error log)...</p>

<p><strong>I used:</strong> <code>sudo apt-get install libapache2-mod-wsgi-py3</code> to install mod_wsgi for Apache 2.4</p>

<p><strong>I used:</strong> <code>./configure --with-python=/usr/local/bin/python3.6</code> and <code>make</code> with <code>make install</code> to install mod_wsgi for Python 3.6</p>

<p><em>I must be doing something wrong - please correct me!</em></p>

<p>Here is my Apache Error Log (cleaned a bit) - and yes I know it fails on the f"""" string line (python 3.6 feature not in 3.5) </p>

<pre><code>[wsgi:warn] mod_wsgi: Compiled for Python/3.5.1+.
[wsgi:warn] mod_wsgi: Runtime using Python/3.5.2.
[wsgi:warn] AH00489: Apache/2.4.18 (Ubuntu) mod_wsgi/4.3.0 Python/3.5.2 configured -- resuming normal operations
[wsgi:warn] AH00094: Command line: '/usr/sbin/apache2'
[wsgi:error] mod_wsgi (pid=12963): Target WSGI script '/home/jamin/www/dev.tir.com/tir/tir/wsgi.py' cannot be loaded as Python module.
[wsgi:error] mod_wsgi (pid=12963): Exception occurred processing WSGI script '/home/jamin/www/dev.tir.com/tir/tir/wsgi.py'.
[wsgi:error] Traceback (most recent call last):
[wsgi:error]   File ""/home/jamin/www/dev.tir.com/tir/tir/wsgi.py"", line 21, in &lt;module&gt;
[wsgi:error]     application = get_wsgi_application()
[wsgi:error]   File ""/home/jamin/www/dev.tir.com/py361ve/lib/python3.6/site-packages/django/core/wsgi.py"", line 13, in get_wsgi_application
[wsgi:error]     django.setup(set_prefix=False)
[wsgi:error]   File ""/home/jamin/www/dev.tir.com/py361ve/lib/python3.6/site-packages/django/__init__.py"", line 27, in setup
[wsgi:error]     apps.populate(settings.INSTALLED_APPS)
[wsgi:error]   File ""/home/jamin/www/dev.tir.com/py361ve/lib/python3.6/site-packages/django/apps/registry.py"", line 116, in populate
[wsgi:error]     app_config.ready()
[wsgi:error]   File ""/home/jamin/www/dev.tir.com/py361ve/lib/python3.6/site-packages/django/contrib/admin/apps.py"", line 23, in ready
[wsgi:error]     self.module.autodiscover()
[wsgi:error]   File ""/home/jamin/www/dev.tir.com/py361ve/lib/python3.6/site-packages/django/contrib/admin/__init__.py"", line 26, in autodiscover
[wsgi:error]     autodiscover_modules('admin', register_to=site)
[wsgi:error]   File ""/home/jamin/www/dev.tir.com/py361ve/lib/python3.6/site-packages/django/utils/module_loading.py"", line 50, in autodiscover_modules
[wsgi:error]     import_module('%s.%s' % (app_config.name, module_to_search))
[wsgi:error]   File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
[wsgi:error]     return _bootstrap._gcd_import(name[level:], package, level)
[wsgi:error]   File ""&lt;frozen importlib._bootstrap&gt;"", line 986, in _gcd_import
[wsgi:error]   File ""&lt;frozen importlib._bootstrap&gt;"", line 969, in _find_and_load
[wsgi:error]   File ""&lt;frozen importlib._bootstrap&gt;"", line 958, in _find_and_load_unlocked
[wsgi:error]   File ""&lt;frozen importlib._bootstrap&gt;"", line 673, in _load_unlocked
[wsgi:error]   File ""&lt;frozen importlib._bootstrap_external&gt;"", line 665, in exec_module
[wsgi:error]   File ""&lt;frozen importlib._bootstrap&gt;"", line 222, in _call_with_frames_removed
[wsgi:error]   File ""/home/jamin/www/dev.tir.com/tir/company/admin.py"", line 13, in &lt;module&gt;
[wsgi:error]     from .forms import AdminInteractionForm
[wsgi:error]   File ""/home/jamin/www/dev.tir.com/tir/company/forms.py"", line 87
[wsgi:error]     resp = f""Hi {user.first_name}, you'll need to login to send any more suggestions. \\
[wsgi:error]                              \\n\\nFirst Time? Check your email/spam for login instructions from us.""
[wsgi:error]                                                                                        
[wsgi:error]                                                                                                   ^
[wsgi:error] SyntaxError: invalid syntax
</code></pre>

<p>*also tir.com is shorthand for my actual site - i do not have anything to do with that domain</p>
","1840601","","","","","2019-10-13 12:13:45","Install mod_wsgi on Ubuntu with Python 3.6, Apache 2.4, and Django 1.11","<python><django><apache><python-3.x><mod-wsgi>","2","1","8","","","CC BY-SA 3.0","0"
"53217767","1","","","2018-11-08 23:37:20","","25","22127","<p>I am currently on JRE: 1.8.0_181, Python: 3.6.4, spark: 2.3.2</p>

<p>I am trying to execute following code in Python:</p>

<pre><code>from pyspark.sql import SparkSession

spark = SparkSession.builder.appName('Basics').getOrCreate()
</code></pre>

<p>This fails with following error:</p>

<blockquote>
  <p>spark = SparkSession.builder.appName('Basics').getOrCreate()
  Traceback (most recent call last):
    File """", line 1, in 
    File ""C:\Tools\Anaconda3\lib\site-packages\pyspark\sql\session.py"", line 173, in getOrCreate
      sc = SparkContext.getOrCreate(sparkConf)
    File ""C:\Tools\Anaconda3\lib\site-packages\pyspark\context.py"", line 349, in getOrCreate
      SparkContext(conf=conf or SparkConf())
    File ""C:\Tools\Anaconda3\lib\site-packages\pyspark\context.py"", line 118, in <strong>init</strong>
      conf, jsc, profiler_cls)
    File ""C:\Tools\Anaconda3\lib\site-packages\pyspark\context.py"", line 195, in _do_init
      self._encryption_enabled = self._jvm.PythonUtils.getEncryptionEnabled(self._jsc)
    File ""C:\Tools\Anaconda3\lib\site-packages\py4j\java_gateway.py"", line 1487, in <strong>getattr</strong>
      ""{0}.{1} does not exist in the JVM"".format(self._fqn, name))
  <strong>py4j.protocol.Py4JError: org.apache.spark.api.python.PythonUtils.getEncryptionEnabled does not exist in the JVM</strong></p>
</blockquote>

<p>Any one has any idea on what can be a potential issue here? </p>

<p>Appreciate any help or feedback here. Thank you!</p>
","10626408","","","","","2020-08-24 16:13:34","py4j.protocol.Py4JError: org.apache.spark.api.python.PythonUtils.getEncryptionEnabled does not exist in the JVM","<python><python-3.x><pyspark>","6","2","2","","","CC BY-SA 4.0","0"
"41588925","1","41588948","","2017-01-11 10:53:25","","12","22123","<p>If I run <code>pip install Django</code> I get</p>

<blockquote>
  <p>Requirement already satisfied: Django in
  /usr/local/lib/python2.7/dist-packages</p>
</blockquote>

<p>I'd like to use python3.6 instead (which is already installed in <code>/usr/bin/python3.6</code>). What's the correct <code>pip</code> syntax to install the last version of Django on python 3.6?</p>
","5012854","","4952130","","2017-01-11 10:57:51","2017-01-11 11:20:38","pip install Django on python3.6","<python><django><python-3.x><pip><python-3.6>","5","0","2","","","CC BY-SA 3.0","0"
"42222096","1","42225189","","2017-02-14 09:18:01","","7","22027","<p>I work on Ubuntu 14. I install python3 and pip3.
When I try to use pip3, I have this error</p>

<pre><code>Traceback (most recent call last):
  File ""/usr/local/bin/pip3"", line 6, in &lt;module&gt;
    from pkg_resources import load_entry_point
  File ""/usr/local/lib/python3.5/dist-packages/pkg_resources/__init__.py"", line 70, i
n &lt;module&gt;
    import packaging.version
ImportError: No module named 'packaging'
</code></pre>

<p>Does someone know what is the issue?</p>

<p>Many thanks</p>
","6454030","","3374996","","2017-02-14 11:13:04","2017-03-30 09:51:28","No module named packaging","<python><python-3.x><ubuntu><pip>","2","4","3","","","CC BY-SA 3.0","0"
"32115607","1","32116102","","2015-08-20 10:18:53","","5","22014","<p>I use Arch Linux, python 3.4, openSSL 1.0.2d. When I make request to <a href=""https://www.supercash.cz/"" rel=""noreferrer"">https://www.supercash.cz/</a> I get this error. It doesn't matter if I use requests or build in urllib there is always the same error. SSL certificate for this site seams to be OK in Chrome browser.</p>

<pre><code>File ""/usr/lib64/python3.4/urllib/request.py"", line 463, in open
    response = self._open(req, data)
File ""/usr/lib64/python3.4/urllib/request.py"", line 481, in _open
    '_open', req)
File ""/usr/lib64/python3.4/urllib/request.py"", line 441, in _call_chain
    result = func(*args)
File ""/usr/lib64/python3.4/urllib/request.py"", line 1225, in https_open
    context=self._context, check_hostname=self._check_hostname)
File ""/usr/lib64/python3.4/urllib/request.py"", line 1184, in do_open
    raise URLError(err)
urllib.error.URLError: &lt;urlopen error EOF occurred in violation of protocol (_ssl.c:600)&gt;
</code></pre>

<p>I tried this but it only works in python2.7
<a href=""https://stackoverflow.com/questions/11772847/error-urlopen-error-errno-8-ssl-c504-eof-occurred-in-violation-of-protoco"">Error - urlopen error [Errno 8] _ssl.c:504: EOF occurred in violation of protocol , help needed</a></p>

<p>This is result of ssl test <a href=""https://www.ssllabs.com/ssltest/analyze.html?d=supercash.cz"" rel=""noreferrer"">https://www.ssllabs.com/ssltest/analyze.html?d=supercash.cz</a></p>
","2355557","","2355557","","2015-08-20 10:24:01","2019-03-29 12:06:22","Python 3.4 SSL error urlopen error EOF occurred in violation of protocol (_ssl.c:600)","<python><python-3.x><ssl><python-3.4>","2","0","","","","CC BY-SA 3.0","0"
"43279256","1","43280020","","2017-04-07 13:20:59","","9","21971","<p>So I have a function with several optional arguments like so:</p>

<pre><code>def func1(arg1, arg2, optarg1=None, optarg2=None, optarg3=None):
</code></pre>

<p>Optarg1 &amp; optarg2 are <em>usually</em> used together and if these 2 args are specified then optarg3 is not used.  By contrast, if optarg3 is specified then optarg1 &amp; optarg2 are not used.  If it were one optional argument it'd be easy for the function to ""know"" which argument to use: </p>

<pre><code>if optarg1 != None:
    do something
else:
    do something else 
</code></pre>

<p>My question is how to I ""tell"" the function which optional argument to use when there's multiple optional arguments and not all of them are always specified?  Is parsing the arguments with **kwargs the way to go? </p>
","6525959","","","","","2017-04-07 14:08:08","Multiple optional arguments python","<python-3.x><optional-arguments>","2","1","2","","","CC BY-SA 3.0","0"
"37944806","1","37945059","","2016-06-21 12:44:27","","13","21958","<p>I'm new to Python and I do have an issue that is bothering me.</p>

<p>I use the following code to get a base64 string representation of my zip file.</p>

<pre><code>with open( ""C:\\Users\\Mario\\Downloads\\exportTest1.zip"",'rb' ) as file:
    zipContents = file.read()
    encodedZip = base64.encodestring(zipContents)
</code></pre>

<p>Now, if I output the string it is contained inside a b'' representation. This for me is not necessary and I would like to avoid it. Also it adds a newlines character every 76 characters which is another issue. Is there a way to get the binary content and represent it without the newline characters and trailing and leading b''?</p>

<p>Just for comparison, if I do the following in PowerShell:</p>

<pre><code>$fileName = ""C:\Users\Mario\Downloads\exportTest1.zip""
$fileContentBytes = [System.IO.File]::ReadAllBytes($fileName)
$fileContentEncoded = [System.Convert]::ToBase64String($fileContentBytes) 
</code></pre>

<p>I do get the exact string I'm looking for, no b'' and no \n every 76 chars.</p>
","5163862","","5163862","","2016-06-21 12:53:38","2019-08-18 23:48:43","Python 3 and base64 encoding of a binary file","<python><python-3.x><base64><binaries>","2","2","1","","","CC BY-SA 3.0","0"
"36347786","1","","","2016-04-01 03:58:55","","7","21954","<p>my problem starts here:</p>

<p><a href=""https://stackoverflow.com/questions/36323564/pyttsx-and-gtts-module-errors"">pyttsx and gTTS module errors</a> </p>

<p>gTTS works well, takes text from text file, but first creates mp3 file, then if I want listen, I must call this mp3, so it is good but it would be better if I can avoid any audio files, and get just read from text file. maybe somehow I can use google voice to read from text file..? anyway main question now is other</p>

<p>if I can use only gTTS what is the best way to play mp3 on Windows 10-64 bit, Python 3.5 </p>

<p>ok with os: </p>

<pre><code>import os
os.startfile(""D:\my path/rec1.mp3"") 
</code></pre>

<p>it is good, but I don't want use default player, need something like simpleaudio for mp3... </p>

<p>with pygame I have installation problem and  not sure, how good is use it this way:</p>

<pre><code>from pygame import mixer 

mixer.init()
mixer.music.load('D:/my path/rec1.mp3')
mixer.music.play()
</code></pre>

<p>vlc just how to install it? with <code>easy_install vlc</code> I got error: <code>could not find suitable distribution for requirement.parse ('vlc')</code> and with <code>pip install vlc</code> error: <code>could not find a version that satisfies the requirement vlc (from versions: ) no matching distribution found for vlc</code> </p>

<pre><code>import vlc

p = vlc.MediaPlayer(""file:/my path/rec1.mp3"")
p.play()
p.stop()
</code></pre>

<p>with pyglet: </p>

<pre><code>import pyglet

music=pyglet.media.load('D:/my path/rec1.mp3')
music.play()
pyglet.app.run()
</code></pre>

<p>I got this error:</p>

<pre><code>'AVbin is required to decode compressed media')
pyglet.media.riff.WAVEFormatException: AVbin is required to decode compressed media
</code></pre>

<p>subprocess also uses default player:</p>

<pre><code>import subprocess

sound_program = ""path to player""
sound_file = ""D:/my path/rec1.mp3""
subprocess.call([sound_program, sound_file])
</code></pre>

<p>with mp3play, not sure how to use it: </p>

<pre><code>import mp3play

filename = (r'D:\my path/rec1.mp3')
clip = mp3play.load(filename)
clip.play()
</code></pre>

<p>I tried it this way: </p>

<pre><code>  filename = ('D:\my path/rec1.mp3')
</code></pre>

<p>this way: </p>

<pre><code>  filename = r'D:\my path/rec1.mp3'
</code></pre>

<p>In all cases I got error: </p>

<pre><code>Traceback (most recent call last):
  File ""D:/dt/PyCharm_project/0_ASK.py"", line 18, in &lt;module&gt;
    import mp3play
  File ""C:\Users\User\AppData\Roaming\Python\Python35\site-packages\mp3play\__init__.py"", line 4, in &lt;module&gt;
    from .windows import AudioClip as _PlatformSpecificAudioClip
  File ""C:\Users\User\AppData\Roaming\Python\Python35\site-packages\mp3play\windows.py"", line 27
    print 'Error %s for ""%s"": %s' % (str(err), txt, buf)
                                ^
SyntaxError: invalid syntax
</code></pre>

<p>ok so with pydub: </p>

<pre><code>from pydub import AudioSegment 
from gtts import gTTS
import simpleaudio as sa

blabla = ('my voice')
tts = gTTS(text=blabla, lang='en')
tts.save(""D:/my path/rec.mp3"")

rec = AudioSegment.from_mp3(""D:\my path\rec.mp3"")
rec.export(""rec.wav"", format=""wav"")

#rec = AudioSegment.ffmpeg (""D:\my path\rec.mp3"")
#rec.export(""rec.wav"", format=""wav"")

#rec = AudioSegment.converter (""D:\my path\rec.mp3"")
#rec.export(""rec.wav"", format=""wav"")

wave_obj = sa.WaveObject.from_wave_file('D:/my path/rec.wav'')
play_obj = wave_obj.play()
play_obj.wait_done()
</code></pre>

<p>Errors in the sequence first with <code>AudioSegment.from_mp3</code> :</p>

<pre><code>RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
  warn(""Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work"", RuntimeWarning)
Traceback (most recent call last):
  File "" D:/dt/PyCharm_project/0_ASK.py"", line 9, in &lt;module&gt;
    rec = AudioSegment.from_mp3(""D:\my path\rec.mp3"")
  File ""C:\Users\User\AppData\Roaming\Python\Python35\site-packages\pydub\audio_segment.py"", line 438, in from_mp3
    return cls.from_file(file, 'mp3')
  File ""C:\Users\User\AppData\Roaming\Python\Python35\site-packages\pydub\audio_segment.py"", line 366, in from_file
    file = _fd_or_path_or_tempfile(file, 'rb', tempfile=False)
  File ""C:\Users\User\AppData\Roaming\Python\Python35\site-packages\pydub\utils.py"", line 59, in _fd_or_path_or_tempfile
    fd = open(fd, mode=mode)
OSError: [Errno 22] Invalid argument: 'D:\my path\rec.mp3'
</code></pre>

<p>with <code>AudioSegment.ffmpeg</code> :</p>

<pre><code>  warn(""Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work"", RuntimeWarning)
Traceback (most recent call last):
  File ""D:/dt/PyCharm_project/0_ASK.py "", line 12, in &lt;module&gt;
    rec = AudioSegment.ffmpeg (""D:\my path\rec.mp3"")
TypeError: 'str' object is not callable
</code></pre>

<p>with <code>AudioSegment.converter</code>:</p>

<pre><code>RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work
  warn(""Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work"", RuntimeWarning)
Traceback (most recent call last):
  File ""D:/dt/PyCharm_project/0_ASK.py"", line 15, in &lt;module&gt;
    rec = AudioSegment.converter (""D:\my path\rec.mp3"")
TypeError: 'str' object is not callable
</code></pre>

<p>not sure maybe webbrowser, but how to install it? </p>

<pre><code>import webbrowser
webbrowser.open(""D:/my path/rec1.mp3"")
</code></pre>
","","user6067640","-1","user6067640","2017-05-23 11:46:41","2019-02-19 06:20:12","how to play mp3","<python-3.x><pygame><vlc><pyglet><pydub>","12","0","4","","","CC BY-SA 3.0","0"
"47823335","1","47823420","","2017-12-14 22:50:28","","7","21952","<p>I have numpy array, and I want to select a number of values based on their index number. I am using Python3.6</p>

<p>for example:</p>

<pre><code>np.array:
index#
[0]
[1]  + + + + + + + + + +         + + + + + + + + + +
[2]  + I want to selct +         + I don't want to select:
[3]  + the indexs:     +         +                  [0]
[4]  +      [2]        +         +                  [10]
[5]  +      [4]        +         +       Or any between
[6]  +      [6]        +         + 
[7]  +      [8]        +         + + + + + + + + + +
[8]  + + + + + + + + + +
[9]
[10]
</code></pre>

<p>so as you can see for the example above I want to select the index number:</p>

<pre><code>if x = 2, 4, 8
</code></pre>

<p>this will work if I just specify the numbers in x but if I want to make x variable I tried for example:</p>

<pre><code>if x:
for i in np.arange(x+2, x-last_index_number, x+2):
return whatever 
</code></pre>

<p>Where
x+2 = the first index I want (the start point).
x-last_index_number = the last index I want (the last point).
x+2 = the step (I want it to go the next index number by adding 2 to x and so on.
but this didn't work.</p>

<p>So my question can I specify numbers to select in a certain order: </p>

<pre><code>[5][10][15][20][25][30][35]
or
[4][8][12][16][20]
</code></pre>
","7632116","","6622587","","2017-12-15 00:02:17","2017-12-15 00:02:17","I want to select specific range of indexes from an array","<python><arrays><python-3.x><numpy><indexing>","2","1","1","","","CC BY-SA 3.0","0"
"40640956","1","40641103","","2016-11-16 19:44:50","","6","21927","<p>It seems strange to me that</p>

<pre><code>with open(file, 'r')
</code></pre>

<p>can report </p>

<pre><code>FileNotFoundError: [Errno 2]
</code></pre>

<p>but I can't catch that in some way and continue on.  Am I missing something here or is it really expected that you use isfile() or similar before a with open() ?</p>
","7169448","","1222951","","2018-09-19 18:13:20","2018-09-19 18:13:20","Python - with open() except (FileNotFoundError)?","<python><python-3.x>","2","0","","2018-09-19 18:14:33","","CC BY-SA 3.0","0"
"42593771","1","","","2017-03-04 08:15:36","","7","21923","<p>I have been trying to iterate the tensorflow model using the answer provided on 
<a href=""https://stackoverflow.com/questions/35695183/tensorflow-memory-leak-even-while-closing-session"">Tensorflow : Memory leak even while closing Session?</a>
I have the same error as mentioned in one of the comments i.e. ""The Session graph is empty.  Add operations to the graph before calling run()."" I cannot figure out how to solve it.</p>
","7337638","","-1","","2017-05-23 12:00:13","2019-01-21 18:37:25","Session graph is empty","<python-3.x><tensorflow>","2","1","3","","","CC BY-SA 3.0","0"
"38855641","1","","","2016-08-09 16:06:36","","3","21896","<p>I am looking for some assistance with writing API results to a .CSV file using Python. At this point, I'm successfully writing to .CSV, but I cannot seem to nail down the code behind the .CSV format I'm looking for, which is the standard one field = one column format.</p>

<p>Any help is appreciated! Details are below. Thanks!</p>

<p><strong>My code:</strong></p>

<pre><code>import requests
import json
import csv

urlcomp = 'http://url_ommitted/api/reports/completion?status=COMPLETED&amp;from=2016-06-01&amp;to=2016-08-06'
headers = {'authorization': ""Basic API Key Ommitted"", 'accept': ""application/json"", 'accept': ""text/csv""}

## API Call to retrieve report
rcomp = requests.get(urlcomp, headers=headers)

## API Results
data = rcomp.text

## Write API Results to CSV
with open('C:\_Python\\testCompletionReport.csv', ""wb"") as csvFile:
    writer = csv.writer(csvFile, delimiter=',')
    for line in data:
        writer.writerow(line)
</code></pre>

<p>The code above creates a .CSV with the correct output, but it's writing each character from the API results into a new cell in Column A of the output file. <strong>Screenshot below:</strong></p>

<p><a href=""https://i.stack.imgur.com/8XIaD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8XIaD.png"" alt=""Screenshot below:""></a></p>

<p>I've also attempted the code below, which writes the entire API result set into a single cell in the .CSV output file.</p>

<p><strong>Code:</strong></p>

<pre><code>data = rcomp.text

with open('C:\_Python\\CompletionReportOutput.csv', 'wb') as csvFile:
    writer = csv.writer(csvFile, delimiter = ',')
    writer.writerow([data])
</code></pre>

<p><strong>Output:</strong></p>

<p><a href=""https://i.stack.imgur.com/O88Yx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O88Yx.png"" alt=""enter image description here""></a></p>

<p><strong>Below is a screenshot of some sample API result data returned from my call:</strong>
<a href=""https://i.stack.imgur.com/QcuUm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QcuUm.png"" alt=""enter image description here""></a></p>

<p><strong>Example of what I'm looking for in the final .CSV output file:</strong>
<a href=""https://i.stack.imgur.com/kfEtP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kfEtP.png"" alt=""enter image description here""></a></p>

<p><strong>EDIT - Sample API Response:</strong></p>

<p>""PACKAGE CREATED"",""PACKAGE ID"",""PACKAGE NAME"",""PACKAGE STATUS"",""PACKAGE TRASHED"",""PACKAGE UPDATED"",""SENDER ID"",""SENDER NAME"",""SENDER COMPANY"",""SENDER CREATED"",""SENDER EMAIL"",""SENDER FIRSTNAME"",""SENDER LANGUAGE"",""SENDER LASTNAME"",""SENDER PHONE"",""SENDER TITLE"",""SENDER UPDATED"",""SENDER ACTIVATED"",""SENDER LOCKED"",""SENDER STATUS"",""SENDER TYPE""
""Thu Aug 04 14:52:57 CDT 2016"",""ulw5MTQo8WjBfoCTKqz9LNCFpV4="",""TestOne to TestTwo - Flatten PDF Removed"",""COMPLETED"",""false"",""Thu Aug 04 14:53:30 CDT 2016"",""tKpohv2kZ2oU"","""","""",""2016-08-03 14:12:06.904"",""testaccount@test.com"",""John"",""en"",""Smith"","""","""",""2016-08-03 14:12:06.942118"",""null"",""null"",""INVITED"",""REGULAR""
""Thu Aug 04 09:39:22 CDT 2016"",""IJV3U_yjPlxS-TVQgMrNgVUUSss="",""TestOne to TestTwo - Email Test"",""COMPLETED"",""false"",""Thu Aug 04 10:11:29 CDT 2016"",""tKpohv2kZ2oU"","""","""",""2016-08-03 14:12:06.904"",""testaccount@test.com"",""John"",""en"",""Smith"","""","""",""2016-08-03 14:12:06.942118"",""null"",""null"",""INVITED"",""REGULAR""</p>

<p><strong>SECOND EDIT - Output from Lee's suggestion:</strong></p>

<p><a href=""https://i.stack.imgur.com/xiTTd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xiTTd.png"" alt=""enter image description here""></a></p>
","1060275","","1060275","","2016-08-09 16:53:43","2017-09-12 16:12:36","Writing API Results to CSV in Python","<python><python-2.7><python-3.x><csv><export-to-csv>","2","4","1","","","CC BY-SA 3.0","0"
"44124059","1","","","2017-05-23 00:41:14","","0","21874","<p>I'm taking a class in Python and our prof wants us to write a program that prompts the user to enter an integer repeatedly until they enter 0. Then, have the program ignore all negative numbers, if any, and display the number of even integers, the number of odd integers, the sum of the even integers, the sum of the odd numbers, and the number of positive integers.</p>

<p>I've been trying and trying to do this program in small parts. However, I always end up getting stuck. I've started over about 5 times now and I would really appreciate if someone were to point me in the right direction. </p>

<p>So far, I have this: </p>

<pre><code> num_str = input(""Input an integer (0 terminates):"")
 num_int=int(num_str)
 even_count=0
 odd_count=0
 even_sum=0
 odd_sum=0 


while num_int !=0:
   num_str = input(""Input an integer (0 terminates):"")
   num_int=int(num_str)
   for num_int in num_str: 
       if num_int%2 == 0: 
           even_count += 1
       else: 
           odd_count +=1


print("""")
print(""Sum of odds:"", odd_sum)
print(""Sum of evens:"", even_sum)
print(""Even count:"", even_count)
print(""Odd count:"", odd_count)
print(""Total positive int count:"")
</code></pre>

<p>I know it's not much and I'm missing a whole lot, but I just wrote what I know needs to be included so far. I keep getting stuck because the program keeps giving me errors. Any sort of help is very appreciated because I have really no idea where to start!</p>
","8050757","","1402846","","2017-05-23 00:57:16","2020-08-11 01:03:50","Python program prompts user to enter number until they enter 0, then program adds even and odd integers","<python><python-3.x>","5","4","","","","CC BY-SA 3.0","0"
"34133976","1","34134012","","2015-12-07 12:49:55","","3","21848","<p>I am trying to using split line on a text file that looks like this:</p>

<pre><code>12/1 AH120 LAX PEK 12:30 21:00
12/2 C32 PEK HNA 12:40 20:00
12/3 D34 MON DOE 3:00 4:00
12/5 A100 ICS SEL 4:00 12:00
</code></pre>

<p>Here is the code : </p>

<pre><code>f = open('flights.txt', 'r')

for line in f:
    x = f.readline()
    y = x.split()
    print(y)
</code></pre>

<p>The problem I have is that instead of giving me lists of each line, it skips a few lines and the output is such that is looks like this:</p>

<pre><code>['12/2', 'C32', 'PEK', 'HNA', '12:40', '20:00']
['12/3', 'D34', 'MON', 'DOE', '3:00', '4:00']
</code></pre>

<p>As you can see it is missing the lines beginning with 12/1 and 12/5.
I don't know why I am running into this problem. Can anyone tell me what I am doing wrong?</p>
","4914525","","","","","2019-01-29 18:38:32","Splitting lines from a text file in Python 3","<python><python-3.x>","2","0","","","","CC BY-SA 3.0","0"
"32006771","1","42033743","","2015-08-14 09:29:45","","12","21818","<p>I'm trying to switch from Notepad++ to Atom, but I just can't manage to get my scripts executed in Atom.</p>

<p>I followed <a href=""https://stackoverflow.com/a/25586345/2605073""><strong>this answer</strong></a> (so I already installed <strong>script</strong>) which is not really extensive and also the rest on the web doesn't offer anything comprehensible for beginners.</p>

<p>In Notepad++ NPPexec I used to </p>

<pre><code>NPP_SAVE
cd ""$(FULL_CURRENT_PATH)""
C:\Python34\python.exe -u ""$(FULL_CURRENT_PATH)""
</code></pre>

<p>and in Sublime Text 2 I made it run by creating a new ""Build System"":</p>

<pre><code>{
    ""cmd"": [""C:\\python34\\python.exe"", ""-u"", ""$file""],
    ""file_regex"": ""^[ ]*File \""(...*?)\"", line ([0-9]*)"",
    ""selector"": ""source.python""
}
</code></pre>

<p><strong>Can you please guide me how to setup Atom to be able to execute Python scripts with Python 3.4 scripts with a keyboard short-cut?</strong></p>

<hr>

<p>I already tried to set my init-script to:</p>

<pre><code>process.env.path = [""C:\Python34\python.exe"",process.env.PATH].join("";"")
</code></pre>

<p>respectively</p>

<pre><code>process.env.path = [""C:\Python34"",process.env.PATH].join("";"")
</code></pre>

<p>with no success.</p>

<hr>

<p>When I go to <strong>Packages -> Script -> Configure Script</strong> and type</p>

<pre><code>C:\\Python34\\python.exe
</code></pre>

<p>it works. But thats not a permanent solution.</p>

<hr>

<p>When I press <strong>Ctrl+Shift+B</strong> to run a script, without configuring it before (as it is supposed to work), I get (suggestion of ig0774's comment implemented):</p>

<p><a href=""https://i.stack.imgur.com/1Ucnd.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1Ucnd.png"" alt=""enter image description here""></a></p>

<p>(it doesn't matter whether it is <code>C:\Python34</code> or <code>C:\Python34\</code>)</p>

<p>It complains that <strong>python is not in my path</strong> - but it is.</p>

<hr>

<p><strong>I read multiple times that Windows 7/8 64bit together with Python 3.x could cause issues with certain packages.</strong> May this be the reason in ths case as well? I have Windows 7 Pro x64.</p>

<hr>

<h2>Update</h2>

<p>As I've switched to VSCode and probably stay there, I'm not willing/don't have the time to try out all the answers, so I let the community judge the answers and accept always the highest voted. Please ping me, if it's not correct anymore.</p>
","2605073","","2605073","","2017-11-19 11:53:08","2018-07-31 04:48:32","How to setup Atom's script to run Python 3.x scripts? May the combination with Windows 7 Pro x64 be the issue?","<python><python-3.x><atom-editor>","8","2","8","","","CC BY-SA 3.0","0"
"41588383","1","41597238","","2017-01-11 10:29:30","","17","21802","<p>I'm using Keras with Tensorflow backend on a cluster (creating neural networks). How can I run it in a multi-threaded way on the cluster (on several cores) or is this done automatically by Keras? For example in Java one can create several threads, each thread running on a core.</p>

<p>If possible, how many cores should be used?</p>
","3591044","","3591044","","2017-01-11 10:59:31","2020-10-25 09:45:28","How to run Keras on multiple cores?","<python><multithreading><python-3.x><tensorflow><keras>","2","0","5","","","CC BY-SA 3.0","0"
"50157962","1","50158176","","2018-05-03 14:50:20","","1","21765","<p>I have my data stored in json format. And by using json:load() I am reading the data.</p>
<p>What I am trying to do is based on argument passed , I am trying to extract specific part of Key:Value from the list and storing them into a new list for later processing.</p>
<h1>Code</h1>
<pre><code>selected_env='mydev2'
server_list=[{'mydev': ['192.168.56.102', '192.168.56.102', '192.168.56.102']}, {'mydev2': ['192.168.56.102', '192.168.56.102', '192.168.56.102']}]         
for item in server_list :
    host_list=[item for item in server_list[selected_env] if selected_env in server_list]

print(host_list)
</code></pre>
<p>And I am getting error as</p>
<p><code>TypeError: list indices must be integers or slices, not str</code></p>
<p><strong>Note</strong>: I have achieved already one way of extraction as below</p>
<pre><code>for item in server_list:
  for element in item :
    if element == selected_env :
       host_list=item[selected_env]  
</code></pre>
<p>even though its working, I thought of optimizing it and went to read some articles on list extraction and kind of stuck with above error.</p>
<p>Any help, greatly appreciated.</p>
","1293013","","-1","","2020-06-20 09:12:55","2018-05-03 17:19:04","Python3: TypeError: list indices must be integers or slices, not str","<python><python-3.x><list>","3","0","2","","","CC BY-SA 4.0","0"
"33681952","1","33681953","","2015-11-12 21:39:49","","26","21750","<p>Python 3, Django 1.8.5, Postgres</p>

<p>I have a model <code>Sites</code> that has been working fine. I recently tried to add a field, airport_code, and migrate the data.</p>

<pre><code>class Site(BaseModel):

  objects = SiteManager()

  name = models.CharField(max_length=200, unique=True)
  domain = models.CharField(max_length=200, unique=True)
  weather = models.CharField(max_length=10)
  nearby_sites = models.ManyToManyField('self', symmetrical=False, blank=True)
  users = models.ManyToManyField(settings.AUTH_USER_MODEL, blank=True)
  facebook = models.URLField(max_length=200)
  twitter = models.URLField(max_length=200)
  header_override = models.TextField(blank=True)
  email_header_override = models.TextField(blank=True)
  timely_site_tag_id = models.IntegerField()
  timely_featured_tag_id = models.IntegerField()
  timely_domain = models.CharField(max_length=255)
  sitemap_public_id = models.CharField(max_length=255)
  state = models.CharField(max_length=24)
  airport_code = JSONField()
</code></pre>

<p>However, when I ran <code>makemigrations</code> I got an error:</p>

<p><code>django.db.utils.ProgrammingError: column sites_site.airport_code does not exist
LINE 1: ..._site"".""sitemap_public_id"", ""sites_site"".""state"", ""sites_sit...</code></p>

<p>Of course, this doesn't make sense, because the column obviously does not exist when I'm trying to create it within the migration.</p>

<p>I have seen many questions about this bug on Stack Overflow that are unanswered, or have a solution to manually create the migration file, or destroy and rebuild the database. This is not an okay solution.</p>
","722798","","2321643","","2015-11-12 21:53:47","2020-09-25 06:00:50","Django Migration Error: Column does not exist","<django><postgresql><python-3.x><django-models>","15","0","2","","","CC BY-SA 3.0","0"
"41720578","1","41722610","","2017-01-18 13:21:32","","15","21747","<p>I <a href=""https://askubuntu.com/questions/865554/how-do-i-install-python-3-6-using-apt-get"">installed Python 3.6 on Ubuntu 16.04</a> by using <a href=""https://launchpad.net/~jonathonf/+archive/ubuntu/python-3.6"" rel=""noreferrer"">Jonathon Fernyhough's PPA</a>:</p>

<pre><code>sudo add-apt-repository ppa:jonathonf/python-3.6
sudo apt-get update
sudo apt-get install python3.6
</code></pre>

<p>I made a string, using the new literal string interpolation, but I supplied an invalid format specifier. I not only got the expected <code>ValueError: Invalid format specifier</code>, but also the unexpected <code>ModuleNotFoundError: No module named 'apt_pkg'</code>.</p>

<pre><code>$ python3.6
Python 3.6.0 (default, Dec 29 2016, 21:40:36) 
[GCC 5.4.1 20161202] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; value = 4 * 20
&gt;&gt;&gt; f'the value is {value:%A}'
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ValueError: Invalid format specifier
Error in sys.excepthook:
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 63, in apport_excepthook
    from apport.fileutils import likely_packaged, get_recent_crashes
  File ""/usr/lib/python3/dist-packages/apport/__init__.py"", line 5, in &lt;module&gt;
    from apport.report import Report
  File ""/usr/lib/python3/dist-packages/apport/report.py"", line 30, in &lt;module&gt;
    import apport.fileutils
  File ""/usr/lib/python3/dist-packages/apport/fileutils.py"", line 23, in &lt;module&gt;
    from apport.packaging_impl import impl as packaging
  File ""/usr/lib/python3/dist-packages/apport/packaging_impl.py"", line 23, in &lt;module&gt;
    import apt
  File ""/usr/lib/python3/dist-packages/apt/__init__.py"", line 23, in &lt;module&gt;
    import apt_pkg
ModuleNotFoundError: No module named 'apt_pkg'

Original exception was:
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
ValueError: Invalid format specifier
</code></pre>

<p>I reported this to the <a href=""http://bugs.python.org/issue29307"" rel=""noreferrer"">Python bug tracker</a>. There it was noted that:</p>

<blockquote>
  <p>It seems to be vendor's issue not CPython itself. This same issue also happens in Ubuntu 16.10's Python 3.6. Raise any exception can cause this:</p>
</blockquote>

<pre><code>Python 3.6.0b2 (default, Oct 11 2016, 05:27:10) 
[GCC 6.2.0 20161005] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; raise Exception
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
Exception
Error in sys.excepthook:
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 63, in apport_excepthook
    from apport.fileutils import likely_packaged, get_recent_crashes
  File ""/usr/lib/python3/dist-packages/apport/__init__.py"", line 5, in &lt;module&gt;
    from apport.report import Report
  File ""/usr/lib/python3/dist-packages/apport/report.py"", line 30, in &lt;module&gt;
    import apport.fileutils
  File ""/usr/lib/python3/dist-packages/apport/fileutils.py"", line 23, in &lt;module&gt;
    from apport.packaging_impl import impl as packaging
  File ""/usr/lib/python3/dist-packages/apport/packaging_impl.py"", line 23, in &lt;module&gt;
    import apt
  File ""/usr/lib/python3/dist-packages/apt/__init__.py"", line 23, in &lt;module&gt;
    import apt_pkg
ModuleNotFoundError: No module named 'apt_pkg'

Original exception was:
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
Exception
&gt;&gt;&gt; 
</code></pre>

<blockquote>
  <p>Also see <a href=""https://bugs.launchpad.net/ubuntu/+source/python3.6/+bug/1631367"" rel=""noreferrer"">https://bugs.launchpad.net/ubuntu/+source/python3.6/+bug/1631367</a>.</p>
</blockquote>

<p>Finally, the issue was closed with the comment</p>

<blockquote>
  <p>Yes, this appears to be the vendor's failure reporting infrastructure
  that is failing.  Why they'd want a report for every traceback at the
  interactive prompt is beyond me, but that appears to be what they are
  trying to do.</p>
</blockquote>

<p>My questions now are:</p>

<ol>
<li>How do I interpret this comment? Is the vendor in this case Jonathon Fernyhough's PPA? And did he change something to to the Python code he distributes so that it tries to file a report for every Exception that produces a traceback?</li>
<li>Who do I need to notify or where do I need to file a bug to get this resolved?</li>
</ol>
","50065","","4952806","","2018-04-17 14:51:40","2019-12-23 16:58:54","ModuleNotFoundError in tracebacks with Python3.6 on linux","<python-3.x><exception-handling><python-3.6>","4","2","3","","","CC BY-SA 3.0","0"
"40428931","1","40429176","","2016-11-04 17:50:16","","20","21746","<p>I seem to remember there is a package that printed the versions and relevant information about Python packages used in a Jupyter notebook so the results in it were reproducible. But I cannot remember the name of the package. Can any of you point me in the right direction?</p>

<p>Thanks in advance!</p>
","7096249","","","","","2020-09-21 09:13:15","Package for listing version of packages used in a Jupyter notebook","<python><python-3.x>","7","3","8","","","CC BY-SA 3.0","0"
"50692522","1","50692647","","2018-06-05 05:18:00","","4","21721","<p><a href=""https://i.stack.imgur.com/u3pIo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/u3pIo.png"" alt=""Why am I getting this error ""></a></p>

<p>Why am I getting this error (see linting) saying <strong>'End of statement expected'</strong> in pycharm? </p>

<p>I am very new to python.</p>
","1461967","","","","","2018-06-05 05:30:22","'End of statement expected' in pycharm","<python><python-3.x><numpy><pycharm>","1","6","1","2018-06-05 06:35:45","","CC BY-SA 4.0","0"
"27750608","1","27764910","","2015-01-03 00:51:01","","4","21717","<p>I have installed the nltk package. Following that I am trying to download the supporting packages using nltk.download() and am getting error:</p>

<p>[Errno 11001] getaddrinfo </p>

<p>My machine / software details are:</p>

<p>OS: Windows 8.1
Python: 3.3.4
NLTK Package: 3.0</p>

<p>Below are the commands run in python:</p>

<pre><code>Python 3.3.4 (v3.3.4:7ff62415e426, Feb 10 2014, 18:13:51) [MSC v.1600 64 bit (AMD64)] on win32
Type ""copyright"", ""credits"" or ""license()"" for more information.

import nltk

nltk.download()
showing info http://nltk.github.com/nltk_data/
True

nltk.download(""all"")
[nltk_data] Error loading all: &lt;urlopen error [Errno 11001]
[nltk_data]     getaddrinfo failed&gt;
False
</code></pre>

<p><img src=""https://i.stack.imgur.com/KAnf3.jpg"" alt=""enter image description here""></p>

<p>It looks like it is going to  <a href=""http://nltk.github.com/nltk_data/"" rel=""nofollow noreferrer"">http://nltk.github.com/nltk_data/</a> whereas it should Ideally try to get the data from <a href=""http://www.nltk.org/nltk_data/"" rel=""nofollow noreferrer"">http://www.nltk.org/nltk_data/</a>. </p>

<p>On another machine when we type <a href=""http://nltk.github.com/nltk_data/"" rel=""nofollow noreferrer"">http://nltk.github.com/nltk_data/</a> in the browser, it redirects to <a href=""http://www.nltk.org/nltk_data/"" rel=""nofollow noreferrer"">http://www.nltk.org/nltk_data/</a>. I am not understanding why the redirection is not happening on my laptop.</p>

<p>I feel that this might be the issue.</p>

<p>Kindly help.</p>

<p>I have added the command prompt screenshot. Need help..</p>

<p><img src=""https://i.stack.imgur.com/zNSzA.jpg"" alt=""enter image description here""></p>

<p>Regards,
Bonson </p>
","1181744","","1181744","","2015-01-04 03:32:22","2019-09-16 10:08:47","error installing nltk supporting packages : nltk.download()","<python><python-3.x><nltk>","5","1","5","","","CC BY-SA 3.0","0"
"51026315","1","51027262","","2018-06-25 14:50:38","","7","21688","<p>I am switched from Python 2.7 to Python 3.6.</p>

<p>I have scripts that deal with some non-English content.</p>

<p>I usually run scripts via Cron and also in Terminal.</p>

<p>I had UnicodeDecodeError in my Python 2.7 scripts and I solved by this.</p>

<pre><code># encoding=utf8  
import sys  

reload(sys)  
sys.setdefaultencoding('utf8')
</code></pre>

<p>Now in Python 3.6, it doesnt work. I have print statements like <code>print(""Here %s"" % (myvar))</code> and it throws error. I can solve this issue by replacing it to <code>myvar.encode(""utf-8"")</code> but I don't want to write with each print statement.</p>

<p>I did <code>PYTHONIOENCODING=utf-8</code> in my terminal and I have still that issue.</p>

<p>Is there a cleaner way to solve <code>UnicodeDecodeError</code> issue in Python 3.6?</p>

<p>is there any way to tell Python3 to print everything in utf-8? just like I did in Python2?</p>
","4094231","","1554386","","2020-04-03 09:23:01","2020-04-03 09:23:01","How to solve UnicodeDecodeError in Python 3.6?","<python><python-3.x><linux><ubuntu><unicode>","5","18","3","","","CC BY-SA 4.0","0"
"44976172","1","44976188","","2017-07-07 16:59:10","","-8","21652","<p>I have written a code in Python3 using the IDE Wing, but I want to change a variable name, e.g., <code>var_1</code>, to <code>var_2</code> in all places in my file. In MATLAB, when you change a variable name in one place, by <code>Shift</code>+<code>Enter</code> it is automatically done for all other occurrences of that variable in the file. Is there any similar way to do it in Python?</p>
","8196461","","8196461","","2017-07-07 17:04:00","2017-07-07 17:08:03","Change a variable name in all places in Python","<python><python-3.x>","5","1","1","2017-07-07 17:17:03","","CC BY-SA 3.0","0"
"55409641","1","55409674","","2019-03-29 02:24:30","","25","21637","<p>I would like to use asyncio to get webpage html.</p>
<p>I run the following code in jupyter notebook:</p>
<pre><code>import aiofiles
import aiohttp
from aiohttp import ClientSession

async def get_info(url, session):
    resp = await session.request(method=&quot;GET&quot;, url=url)
    resp.raise_for_status()
    html = await resp.text(encoding='GB18030')
    with open('test_asyncio.html', 'w', encoding='utf-8-sig') as f:
        f.write(html)
    return html
    
async def main(urls):
    async with ClientSession() as session:
        tasks = [get_info(url, session) for url in urls]
        return await asyncio.gather(*tasks)

if __name__ == &quot;__main__&quot;:
    url = ['http://huanyuntianxiazh.fang.com/house/1010123799/housedetail.htm', 'http://zhaoshangyonghefu010.fang.com/house/1010126863/housedetail.htm']
    result = asyncio.run(main(url))
</code></pre>
<p>However, it returns <code>RuntimeError: asyncio.run() cannot be called from a running event loop</code></p>
<p>What is the problem?</p>
<p>How to solve it?</p>
","9087866","","5446749","","2020-07-24 11:22:34","2020-10-20 11:03:21","asyncio.run() cannot be called from a running event loop","<python><python-3.x><jupyter-notebook><python-asyncio>","5","0","10","","","CC BY-SA 4.0","0"
"32812463","1","","","2015-09-27 20:22:40","","18","21631","<p>Our system is running on Ubuntu, python 3.4, postgres 9.4.x and psycopg2.</p>

<p>We (will in the furture) split between <code>dev</code>, <code>test</code> and <code>prod</code> environments using schemas. I've create a convenience method for creating connections to our database. It uses json connection configuration files in order create the connection string. I want to configure the connection to use a particular schema for all following queries using the returned connection. I don't want my queries to have hardcoded schemas, because we should be able to easily switch between them depending on if we are in development, testing or production phase/environment.</p>

<p>Currently the convenience method looks like the following:</p>

<pre><code>def connect(conn_config_file = 'Commons/config/conn_commons.json'):
    with open(conn_config_file) as config_file:    
        conn_config = json.load(config_file)

    conn = psycopg2.connect(
        ""dbname='"" + conn_config['dbname'] + ""' "" +
        ""user='"" + conn_config['user'] + ""' "" +
        ""host='"" + conn_config['host'] + ""' "" +
        ""password='"" + conn_config['password'] + ""' "" +
        ""port="" + conn_config['port'] + "" ""
    )
    cur = conn.cursor()
    cur.execute(""SET search_path TO "" + conn_config['schema'])

    return conn
</code></pre>

<p>It works fine as long as you give it time to execute the set <code>search_path</code> query. Unfortunately, if I'm too fast with executing a following query a race condition happens where the <code>search_path</code> isn't set. I've tried to force the execution with doing a <code>conn.commit()</code> before the <code>return conn</code>, however, this resets the <code>search_path</code> to the default schema <code>postgres</code> so that it doesn't use, say, <code>prod</code>. Suggestions at the database or application layer is preferable, however, I know we probably could solve this at the OS level too, any suggestions in that direction are also welcomed. </p>

<p>An example json configuration file looks like the following:</p>

<pre><code>{
    ""dbname"": ""thedatabase"",
    ""user"": ""theuser"",
    ""host"": ""localhost"",
    ""password"": ""theusers_secret_password"",
    ""port"": ""6432"",
    ""schema"": ""prod""
}
</code></pre>

<p>Any suggestion is very appreciated.</p>
","604048","","604048","","2015-09-27 20:29:10","2018-04-25 02:44:23","Setting schema for all queries of a connection in psycopg2: Getting race condition when setting search_path","<python><postgresql><python-3.x><schema><psycopg2>","2","6","3","","","CC BY-SA 3.0","0"
"52138280","1","52138321","","2018-09-02 15:12:17","","7","21629","<p>I have Python 3.7 installed on my computer. I want to use tensorflow and just found out that it basically doesn't support 3.7, so I'd like to (also) install Python 3.6.</p>

<p>Any suggestions of how to do that? Do I have to uninstall 3.7 and replace it by 3.6 or is there a way to just use 3.6 for the stuff related to tensorflow?</p>
","5248259","","3890632","","2018-09-02 15:15:56","2020-03-02 10:13:24","(Easiest) Way to use Python 3.6 and 3.7 on same computer?","<python><python-3.x>","2","4","","","","CC BY-SA 4.0","0"
"35422490","1","35432125","","2016-02-16 02:14:00","","2","21613","<p>New to PyQt5... Here is a very basic question.</p>

<p>I would like to add an image inside the layout of a widget. This widget is the Main Window / root widget of my application. I use the following code, but I get an error message.</p>

<pre><code>import sys

from PyQt5.QtGui import QImage
from PyQt5.QtWidgets import *

class MainWindow(QWidget):

    def __init__(self):

       super().__init__()

       self.setGeometry(300,300,300,220)
       self.setWindowTitle(""Hello !"")

       oImage = QImage(""backgound.png"")

       oLayout = QVBoxLayout()
       oLayout.addWidget(oImage)

       self.setLayout(oLayout)

       self.show()

if __name__ == ""__main__"":

    app = QApplication(sys.argv)

    oMainwindow = MainWindow()

    sys.exit(app.exec_())


TypeError: QBoxLayout.addWidget(QWidget, int stretch=0, Qt.Alignment alignment=0): argument 1 has unexpected type 'QImage'
</code></pre>

<p>Apparently a QLayoutWidget does not accept a QImage as an input. Is there a workaround to have an image appear as a brackground in a QWidget ?</p>
","5059589","","5059589","","2016-02-17 22:22:49","2019-11-15 09:32:59","PyQt5 - Add image in background of MainWindow layout","<python-3.x><qwidget><pyqt5><qimage><qlayout>","1","0","3","","","CC BY-SA 3.0","0"
"30208044","1","30208145","","2015-05-13 07:12:41","","1","21575","<p>Let's say I have dict = {'a': 1, 'b': 2'} and I also have a list = ['a', 'b, 'c', 'd', 'e']. Goal is to add the list elements into the dictionary and print out the new dict values along with the sum of those values. Should look like:</p>

<pre><code>2 a
3 b
1 c
1 d
1 e
Total number of items: 8
</code></pre>

<p>Instead I get:</p>

<pre><code>1 a
2 b
1 c
1 d
1 e
Total number of items: 6
</code></pre>

<p>What I have so far:</p>

<pre><code>def addToInventory(inventory, addedItems)
    for items in list():
        dict.setdefault(item, [])

def displayInventory(inventory):
    print('Inventory:')
    item_count = 0
    for k, v in inventory.items():
       print(str(v) + ' ' + k)
       item_count += int(v)
    print('Total number of items: ' + str(item_count))

newInventory=addToInventory(dict, list)
displayInventory(dict)
</code></pre>

<p>Any help will be appreciated!</p>
","4894146","","1903116","","2015-05-13 07:38:08","2019-11-10 22:59:05","How to add list elements into dictionary","<python><list><python-3.x><dictionary>","11","4","3","","","CC BY-SA 3.0","0"
"51075666","1","51082386","","2018-06-28 06:01:30","","5","21569","<p>I have been trying to merge the following sequential models but haven't been able to. Could somebody please point out my mistake, thank you.</p>

<p>The code compiles while using""merge"" but give the following error ""TypeError: 'module' object is not callable""
However it doesn't even compile while using ""Merge""</p>

<p>I am using keras version 2.2.0 and python 3.6</p>

<pre><code>from keras.layers import merge
def linear_model_combined(optimizer='Adadelta'):    
    modela = Sequential()
    modela.add(Flatten(input_shape=(100, 34)))
    modela.add(Dense(1024))
    modela.add(Activation('relu'))
    modela.add(Dense(512))

    modelb = Sequential()
    modelb.add(Flatten(input_shape=(100, 34)))
    modelb.add(Dense(1024))
    modelb.add(Activation('relu'))
    modelb.add(Dense(512))

    model_combined = Sequential()

    model_combined.add(Merge([modela, modelb], mode='concat'))

    model_combined.add(Activation('relu'))
    model_combined.add(Dense(256))
    model_combined.add(Activation('relu'))

    model_combined.add(Dense(4))
    model_combined.add(Activation('softmax'))

    model_combined.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

    return model_combined
</code></pre>
","6551392","","6551392","","2018-06-28 06:14:28","2020-04-25 19:49:50","How to implement Merge from Keras.layers","<python><python-3.x><neural-network><keras><keras-layer>","3","0","","","","CC BY-SA 4.0","0"
"49901806","1","49905997","","2018-04-18 14:04:24","","20","21531","<p>I'm doing a simple tutorial using Tensorflow, I have just installed so it should be updated, first I load the mnist data using the following code:</p>

<pre><code>import numpy as np
import os
from tensorflow.examples.tutorials.mnist import input_data
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)
train_data = mnist.train.images  # Returns np.array
train_labels = np.asarray(mnist.train.labels, dtype=np.int32)
eval_data = mnist.test.images  # Returns np.array
eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)
</code></pre>

<p>But when I run it I get the following warning:</p>

<pre><code>WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Use the retry module or similar alternatives.
WARNING:tensorflow:From C:/Users/user/PycharmProjects/TensorFlowRNN/sample.py:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST_data/train-images-idx3-ubyte.gz
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST_data/train-labels-idx1-ubyte.gz
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.one_hot on tensors.
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From C:\Users\user\PycharmProjects\TensorFlowRNN\venv\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
</code></pre>

<p>I have used the line <code>os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'</code> which should avoid getting warnings and tried other alternatives to obtain mnist, however always appear the same warnings, can someone help me figure out is this happening?</p>

<p>PD: I am using Python 3.6 in windows 10, in case it helps.</p>
","9349674","","3924118","","2019-04-20 14:42:46","2019-04-20 14:42:46","Warning: Please use alternatives such as official/mnist/dataset.py from tensorflow/models","<python><python-3.x><tensorflow>","2","0","6","","","CC BY-SA 4.0","0"
"29132599","1","","","2015-03-18 21:02:53","","0","21520","<p>How to make word inputs in Python
I want to be able to have the computer to ask a question to the user like</p>

<pre><code>test = int(input('This only takes a number as an answer'))
</code></pre>

<p>I want to be able to have 'test' not be a number, rather a word, or letter.</p>
","4687236","","3204551","","2017-02-14 19:17:23","2018-04-14 16:05:34","Accept an input as a word and not an integer","<python><python-3.x><input><words>","5","1","","","","CC BY-SA 3.0","0"
"39930363","1","39930454","","2016-10-08 08:32:52","","11","21516","<p>For example, I have a frozen set</p>

<pre><code>[frozenset({'a', 'c,'}), frozenset({'h,', 'a,'})]
</code></pre>

<p>I want to convert it to a normal list like</p>

<pre><code>[['a', 'c,'],['a,', 'd,']...]
</code></pre>

<p>What method should I use?</p>
","6940418","","3001761","","2016-10-08 08:42:14","2020-04-19 06:37:52","How to convert frozenset to normal sets or list?","<python><python-3.x>","2","1","4","","","CC BY-SA 3.0","0"
"33895739","1","","","2015-11-24 14:06:31","","10","21485","<p>I am trying to fetch data from an url using python 3.5 using the following code</p>

<pre><code>import requests
url ='http://eutils.ncbi.nlm.nih.gov/entrez/eutils/einfo.fcgi'
r = requests.get(url)
r.content
</code></pre>

<p>The url can be opened with no problems in the browser.</p>

<p>However, I am getting an error (for this URL and any other URL I try) as follows:</p>

<pre><code>-------------------------------------------------------------------------- 
TypeError                                 Traceback (most recent call last)
C:\Anaconda3\lib\site-packages\requests\packages\urllib3\connectionpool.py in _make_request(self, conn, method, url, timeout, **httplib_request_kw)
     375             try:  # Python 2.7, use buffering of HTTP responses
 --&gt; 376                 httplib_response = conn.getresponse(buffering=True)
     377             except TypeError:  # Python 2.6 and older

 TypeError: getresponse() got an unexpected keyword argument 'buffering'

 During handling of the above exception, another exception occurred:

 RemoteDisconnected                        Traceback (most recent call last)
 C:\Anaconda3\lib\site-packages\requests\packages\urllib3\connectionpool.py
 in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, **response_kw)
     558                                                  timeout=timeout_obj,
 --&gt; 559                                                   body=body, headers=headers)
     560 

 C:\Anaconda3\lib\site-packages\requests\packages\urllib3\connectionpool.py
 in _make_request(self, conn, method, url, timeout,
 **httplib_request_kw)
     377             except TypeError:  # Python 2.6 and older
 --&gt; 378                 httplib_response = conn.getresponse()
     379         except (SocketTimeout, BaseSSLError, SocketError) as e:

 C:\Anaconda3\lib\http\client.py in getresponse(self)    1173          
 try:
 -&gt; 1174                 response.begin()    1175             except ConnectionError:

 C:\Anaconda3\lib\http\client.py in begin(self)
     281         while True:
 --&gt; 282             version, status, reason = self._read_status()
     283             if status != CONTINUE:
&gt; 
&gt; C:\Anaconda3\lib\http\client.py in _read_status(self)
&gt;     250             # sending a valid response.
&gt; --&gt; 251             raise RemoteDisconnected(""Remote end closed connection without""
&gt;     252                                      "" response"")
&gt; 
&gt; RemoteDisconnected: Remote end closed connection without response
&gt; 
&gt; During handling of the above exception, another exception occurred:
&gt; 
&gt; ProtocolError                             Traceback (most recent call
&gt; last) C:\Anaconda3\lib\site-packages\requests\adapters.py in
&gt; send(self, request, stream, timeout, verify, cert, proxies)
&gt;     369                     retries=self.max_retries,
&gt; --&gt; 370                     timeout=timeout
&gt;     371                 )
&gt; 
&gt; C:\Anaconda3\lib\site-packages\requests\packages\urllib3\connectionpool.py
&gt; in urlopen(self, method, url, body, headers, retries, redirect,
&gt; assert_same_host, timeout, pool_timeout, release_conn, **response_kw)
&gt;     608             retries = retries.increment(method, url, error=e, _pool=self,
&gt; --&gt; 609                                         _stacktrace=sys.exc_info()[2])
&gt;     610             retries.sleep()
&gt; 
&gt; C:\Anaconda3\lib\site-packages\requests\packages\urllib3\util\retry.py
&gt; in increment(self, method, url, response, error, _pool, _stacktrace)
&gt;     244             if read is False:
&gt; --&gt; 245                 raise six.reraise(type(error), error, _stacktrace)
&gt;     246             elif read is not None:
&gt; 
&gt; C:\Anaconda3\lib\site-packages\requests\packages\urllib3\packages\six.py
&gt; in reraise(tp, value, tb)
&gt;     308         if value.__traceback__ is not tb:
&gt; --&gt; 309             raise value.with_traceback(tb)
&gt;     310         raise value
&gt; 
&gt; C:\Anaconda3\lib\site-packages\requests\packages\urllib3\connectionpool.py
&gt; in urlopen(self, method, url, body, headers, retries, redirect,
&gt; assert_same_host, timeout, pool_timeout, release_conn, **response_kw)
&gt;     558                                                   timeout=timeout_obj,
&gt; --&gt; 559                                                   body=body, headers=headers)
&gt;     560 
&gt; 
&gt; C:\Anaconda3\lib\site-packages\requests\packages\urllib3\connectionpool.py
&gt; in _make_request(self, conn, method, url, timeout,
&gt; **httplib_request_kw)
&gt;     377             except TypeError:  # Python 2.6 and older
&gt; --&gt; 378                 httplib_response = conn.getresponse()
&gt;     379         except (SocketTimeout, BaseSSLError, SocketError) as e:
&gt; 
&gt; C:\Anaconda3\lib\http\client.py in getresponse(self)    1173          
&gt; try:
&gt; -&gt; 1174                 response.begin()    1175             except ConnectionError:
&gt; 
&gt; C:\Anaconda3\lib\http\client.py in begin(self)
&gt;     281         while True:
&gt; --&gt; 282             version, status, reason = self._read_status()
&gt;     283             if status != CONTINUE:
&gt; 
&gt; C:\Anaconda3\lib\http\client.py in _read_status(self)
&gt;     250             # sending a valid response.
&gt; --&gt; 251             raise RemoteDisconnected(""Remote end closed connection without""
&gt;     252                                      "" response"")
&gt; 
&gt; ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end
&gt; closed connection without response',))
&gt; 
&gt; During handling of the above exception, another exception occurred:
&gt; 
&gt; ConnectionError                           Traceback (most recent call
&gt; last) &lt;ipython-input-16-598f53da7af3&gt; in &lt;module&gt;()
&gt;       3 import requests
&gt;       4 url ='http://eutils.ncbi.nlm.nih.gov/entrez/eutils/einfo.fcgi'
&gt; ----&gt; 5 r = requests.get(url)
&gt;       6 r.content
&gt; 
&gt; C:\Anaconda3\lib\site-packages\requests\api.py in get(url, params,
&gt; **kwargs)
&gt;      67 
&gt;      68     kwargs.setdefault('allow_redirects', True)
&gt; ---&gt; 69     return request('get', url, params=params, **kwargs)
&gt;      70 
&gt;      71 
&gt; 
&gt; C:\Anaconda3\lib\site-packages\requests\api.py in request(method, url,
&gt; **kwargs)
&gt;      48 
&gt;      49     session = sessions.Session()
&gt; ---&gt; 50     response = session.request(method=method, url=url, **kwargs)
&gt;      51     # By explicitly closing the session, we avoid leaving sockets open which
&gt;      52     # can trigger a ResourceWarning in some cases, and look like a memory leak
&gt; 
&gt; C:\Anaconda3\lib\site-packages\requests\sessions.py in request(self,
&gt; method, url, params, data, headers, cookies, files, auth, timeout,
&gt; allow_redirects, proxies, hooks, stream, verify, cert, json)
&gt;     466         }
&gt;     467         send_kwargs.update(settings)
&gt; --&gt; 468         resp = self.send(prep, **send_kwargs)
&gt;     469 
&gt;     470         return resp
&gt; 
&gt; C:\Anaconda3\lib\site-packages\requests\sessions.py in send(self,
&gt; request, **kwargs)
&gt;     574 
&gt;     575         # Send the request
&gt; --&gt; 576         r = adapter.send(request, **kwargs)
&gt;     577 
&gt;     578         # Total elapsed time of the request (approximately)
&gt; 
&gt; C:\Anaconda3\lib\site-packages\requests\adapters.py in send(self,
&gt; request, stream, timeout, verify, cert, proxies)
&gt;     410 
&gt;     411         except (ProtocolError, socket.error) as err:
&gt; --&gt; 412             raise ConnectionError(err, request=request)
&gt;     413 
&gt;     414         except MaxRetryError as e:
&gt; 
&gt; ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote
&gt; end closed connection without response',))
</code></pre>
","314548","","3088138","","2019-11-06 16:24:21","2020-08-31 12:47:44","Python requests module Error - cant load any url: 'Remote end closed connection without response'","<python-3.x><python-requests><anaconda>","3","0","3","","","CC BY-SA 4.0","0"
"58047454","1","58639231","","2019-09-22 08:34:26","","8","21477","<p>While training the yolov3 framework, there's always this module error</p>

<p>I have tried reinstalling keras and tensorflow, and the version of keras is 2.3.0 and the version of tensorflow is 1.14.0.</p>

<pre><code>Traceback (most recent call last):
  File ""train.py"", line 6, in &lt;module&gt;
    import keras.backend as K
  File ""F:\Anacoda\lib\site-packages\keras\__init__.py"", line 3, in &lt;module&gt;
    from . import utils
  File ""F:\Anacoda\lib\site-packages\keras\utils\__init__.py"", line 27, in &lt;module&gt;
    from .multi_gpu_utils import multi_gpu_model
  File ""F:\Anacoda\lib\site-packages\keras\utils\multi_gpu_utils.py"", line 7, in &lt;module&gt;
    from ..layers.merge import concatenate
  File ""F:\Anacoda\lib\site-packages\keras\layers\__init__.py"", line 4, in &lt;module&gt;
    from ..engine.base_layer import Layer
  File ""F:\Anacoda\lib\site-packages\keras\engine\__init__.py"", line 8, in &lt;module&gt;
    from .training import Model
  File ""F:\Anacoda\lib\site-packages\keras\engine\training.py"", line 21, in &lt;module&gt;
    from . import training_arrays
  File ""F:\Anacoda\lib\site-packages\keras\engine\training_arrays.py"", line 14, in &lt;module&gt;
    from .. import callbacks as cbks
  File ""F:\Anacoda\lib\site-packages\keras\callbacks\__init__.py"", line 19, in &lt;module&gt;
    if K.backend() == 'tensorflow' and not K.tensorflow_backend._is_tf_1():
AttributeError: module 'keras.backend.tensorflow_backend' has no attribute '_is_tf_1'
</code></pre>
","12102026","","","","","2020-07-20 08:36:13","How to fix ' module 'keras.backend.tensorflow_backend' has no attribute '_is_tf_1''","<python-3.x><tensorflow><keras>","4","0","","","","CC BY-SA 4.0","0"
"49848517","1","","","2018-04-16 01:33:53","","14","21474","<p>FYI: this is NOT a duplicate! </p>

<p>Before running my python code I installed biopython in the cmd prompt:</p>

<pre><code>pip install biopython
</code></pre>

<p>I then get an error saying 'No module named Bio' when try to import it in python</p>

<pre><code>import Bio
</code></pre>

<p>The same thing happens with</p>

<pre><code>import biopython     
</code></pre>

<p>It should be noted I have updated PIP and run python 3.5.2<br>
 I appreciate anyone's help.</p>
","6318255","","6260170","","2020-06-01 16:00:34","2020-10-23 17:30:58","biopython no module named Bio","<python><python-3.x><pip><bioinformatics><biopython>","11","4","","","","CC BY-SA 3.0","0"
"35617246","1","35620064","","2016-02-25 02:43:35","","5","21455","<p>I'm currently making a game using PyGame (Python 3), and I'm looking for a way to make the game run at a fixed FPS.</p>

<p>Most of the game is located inside a giant while loop, where the user input is taken, sprites are rendered, etc. every tick. My goal is to be able to set a fixed FPS that will make the game run at the same speed on a fast or slow computer.</p>

<p>I can, of course, use the clock module in pygame:</p>

<pre><code>clock = pygame.time.Clock()
</code></pre>

<p>and then call this every loop:</p>

<pre><code>clock.tick(30)
</code></pre>

<p>but that will keep the game CAPPED at 30 FPS. So if I set it to 500 FPS it might still run as fast as it did before. My goal is that if I set it to 500 FPS it will run at the same SPEED as it would at 500 FPS...</p>

<p>So is it possible to make the game run at a fixed FPS (or make an illusion of that), regardless of the speed of the computer - or at least run at the same speed through the use of some frame-skip algorithm?</p>

<p>Sorry if that wording was rather confusing.</p>
","5129275","","","","","2016-02-26 06:17:37","Setting a fixed FPS in Pygame, Python 3","<python><performance><python-3.x><pygame><frame-rate>","1","0","2","","","CC BY-SA 3.0","0"
"41110531","1","41110903","","2016-12-12 22:08:27","","5","21438","<p>I'm working on a script to download a group of files. I successfully completed this and it's working decently. Now I have tried adding a dynamic printout of the progression of the download. </p>

<p>For small downloads (it's of .mp4 files by the way) such as 5MB, the progression works great and the file closes successfully, thus resulting in a complete and working downloaded .mp4 file. For larger files, such as 250MB and above, it does not work successfully, I get the following error: </p>

<p><a href=""https://i.stack.imgur.com/hztx7.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hztx7.png"" alt=""enter image description here""></a></p>

<p>And here's my code: </p>

<pre><code>import urllib.request
import shutil
import os
import sys
import io

script_dir = os.path.dirname('C:/Users/Kenny/Desktop/')
rel_path = 'stupid_folder/video.mp4'
abs_file_path = os.path.join(script_dir, rel_path)
url = 'https://archive.org/download/SF145/SF145_512kb.mp4'
# Download the file from `url` and save it locally under `file_name`:

with urllib.request.urlopen(url) as response, open(abs_file_path, 'wb') as out_file:

    eventID = 123456

    resp = urllib.request.urlopen(url)
    length = resp.getheader('content-length')
    if length:
        length = int(length)
        blocksize = max(4096, length//100)
    else:
        blocksize = 1000000 # just made something up

    # print(length, blocksize)

    buf = io.BytesIO()
    size = 0
    while True:
        buf1 = resp.read(blocksize)
        if not buf1:
            break
        buf.write(buf1)
        size += len(buf1)
        if length:
            print('\r[{:.1f}%] Downloading: {}'.format(size/length*100, eventID), end='')#print('\rDownloading: {:.1f}%'.format(size/length*100), end='')
    print()

    shutil.copyfileobj(response, out_file)
</code></pre>

<p>This works perfectly with small files, but larger ones I get the error. Now, I do NOT get the error, however, with larger files if I comment out the <em>progress</em> indicator code: </p>

<pre><code>with urllib.request.urlopen(url) as response, open(abs_file_path, 'wb') as out_file:

    # eventID = 123456
    # 
    # resp = urllib.request.urlopen(url)
    # length = resp.getheader('content-length')
    # if length:
    #     length = int(length)
    #     blocksize = max(4096, length//100)
    # else:
    #     blocksize = 1000000 # just made something up
    # 
    # # print(length, blocksize)
    # 
    # buf = io.BytesIO()
    # size = 0
    # while True:
    #     buf1 = resp.read(blocksize)
    #     if not buf1:
    #         break
    #     buf.write(buf1)
    #     size += len(buf1)
    #     if length:
    #         print('\r[{:.1f}%] Downloading: {}'.format(size/length*100, eventID), end='')#print('\rDownloading: {:.1f}%'.format(size/length*100), end='')
    # print()

    shutil.copyfileobj(response, out_file)
</code></pre>

<p>Does anyone have any ideas? This is the last part of my project and I would really like to be able to see the progress. Once again, this is Python 3.5. Thanks for any help provided!</p>
","2719598","","68587","","2016-12-12 22:42:39","2016-12-12 22:56:56","ConnectionResetError: An existing connection was forcibly closed by the remote host","<python><python-3.x>","1","6","","","","CC BY-SA 3.0","0"
"51824628","1","51824838","","2018-08-13 14:24:41","","5","21414","<p>I have installed pycryptodomex module on python 3.6.5 but when i try to execute the below call, i get the error mentioned in the headline</p>

<pre><code>from Crypto.Cipher import AES
</code></pre>

<p>I want to encrypt a file using AES. How to proceed now ?</p>
","9648095","","1222951","","2018-08-13 14:25:19","2020-05-22 22:00:45","ModuleNotFoundError: No module named 'Crypto' Error","<python><python-3.x><pycryptodome>","5","1","1","","","CC BY-SA 4.0","0"
"49281713","1","","","2018-03-14 15:28:26","","4","21413","<pre><code>file_location3 = ""F:/python/course1_downloads/City_Zhvi_AllHomes.csv""

housing = pd.read_csv(file_location3)    
housing.set_index(['State','RegionName'],inplace=True)
housing = housing.iloc[:, 49:]

housing = housing.groupby(pd.PeriodIndex(housing.columns,freq='Q'),axis=1).mean()

data = housing
data = data.iloc[:,'2008q3' : '2009q2']
</code></pre>

<p>The error that I am getting is:</p>

<blockquote>
  <p>cannot do slice indexing on <code>'&lt;class
  'pandas.core.indexes.period.PeriodIndex'&gt;</code> with these indexers <code>[2008q3]</code>
  of <code>&lt;'class 'str'&gt;</code></p>
</blockquote>

<h2>Now I'm getting another error</h2>

<pre><code>def price_ratio(row):
    return (row['2008q3'] - row['2009q2']) / row['2008q3']

data['up&amp;down'] = data.apply(price_ratio, axis=1)
</code></pre>

<p>This gives me error: <code>KeyError: ('2008q3', 'occurred at index 0')</code></p>
","9480915","","2181238","","2019-05-09 08:54:55","2019-10-01 22:12:34","cannot do slice indexing on <class 'pandas.core.indexes.period.PeriodIndex'> with these indexers","<python-3.x><pandas>","3","2","1","","","CC BY-SA 4.0","1"
"43325501","1","43329799","","2017-04-10 14:10:41","","26","21309","<p>Is it possible to write a sequence of <a href=""https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Promise"" rel=""noreferrer"">promise</a> (or tasks) using <strong>only</strong> <a href=""https://docs.python.org/3.6/library/index.html"" rel=""noreferrer"">Python 3.6.1 Standard Library</a>?</p>

<p>For example, a sequence promises in JavaScript is written as:</p>

<pre><code>const SLEEP_INTERVAL_IN_MILLISECONDS = 200;

const alpha = function alpha (number) {
    return new Promise(function (resolve, reject) {
        const fulfill = function() {
            return resolve(number + 1);
        };

        return setTimeout(fulfill, SLEEP_INTERVAL_IN_MILLISECONDS);
    });
};

const bravo = function bravo (number) {
    return new Promise(function (resolve, reject) {
        const fulfill = function() {
            return resolve(Math.ceil(1000*Math.random()) + number);
        };
        return setTimeout(fulfill, SLEEP_INTERVAL_IN_MILLISECONDS);
    });
};

const charlie = function charlie (number) {
    return new Promise(function (resolve, reject) {
        return (number%2 == 0) ? reject(number) : resolve(number);
    });
};

function run() {
    return Promise.resolve(42)
        .then(alpha)
        .then(bravo)
        .then(charlie)
        .then((number) =&gt; {
            console.log('success: ' + number)
        })
        .catch((error) =&gt; {
            console.log('error: ' + error);
        });
}

run();
</code></pre>

<p>Each function <a href=""https://developers.google.com/web/fundamentals/getting-started/primers/promises"" rel=""noreferrer"">also returns a Promise</a> with asynchronous processing result, that would be resolved/rejected by the immediately following promise.</p>

<p>I am aware of libraries such as <a href=""https://pypi.python.org/pypi/promise"" rel=""noreferrer""><code>promises-2.01b</code></a> and <a href=""https://pypi.python.org/pypi/asyncio/3.4.3"" rel=""noreferrer""><code>asyncio 3.4.3</code></a> and I am looking for a Python STL solution. Thus, if I need to import a non-STL library, I prefer using <a href=""https://github.com/ReactiveX/RxPY"" rel=""noreferrer"">RxPython</a> instead.</p>
","2886392","","2886392","","2017-04-10 14:55:37","2019-10-04 10:50:26","How do I write a sequence of promises in Python?","<javascript><python><python-3.x><promise><python-asyncio>","2","0","10","","","CC BY-SA 3.0","0"
"55508303","1","55513660","","2019-04-04 05:12:16","","4","21285","<p>How to write a list of list into excel using python 3?</p>

<pre><code>new_list = [[&quot;first&quot;, &quot;second&quot;], [&quot;third&quot;, &quot;fourth&quot;]]
with open(&quot;file_name.csv&quot;, 'w') as f:
    fc = csv.writer(f, lineterminator='\n')
fc.writerows(new_list)
</code></pre>
<p>I can write this list into <code>csv</code> file but How canI want to write in excel file?</p>
","10651396","","1092815","","2020-07-28 18:37:53","2020-07-28 18:37:53","How to write a list of list into excel using python?","<python><excel><python-3.x>","5","1","3","","","CC BY-SA 4.0","0"
"51697468","1","","","2018-08-05 19:11:47","","5","21281","<p>My machine has Geforce 940mx GDDR5 GPU.</p>

<p>I have installed all requirements to run GPU accelerated dlib (with GPU support):</p>

<ol>
<li><p>CUDA 9.0 toolkit with all 3 patches updates from <a href=""https://developer.nvidia.com/cuda-90-download-archive?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exelocal"" rel=""noreferrer"">https://developer.nvidia.com/cuda-90-download-archive?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exelocal</a></p></li>
<li><p>cuDNN 7.1.4 </p></li>
</ol>

<p>Then I executed all those below command after cloning dlib/davisKing repository on Github for compliling dlib with GPU support
:</p>

<pre><code>$ git clone https://github.com/davisking/dlib.git
$ cd dlib
$ mkdir build
$ cd build
$ cmake .. -DDLIB_USE_CUDA=1 -DUSE_AVX_INSTRUCTIONS=1
$ cmake --build .
$ cd ..
$ python setup.py install --yes USE_AVX_INSTRUCTIONS --yes DLIB_USE_CUDA
</code></pre>

<p>Now how could I possibly check/confirm if dlib(or other libraries depend on dlib like face_recognition of Adam Geitgey) is using GPU inside python shell/Anaconda(jupyter Notebook)?</p>
","9875693","","681865","","2018-08-06 04:43:20","2020-04-09 14:08:25","How to check if dlib is using GPU or not?","<python-3.x><tensorflow><face-recognition><dlib>","3","0","2","","","CC BY-SA 4.0","0"
"43668827","1","","","2017-04-27 22:19:47","","-1","21254","<p>I understand that  the Modulo function returns the remainder of a division problem.
Ex: 16 % 5 = 3 with a remainder of 1.  So 1 would be returned.</p>

<pre><code>&gt;&gt;&gt; 1 % 3      Three goes into 1 zero times remainder 1
1
&gt;&gt;&gt; 2 % 3      Three goes into 2 zero times remainder 2
2
&gt;&gt;&gt; 0 % 3      What happens here?  3 goes into zero, zero times remainder 3 
</code></pre>

<p>if we        follow the logic of the previous two illustrations, that is not what was returned, zero was.  Why?</p>

<pre><code>&gt;&gt;&gt; 0 % 3 
0
</code></pre>
","7907438","","2750492","","2017-04-27 22:30:29","2018-06-20 05:52:01","Python Modulo Function","<python-3.x><modulo>","2","2","1","","","CC BY-SA 3.0","0"
"28876791","1","28876824","","2015-03-05 11:27:11","","12","21253","<p>Code:</p>

<pre><code>from html.parser import HTMLParser
</code></pre>

<p>Traceback (most recent call last):</p>

<pre><code>  File ""program.py"", line 7, in &lt;module&gt;
    from html.parser import HTMLParser
ImportError: No module named 'html.parser'; 'html' is not a package
</code></pre>

<p>I call it with <code>python3 program.py</code></p>

<p>Python version: Python 3.4.0</p>
","3367446","","100297","","2015-03-05 11:29:04","2015-03-05 11:29:08","ImportError: No module named 'html.parser'; 'html' is not a package (python3)","<python><python-3.x><python-import>","2","0","2","2019-10-16 06:43:33","","CC BY-SA 3.0","0"
"35090948","1","","","2016-01-29 17:53:38","","5","21211","<p>I'm trying to extract text from a PDF (<a href=""https://www.sec.gov/litigation/admin/2015/34-76574.pdf"" rel=""noreferrer"">https://www.sec.gov/litigation/admin/2015/34-76574.pdf</a>) using PyPDF2, and the only result I'm getting is the following string:</p>

<pre><code>b''
</code></pre>

<p>Here is my code:</p>

<pre><code>import PyPDF2
import urllib.request
import io

url = 'https://www.sec.gov/litigation/admin/2015/34-76574.pdf'
remote_file = urllib.request.urlopen(url).read()
memory_file = io.BytesIO(remote_file)

read_pdf = PyPDF2.PdfFileReader(memory_file)
number_of_pages = read_pdf.getNumPages()
page = read_pdf.getPage(1)
page_content = page.extractText()
print(page_content.encode('utf-8'))
</code></pre>

<p>This code worked correctly on a few of the PDFs I'm working with (e.g. <a href=""https://www.sec.gov/litigation/admin/2016/34-76837-proposed-amended-distribution-plan.pdf"" rel=""noreferrer"">https://www.sec.gov/litigation/admin/2016/34-76837-proposed-amended-distribution-plan.pdf</a>), but the others like the file above didn't work. Any idea what's wrong?</p>
","5858459","","","","","2017-07-13 14:34:48","PyPDF2 won't extract all text from PDF","<python><python-3.x><pdf><pypdf>","2","3","3","","","CC BY-SA 3.0","0"
"43224000","1","43226253","","2017-04-05 06:59:41","","9","21205","<p>I have a list of files that I pass into a for loop and do a whole bunch of functions. Whats the easiest way to parallelize this? Not sure I could find this exact thing anywhere and I think my current implementation is incorrect because I only saw one file being run. From some reading I've done, I think this should be a perfectly parallel case.</p>

<p>Old code is something like this:</p>

<pre><code>import pandas as pd
filenames = ['file1.csv', 'file2.csv', 'file3.csv', 'file4.csv']
for file in filenames:
    file1 = pd.read_csv(file)
    print('running ' + str(file))
    a = function1(file1)
    b = function2(a)
    c = function3(b)
    for d in range(1,6):
            e = function4(c, d)
    c.to_csv('output.csv')
</code></pre>

<p>(incorrectly) Parallelized code</p>

<pre><code>import pandas as pd
from multiprocessing import Pool
filenames = ['file1.csv', 'file2.csv', 'file3.csv', 'file4.csv']
def multip(filenames):
    file1 = pd.read_csv(file)
    print('running ' + str(file))
    a = function1(file1)
    b = function2(a)
    c = function3(b)
    for d in range(1,6):
            e = function4(c, d)
    c.to_csv('output.csv')

if __name__ == '__main__'
    pool = Pool(processes=4)
    runstuff = pool.map(multip(filenames))
</code></pre>

<p>What I <strong><em>(think)</em></strong> I want to do is have <strong>one file</strong> be computed <strong>per core</strong> (maybe per process?). I also did</p>

<pre><code>multiprocessing.cpu_count()
</code></pre>

<p>and got 8 (I have a quad so its probably taking into account threads). Since I have around 10 files total, if I can put one file per process to speed things up that would be great! I would hope the remaining 2 files would find a process after the processes from the first round complete as well.</p>

<p>Edit: 
for further clarity, the functions (i.e. function1, function2 etc) also feed into other functions (i.e function1a, function1b) inside their respective files. I call function 1 using an import statement. </p>

<p>I get the following error:</p>

<pre><code>OSError: Expected file path name or file-like object, got &lt;class 'list'&gt; type
</code></pre>

<p>Apparently doesn't like being passed a list but i don't want to do filenames[0] in the if statement because that only runs one file</p>
","7470810","","7470810","","2017-04-05 08:07:57","2017-04-05 14:20:31","Multiprocessing Pool with a for loop","<python-3.x><python-multiprocessing>","1","0","1","","","CC BY-SA 3.0","0"
"31222137","1","31222251","","2015-07-04 15:10:06","","7","21156","<p>There is an mkv file in a folder named ""<code>export</code>"". What I want to do is to make a python script which fetches the file name from that export folder.
Let's say the folder is at ""<code>C:\Users\UserName\Desktop\New_folder\export</code>"".</p>

<p>How do I fetch the name?</p>

<p>I tried using this <code>os.path.basename</code> and <code>os.path.splitext</code> .. well.. didn't work out like I expected.</p>
","2408212","","2867928","","2018-03-14 09:41:25","2020-01-24 09:14:51","how to get name of a file in directory using python","<python><python-3.x><python-2.7><file>","6","2","3","","","CC BY-SA 3.0","0"
"48016351","1","48016620","","2017-12-29 01:26:25","","24","21141","<p><a href=""https://i.stack.imgur.com/KLHi2.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/KLHi2.png"" alt=""enter image description here""></a></p>

<p>I installed anaconda in C:\Program Files\Anaconda3. Every time to create a new env, I just do cmd and write:</p>

<pre><code>conda create --name envname python=3.5
</code></pre>

<p>But how can i install a new env from the ""environments.yml"" file </p>

<p><a href=""https://i.stack.imgur.com/7c0K9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7c0K9.png"" alt=""enter image description here""></a></p>
","7372400","","837534","","2017-12-29 02:18:20","2020-09-26 04:42:09","How to make new anaconda env from yml file","<python><python-3.x><anaconda>","5","0","4","","","CC BY-SA 3.0","0"
"41788017","1","41794225","","2017-01-22 06:32:18","","17","21138","<p>Python 3.x, Celery 4.x...</p>

<p>I have a class-based task.</p>

<p><code>myproj/celery.py</code></p>

<pre><code>from celery import Celery

# django settings stuff...

app = Celery('myproj')
app.autodiscover_tasks()
</code></pre>

<p><code>app1/tasks.py</code></p>

<pre><code>import celery

class EmailTask(celery.Task):
    def run(self, *args, **kwargs):
        self.do_something()
</code></pre>

<p>If I do:</p>

<pre><code>$ celery worker -A myproj -l info
[tasks]
  . app2.tasks.debug_task
  . app2.tasks.test
</code></pre>

<p>So, the celery decorators work to register tasks, but the class-based task is not registered.</p>

<h1>How do I get the class-based tasks to register?</h1>

<p><strong>Update 1:</strong></p>

<p>If I add the following lines to <code>app1/tasks.py</code></p>

<pre><code>from myproj.celery import app
email_task = app.tasks[EmailTask.name]
</code></pre>

<p>.</p>

<pre><code>$ celery worker -A myproj -l info
  File ""myproj/app1/tasks.py"", line 405, in &lt;module&gt;
    email_task = app.tasks[EmailTask.name]
  File ""/usr/local/lib/python3.5/site-packages/celery/app/registry.py"", line 19, in __missing__
    raise self.NotRegistered(key)
celery.exceptions.NotRegistered
</code></pre>

<p><strong>Update 2:</strong></p>

<p>I am able to execute my task synchronously (<code>run</code>) via a wrapper. However, I cannot run the task async, i.e., via <code>delay</code>.</p>

<p><code>app1/tasks.py</code></p>

<pre><code>@app.task
def email_task():
    """"""
    Wrapper to call class based task
    """"""
    task = EmailTask()
    # task.delay()  # Won't work!!!
    task.run()
</code></pre>

<p>.</p>

<pre><code>$./manage.py shell
&gt; from app1.tasks import EmailTask
&gt; task1 = EmailTask()
&gt; task1.run() # a-okay
&gt; task2 = EmailTask()
&gt; task2.delay() # nope
  &lt;AsyncResult: 1c03bad9-169a-4a4e-a56f-7d83892e8bbc&gt;

# And on the worker...
[2017-01-22 08:07:28,120: INFO/PoolWorker-1] Task app1.tasks.email_task[41e5bc7d-058a-400e-9f73-c853c0f60a2a] succeeded in 0.0701281649817247s: None
[2017-01-22 08:10:31,909: ERROR/MainProcess] Received unregistered task of type None.
The message has been ignored and discarded.
</code></pre>
","1870013","","1870013","","2017-01-22 08:20:23","2019-02-27 15:51:58","Register Celery Class-based Task","<python><python-3.x><celery>","2","1","4","","","CC BY-SA 3.0","0"
"31327762","1","31328058","","2015-07-09 20:38:56","","33","21137","<p>I know of these two questions about <code>__init__.py</code> and <code>__main__.py</code>:</p>

<p><a href=""https://stackoverflow.com/questions/448271"">What is __init__.py for?</a></p>

<p><a href=""https://stackoverflow.com/questions/4042905"">What is __main__.py?</a></p>

<p>But I don't really understand the difference between them.</p>
","4865723","","837710","","2019-12-26 05:07:28","2019-12-26 05:07:28","What is the difference between __init__.py and __main__.py?","<python><python-3.x>","2","1","10","2015-07-09 22:35:23","","CC BY-SA 4.0","0"
"42725140","1","","","2017-03-10 18:13:49","","3","21078","<p>Following the tutorial:</p>

<p><a href=""http://www.pyimagesearch.com/2016/08/10/imagenet-classification-with-python-and-keras/#comment-419896"" rel=""nofollow noreferrer"">http://www.pyimagesearch.com/2016/08/10/imagenet-classification-with-python-and-keras/#comment-419896</a></p>

<p>Using these files:</p>

<pre><code> https://github.com/fchollet/deep-learning-models
</code></pre>

<p>I get 2 separate errors depending on how I execute:</p>

<p>Running in PyCharm:</p>

<pre><code> Using TensorFlow backend.
 usage: test_imagenet.py [-h] -i IMAGE
 test_imagenet.py: error: the following arguments are required: -i/--image
</code></pre>

<p>Running in cmd line:</p>

<pre><code>     C:\Users\AppData\Local\Programs\Python\Python35\Scripts&gt;python deep-learning-models/test_imagenet.py --image deep-learning-models/images/dog.jpg
Traceback (most recent call last):
  File ""deep-learning-models/test_imagenet.py"", line 2, in &lt;module&gt;
    from keras.preprocessing import image as image_utils
ImportError: No module named keras.preprocessing
</code></pre>

<p>How do I resolve?</p>
","1239984","","","","","2017-05-10 14:11:45","ImportError: No module named keras.preprocessing","<python-3.x><tensorflow><keras>","3","0","1","","","CC BY-SA 3.0","0"
"53416226","1","53494582","","2018-11-21 16:13:31","","15","21072","<p>I have a pandas dataframe. i want to write this dataframe to parquet file in S3.
I need a sample code for the same.I tried to google it. but i could not get a working sample code.</p>
","10395566","","","","","2020-09-12 12:37:15","How to write parquet file from pandas dataframe in S3 in python","<python-3.x><amazon-s3><parquet>","4","0","3","","","CC BY-SA 4.0","0"
"41354205","1","","","2016-12-28 01:28:27","","10","21071","<p>I am trying to write a token based auth in flask for my android app. For that I need a unique token using which I can verify the user. </p>

<p>Itsdangerous library provide a JSONWebSignatureSerializer function using which I can create JWT token. So my first question is, is it safe to use JWT for mobile based auth ?</p>

<p>Secondly, I did a little bit research on how django rest framework generates its token.</p>

<pre><code>def generate_key(self):
    return binascii.hexlify(os.urandom(20)).decode()  
</code></pre>

<p>Is this token unique or just a random one? Which one should I use for a mobile based auth?</p>

<p>What is the based way to generate a unique token for mobile application in  python ?</p>
","4543743","","","","","2018-06-09 15:20:30","How to generate a unique auth token in python?","<python><django><python-3.x><flask>","4","1","3","","","CC BY-SA 3.0","0"
"32015356","1","32015481","","2015-08-14 17:11:08","","2","21066","<p>I have a string similar to <code>""dasdasdsafs[image : image name : image]vvfd gvdfvg dfvgd""</code>. From this string, I want to remove the part which stars from  <code>[image :</code> and ends at <code>: image]</code> . I tried to find the 'sub-string' using following code-  </p>

<pre><code>result = re.search('%s(.*)%s' % (start, end), st).group(1)
</code></pre>

<p>but it doesn't give me the required result. 
Help me to find the correct way to remove the sub-string from the string.</p>
","4124198","","1382937","","2015-08-15 03:58:09","2017-04-24 07:29:01","Find and remove a string starting and ending with a specific substring in python","<python><regex><python-2.7><python-3.x>","4","1","2","","","CC BY-SA 3.0","0"
"34271982","1","34272200","","2015-12-14 16:29:03","","20","21028","<p>I  was able to move to Linux mint 17.3 64 bit version from my Linux mint 16. This was long awaited migration.</p>

<p>After moving to Linux Mint 17.3, I am not able to the install python3-venv module, which is said to be the replacement for virtualenv in python 3.x. In my linux mint 16 I had access to pyvenv-3.4 tool. I dont know when I installed that module in Linux mint 16.</p>

<p>Anybody faced this issue ?</p>

<pre><code>python -m venv test
The virtual environment was not created successfully because ensurepip is not
available. On Debian/Ubuntu systems, you need to install the python3-venv
package using the following command.

apt-get install python3-venv

You may need to use sudo with that command. After installing the python3-venv
package, recreate your virtual environment.

izero@Ganesha ~/devel $ sudo apt-get install python3-venv
[sudo] password for izero:
Reading package lists... Done
Building dependency tree
Reading state information... Done
E: Unable to locate package python3-venv
</code></pre>
","906410","","","","","2017-07-20 09:15:46","Install python3-venv module on linux mint","<python><linux><python-3.x><linux-mint><python-venv>","3","0","7","","","CC BY-SA 3.0","0"
"41667617","1","41671941","","2017-01-15 23:43:50","","4","20980","<p>I would like to know how to overwrite a file in python. When I'm using <code>""w""</code> in the <code>open</code> statement, I still get only one line in my output file.</p>

<pre><code>article = open(""article.txt"", ""w"")
article.write(str(new_line))
article.close()
</code></pre>

<p>Can you tell me please how can I fix my problem?</p>
","7423060","","355230","","2020-02-16 19:57:48","2020-06-10 05:58:13","How to overwrite a file correctly?","<python><python-3.x>","2","1","1","","","CC BY-SA 4.0","0"
"39658864","1","39658973","","2016-09-23 10:46:02","","8","20976","<p>I am using PyCharm for executing my python programs. Today, I had tried updating all the packages using Project Interpreter. I received the following error in the process: </p>

<p><a href=""https://i.stack.imgur.com/8OWlz.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8OWlz.png"" alt=""enter image description here""></a></p>

<p>After which none of my python libraries are shown in Project Interpreter list. Could anyone please help me in fixing this problem?</p>

<p>I am using PyCharm Community Edition 2016.2.3 in Mac OS X 10.11.6. </p>

<p>Thanks. </p>
","6335670","","6622587","","2019-06-27 11:13:12","2019-07-26 13:17:56","PyCharm error: [Errno 13] Permission denied","<python><python-3.x><pycharm>","2","0","1","","","CC BY-SA 3.0","0"
"29840849","1","29841278","","2015-04-24 07:05:44","","15","20972","<p>EDIT: I put it in the title, but just realized I didn't mention it in the body. This seems to be specific to Windows.</p>

<p>I'm having a hard time writing output using the <code>csv</code> Python module in a script that works with both Python 2.7 and 3.3.</p>

<p>First try which works as expected in Python 2.7:</p>

<pre><code>with open('test.csv', 'wb') as csv_file:
    writer = csv.DictWriter(csv_file, ['header1', 'header2'])
    writer.writeheader()
    for item in items:
        writer.writerow(item)
</code></pre>

<p>However, when that same thing is run in Python 3.3 you wind up with:</p>

<pre><code>TypeError: 'str' does not support the buffer interface
</code></pre>

<p>So I change <code>'wb'</code> to <code>'wt'</code> and it runs, but now I have an extra blank row every other line in the file.</p>

<p>To fix that, I change:</p>

<pre><code>with open('test.csv', 'wt') as csv_file:
</code></pre>

<p>to:</p>

<pre><code>with open('test.csv', 'wt', newline='') as csv_file:
</code></pre>

<p>But now, it breaks Python 2.7:</p>

<pre><code>TypeError: 'newline' is an invalid keyword argument for this function
</code></pre>

<p>I know I could just do something like:</p>

<pre><code>try:
    with open('test.csv', 'wt', newline='') as csv_file:
        writer = csv.DictWriter(csv_file, ['header1', 'header2'])
        writer.writeheader()
        for item in items:
            writer.writerow(item)
except TypeError:
    with open('test.csv', 'wb') as csv_file:
        writer = csv.DictWriter(csv_file, ['header1', 'header2'])
        writer.writeheader()
        for item in items:
            writer.writerow(item)
</code></pre>

<p>However, that has some seriously bad duplication.</p>

<p>Does anyone have a cleaner way of doing this?</p>

<p>EDIT: The test data is simple and has no newlines or anything:</p>

<pre><code>items = [{'header1': 'value', 'header2': 'value2'},
         {'header1': 'blah1', 'header2': 'blah2'}]
</code></pre>
","810870","","810870","","2015-04-24 08:13:16","2017-10-05 02:44:07","Writing a .CSV file in Python that works for both Python 2.7+ and Python 3.3+ in Windows","<python><python-2.7><csv><python-3.x><python-3.3>","2","3","2","","","CC BY-SA 3.0","0"
"35533344","1","35533423","","2016-02-21 07:11:27","","3","20960","<blockquote>
  <p>Write a function that accepts an input list and returns a new list
  which contains only the unique elements (Elements should only appear
  one time in the list and the order of the elements must be preserved
  as the original list. ).</p>
</blockquote>

<pre><code>def unique_elements (list):
    new_list = []
    length = len(list)
    i = 0
    while (length != 0):
        if (list[i] != list [i + 1]):
            new_list.append(list[i])
        i = i + 1
        length = length - 1
    '''new_list = set(list)'''
    return (new_list)

#Main program
n = int(input(""Enter length of the list: ""))
list = []
for i in range (0, n):
    item = int(input(""Enter only integer values: ""))
    list.append(item)
print (""This is your list: "", list)
result = unique_elements (list)
print (result)
</code></pre>

<p>I am stuck with this error:</p>

<blockquote>
  <p>IndexError: list index out of range</p>
</blockquote>
","5406239","","1534017","","2016-02-22 09:07:13","2018-06-26 18:17:00","How to find unique elements in a list in python? (Without using set)","<python><python-2.7><python-3.x>","7","6","","","","CC BY-SA 3.0","0"
"38110384","1","38110429","","2016-06-29 21:31:25","","11","20955","<p>I want to convert any audio file (flac, wav,...) to mp3 with python
I am a noob , I tried <code>pydub</code> but I didn't found out how to make ffmpeg work with it, and If I'm right it can't convert flac file. </p>

<p>The idea of my project is to :
Make musicBee send the path of the 'now playing' track (by pressing the assigned shortcut) to my python file which would convert the music if it is not in mp3 and send it to a folder. (Everything in background so I don't have to leave what I'm doing to make the operation)</p>
","5472210","","","","","2020-03-20 15:28:48","Convert any audio file to mp3 with python","<python-3.x><audio><converter>","1","1","2","","","CC BY-SA 3.0","0"
"43629270","1","43629415","","2017-04-26 08:50:01","","17","20952","<p>I saw <a href=""https://stackoverflow.com/questions/20145902/how-to-extract-dictionary-single-key-value-pair-in-variables"">How to extract dictionary single key-value pair in variables</a> suggesting:</p>

<pre><code>d = {""a"":1}
(k, v), = d.items()
</code></pre>

<p>But: I only care about the <em>value</em>. And I want to pass that value to a method; like:</p>

<pre><code>foo(v)
</code></pre>

<p>So the question is: is there a simple command that works for both python2 and python3 that gives me that value directly, without the detour of the ""tuple"" assignment? </p>

<p>Or is there a way to make the tuple assignment work for my usecase of calling a method?</p>
","1531124","","9209546","","2018-07-10 11:41:36","2018-07-10 11:41:36","How to get single value from dict with single entry?","<python><python-3.x><dictionary>","4","2","2","","","CC BY-SA 3.0","0"
"53968641","1","","","2018-12-29 10:19:08","","5","20948","<p>I'm studying packages, and I have a question on creating a requirements.txt file.</p>

<p>If a module is imported from python, does it need to be mentioned in the requirements.txt file or can be left out?</p>

<p>I know that for separate modules, they need to be pip installed. For those modules that need to be pip installed, should they be written in the requirements.txt file as 'pip install modulename'?</p>
","2298759","","","","","2018-12-29 13:13:26","requirements.txt in python package","<python-3.x><package>","1","0","2","","","CC BY-SA 4.0","0"
"39424052","1","39424357","","2016-09-10 08:13:11","","9","20932","<p>I don't know how to set the coordinates to crop an image in <code>PIL</code>s <code>crop()</code>:</p>

<pre><code>from PIL import Image
img = Image.open(""Supernatural.xlsxscreenshot.png"")
img2 = img.crop((0, 0, 201, 335))
img2.save(""img2.jpg"")
</code></pre>

<p>I tried with <code>gThumb</code> to get coordinates, but if I take an area which I would like to crop, I can only find position 194 336. Could someone help me please?</p>

<p>This is my picture:</p>

<p><a href=""https://i.stack.imgur.com/1twQc.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1twQc.png"" alt=""enter image description here""></a></p>

<p>I wish to crop to this:</p>

<p><a href=""https://i.stack.imgur.com/hxqTZ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hxqTZ.png"" alt=""enter image description here""></a></p>
","3904216","","1391444","","2016-09-10 13:44:06","2016-09-10 13:44:06","How to set coordinates when cropping an image with PIL?","<python><image><python-2.7><python-3.x><image-processing>","1","0","3","","","CC BY-SA 3.0","0"
"32154121","1","32209886","","2015-08-22 08:39:41","","20","20886","<p>I am trying to connect to wss://api.poloniex.com and subscribe to ticker. I can't find any working example in python. I have tried to use autobahn/twisted and websocket-client 0.32.0.</p>

<p>The purpose of this is to get real time ticker data and store it in a mysql database.</p>

<p>So far I have tried to use examples provided in library documentation. They work for localhost or the test server but if I change to wss://api.poloniex.com I get a bunch of errors.</p>

<p>here is my attempt using websocket-client 0.32.0:</p>

<pre><code>from websocket import create_connection
ws = create_connection(""wss://api.poloniex.com"")
ws.send(""ticker"")
result = ws.recv()
print ""Received '%s'"" % result
ws.close()
</code></pre>

<p>and this is using autobahn/twisted:</p>

<pre><code>from autobahn.twisted.websocket import WebSocketClientProtocol
from autobahn.twisted.websocket import WebSocketClientFactory


class MyClientProtocol(WebSocketClientProtocol):

    def onConnect(self, response):
        print(""Server connected: {0}"".format(response.peer))

    def onOpen(self):
        print(""WebSocket connection open."")

        def hello():
            self.sendMessage(u""ticker"".encode('utf8'))
            self.sendMessage(b""\x00\x01\x03\x04"", isBinary=True)
            self.factory.reactor.callLater(1, hello)

        # start sending messages every second ..
        hello()

    def onMessage(self, payload, isBinary):
        if isBinary:
            print(""Binary message received: {0} bytes"".format(len(payload)))
        else:
            print(""Text message received: {0}"".format(payload.decode('utf8')))

    def onClose(self, wasClean, code, reason):
        print(""WebSocket connection closed: {0}"".format(reason))


if __name__ == '__main__':

    import sys

    from twisted.python import log
    from twisted.internet import reactor

    log.startLogging(sys.stdout)

    factory = WebSocketClientFactory(""wss://api.poloniex.com"", debug=False)
    factory.protocol = MyClientProtocol

    reactor.connectTCP(""wss://api.poloniex.com"", 9000, factory)
    reactor.run()
</code></pre>

<p>A complete but simple example showing how to connect and subscribe to to a websocket push api using any python library would be greatly appreciated.</p>
","4957074","","2335118","","2015-08-24 18:04:25","2017-12-30 21:56:45","How to connect to poloniex.com websocket api using a python library","<api><python-3.x><websocket>","4","0","18","","","CC BY-SA 3.0","0"
"47145934","1","47146640","","2017-11-06 21:16:36","","0","20885","<p>I have an experience in java and c++ for 7 years now. I recently started learning python. Can someone please help me on how to read the input for the matrix and display the same in matrix format. This is the code I wrote:</p>

<pre><code>import sys

# no of rows are equal to the number of columns.
n = int(input(""Enter the number of rows in a matrix""))
a = [[0 for x in range (n)] for y in range(n)]
for i in range (n):
    for j in range(n):
        a[i][j]=int(input())
        print (a[i][j])
    print(""\n"")
</code></pre>
","4531702","","4531702","","2018-07-03 19:24:03","2020-09-24 16:04:17","How to take matrix input from the user and display the matrix in Python?","<python><python-3.x>","6","1","","","","CC BY-SA 3.0","0"
"43165341","1","43167631","","2017-04-02 06:36:28","","10","20849","<p>This is my code.</p>

<pre><code>import requests
r = requests.get('https://academic.oup.com/journals')
</code></pre>

<p>then I got the error below;</p>

<pre><code>ConnectionError: ('Connection aborted.', OSError(""(104, 'ECONNRESET')"",))
</code></pre>

<p>Why I got the error?
What should I do?
the Request works well with other URLs like <a href=""https://www.google.com"" rel=""noreferrer"">https://www.google.com</a></p>
","7801849","","","","","2017-04-02 11:11:46","Python3 Requests ConnectionError: ('Connection aborted.', OSError(""(104, 'ECONNRESET')"",)) with a specific URL","<python-3.x><python-requests><anaconda>","1","0","3","","","CC BY-SA 3.0","0"
"41036535","1","41036763","","2016-12-08 09:58:22","","2","20845","<p>I am writing a program that needs to check if a users input is a decimal (the users input must be a deciamal number),I was wondering how I could test a varible to see if it contains only a decimal number.</p>

<p>Thanks,
Jarvey</p>
","5487247","","","","","2016-12-08 10:28:35","Python - how to test if a users input is a decimal number","<python><python-3.x><variables><input><decimal>","6","1","1","2016-12-08 22:08:26","","CC BY-SA 3.0","0"
"44641941","1","","","2017-06-20 00:34:34","","9","20814","<p>I am using pyinstaller(v3.2.1) to build a --onefile windows exe. I am using multiprocessing within my python (v3.5.3) script. I have implemented the below mentioned workaround for windows.</p>

<p><a href=""https://github.com/pyinstaller/pyinstaller/wiki/Recipe-Multiprocessing"" rel=""noreferrer"">Recipe Multiprocessing</a></p>

<p>Logically, my python script does not span multiple process unless required / conditions are met and is working as expected. The issue I have is that, whenever multiple processes are involved, everything seems fine.
But in case, if multiprocess is not involed, the below ""Fatal: Could not execute the script"" dialog box flashes for couple of seconds or more and then disappears still returning the expected results.</p>

<p><a href=""https://i.stack.imgur.com/zLUNf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zLUNf.png"" alt=""enter image description here""></a></p>

<p>Is there anything I am missing that is causing the fatal error dialog to appear and disappear ? I suspect the <code>multiprocessing.freeze()</code> statement right after <code>if __name__ == __main__ :</code> might be causing the issue when new processes are not created! </p>
","2425218","","","","","2019-09-18 13:46:37","Pyinstaller - ""Fatal error ! Failed to execute script"" when using multiprocessing.freeze_support","<python><python-3.x><multiprocessing><pyinstaller>","4","1","3","","","CC BY-SA 3.0","0"
"45786059","1","","","2017-08-20 19:31:29","","10","20796","<p>I used <code>nltk</code> in my code for a few days, but now, when I try to <code>import nltk</code>, I get the error:</p>

<pre><code>File ""C:\Users\Nada\Anaconda\lib\site-packages\nltk\corpus\reader\plaintext.py"", line 42, in PlaintextCorpusReader 
sent_tokenizer=nltk.data.LazyLoader(

AttributeError: module 'nltk' has no attribute 'data'
</code></pre>

<p>I installed <code>weka package</code> yesterday but it didn't work, I don't know in this has anything to do with that...</p>

<p>I tried to update it, but the error keeps showing up.</p>
","","user8451312","2145789","","2018-03-05 05:07:20","2020-08-17 23:13:59","NLTK - AttributeError: module 'nltk' has no attribute 'data'","<python-3.x><import><nltk>","7","2","","","","CC BY-SA 3.0","0"
"32058745","1","32058758","","2015-08-17 20:03:28","","31","20795","<p>I don't know how to interpret the output from Python's timeit.timeit() function. My code is as follows:</p>

<pre><code>import timeit

setup = """"""
import pydash
list_of_objs = [
    {},
    {'a': 1, 'b': 2, 0: 0},
    {'a': 1, 'c': 1, 'p': lambda x: x}
]
""""""
print(timeit.timeit(""pydash.filter_(list_of_objs, {'a': 1})"", setup=setup))
</code></pre>

<p>The output from this is <code>11.85382745500101</code>. How do I interpret this number?</p>
","4774955","","4774955","","2015-08-17 20:05:03","2015-08-17 20:06:16","What unit of time does timeit return?","<python><python-3.x><python-3.4><timeit>","1","0","2","","","CC BY-SA 3.0","0"
"47302343","1","47302961","","2017-11-15 08:12:37","","10","20782","<p>I have this code :</p>

<pre><code>plt.scatter(data_projected[:,0],data_projected[:,1],c=digits.target
        ,edgecolors='none',alpha=0.5,cmap=plt.cm.get_cmap('nipy_spectral',10));    
</code></pre>

<p>My confusion comes from <strong>plt.cm.get_cmap('nipy_spectral',10)</strong>. Sometimes there will be <strong>plt.cm.get_cmap('RdYlBu')</strong> instead.  </p>

<p>Is the <code>'RdYlBu'</code>,<code>'nipy_spectral'</code> the name of a color? And is there any other names to use instead?</p>

<p>Is there a list of all colors available?</p>

<p>I have read <a href=""http://matplotlib.org/api/cm_api.html"" rel=""noreferrer"">the document</a> but it does not seem to help or I do not understand it.</p>
","6032275","","6832816","","2019-04-08 17:58:22","2019-04-08 17:58:22","What names can be used in plt.cm.get_cmap?","<python><python-3.x><matplotlib><data-analysis><matplotlib-basemap>","2","1","2","","","CC BY-SA 4.0","0"
"51221710","1","51222722","","2018-07-07 09:20:49","","0","20773","<pre><code> from newsapi.sources import Sources
 import json
 api_key ='*******************'
 s = Sources(API_KEY=api_key)
</code></pre>

<p>they input the category of news they want</p>

<pre><code> wanted = input('&gt; ')
 source_list = s.get(category=wanted, language='en')

 index = 0
 sources = []
</code></pre>

<p>getting the sources
     for source in source_list[""sources""]:</p>

<pre><code>     data = json.dumps(source_list)
     data = json.loads(data)

     source = (data[""sources""][index][""url""])
     sources.append(source)
     index += 1


 from newspaper import Article

 i = len(sources) - 1
</code></pre>

<p>looping through the source list and printing the articles
     for source in sources:</p>

<pre><code>     url_ = sources[i]

     a = Article[url_]  
     print(a)

     i -= 1
</code></pre>

<p>getting error 'type' object is not subscriptable on the line <code>a = Article[url_]</code> have researched but still do not understand why in my case.</p>
","9044633","","","","","2018-07-09 23:04:15","type object not subscriptable - python","<python><python-3.x>","1","6","1","","","CC BY-SA 4.0","0"
"32002207","1","32002238","","2015-08-14 04:18:58","","18","20766","<p>Is it like a regular python set? </p>

<p>Suppose I have the following queryset</p>

<pre><code>entry_set = Entry.objects.all()
</code></pre>

<p>How do I check if Entry Object e is present in entry_set?</p>
","4130471","","","","","2020-02-27 23:06:06","How to check if an element is present in a Django queryset?","<python><django><python-2.7><python-3.x><django-models>","4","0","3","","","CC BY-SA 3.0","0"
"46657221","1","46657489","","2017-10-10 01:12:26","","15","20742","<p>Imagine I have a series of 4 possible Markovian states (A, B, C, D): </p>

<pre><code>X = [A, B, B, C, B, A, D, D, A, B, A, D, ....]
</code></pre>

<p>How can I generate a Markov transformation matrix using Python? The matrix must be 4 by 4, showing the probability of moving from each state to the other 3 states.
I've been looking at many examples online but in all of them, the matrix is given, not calculated based on data.
I also looked into hmmlearn but nowhere I read on how to have it spit out the transition matrix. Is there a library that I can use for this purpose?</p>

<p>Here is an R code for the exact thing I am trying to do in Python:
<a href=""https://stats.stackexchange.com/questions/26722/calculate-transition-matrix-markov-in-r"">https://stats.stackexchange.com/questions/26722/calculate-transition-matrix-markov-in-r</a></p>
","5411606","","5411606","","2017-10-10 17:13:46","2020-09-29 15:30:06","Generating Markov transition matrix in Python","<python-3.x><markov-chains><markov-models>","2","1","8","","","CC BY-SA 3.0","0"
"38469362","1","","","2016-07-19 22:11:06","","0","20738","<p>I'm writing a payroll calculator for school in python 3. The user input starts by asking for your name or ""0"" to quit the program. whenever I enter ""0"" at the start the program closes as it should, but if I enter it after calculating a users pay it prints (end of report and the previous payroll information). I can't figure out how to get it to stop printing the payroll information after you end it. This is what I have so far.</p>

<p>This is the code:
    One Stop Shop Payroll Calculator</p>

<pre><code>user = str
end = ""0""
hours = round(40,2)
print(""One Stop Shop Payroll Calculator"")
while user != end:
    print()
    user = input(""Please enter your name or type '0' to quit: "")
if user == end:
    print(""End of Report"")
else:
    hours = (float(input(""Please enter hours worked: "", )))
    payrate =(float(input(""Please enter your payrate: $"", )))
if hours &lt; 40:
    print(""Employee's name: "", user)
    print(""Overtime hours: 0"")
    print(""Overtime Pay: $0.00"")
    regularpay = round(hours * payrate, 2)
    print(""Gross Pay: $"", regularpay)
elif hours &gt; 40:
    overtimehours = round(hours - 40.00,2)
    print(""Overtime hours: "", overtimehours)
    print(""Employee's name: "", user)
    regularpay = round(hours * payrate,2)
    overtimerate = round(payrate * 1.5, 2)
    overtimepay = round(overtimehours * overtimerate)
    grosspay = round(regularpay+overtimepay,2)
    print(""Regular Pay: $"", regularpay)
    print(""Overtime Pay: $"",overtimepay)
    print(""Gross Pay: $"", grosspay)
</code></pre>

<p>This is how it shows up when you run it:</p>

<pre><code>One Stop Shop Payroll Calculator

Please enter your name or type '0' to quit: Brandon
Please enter hours worked: 50
Please enter your payrate: $10
Overtime hours:  10.0
Employee's name:  Brandon
Regular Pay: $ 500.0
Overtime Pay: $ 150
Gross Pay: $ 650.0

Please enter your name or type '0' to quit: Brandon
Please enter hours worked: 30
Please enter your payrate: $10
Employee's name:  Brandon
Overtime hours: 0
Overtime Pay: $0.00
Gross Pay: $ 300.0

Please enter your name or type '0' to quit: 0
End of Report
Employee's name:  0
Overtime hours: 0
Overtime Pay: $0.00
Gross Pay: $ 300.0

Process finished with exit code 0
</code></pre>

<p><a href=""http://i.stack.imgur.com/ycFto.png"" rel=""nofollow"">code</a></p>

<p><a href=""http://i.stack.imgur.com/lmAo0.png"" rel=""nofollow"">code being executed</a></p>
","6598353","","2912340","","2016-07-20 16:59:34","2020-08-20 14:03:40","Payroll Calculator in python","<python-3.x><pycharm>","3","1","","","","CC BY-SA 3.0","0"
"31187407","1","31187681","","2015-07-02 14:31:31","","7","20720","<p>I am trying to send the followings  ASCII command:
close1</p>

<p>using PySerial, below is my attempt:</p>

<pre><code>import serial

#Using  pyserial Library to establish connection
#Global Variables
ser = 0

#Initialize Serial Port
def serial_connection():
    COMPORT = 3
    global ser
    ser = serial.Serial()
    ser.baudrate = 38400 
    ser.port = COMPORT - 1 #counter for port name starts at 0




    #check to see if port is open or closed
    if (ser.isOpen() == False):
        print ('The Port %d is Open '%COMPORT + ser.portstr)
          #timeout in seconds
        ser.timeout = 10
        ser.open()

    else:
        print ('The Port %d is closed' %COMPORT)


#call the serial_connection() function
serial_connection()
ser.write('open1\r\n')
</code></pre>

<p>but as a result I am receiving the following error:</p>

<pre><code>Traceback (most recent call last):
      , line 31, in &lt;module&gt;
        ser.write('open1\r\n')
      , line 283, in write
        data = to_bytes(data)
      File ""C:\Python34\lib\site-packages\serial\serialutil.py"", line 76, in to_bytes
        b.append(item)  # this one handles int and str for our emulation and ints for Python 3.x
    TypeError: an integer is required
</code></pre>

<p>Not sure how I would be able to resolve that. close1 is just an example of an ASCII command I want to send there is also status1 to see if my locks are open or close, etc.</p>

<p>Thanks in advance</p>
","4534337","","","","","2015-07-02 14:43:38","Sending ASCII Command using PySerial","<python><python-3.x><serial-port><hardware><pyserial>","1","0","1","","","CC BY-SA 3.0","0"
"29715249","1","","","2015-04-18 09:13:30","","30","20713","<p>I want to output the <code>requirements.txt</code> for my Python 3 project in PyCharm. Any ideas?</p>
","3741571","","3001761","","2015-04-18 09:17:06","2020-09-06 19:13:25","Is there any way to output requirements.txt automatically?","<python><python-3.x><pycharm>","5","8","7","","","CC BY-SA 3.0","0"
"36671077","1","36671208","","2016-04-16 23:42:46","","20","20688","<p>In Python, it is possible to use one-liners to set values with special conditions (such as defaults or conditions) in a simple, intuitive way.</p>

<pre><code>result = 0 or ""Does not exist.""  # ""Does not exist.""

result = ""Found user!"" if user in user_list else ""User not found.""
</code></pre>

<p>Is it possible to write a similar statement that catches exceptions?</p>

<pre><code>from json import loads

result = loads('{""value"": true}') or ""Oh no, explosions occurred!""
# {'value': True}

result = loads(None) or ""Oh no, explosions occurred!""
# ""Oh no, explosions occurred!"" is desired, but a TypeError is raised.
</code></pre>
","6119465","","6119465","","2016-04-17 21:49:09","2018-01-22 02:08:40","One-Line Exception Handling","<python><python-3.x><exception><exception-handling>","2","10","7","","","CC BY-SA 3.0","0"
"34709576","1","","","2016-01-10 19:02:44","","6","20670","<p>I'm having an issue with modifying an array, by adding the percentage of each item compared to its row to a new matrix. This is the code providing error:</p>

<pre><code>for j in range(1,27):
        for k in range(1,27):
                let_prob[j,k] = let_mat[j,k]*100/(let_mat[j].sum())
</code></pre>

<p>I get the error:</p>

<blockquote>
  <p>RuntimeWarning: invalid value encountered in long_scalars</p>
</blockquote>

<p>I have tried rounding the denominator to no success.</p>
","4961318","","5276734","","2017-11-13 20:03:38","2018-12-19 13:14:02","RuntimeWarning: invalid value encountered in long_scalars","<python><arrays><python-3.x><numpy><runtime-error>","1","0","","","","CC BY-SA 3.0","0"
"43284811","1","43298736","","2017-04-07 18:24:43","","4","20657","<p>I have some interesting user data. It gives some information on the timeliness of certain tasks the users were asked to perform. I am trying to find out, if <code>late</code> - which tells me if users are on time (<code>0</code>), a little late (<code>1</code>), or quite late (<code>2</code>) - is predictable/explainable. I generate <code>late</code> from a column giving traffic light information (green = not late, red = super late). </p>

<p>Here is what I do: </p>

<pre><code>  #imports
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn import preprocessing
  from sklearn import svm
  import sklearn.metrics as sm




  #load user data
  df = pd.read_csv('April.csv', error_bad_lines=False, encoding='iso8859_15', delimiter=';')


  #convert objects to datetime data types
  cols = ['Planned Start', 'Actual Start', 'Planned End', 'Actual End']
  df = df[cols].apply(
  pd.to_datetime, dayfirst=True, errors='ignore'
  ).join(df.drop(cols, 1))

  #convert datetime to numeric data types
  cols = ['Planned Start', 'Actual Start', 'Planned End', 'Actual End']
  df = df[cols].apply(
  pd.to_numeric, errors='ignore'
  ).join(df.drop(cols, 1))


  #add likert scale for green, yellow and red traffic lights
  df['late'] = 0
  df.ix[df['End Time Traffic Light'].isin(['Yellow']), 'late'] = 1
  df.ix[df['End Time Traffic Light'].isin(['Red']), 'late'] = 2

  #Supervised Learning

    #X and y arrays
  # X = np.array(df.drop(['late'], axis=1))
  X = df[['Planned Start', 'Actual Start', 'Planned End', 'Actual End', 'Measure Package', 'Measure' , 'Responsible User']].as_matrix()

  y = np.array(df['late'])

    #preprocessing the data
  X = preprocessing.scale(X)


  #Supper Vector Machine
  clf = svm.SVC(decision_function_shape='ovo')
  clf.fit(X, y) 
  print(clf.score(X, y))
</code></pre>

<p>I am now trying to understand how to plot the decision boundaries.My goal is to plot a 2-way scatter with <code>Actual End</code> and <code>Planned End</code>. Naturally, I checked the documentation (see e.g. <a href=""http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html#sphx-glr-auto-examples-svm-plot-iris-py"" rel=""nofollow noreferrer"">here</a>). But I can't wrap my head around it. How does this work?</p>
","5779017","","5779017","","2017-04-08 08:03:28","2017-08-04 17:33:55","Plot SVM with Matplotlib?","<python-3.x><plot><scikit-learn><supervised-learning>","1","2","4","","","CC BY-SA 3.0","0"
"55823744","1","55825571","","2019-04-24 06:36:01","","14","20645","<p>I am establishing a connection to oracle 11g which is in a remote server using cx_oracle 7 with python 3.6.7. my OS in Ubuntu 18.04</p>

<p>I have installed oracle instant client library with libclntsh.so but I am not getting the expected output.</p>

<p>here is the code which i am using to connect to the oracle db</p>

<pre><code>connection = cx_Oracle.connect(""username/password@host/port"")
print (connection.version)
connection.close()
</code></pre>

<p>when the script runs i expect to get the connection version instead i am getting the following error message</p>

<blockquote>
  <p>File ""script.py"", line 13, in 
      connection = cx_Oracle.connect(""username/password@host/port"") cx_Oracle.DatabaseError: DPI-1047: Cannot locate a 64-bit Oracle
  Client library: ""libclntsh.so: cannot open shared object file: No such
  file or directory"". See
  <a href=""https://oracle.github.io/odpi/doc/installation.html#linux"" rel=""noreferrer"">https://oracle.github.io/odpi/doc/installation.html#linux</a> for help</p>
</blockquote>
","5841507","","","","","2020-09-25 22:24:33","How to fix: cx_Oracle.DatabaseError: DPI-1047: Cannot locate a 64-bit Oracle Client library - Python","<python-3.x><ubuntu-18.04><cx-oracle>","3","0","2","","","CC BY-SA 4.0","0"
"35149861","1","35150068","","2016-02-02 09:45:46","","13","20620","<p>What is the equivalent of <code>urllib.parse.quote</code></p>

<p>It's <code>urllib.urlencode()</code>?</p>

<p>Thanks</p>
","2362778","","","","","2016-03-06 20:47:02","Equivalent urllib.parse.quote() in python 2.7","<python><python-2.7><python-3.x>","3","0","3","","","CC BY-SA 3.0","0"
"29943146","1","","","2015-04-29 12:01:50","","10","20587","<p>I am new to python, trying to port a script in 2.x to 3.x i am encountering the error TypeError; Must use key word argument or key function in python 3.x. Below is the piece of code: Please help</p>

<pre><code>def resort_working_array( self, chosen_values_arr, num ):
    for item in self.__working_arr[num]:
        data_node = self.__pairs.get_node_info( item )

        new_combs = []
        for i in range(0, self.__n):
            # numbers of new combinations to be created if this item is appended to array
            new_combs.append( set([pairs_storage.key(z) for z in xuniqueCombinations( chosen_values_arr+[item], i+1)]) - self.__pairs.get_combs()[i] )
        # weighting the node
        item.weights =  [ -len(new_combs[-1]) ]    # node that creates most of new pairs is the best
        item.weights += [ len(data_node.out) ] # less used outbound connections most likely to produce more new pairs while search continues
        item.weights += [ len(x) for x in reversed(new_combs[:-1])]
        item.weights += [ -data_node.counter ]  # less used node is better
        item.weights += [ -len(data_node.in_) ] # otherwise we will prefer node with most of free inbound connections; somehow it works out better ;)

    self.__working_arr[num].sort( key = lambda a,b: cmp(a.weights, b.weights) )
</code></pre>
","4846265","","2867928","","2015-04-29 12:02:50","2018-12-13 00:22:30","TypeError; Must use key word argument or key function in python 3.x","<python-3.x>","3","3","1","","","CC BY-SA 3.0","0"
"31445728","1","31445907","","2015-07-16 05:07:59","","34","20586","<p>There is a <code>-&gt;</code>, or dash-greater-than symbol at the end of a python method, and I'm not sure what it means. One might call it an arrow as well.</p>

<p>Here is the example:</p>

<pre><code>@property
def get_foo(self) -&gt; Foo:
    return self._foo
</code></pre>

<p>where <code>self._foo</code> is an instance of Foo. </p>

<p>My guess is that it is some kind of static type declaration, to tell the interpreter that <code>self._foo</code> is of type Foo. But when I tested this, if <code>self._foo</code> is not an instance of Foo, nothing unusual happens. Also, if <code>self._foo</code> is of a type other than Foo, let's say it was an <code>int</code>, then <code>type(SomeClass.get_foo())</code> returns <code>int</code>. So, what's the point of <code>-&gt; Foo</code>?</p>

<p>This concept is hard to lookup because it is a symbol without a common name, and the term ""arrow"" is misleading.</p>
","1884158","","3100115","","2015-07-16 06:10:29","2015-11-25 23:08:11","What does the -> (dash-greater-than arrow symbol) mean in a Python method signature?","<python><python-3.x>","1","0","11","2018-12-05 23:27:21","","CC BY-SA 3.0","0"
"44798950","1","44800190","","2017-06-28 09:41:22","","4","20584","<p>I am new to Python and even newer to tkinter.  </p>

<p>I've utilised code from stackoverflow (<a href=""https://stackoverflow.com/questions/7546050/switch-between-two-frames-in-tkinter"">Switch between two frames in tkinter</a>) to produce a program where new frames are called and placed on top of each other depending on what options the user selects.  A stripped down version of my code is below.  There are a lot more frames. </p>

<pre><code>import tkinter as tk                
from tkinter import font  as tkfont 
import pandas as pd

class My_GUI(tk.Tk):

    def __init__(self, *args, **kwargs):
        tk.Tk.__init__(self, *args, **kwargs)

        self.title_font = tkfont.Font(family='Helvetica', size=18, weight=""bold"", slant=""italic"")


        container = tk.Frame(self)
        container.pack(side=""top"", fill=""both"", expand=True)
        container.grid_rowconfigure(0, weight=1)
        container.grid_columnconfigure(0, weight=1)

        self.frames = {}
        for F in (StartPage, Page_2):
            page_name = F.__name__
            frame = F(parent=container, controller=self)
            self.frames[page_name] = frame


            frame.grid(row=0, column=0, sticky=""nsew"")

        self.show_frame(""StartPage"")

    def show_frame(self, page_name):
        '''Show a frame for the given page name'''
        frame = self.frames[page_name]
        frame.tkraise()

class StartPage(tk.Frame):

    def __init__(self, parent, controller):
        tk.Frame.__init__(self, parent)
        self.controller = controller
        label = tk.Label(self, text=""Welcome to...."", font=controller.title_font)
        label.pack(side=""top"", fill=""x"", pady=10)

        button1 = tk.Button(self, text=""Option selected"",
                            command=lambda: controller.show_frame(""Page_2""))
        button1.pack()



class Page_2(tk.Frame):

    def __init__(self, parent, controller):
        tk.Frame.__init__(self, parent)
        self.controller = controller
        label = tk.Label(self, text=""The payment options are displayed below"", font=controller.title_font)
        label.pack(side=""top"", fill=""x"", pady=10)

        #I want the able to be display the dataframe here

        button = tk.Button(self, text=""Restart"",
                           command=lambda: controller.show_frame(""StartPage""))
        button.pack()

a = {'Option_1':[150,82.50,150,157.50,78.75],
     'Option2':[245,134.75,245,257.25,128.63]}
df = pd.DataFrame(a,index=['a',
                    'b',
                    'c',
                    'd',
                    'e']) 

print(df.iloc[:6,1:2])

if __name__ == ""__main__"":
    app = My_GUI()
    app.mainloop()
</code></pre>

<p>When Page_2 appears I want it to display a dataframe with the code below.</p>

<pre><code>a = {'Option_1':[150,82.50,150,157.50,78.75],
     'Option2':[245,134.75,245,257.25,128.63]}
df = pd.DataFrame(a,index=['a',
                    'b',
                    'c',
                    'd',
                    'e']) 

print(df.iloc[:6,1:2])
</code></pre>

<p>I've searched SO e.g. <a href=""https://stackoverflow.com/questions/42445059/how-to-display-a-pandas-dataframe-in-a-tkinter-window-tk-frame-to-be-precise"">How to display a pandas dataframe in a tkinter window (tk frame to be precise)</a> (no answer provided) and other websites for an answer to similar question but without success.</p>

<p>How and where would I place my dataframe code selection to appear in the area I want when I select Page_2?  </p>
","6414699","","","","","2020-05-03 17:11:02","How to display a dataframe in tkinter","<python-3.x><dataframe><tkinter>","2","0","2","","","CC BY-SA 3.0","0"
"33498061","1","33499916","","2015-11-03 11:32:36","","24","19848","<p>I am working on a program that involves large amounts of data. I am using the python pandas module to look for errors in my data. This usually works very fast. However this current piece of code I wrote seems to be way slower than it should be and I am looking for a way to speed it up.</p>

<p>In order for you guys to properly test it I uploaded a rather large piece of code. You should be able to run it as is. The comments in the code should explain what I am trying to do here. Any help would be greatly appreciated.</p>

<pre><code># -*- coding: utf-8 -*-

import pandas as pd
import numpy as np

# Filling dataframe with data
# Just ignore this part for now, real data comes from csv files, this is an example of how it looks
TimeOfDay_options = ['Day','Evening','Night']
TypeOfCargo_options = ['Goods','Passengers']
np.random.seed(1234)
n = 10000

df = pd.DataFrame()
df['ID_number'] = np.random.randint(3, size=n)
df['TimeOfDay'] = np.random.choice(TimeOfDay_options, size=n)
df['TypeOfCargo'] = np.random.choice(TypeOfCargo_options, size=n)
df['TrackStart'] = np.random.randint(400, size=n) * 900
df['SectionStart'] = np.nan
df['SectionStop'] = np.nan

grouped_df = df.groupby(['ID_number','TimeOfDay','TypeOfCargo','TrackStart'])
for index, group in grouped_df:
    if len(group) == 1:
        df.loc[group.index,['SectionStart']] = group['TrackStart']
        df.loc[group.index,['SectionStop']] = group['TrackStart'] + 899

    if len(group) &gt; 1:
        track_start = group.loc[group.index[0],'TrackStart']
        track_end = track_start + 899
        section_stops = np.random.randint(track_start, track_end, size=len(group))
        section_stops[-1] = track_end
        section_stops = np.sort(section_stops)
        section_starts = np.insert(section_stops, 0, track_start)

        for i,start,stop in zip(group.index,section_starts,section_stops):
            df.loc[i,['SectionStart']] = start
            df.loc[i,['SectionStop']] = stop

#%% This is what a random group looks like without errors
#Note that each section neatly starts where the previous section ended
#There are no gaps (The whole track is defined)
grouped_df.get_group((2, 'Night', 'Passengers', 323100))

#%% Introducing errors to the data
df.loc[2640,'SectionStart'] += 100
df.loc[5390,'SectionStart'] += 7

#%% This is what the same group looks like after introducing errors 
#Note that the 'SectionStop' of row 1525 is no longer similar to the 'SectionStart' of row 2640
#This track now has a gap of 100, it is not completely defined from start to end
grouped_df.get_group((2, 'Night', 'Passengers', 323100))

#%% Try to locate the errors
#This is the part of the code I need to speed up

def Full_coverage(group):
    if len(group) &gt; 1:
        #Sort the grouped data by column 'SectionStart' from low to high

        #Updated for newer pandas version
        #group.sort('SectionStart', ascending=True, inplace=True)
        group.sort_values('SectionStart', ascending=True, inplace=True)

        #Some initial values, overwritten at the end of each loop  
        #These variables correspond to the first row of the group
        start_km = group.iloc[0,4]
        end_km = group.iloc[0,5]
        end_km_index = group.index[0]

        #Loop through all the rows in the group
        #index is the index of the row
        #i is the 'SectionStart' of the row
        #j is the 'SectionStop' of the row
        #The loop starts from the 2nd row in the group
        for index, (i, j) in group.iloc[1:,[4,5]].iterrows():

            #The start of the next row must be equal to the end of the previous row in the group
            if i != end_km: 

                #Add the faulty data to the error list
                incomplete_coverage.append(('Expected startpoint: '+str(end_km)+' (row '+str(end_km_index)+')', \
                                    'Found startpoint: '+str(i)+' (row '+str(index)+')'))                

            #Overwrite these values for the next loop
            start_km = i
            end_km = j
            end_km_index = index

    return group

#Check if the complete track is completely defined (from start to end) for each combination of:
    #'ID_number','TimeOfDay','TypeOfCargo','TrackStart'
incomplete_coverage = [] #Create empty list for storing the error messages
df_grouped = df.groupby(['ID_number','TimeOfDay','TypeOfCargo','TrackStart']).apply(lambda x: Full_coverage(x))

#Print the error list
print('\nFound incomplete coverage in the following rows:')
for i,j in incomplete_coverage:
    print(i)
    print(j)
    print() 

#%%Time the procedure -- It is very slow, taking about 6.6 seconds on my pc
%timeit df.groupby(['ID_number','TimeOfDay','TypeOfCargo','TrackStart']).apply(lambda x: Full_coverage(x))
</code></pre>
","3685856","","3685856","","2019-01-17 12:43:00","2019-01-17 12:43:00","Pandas groupby apply performing slow","<python><python-3.x><pandas>","2","3","4","","","CC BY-SA 4.0","1"
"41681693","1","41681724","","2017-01-16 17:18:30","","3","18433","<p>I have a dataframe which has multiple columns. I'd like to iterate through the columns, counting for each column how many null values there are and produce a new dataframe which displays the sum of <code>isnull</code> values alongside the column header names. </p>

<p>If I do: </p>

<pre><code>for col in main_df:
    print(sum(pd.isnull(data[col])))
</code></pre>

<p>I get a list of the null count for each column:</p>

<pre><code>0
1
100
</code></pre>

<p>What I'm trying to do is create a new dataframe which has the column header alongside the null count, e.g. </p>

<pre><code>col1 | 0
col2 | 1
col3 | 100
</code></pre>
","5665505","","3930542","","2017-01-16 19:26:43","2020-05-01 21:54:41","pandas isnull sum with column headers","<python><python-3.x><pandas>","2","1","5","","","CC BY-SA 3.0","1"
"33204763","1","33204819","","2015-10-19 00:16:31","","10","15920","<p>I have a method called counting that takes 2 arguments. I need to call this method using the apply() method. However when I am passing the two parameters to the apply method it is giving the following error:</p>

<blockquote>
  <p>TypeError: counting() takes exactly 2 arguments (1 given)</p>
</blockquote>

<p>I have seen the following thread <a href=""https://stackoverflow.com/questions/21188504/python-pandas-apply-a-function-with-arguments-to-a-series-update?lq=1"">python pandas: apply a function with arguments to a series. Update</a> and I do not want to use functool.partial as I do not want to import additional classes to be able to pass parameters.</p>

<pre><code>def counting(dic, strWord):
    if strWord in dic:
        return dic[strWord]
    else:
        return 0

DF['new_column'] = DF['dic_column'].apply(counting, 'word')
</code></pre>

<p>If I give a single parameter, it works:</p>

<pre><code>def awesome_count(dic):
    if strWord in dic:
       return dic[strWord]
    else:
       return 0

DF['new_column'] = DF['dic_column'].apply(counting)
</code></pre>
","1181744","","-1","","2017-05-23 11:46:50","2016-04-30 09:36:25","How to pass multiple arguments to the apply function","<python><python-3.x><pandas>","2","3","4","","","CC BY-SA 3.0","1"
"49566862","1","49566995","","2018-03-30 00:21:24","","0","12308","<p>I am new to python. Just following the tutorial: <a href=""https://www.hackerearth.com/practice/machine-learning/machine-learning-projects/python-project/tutorial/"" rel=""nofollow noreferrer"">https://www.hackerearth.com/practice/machine-learning/machine-learning-projects/python-project/tutorial/</a></p>

<p>This is the dataframe miss:</p>

<pre><code>miss = train.isnull().sum()/len(train)
miss = miss[miss&gt;0]
miss.sort_values(inplace = True)
miss

Electrical      0.000685
MasVnrType      0.005479
MasVnrArea      0.005479
BsmtQual        0.025342
BsmtCond        0.025342
BsmtFinType1    0.025342
BsmtExposure    0.026027
BsmtFinType2    0.026027
GarageCond      0.055479
GarageQual      0.055479
GarageFinish    0.055479
GarageType      0.055479
GarageYrBlt     0.055479
LotFrontage     0.177397
FireplaceQu     0.472603
Fence           0.807534
Alley           0.937671
MiscFeature     0.963014
PoolQC          0.995205
dtype: float64
</code></pre>

<p>Now I just want to visualize those missing values""</p>

<pre><code>#visualising missing values
miss = miss.to_frame()
miss.columns = ['count']
miss.index.names = ['Name']
miss['Name'] = miss.index
</code></pre>

<p>And this is the error I got:</p>

<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-42-cd3b25e8862a&gt; in &lt;module&gt;()
      1 #visualising missing values
----&gt; 2 miss = miss.to_frame()

    C:\Users\Username\AppData\Local\Continuum\Anaconda3\lib\site-packages\pandas\core\generic.py in __getattr__(self, name)
       2742             if name in self._info_axis:
       2743                 return self[name]
    -&gt; 2744             return object.__getattribute__(self, name)
       2745 
       2746     def __setattr__(self, name, value):

    AttributeError: 'DataFrame' object has no attribute 'to_frame'
</code></pre>

<p>What am I missing here? </p>
","6043544","","6043544","","2018-03-30 00:25:35","2018-03-30 00:50:32","'DataFrame' object has no attribute 'to_frame'","<python><python-3.x><pandas>","1","3","1","","","CC BY-SA 3.0","1"
"48521740","1","48684022","","2018-01-30 12:40:19","","12","12275","<p>I am trying to use the <a href=""https://github.com/esafak/mca/blob/master/docs/usage.rst"" rel=""noreferrer"">mca package</a> to do multiple correspondence analysis in Python. </p>

<p>I am a bit confused as to how to use it. With <code>PCA</code> I would expect to <em>fit</em> some data (i.e. find principal components for those data) and then later I would be able to use the principal components that I found to <em>transform</em> unseen data.</p>

<p>Based on the MCA documentation, I cannot work out how to do this last step. I also don't understand what any of the weirdly cryptically named properties and methods do (i.e. <code>.E</code>, <code>.L</code>, <code>.K</code>, <code>.k</code> etc).</p>

<p>So far if I have a DataFrame with a column containing strings (assume this is the only column in the DF) I would do something like</p>

<pre><code>import mca
ca = mca.MCA(pd.get_dummies(df, drop_first=True))
</code></pre>

<p>from what I can gather</p>

<pre><code>ca.fs_r(1)
</code></pre>

<p>is the transformation of the data in <code>df</code> and </p>

<pre><code>ca.L
</code></pre>

<p>is supposed to be the eigenvalues (although I get a vector of <code>1</code>s that is one element fewer that my number of features?).</p>

<p>now if I had some more data with the same features, let's say <code>df_new</code> and assuming I've already converted this correctly to dummy variables, how do I find the equivalent of <code>ca.fs_r(1)</code> for the new data</p>
","1011724","","1011724","","2018-01-30 14:26:47","2020-04-14 16:02:51","Using mca package in Python","<python-3.x><pandas><scikit-learn><pca>","2","7","5","","","CC BY-SA 3.0","1"
"41400381","1","41400433","","2016-12-30 17:43:55","","10","10491","<p>In python pandas, there is a Series/dataframe column of str values to combine into one long string:</p>

<pre><code>df = pd.DataFrame({'text' : pd.Series(['Hello', 'world', '!'], index=['a', 'b', 'c'])})
</code></pre>

<p>Goal: 'Hello world !'</p>

<p>Thus far methods such as <code>df['text'].apply(lambda x: ' '.join(x))</code> are only returning the Series. </p>

<p>What is the best way to get to the goal concatenated string?</p>
","6552952","","704848","","2016-12-30 17:50:55","2016-12-31 11:53:11","Python Pandas concatenate a Series of strings into one string","<string><python-3.x><pandas><string-concatenation><series>","2","0","","","","CC BY-SA 3.0","1"
"48775841","1","48776205","","2018-02-13 21:21:31","","10","10230","<p>I am trying to use Python (with Pandas) to calculate the 20-day Exponential Moving Averages (EMA) of daily stock data for Intel (INTC). Pandas has a number of ways of doing this, and I've also tried stockstats, which runs on Pandas, but they never return the same EMA as I get from stock/finance websites.</p>

<p>I've double checked the close prices, and they match, but the EMA always comes out ""wrong"".</p>

<p>This is the CSV I'm using: <a href=""https://pastebin.com/f8F2Jgdh"" rel=""noreferrer"">INTC Stock Data</a></p>

<p>It contains the daily Date, Month Name, Open, High, Low, Close, Day Avg, and Volume for Intel's stock (Ticker: INTC) from 4/20/2016 to 2/1/2018.</p>

<p>When I look to the bigger stock websites like <a href=""https://www.marketwatch.com/investing/stock/intc/charts"" rel=""noreferrer"">MarketWatch</a> or <a href=""https://eresearch.fidelity.com/eresearch/evaluate/snapshot.jhtml?symbols=intc"" rel=""noreferrer"">Fidelity</a>, their numbers don't match mine. They match <em>each other</em>, but not me.</p>

<p>For example...</p>

<pre><code>df2['Close'].ewm(span=20,min_periods=0,adjust=False,ignore_na=False).mean()
</code></pre>

<p>or...</p>

<pre><code>df2['Close'].ewm(span=20, min_periods=20, adjust=True).mean()
</code></pre>

<p>or...</p>

<pre><code>df2[""Close""].shift().fillna(df[""Close""]).ewm(com=1, adjust=False).mean()
</code></pre>

<p>Give me EMA's for 2/1/2018 like $44.71, $47.65, $46.15, etc. when the <em>real 20-Day EMA</em> on <em>any</em> finance site is $45.65. And I get the wrong numbers no matter what date I try to compute the EMA for. It's even wrong when I just try for 5-Day EMAs.</p>

<p>I've read, watched and followed tutorials on the subject, but their results also don't match the accepted/published EMA's you'd find on any finance site. The people creating the tutorials and videos simply never check them against each other after Panda's crunches the numbers. And I need my numbers to match.</p>

<p>How do I arrive at the same figures every other finance site on the internet is getting for EMAs? I don't think this has anything to do with adjusted close prices because I'm using old/settled data and my close prices and dates are the same as theirs.</p>
","2495258","","202229","","2019-01-18 21:07:06","2019-01-18 21:07:06","Pandas' EMA not matching the stock's EMA?","<python><python-3.x><pandas><finance><stockquotes>","1","2","6","","","CC BY-SA 4.0","1"
"41481153","1","41481243","","2017-01-05 09:18:54","","8","9637","<p>I am trying to label a scatter/bubble chart I create from matplotlib with entries from a column in a pandas data frame. I have seen plenty of examples and questions related (see e.g. <a href=""https://stackoverflow.com/questions/40979026/how-to-label-data-points-in-matplotlib-scatter-plot-while-looping-through-pandas"">here</a> and <a href=""https://stackoverflow.com/questions/14432557/matplotlib-scatter-plot-with-different-text-at-each-data-point"">here</a>). Hence I tried to annotate the plot accordingly. Here is what I do: </p>

<pre><code>import matplotlib.pyplot as plt
import pandas as pd 
#example data frame
x = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]
y = [100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 600, 600]
s = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]
users =['mark', 'mark', 'mark', 'rachel', 'rachel', 'rachel', 'jeff', 'jeff', 'jeff', 'lauren', 'lauren', 'lauren']

df = pd.DataFrame(dict(x=x, y=y, users=users)

#my attempt to plot things
plt.scatter(x_axis, y_axis, s=area, alpha=0.5)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.annotate(df.users, xy=(x,y))
    plt.show()
</code></pre>

<p>I use a pandas datframe and I somehow get a KeyError- so I guess a <code>dict()</code> object is expected? Is there any other way to label the data using with entries from a pandas data frame? </p>
","5779017","","-1","","2017-05-23 10:31:09","2017-01-05 11:35:52","How to label bubble chart/scatter plot with column from pandas dataframe?","<python-3.x><pandas><matplotlib><scatter-plot>","2","0","5","","","CC BY-SA 3.0","1"
"40539377","1","40539507","","2016-11-11 00:29:33","","6","9254","<p>I have 2 similar data frames structured like this : </p>

<pre><code>ind = pd.MultiIndex.from_product([['Day 1','Day 2'],['D1','D2'],['Mean','StDev','StErr']], names = ['interval','device','stats'])
df =  pd.DataFrame({'col1':[1,2,3,4,5,6,7,8,9,10,11,12]}, index = ind)
print(df)

                       col1
interval device stats      
Day 1    D1     Mean      1
                StDev     2
                StErr     3
         D2     Mean      4
                StDev     5
                StErr     6
Day 2    D1     Mean      7
                StDev     8
                StErr     9
         D2     Mean     10
                StDev    11
                StErr    12

ind2 = pd.MultiIndex.from_product([['Day 1','Day 2'],['D1','D2'],['Ratio']], names = ['interval','device','stats'])
df2 =  pd.DataFrame({'col1':[100,200,300,400]}, index = ind2)
print(df2)

                       col1
interval device stats      
Day 1    D1     Ratio   100
         D2     Ratio   200
Day 2    D1     Ratio   300
         D2     Ratio   400
</code></pre>

<p>I'm trying to merge them to get this : </p>

<pre><code>                       col1
interval device stats      
Day 1    D1     Mean      1
                StDev     2
                StErr     3
                Ratio   100
         D2     Mean      4
                StDev     5
                StErr     6
                Ratio   200
Day 2    D1     Mean      7
                StDev     8
                StErr     9
                Ratio   300
         D2     Mean     10
                StDev    11
                StErr    12
                Ratio   400
</code></pre>

<p>I tried a bunch of different things using <code>join</code>, <code>concat</code>, and <code>merge</code> but the closest I've been able to get is using <code>df3 = pd.concat([df, df2], axis=1)</code>.  Unfortunately that gives me this : </p>

<pre><code>                          col1  col1
interval device stats            
Day 1    D1     Mean      1   NaN
                Ratio   NaN   100
                StDev     2   NaN
                StErr     3   NaN
         D2     Mean      4   NaN
                Ratio   NaN   200
                StDev     5   NaN
                StErr     6   NaN
Day 2    D1     Mean      7   NaN
                Ratio   NaN   300
                StDev     8   NaN
                StErr     9   NaN
         D2     Mean     10   NaN
                Ratio   NaN   400
                StDev    11   NaN
                StErr    12   NaN
</code></pre>
","4727766","","","","","2016-11-11 00:45:20","Merging multiindex dataframe in pandas","<python><python-3.x><pandas>","1","0","3","","","CC BY-SA 3.0","1"
"49564176","1","49584275","","2018-03-29 20:04:05","","6","9189","<p>I've got a machine learning task involving a large amount of text data. I want to identify, and extract, noun-phrases in the training text so I can use them for feature construction later on in the pipeline. 
I've extracted the type of noun-phrases I wanted from text but I'm fairly new to NLTK, so I approached this problem in a way where I can break down each step in list comprehensions like you can see below. </p>

<p>But my real question is, am I reinventing the wheel here? Is there a faster way to do this that I'm not seeing?</p>

<pre><code>import nltk
import pandas as pd

myData = pd.read_excel(""\User\train_.xlsx"")
texts = myData['message']

# Defining a grammar &amp; Parser
NP = ""NP: {(&lt;V\w+&gt;|&lt;NN\w?&gt;)+.*&lt;NN\w?&gt;}""
chunkr = nltk.RegexpParser(NP)

tokens = [nltk.word_tokenize(i) for i in texts]

tag_list = [nltk.pos_tag(w) for w in tokens]

phrases = [chunkr.parse(sublist) for sublist in tag_list]

leaves = [[subtree.leaves() for subtree in tree.subtrees(filter = lambda t: t.label == 'NP')] for tree in phrases]
</code></pre>

<p>flatten the list of lists of lists of tuples that we've ended up with, into
just a list of lists of tuples</p>

<pre><code>leaves = [tupls for sublists in leaves for tupls in sublists]
</code></pre>

<p>Join the extracted terms into one bigram</p>

<pre><code>nounphrases = [unigram[0][1]+' '+unigram[1][0] in leaves]
</code></pre>
","6472571","","610569","","2018-03-31 04:35:35","2018-04-01 01:32:58","Python (NLTK) - more efficient way to extract noun phrases?","<python-3.x><pandas><nlp><nltk><text-chunking>","1","0","2","","","CC BY-SA 3.0","1"
"42281851","1","","","2017-02-16 18:24:46","","2","8563","<p>I am trying to add padding to the left and the right side of my plot.
But when I change xlim and ylim; the image becomes smaller.</p>

<p>what am I doing wrong?</p>

<pre><code>import matplotlib.pyplot as plt
plt.rcParams['text.usetex'] = False
from matplotlib.font_manager import FontProperties
import seaborn as sns
%matplotlib inline



df1=df['Total Acc'].round(4)*100 
labels = ['AWA','Rem', 'S1', 'S2', 'SWS', 'SX', 'ALL'] 


rows = [df1.loc[label] for label in labels] 
for row in rows:  
    row.plot(figsize=(10, 5), marker='o')


# http://matplotlib.org/api/legend_api.html ---set_bbox_to_anchor(bbox, transform=None)
myLegend=plt.legend(labels, bbox_to_anchor=(0., 1.15, 1., .102), prop ={'size':10}, loc=10, ncol=7,  #left, bottom, width, height
                title=r'LEARNING CURVE - Fp1_RF(20)')                                         
myLegend.get_title().set_fontsize('18') 


plt.ylim(97.5, 98.5)
plt.xlim(0, 45) 

plt.xlabel('# of samples per subject')
plt.ylabel('Accuracy')  
</code></pre>

<p><a href=""https://i.stack.imgur.com/d6iQ5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/d6iQ5.png"" alt=""enter image description here""></a></p>
","6106842","","","","","2017-02-16 19:08:32","How to add padding to a plot in python?","<python><python-3.x><pandas><matplotlib><seaborn>","1","0","1","","","CC BY-SA 3.0","1"
"33161769","1","33163442","","2015-10-16 02:50:23","","6","8511","<p>I am trying the kaggle challenge <a href=""https://www.kaggle.com/c/hillary-clinton-emails"" rel=""nofollow"">here</a>, and unfortunately I am stuck at a very basic step. My limited python knowledge has to be blamed for this.
I am trying to read the <a href=""https://www.kaggle.com/c/hillary-clinton-emails/data"" rel=""nofollow"">datasets</a> into a pandas dataframe by executing following command:</p>

<pre><code>test = pd.DataFrame.from_csv(""C:/Name/DataMining/hillary/data/output/emails.csv"")
</code></pre>

<p>The problem is that this file as you would find out has over 300,000 records, but I am reading only 7945, 21.</p>

<pre><code>print (test.shape)
(7945, 21)
</code></pre>

<p>Now I have double checked the file and I cannot find anything special about line number 7945. Any pointers why this could be happening. Seems very ordinary situation, I hope some of you who have ran across this error can help me out.</p>
","4802200","","769871","","2017-03-02 17:54:55","2017-03-02 17:54:55","Not reading all rows while importing csv into pandas dataframe","<python-3.x><csv><pandas><machine-learning><kaggle>","1","0","","","","CC BY-SA 3.0","1"
"40842898","1","42160126","","2016-11-28 11:11:10","","3","8467","<p>I have a table in pandas dataframe df.</p>

<pre><code>   product_id_x    product_id_y    count       date
0   288472           288473          1     2016-11-08 04:02:07
1   288473           2933696         1     2016-11-08 04:02:07
2   288473           85694162        1     2016-11-08 04:02:07
</code></pre>

<p>i want to save this table in mysql database.</p>

<p>i am using MySQLdb package.</p>

<pre><code>import MySQLdb
conn = MySQLdb.connect(host=""xxx.xxx.xx.xx"", user=""name"", passwd=""pwd"", db=""dbname"")

df.to_sql(con = conn, name = 'sample_insert', if_exists = 'append', flavor = 'mysql', index = False)
</code></pre>

<p>i used this query to put it in my db.</p>

<p>but i am getting error.</p>

<pre><code>ValueError: database flavor mysql is not supported
</code></pre>

<p>my datatype is str for all the columns.</p>

<pre><code>type(df['product_id_x'][0]) = str
type(df['product_id_y'][0]) = str
type(df['count'][0])        = str
type(df['date'][0])         = str
</code></pre>

<p>i don't want to use sqlalchemy or other packages, can anyone tell what's the error here.
Thanks in advance</p>
","6803114","","6803114","","2016-11-28 11:34:08","2017-02-10 13:05:17","pandas dataframe to mysql db error database flavor mysql is not supported","<python><python-2.7><python-3.x><pandas>","2","0","1","","","CC BY-SA 3.0","1"
"56607664","1","56607678","","2019-06-15 05:36:04","","0","8189","<p>I search on all columns (except the first) of my DataFrame and add a new column 'Matching_Columns' with the name of the matching column,
When I try to remove all dots before testing if my pattern is contained within a row I receive an error.</p>

<p>This works:</p>

<pre><code>keyword='123456789'
f = lambda row: row.apply(str).str.contains(keyword ,na=False, flags=re.IGNORECASE)
df1 = df.iloc[:,1:].apply(f, axis=1)

df.insert(loc=1, column='Matching_Columns', value=df1.dot(df.columns[1:] + ', ').str.strip(', '))
</code></pre>

<p>This gives me an error:</p>

<pre><code>keyword='123456789'
f = lambda row: row.apply(str).str.replace(""."","""").contains(keyword ,na=False, flags=re.IGNORECASE)
df1 = df.iloc[:,1:].apply(f, axis=1)

df.insert(loc=1, column='Matching_Columns', value=df1.dot(df.columns[1:] + ', ').str.strip(', '))
</code></pre>

<p>Error:</p>

<pre><code>AttributeError: (""'Series' object has no attribute 'contains'"", 'occurred at index 0')
</code></pre>

<p>I can't figure out what's wrong with this. It looks like the <code>str.replace(""."","""")</code> is causing the error.   </p>

<p>Any help would be appreciated</p>
","439621","","","","","2019-06-15 06:04:13","Error: Series' object has no attribute 'contains'""","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"49209925","1","","","2018-03-10 14:01:50","","6","8007","<p>I am trying to use applymap to my dataset to create floats into integers. But I get the ""'Series' object has no attribute 'applymap'"" error.</p>

<pre><code>import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.pyplot import pie, axis, show
from pandas import Series,DataFrame

class Dataset():
def __init__(self, input):
    self.choice = input
    self.file = 0

def read(self):
    if self.choice == (""1""):
        self.file = pd.read_csv('mycsv.csv')
        self.file.plot(kind='bar')
        print(df)

        self.file['Value'].applymap(float)

def __str__(self):
    return str(self.file)

def applymap(self):
    return self.file.applymap(float)

i = (input(""Pick a DataSet= ""))
df = Dataset(i)
df.read()
plt.show()
</code></pre>
","8972207","","","","","2018-03-10 14:18:01","'Series' object has no attribute 'applymap'","<python><python-3.x><pandas>","1","1","1","","","CC BY-SA 3.0","1"
"41730514","1","41730654","","2017-01-18 22:22:14","","2","7786","<p>First question: please be kind. </p>

<p>I am having trouble loading a CSV file into a DataFrame on Spyder, using iPython. When I load an XLS file, it seems to have no problem and populates the new DataFrame variable into the variable explorer.</p>

<p>For example: </p>

<pre><code>import pandas as pd
energy = pd.read_excel('file.xls', skiprows=17)
</code></pre>

<p>The above returns a DataFrame, named energy, populated in the variable explorer (i.e. I can actually see the DataFrame). </p>

<p>However, when I try to load in a CSV file using the same method, it seems to read in the file, however it does not populate the variable explorer. </p>

<p>For example:</p>

<pre><code>import pandas as pd
GDP = pd.read_csv('file.csv')
</code></pre>

<p>When I run the above line, I don't get an error message, but the new DataFrame, <code>GDP</code>, does not populate the variable explorer. If I print <code>GDP</code> I get the values (268 rows x 60 columns). Am I not saving the new DataFrame correctly as a variable?</p>

<p>Thanks!</p>
","7298348","","4492932","","2017-01-18 22:24:16","2017-01-18 22:47:44","Load a csv file into pandas dataframe","<python><python-3.x><csv><pandas><spyder>","1","2","1","","","CC BY-SA 3.0","1"
"41580249","1","41580295","","2017-01-10 23:26:13","","18","7764","<p>I am using python 3.4 on Jupyter Notebook, trying to merge two data frame like below:</p>

<pre><code>df_A.shape
(204479, 2)

df_B.shape
(178, 3)

new_df = pd.merge(df_A, df_B,  how='inner', on='my_icon_number')
new_df.shape
(266788, 4)
</code></pre>

<p>I thought the <code>new_df</code> merged above should have few rows than <code>df_A</code> since merge is like an inner join. But why <code>new_df</code> here actually has more rows than <code>df_A</code>? </p>

<p>Here is what I actually want:</p>

<p>my <code>df_A</code> is like:</p>

<pre><code> id           my_icon_number
-----------------------------
 A1             123             
 B1             234
 C1             123
 D1             235
 E1             235
 F1             400
</code></pre>

<p>and my <code>df_B</code> is like:</p>

<pre><code>my_icon_number    color      size
-------------------------------------
  123              blue      small
  234              red       large 
  235              yellow    medium
</code></pre>

<p>Then I want <code>new_df</code> to be:</p>

<pre><code> id           my_icon_number     color       size
--------------------------------------------------
 A1             123              blue        small
 B1             234              red         large
 C1             123              blue        small
 D1             235              yellow      medium
 E1             235              yellow      medium
</code></pre>

<p>I don't really want to remove duplicates of my_icon_number in df_A. Any idea what I missed here?</p>
","3993270","","3993270","","2017-01-10 23:45:26","2017-01-10 23:45:26","pandas: merged (inner join) data frame has more rows than the original ones","<python><python-3.x><pandas><dataframe>","1","3","5","","","CC BY-SA 3.0","1"
"41714365","1","","","2017-01-18 08:19:08","","3","7437","<p><strong>Data Set in Question:</strong></p>

<p><a href=""https://i.stack.imgur.com/C5Vhb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C5Vhb.png"" alt=""enter image description here""></a> </p>

<p>For the data set show in the above image, I am trying to find the three most populous states while only taking into consideration the three most populous counties for each state. I use <code>CENSUS2010POP</code>.</p>

<p>This function should return a list of string values(in order of highest population to lowest population).</p>

<p><strong>Below is My Code:</strong></p>

<pre><code>x=census_df.groupby('STNAME')['CENSUS2010POP'].nlargest(3)
</code></pre>

<p>This statement returns a series in which it groups the three most populous counties of each state. </p>

<p>Now, what do I do beyond this to further find the most populous states? </p>

<p>Do I use loops or is there another efficient method to approach this? </p>
","7431202","","3752261","","2017-08-18 06:59:00","2020-09-19 21:23:04","how to find the three largest values in a data frame?","<python><python-3.x><pandas><dataframe><group-by>","5","0","2","","","CC BY-SA 3.0","1"
"40582073","1","40582165","","2016-11-14 04:53:31","","2","7229","<p>I'm looking to pull the historical data for ~200 securities in a given index. I import the list of securities from a csv file then iterate over them to pull their respective data from the quandl api. That dataframe for each security has 12 columns, so I create a new column with the name of the security and the Adjusted Close value, so I can later identify the series.</p>

<p>I'm receiving an error when I try to join all the new columns into an empty dataframe. I receive an attribute error:</p>

<pre><code>'''
Print output data
'''
grab_constituent_data()
AttributeError: 'Series' object has no attribute 'join'
</code></pre>

<p>Below is the code I have used to arrive here thus far.</p>

<pre><code>'''
Import the modules necessary for analysis
'''

import quandl
import pandas as pd
import numpy as np

'''
Set file pathes and API keys
'''

ticker_path = ''
auth_key = ''

'''
Pull a list of tickers in the IGM ETF
'''

def ticker_list():
    df = pd.read_csv('{}IGM Tickers.csv'.format(ticker_path))
    # print(df['Ticker'])
    return df['Ticker']

'''
Pull the historical prices for the securities within Ticker List
'''

def grab_constituent_data():
    tickers = ticker_list()
    main_df = pd.DataFrame()

    for abbv in tickers:
        query = 'EOD/{}'.format(str(abbv))
        df = quandl.get(query, authtoken=auth_key)
        print('Competed the query for {}'.format(query))

        df['{} Adj_Close'.format(str(abbv))] = df['Adj_Close'].copy()
        df = df['{} Adj_Close'.format(str(abbv))]
        print('Completed the column adjustment for {}'.format(str(abbv)))

        if main_df.empty:
            main_df = df
        else:
            main_df = main_df.join(df)

    print(main_df.head())
</code></pre>
","6539435","","","","","2016-11-14 05:04:08","Trying to iterate and join Pandas DFs: AttributeError: 'Series' object has no attribute 'join'","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"41954938","1","41954984","","2017-01-31 10:17:41","","3","7086","<p>How do I remove Date, Hours and Seconds from a pandas datetime, so that I'm left with only the minutes? I have a table of dates in the format:</p>

<pre><code>Date
2015-04-18 23:33:58
2015-04-19 14:32:08
2015-04-20 18:42:44
2015-04-20 21:41:19
</code></pre>

<p>and I want:</p>

<pre><code>Date
33
32
42
41
</code></pre>

<p>The code I'm trying to use is:</p>

<pre><code>fiveMin['Date'] = fiveMin['Date'] - pd.Timedelta(fiveMin['Date'], unit='s')
</code></pre>

<p>to remove the seconds, however I'm getting the error:</p>

<pre><code>Value must be Timedelta, string, integer, float, timedelta or convertible
</code></pre>
","6637269","","","","","2017-01-31 10:20:08","How to strip a pandas datetime of date, hours and seconds","<python><python-3.x><pandas><datetime>","1","0","1","","","CC BY-SA 3.0","1"
"49829023","1","49830789","","2018-04-14 07:36:36","","6","6854","<p>I'm currently trying to train a data set with a decision tree classifier but I couldn't get the train_test_split to work.</p>

<p>From the code below CS is the target output and EN SN JT FT PW YR LO LA are features input.</p>

<p>All variables that went through OHL are in sparse matrix format whereas the other are in array taken straight from the dataframe.</p>

<pre><code>def OHL(x, column): #OneHotEncoder
    le = LabelEncoder()
    enc = OneHotEncoder()
    Labeled = le.fit_transform(x[column].astype(str))
    return enc.fit_transform(Labeled.reshape(-1,1))

###------------------------------------------------------------------------

df = pd.read_csv('h1b_kaggle.csv')
df = df.drop(['Unnamed: 0','WORKSITE'],1)

###------------------------------------------------------------------------

CS = OHL(df, 'CASE_STATUS')
EN = OHL(df, 'EMPLOYER_NAME')
SN = OHL(df, 'SOC_NAME')
JT = OHL(df, 'JOB_TITLE')
FT = OHL(df, 'FULL_TIME_POSITION')
PW = np.array(df['PREVAILING_WAGE'])
YR = OHL(df, 'YEAR')
LO = np.array(df['lon'])
LA = np.array(df['lat'])
</code></pre>
","6511845","","6511845","","2018-04-14 08:05:43","2018-04-15 05:15:51","train_test_split with multiple features","<python><python-3.x><pandas><dataframe><scikit-learn>","1","2","3","","","CC BY-SA 3.0","1"
"48837006","1","48837030","","2018-02-17 01:18:34","","11","6695","<p>I'm looking to add a uuid for every row in a single new column in a pandas DataFrame. This obviously fills the column with the same uuid:</p>

<pre><code>import uuid
import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randn(4,3), columns=list('abc'),
                  index=['apple', 'banana', 'cherry', 'date'])
df['uuid'] = uuid.uuid4()
print(df)

               a         b         c                                  uuid
apple   0.687601 -1.332904 -0.166018  34115445-c4b8-4e64-bc96-e120abda1653
banana -2.252191 -0.844470  0.384140  34115445-c4b8-4e64-bc96-e120abda1653
cherry -0.470388  0.642342  0.692454  34115445-c4b8-4e64-bc96-e120abda1653
date   -0.943255  1.450051 -0.296499  34115445-c4b8-4e64-bc96-e120abda1653
</code></pre>

<p>What I am looking for is a new uuid in each row of the 'uuid' column. I have also tried using .apply() and .map() without success. </p>
","1141844","","9209546","","2019-10-21 12:23:47","2019-10-21 12:23:47","Add uuid to a new column in a pandas DataFrame","<python><python-3.x><pandas><dataframe><uuid>","4","0","1","","","CC BY-SA 3.0","1"
"50418139","1","50418270","","2018-05-18 19:33:05","","7","6477","<p>Say my dataframe is:</p>

<pre><code>df = pandas.DataFrame([[[1,0]],[[0,0]],[[1,0]]])
</code></pre>

<p>which yields:</p>

<pre><code>        0
0  [1, 0]
1  [0, 0]
2  [1, 0]
</code></pre>

<p>I want to drop duplicates, and only get elements [1,0] and [0,0], if I write:</p>

<pre><code>df.drop_duplicates()
</code></pre>

<p>I get the following error: TypeError: unhashable type: 'list'</p>

<p>How can I call drop_duplicates()?</p>

<p>More in general:</p>

<pre><code>df = pandas.DataFrame([[[1,0],""a""],[[0,0],""b""],[[1,0],""c""]], columns=[""list"", ""letter""])
</code></pre>

<p>And I want to call df[""list""].drop_duplicates(), so drop_duplicates applies to a Series and not a dataframe?</p>
","2835670","","2835670","","2018-05-18 19:54:00","2020-08-30 19:39:58","Pandas drop duplicates on elements made of lists","<python><python-3.x><pandas>","4","0","2","","","CC BY-SA 4.0","1"
"41681250","1","41681300","","2017-01-16 16:52:34","","14","6420","<p>I've tested it and also checked the documentation with no visible differences.Either way i wanted to ask just in case.</p>

<p>Do you think that read_csv should be used only for csv's even though it works for other types? while read_table works for anything? and if they're the same while they exist?</p>
","4660614","","","","","2020-07-27 20:45:04","Is there a difference between read_table and read_csv in pandas?","<python-3.x><pandas>","3","0","3","","","CC BY-SA 3.0","1"
"41581044","1","41581129","","2017-01-11 00:53:43","","7","6344","<p>I have the following data frame <code>my_df</code>:</p>

<pre><code>team      member
--------------------    
 A         Mary
 B         John
 C         Amy
 A         Dan
 B         Dave
 D         Paul
 B         Alex
 A         Mary
 D         Mary
</code></pre>

<p>I want the new output the new data frame <code>new_df</code> as:</p>

<pre><code>team      members              number
--------------------------------------
 A       [Mary,Dan]              2
 B       [John,Dave,Alex]        3
 C       [Amy]                   1
 D       [Paul,Mary]             2
</code></pre>

<p>I am wondering is there any existing pandas function can perform the above task? Thanks!</p>
","3993270","","3993270","","2017-01-11 01:05:10","2017-01-11 01:12:21","pandas: aggregate rows for a given column and count the number","<python><python-3.x><pandas>","3","2","2","","","CC BY-SA 3.0","1"
"57193292","1","","","2019-07-25 02:01:20","","3","6074","<p>I wanted to use pandas-profiling to do some eda on a dataset but I'm getting an error : AttributeError: 'DataFrame' object has no attribute 'profile_report'</p>

<p>I have created a python script on spyder with the following code : </p>

<p><code>import pandas as pd
import pandas_profiling
data_abc = pd.read_csv('abc.csv')
profile = data_abc.profile_report(title='Pandas Profiling Report')
profile.to_file(output_file=""abc_pandas_profiling.html"")</code></p>

<p>AttributeError: 'DataFrame' object has no attribute 'profile_report'</p>
","7874188","","","","","2020-10-16 14:35:02","Pandas-profiling error AttributeError: 'DataFrame' object has no attribute 'profile_report'","<python-3.x><pandas><pandas-profiling>","6","1","1","","","CC BY-SA 4.0","1"
"41718085","1","","","2017-01-18 11:20:22","","0","5999","<p>I have an excel spreadsheet that I read with this code: </p>

<pre><code>df=pd.ExcelFile('/Users/xxx/Documents/Python/table.xlsx')
ccg=df.parse(""CCG"")
</code></pre>

<p>With the sheet that I want inside the spreadsheet being <code>CCG</code></p>

<p>The sheet looks like this:  </p>

<pre><code>  col1  col2   col3    
x    a     1      2     
x    b     3      4    
x    c     5      6     
x    d     7      8
x    a     9      10
x    b     11     12
x    c     13     14
y    a     15     16
y    b     17     18
y    c     19     20
y    d     21     22
y    a     23     24
</code></pre>

<p>How would I write code that gets values of <code>col 2</code> and <code>col3</code> for rows that contain both <code>a</code> and <code>x</code>. So the proposed output for this table would be: <code>col1=[1,9]</code>, <code>col2=[2,10]</code></p>
","7079686","","3714940","","2017-01-18 12:47:41","2017-01-18 13:31:24","iterate through rows and columns in excel using pandas-Python 3","<excel><python-3.x><loops><pandas>","3","0","","","","CC BY-SA 3.0","1"
"49330523","1","49334104","","2018-03-16 22:51:16","","2","5710","<p>This is my data set</p>

<pre><code>fake_abalone2

   Sex   Length  Diameter  Height   Whole  Shucked  Viscera  Shell  Rings
                                    Weight  Weight  Weight  Weight
0   M    0.455    0.365    0.095    0.5140  0.2245  0.1010  0.1500  15
1   M    0.350    0.265    0.090    0.2255  0.0995  0.0485  0.0700  7
2   F    0.530    0.420    0.135    0.6770  0.2565  0.1415  0.2100  9
3   M    0.440    0.365    0.125    0.5160  0.2155  0.1140  0.1550  10
4   K    0.330    0.255    0.080    0.2050  0.0895  0.0395  0.0550  7
5   K    0.425    0.300    0.095    0.3515  0.1410  0.0775  0.1200  8
</code></pre>

<p>Getting syntax error while using the following method. Please help me out.
I want the value in ""sex"" table to change depending on ""Rings"" table.If ""Rings"" value is less than 10 the corresponding ""sex"" value should be changed to 'K'.Otherwise, no change should be made in ""Sex"" table.</p>

<pre><code> fake_abalone2[""sex""]=fake_abalone2[""Rings""].apply(lambda x:""K"" if x&lt;10)
</code></pre>

<blockquote>
  <p>File """", line 1
          fake_abalone2[""sex""]=fake_abalone2[""Rings""].apply(lambda x:""K"" if x&lt;10)</p>
  
  <p>SyntaxError: invalid syntax</p>
</blockquote>
","9505454","","9505454","","2018-03-17 05:44:55","2020-05-15 09:32:06","Apply Python lambda : if condition giving syntax error","<python-3.x><if-statement><lambda><pandas-apply>","3","3","0","","","CC BY-SA 3.0","1"
"56582506","1","56583444","","2019-06-13 14:14:47","","3","5507","<p>I have a df as follows:</p>

<pre><code>EscRF_P2P_Volt_V    ContextID   StepID  Time_Elapsed    iso_forest
12.4542121887207    7289972 15  32.472  1
27.1062278747559    7289972 15  33.444  1
622.710632324219    7289972 19  37.503  -1
622.710632324219    7289972 19  38.513000000000005  -1
622.710632324219    7289972 19  39.503  -1
622.710632324219    7289972 19  40.503  -1
622.710632324219    7289972 19  41.503  -1
622.710632324219    7289972 19  42.503  -1
622.710632324219    7289972 19  43.503  -1
622.710632324219    7289972 19  44.503  -1
622.710632324219    7289972 19  45.532000000000004  -1
622.710632324219    7289972 19  46.502  -1
622.710632324219    7289972 19  47.501000000000005  -1
622.710632324219    7289972 19  48.501000000000005  -1
622.710632324219    7289972 19  49.501000000000005  -1
622.710632324219    7289972 19  50.501000000000005  -1
622.710632324219    7289972 19  51.501000000000005  -1
622.710632324219    7289972 19  52.501000000000005  -1
622.710632324219    7289972 19  53.502  -1
23.4432239532471    7289973 24  102.49000000000001  1
23.4432239532471    7289973 24  104.078 1
22.7106227874756    7289973 24  104.92800000000001  1
23.4432239532471    7289973 24  105.81800000000001  1
23.4432239532471    7289973 24  106.918 1
22.7106227874756    7289973 24  107.61000000000001  1
23.4432239532471    7289973 24  108.65  1
23.4432239532471    7289973 24  108.73100000000001  1
23.4432239532471    7289973 24  109.19800000000001  1
23.4432239532471    7289973 24  110.12  1
23.4432239532471    7289973 24  111.11000000000001  1
23.4432239532471    7289973 24  112.108 1
22.7106227874756    7289973 24  113.11000000000001  1
21.97802162170409   7289973 24  114.11000000000001  -1
22.7106227874756    7289973 24  115.11000000000001  1
23.4432239532471    7289973 24  116.11000000000001  1
22.7106227874756    7289973 24  117.18  1
23.4432239532471    7289973 24  118.13000000000001  1
23.4432239532471    7289973 24  119.13000000000001  1
</code></pre>

<p>The <code>ContextID</code> is a product and I have 1400 different ContextIDs meaning that I have 1400 different products. I am trying to overlap and plot 200 different ContextIDs at once in one window, the next 200 in the second window and so on. In the end, I get 7 windows for 1400 products. For achieving the same, I wrote the following code:</p>

<pre><code>from itertools import zip_longest

def grouper(iterable, n, fillvalue = None):
    #Collect data into fixed-length chunks or blocks
    #grouper('ABCDEFG', 3, 'x') --&gt; ABC DEF Gxx""
    args = [iter(iterable)] * n
    return zip_longest(*args, fillvalue=fillvalue)

def add_to_axes(ax, data, param, context_id = None):
    data.plot(x = ""Time_Elapsed"", y = param, label = context_id, ax = ax)
    outlier = data[""iso_forest""] == -1
    data[~outlier].plot.scatter(x = ""Time_Elapsed"", y = param, color = ""green"", ax = ax)
    data[outlier].plot.scatter(x = ""Time_Elapsed"", y = param, color = ""red"", ax = ax)

def group_plots(df, param, group_size = 200):
    for group in grouper(df.groupby(""ContextID""), n = group_size):
        fig, ax = plt.subplots()
        for context_id, data in filter(None, group):
            add_to_axes(ax, data, param, context_id)
            ax.legend().set_visible(False)
        yield fig
</code></pre>

<p>and when I do</p>

<pre><code>Epvv['plot_data'] = Epvv['merged_df']
for i, fig in enumerate(group_plots(Epvv['plot_data'], 'EscRF_P2P_Volt_V')):
    show()
</code></pre>

<p>for the above df, it gives me the following error:</p>

<pre><code>    for i, fig in enumerate(group_plots(Epvv['plot_data'], 'EscRF_P2P_Volt_V')):
    show()
Traceback (most recent call last):

  File ""&lt;ipython-input-10-cdcfbb39cc21&gt;"", line 1, in &lt;module&gt;
    for i, fig in enumerate(group_plots(Epvv['plot_data'], 'EscRF_P2P_Volt_V')):

  File ""&lt;ipython-input-1-b8732ccafe84&gt;"", line 46, in group_plots
    add_to_axes(ax, data, param, context_id)

  File ""&lt;ipython-input-1-b8732ccafe84&gt;"", line 39, in add_to_axes
    data[outlier].plot.scatter(x = ""Time_Elapsed"", y = param, color = ""red"", ax = ax)

  File ""C:\Users\kashy\Anaconda3\envs\py36\lib\site-packages\pandas\plotting\_core.py"", line 3516, in scatter
    return self(kind='scatter', x=x, y=y, c=c, s=s, **kwds)

  File ""C:\Users\kashy\Anaconda3\envs\py36\lib\site-packages\pandas\plotting\_core.py"", line 2942, in __call__
    sort_columns=sort_columns, **kwds)

  File ""C:\Users\kashy\Anaconda3\envs\py36\lib\site-packages\pandas\plotting\_core.py"", line 1973, in plot_frame
    **kwds)

  File ""C:\Users\kashy\Anaconda3\envs\py36\lib\site-packages\pandas\plotting\_core.py"", line 1740, in _plot
    kind=kind, **kwds)

  File ""C:\Users\kashy\Anaconda3\envs\py36\lib\site-packages\pandas\plotting\_core.py"", line 860, in __init__
    super(ScatterPlot, self).__init__(data, x, y, s=s, **kwargs)

  File ""C:\Users\kashy\Anaconda3\envs\py36\lib\site-packages\pandas\plotting\_core.py"", line 804, in __init__
    raise ValueError(self._kind + ' requires x column to be numeric')

ValueError: scatter requires x column to be numeric
</code></pre>

<p>But, when I apply the same to a second df</p>

<pre><code>BacksGas_Flow_sccm  ContextID   StepID  Time_Elapsed    iso_forest
1.953125    7289972 1   0.0 1
1.953125    7289972 1   0.055   1
2.05078125  7289972 2   0.156   -1
2.05078125  7289972 2   0.48700000000000004 -1
2.05078125  7289972 2   1.477   -1
1.953125    7289972 2   2.4770000000000003  1
1.7578125   7289972 2   3.4770000000000003  1
1.7578125   7289972 2   4.487   1
1.85546875  7289972 2   5.993   1
1.7578125   7289972 2   6.545000000000001   1
9.08203125  7289972 5   7.9830000000000005  -1
46.6796875  7289972 5   13.093  1
46.6796875  7289972 5   13.384  1
46.6796875  7289972 5   14.388000000000002  1
46.6796875  7289972 5   15.386000000000001  1
46.6796875  7289972 5   16.386000000000003  1
46.6796875  7289972 5   17.396  1
46.6796875  7289972 5   18.406000000000002  1
46.6796875  7289972 5   19.396  1
74.12109375 7289973 19  98.89800000000001   1
74.12109375 7289973 19  99.59800000000001   1
74.12109375 7289973 19  100.488 1
74.90234375 7289973 19  101.798 1
6.4453125   7289973 24  102.49000000000001  -1
3.515625    7289973 24  104.078 -1
2.5390625   7289973 24  104.92800000000001  -1
2.05078125  7289973 24  105.81800000000001  1
2.05078125  7289973 24  106.918 1
2.05078125  7289973 24  107.61000000000001  1
1.953125    7289973 24  108.65  1
</code></pre>

<p>and do</p>

<pre><code>Bgf['plot_data'] = Bgf['merged_df']
for i, fig in enumerate(group_plots(Bgf['plot_data'], 'BacksGas_Flow_sccm')):
    show()
</code></pre>

<p>it works perfectly fine without any errors an gives me 7 different windows, each having a time plot of 200 different products.</p>

<p>I also checked the datatype of both the dataframes and they were the same:</p>

<pre><code>print(Bgf['plot_data'].dtypes)
BacksGas_Flow_sccm    float64
ContextID              object
StepID                 object
Time_Elapsed          float64
iso_forest             object
dtype: object

print(Epvv['plot_data'].dtypes)
EscRF_P2P_Volt_V    float64
ContextID            object
StepID               object
Time_Elapsed        float64
iso_forest           object
dtype: object
</code></pre>

<p>I don't understand what is the mistake I am doing here, since the code is working for one df and not working for the other. Any help would be appreciated.</p>

<p>Thanks</p>
","11135962","","11135962","","2019-06-13 14:16:42","2019-06-13 15:03:49","ValueError: scatter requires x column to be numeric","<python><python-3.x><pandas><matplotlib>","1","6","","","","CC BY-SA 4.0","1"
"41951160","1","41951562","","2017-01-31 06:38:04","","1","5130","<p>I have a Series with Name as the index and a number in scientific notation such as 3.176154e+08. How can I convert this number to 317,615,384.61538464 with a thousands separator? I tried:</p>

<p>format(s, ',')</p>

<p>But it returns TypeError: non-empty format string passed to object.<strong>format</strong>
There are no NaNs in the data. </p>

<p>Thanks for your help!</p>
","7493449","","3930542","","2017-01-31 08:17:57","2020-07-25 15:31:01","Converting scientific notation in Series to commas and thousands separator","<python><python-3.x><pandas>","2","1","1","","","CC BY-SA 3.0","1"
"49172824","1","49172855","","2018-03-08 12:13:35","","2","4918","<p>I have a file with the below table:</p>

<pre><code>    Name        AvailableDate            totalRemaining
0   X3321       2018-03-14 13:00:00      200
1   X3321       2018-03-14 14:00:00      200
2   X3321       2018-03-14 15:00:00      200
3   X3321       2018-03-14 16:00:00      200
4   X3321       2018-03-14 17:00:00      193
</code></pre>

<p>I wanted to return a DataFrame with all the records in a specific <strong>time</strong> period regardless of the actual <strong>date</strong>.</p>

<p>I followed the example here:</p>

<p><a href=""https://stackoverflow.com/questions/35052691/filter-pandas-dataframe-by-time"">filter pandas dataframe by time</a></p>

<p>but when I execute the below:</p>

<pre><code>## setup
import pandas as pd
import numpy as np

### Step 2
### Check available slots
file2 = r'C:\Users\user\Desktop\Files\data.xlsx'

slots = pd.read_excel(file2,na_values='')

## filter the preferred ones
slots['nextAvailableDate'] = pd.to_datetime((slots['nextAvailableDate']))


slots['times'] = pd.to_datetime((slots['nextAvailableDate']))
slots = slots[slots['times'].between('21:00:00', '02:00:00')]
</code></pre>

<p>This returns empty DataFrame as well as this solution:</p>

<pre><code>slots = slots[slots['times'].dt.strftime('%H:%M:%S').between('21:00:00', '02:00:00')]
</code></pre>

<p>Is there a way to do it correctly without creating a columns for time separately? How I should approach this problem please?</p>

<p>My goal:</p>

<pre><code>Name        AvailableDate            totalRemaining
0   X3321       2018-03-14 21:00:00      200
1   X3321       2018-03-14 22:00:00      200
2   X3321       2018-03-14 23:00:00      200
3   X3321       2018-03-14 00:00:00      200
4   X3321       2018-03-14 01:00:00      193
</code></pre>

<p>for every day that appears in the data set.</p>
","7273886","","13302","","2019-09-15 06:40:11","2020-08-06 08:07:21","Pandas how to filter DataFrame on time period","<python><python-3.x><pandas><datetime><python-datetime>","2","0","1","","","CC BY-SA 4.0","1"
"57000903","1","57001947","","2019-07-12 05:54:09","","6","4850","<p>I have a large dataset which I have to convert to .csv format, I have 29 columns and more than a million lines. I am using python and pandas dataframe to handle this job. I figured that as the dataframe gets larger, appending any rows to is it getting more and more time consuming. I wonder if there is any faster way to this, sharing the relevant snippet from the code.</p>

<p>Any recommendations are welcome though.</p>

<pre class=""lang-py prettyprint-override""><code>
df = DataFrame()

for startID in range(0, 100000, 1000):
    s1 = time.time()
    tempdf = DataFrame()
    url = f'https://******/products?startId={startID}&amp;size=1000'

    r = requests.get(url, headers={'****-Token': 'xxxxxx', 'Merchant-Id': '****'})
    jsonList = r.json()  # datatype= list, contains= dict

    normalized = json_normalize(jsonList)
    # type(normal) = pandas.DataFrame
    print(startID / 1000) # status indicator
    for series in normalized.iterrows():  
        series = series[1] # iterrows returns tuple (index, series)
        offers = series['offers']
        series = series.drop(columns='offers')
        length = len(offers)

        for offer in offers:
            n = json_normalize(offer).squeeze()  # squeeze() casts DataFrame into Series
            concatinated = concat([series, n]).to_frame().transpose()
            tempdf = tempdf.append(concatinated, ignore_index=True)

    del normalized
    df = df.append(tempdf)
    f1 = time.time()
    print(f1 - s1, ' seconds')

df.to_csv('out.csv')
</code></pre>
","8562954","","","","","2019-07-12 07:47:05","What is the fastest and most efficient way to append rows to a DataFrame?","<python><python-3.x><pandas><dataframe><series>","1","6","4","","","CC BY-SA 4.0","1"
"41827167","1","","","2017-01-24 11:32:14","","0","4671","<p>I am trying the new version of folium so I am using map.save instead of map.create_map. It was working with the older version but when I am using the new code, I am getting an error again and again saying SyntaxError: Invalid syntax. But I think I ma using the right code:</p>

<p>I am running this script:</p>

<pre><code>import pandas, folium

df = pandas.read_csv("".....txt"")

map = folium.Map(location= [df[""LAT""].mean(), df[""LON""].mean()], zoom_start = 6, tiles = ""Stamen Terrain"")

def color(elev):
    minimum= int(min(df[""ELEV""]))
    step= int((max(df[""ELEV""])-min(df[""ELEV""]))/3 )
    if elev in range(minimum,minimum+step):
        col= ""blue""
    elif elev in range(minimum+step,(minimum+step)*2):
        col= ""orange""
    else:
        col = ""red""
    return col


for lat,lon,name,elev in zip(df['LAT'], df['LON'], df['NAME'], df['ELEV']):
   map.add_child(folium.Marker(location=[lat, lon], popup = name, icon=   folium.Icon(color= color(elev)))       



map.save(outfile= ""test.html"")
</code></pre>

<p>and I am getting this error:</p>

<pre><code>   ...: 
   File ""&lt;ipython-input-2-02945dfe5a14&gt;"", line 27
     map.save(outfile= ""test.html"")
       ^
 SyntaxError: invalid syntax
</code></pre>

<p>Am I doing something wrong?</p>
","7400738","","7453178","","2017-01-24 13:04:07","2017-02-15 19:36:32","Using folium new version map.save and getting a syntax error","<python-3.x><pandas><folium>","0","2","","","","CC BY-SA 3.0","1"
"41681616","1","41681743","","2017-01-16 17:14:25","","3","4506","<p>I'm comparing a set of eight algorithms (<code>solver</code> column) using a set of instances, each instance is executed once for each algorithm and a level of a parameter <code>D</code> (goes from 1 to 10). So, the resulting data frame would look like this:</p>

<pre><code>         instance  D    z             solver
0   1000_ep0.0075  1  994         threatened
1   1000_ep0.0075  1  993               desc
2   1000_ep0.0075  1  994             degree
3   1000_ep0.0075  1  993    threatened_desc
4   1000_ep0.0075  1  993  threatened_degree
5   1000_ep0.0075  1  994         desc_later
6   1000_ep0.0075  1  994       degree_later
7   1000_ep0.0075  1  993         dyn_degree
8   1000_ep0.0075  2  986         threatened
9   1000_ep0.0075  2  987               desc
10  1000_ep0.0075  2  988             degree
11  1000_ep0.0075  2  987    threatened_desc
12  1000_ep0.0075  2  986  threatened_degree
13  1000_ep0.0075  2  987         desc_later
14  1000_ep0.0075  2  988       degree_later
15  1000_ep0.0075  2  987         dyn_degree
....
</code></pre>

<p>Where the <code>z</code> column corresponds to the value found by the algorithm (smaller the better).</p>

<p>I would like to add a column to the dataframe, corresponding to the rank of each algorithm according to the value of <code>z</code> for each combination <code>&lt;instance, D&gt;</code>. For the example above, would be something like this:</p>

<pre><code>         instance  D    z             solver z_rank
0   1000_ep0.0075  1  994         threatened 2
1   1000_ep0.0075  1  993               desc 1
2   1000_ep0.0075  1  994             degree 2
3   1000_ep0.0075  1  993    threatened_desc 1
4   1000_ep0.0075  1  993  threatened_degree 1
5   1000_ep0.0075  1  994         desc_later 2
6   1000_ep0.0075  1  994       degree_later 2
7   1000_ep0.0075  1  993         dyn_degree 1
8   1000_ep0.0075  2  986         threatened 1
9   1000_ep0.0075  2  987               desc 2
10  1000_ep0.0075  2  988             degree 3
11  1000_ep0.0075  2  987    threatened_desc 2
12  1000_ep0.0075  2  986  threatened_degree 1
13  1000_ep0.0075  2  987         desc_later 2
14  1000_ep0.0075  2  988       degree_later 3
15  1000_ep0.0075  2  987         dyn_degree 2
...
</code></pre>

<p>Using <code>python-pandas</code>, this is what I could get so far:</p>

<pre><code>df.loc[:, 'z_rank'] = df_rg.groupby(['instance', 'D'])['z'].rank()
df.head(16)
         instance  D    z             solver  z_rank
0   1000_ep0.0075  1  994         threatened    47.5
1   1000_ep0.0075  1  993               desc    16.5
2   1000_ep0.0075  1  994             degree    47.5
3   1000_ep0.0075  1  993    threatened_desc    16.5
4   1000_ep0.0075  1  993  threatened_degree    16.5
5   1000_ep0.0075  1  994         desc_later    47.5
6   1000_ep0.0075  1  994       degree_later    47.5
7   1000_ep0.0075  1  993         dyn_degree    16.5
8   1000_ep0.0075  2  986         threatened     7.0
9   1000_ep0.0075  2  987               desc    18.5
10  1000_ep0.0075  2  988             degree    44.5
11  1000_ep0.0075  2  987    threatened_desc    18.5
12  1000_ep0.0075  2  986  threatened_degree     7.0
13  1000_ep0.0075  2  987         desc_later    18.5
14  1000_ep0.0075  2  988       degree_later    44.5
15  1000_ep0.0075  2  987         dyn_degree    18.5
</code></pre>

<p>Which is clearly not what I want.</p>

<p>Could somebody help me with that?</p>
","4594043","","","","","2017-10-03 21:31:18","Rank within groups using python-pandas","<python-3.x><pandas><data-analysis>","2","0","1","","","CC BY-SA 3.0","1"
"57165247","1","57165281","","2019-07-23 13:27:04","","2","4497","<p>in one of the columns in my dataframe I have five values:</p>

<pre><code>1,G,2,3,4
</code></pre>

<p>How to make it change the name of all ""G"" to 1</p>

<p>I tried:</p>

<pre><code>df = df['col_name'].replace({'G': 1})
</code></pre>

<p>I also tried: </p>

<pre><code>df = df['col_name'].replace('G',1)
</code></pre>

<p>""G"" is in fact 1 (I do not know why there is a mixed naming)</p>

<p>Edit:</p>

<p>works correctly with:</p>

<pre><code>df['col_name'] = df['col_name'].replace({'G': 1})
</code></pre>
","11552661","","11552661","","2019-07-23 13:49:41","2020-05-20 16:58:49","Rename column values using pandas DataFrame","<python><python-3.x><pandas><dataframe>","3","2","","","","CC BY-SA 4.0","1"
"48553303","1","48554976","","2018-01-31 23:47:41","","3","4477","<p>I want to write a pandas dataframe to csv.  One of the columns of the df has entries which are lists, e.g. [1, 2], [3, 4], ...</p>

<p>When I use df.to_csv('output.csv') and I open the output csv file, the commas are gone.  That is, the corresponding column of the csv has entries [1  2], [3  4], ....</p>

<p>Is there a way to write a dataframe to csv without removing the commas in these lists?  I've also tried using csv.writer.</p>
","3955122","","","","","2020-07-04 02:46:07","Writing dataframe to csv WITHOUT removing commas","<python><python-3.x><pandas><csv>","3","1","","","","CC BY-SA 3.0","1"
"57334052","1","57431666","","2019-08-02 22:24:22","","3","4470","<p>I am watching <a href=""https://youtu.be/NM8Ue4znLP8?t=219"" rel=""nofollow noreferrer"">this</a> tutorial where I am trying to use <a href=""https://addisonlynch.github.io/iexfinance/stable/configuration.html#output-formatting"" rel=""nofollow noreferrer"">iexfinance</a> to get stock data. You have the option to choose the data type when requesting the data, I chose ""pandas"". When I run the built-in function I get an error that reads <code>AttributeError: module 'pandas.compat' has no attribute 'string_types'</code></p>

<p>I am using python 3.7. I have uninstalled and reinstalled both iexfinance and pandas. I also created and IEX cloud account and passed in a secret key like the documentation states, but the same error. The tutorial doesn't mention any of these steps and its confusing why his works and mine isn't. </p>

<p>I have tried to make the code simpler by following examples on the website: Even when running:</p>

<pre><code>from iexfinance.stocks import Stock

df = Stock(""AAPL"", output_format=""pandas"")

print(df.get_quote().head())

</code></pre>

<p>The error persists</p>

<p>The expected output is: </p>

<pre><code>                      AAPL
avgTotalVolume    30578248
calculationPrice     close
change               -0.58
changePercent     -0.00298
close               207.27
</code></pre>

<p>The output I am receiving is:</p>

<pre><code>Traceback (most recent call last):
  File ""app.py"", line 18, in &lt;module&gt;
    df = Stock(""AAPL"", output_format=""pandas"")
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/iexfinance/stocks/base.py"", line 45, in __init__
    self.symbols = list(map(lambda x: x.upper(), _handle_lists(symbols)))
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/iexfinance/utils/__init__.py"", line 43, in _handle_lists
    if isinstance(l, (compat.string_types, int)):
AttributeError: module 'pandas.compat' has no attribute 'string_types'
</code></pre>
","8684435","","","","","2019-10-02 10:45:11","How to fix ""'pandas.compat' has no attribute 'string_types'"" error in python3","<python><python-3.x><pandas>","3","0","","","","CC BY-SA 4.0","1"
"41244981","1","41245443","","2016-12-20 14:45:43","","3","4417","<p>This is my dataframe (where the values in the authors column are comma separated strings):</p>
<pre><code>authors            book
Jim, Charles       The Greatest Book in the World
Jim                An OK book
Charlotte          A book about books
Charlotte, Jim     The last book
</code></pre>
<p>How do I transform it to a long format, like this:</p>
<pre><code>authors            book
Jim                The Greatest Book in the World
Jim                An OK book
Jim                The last book
Charles            The Greatest Book in the World
Charlotte          A book about books
Charlotte          The last book
</code></pre>
<p>I've tried extracting the individual authors to a list, <code>authors = list(df['authors'].str.split(','))</code>, flatten that list, matched every author to every book, and construct a new list of dicts with every match. But that doesn't seem very pythonic to me, and I'm guessing pandas has a cleaner way to do this.</p>
","2908879","","7758804","","2020-08-22 16:39:11","2020-10-21 00:49:50","How to extract comma separated values to individual rows in pandas?","<python-3.x><pandas>","2","0","3","","","CC BY-SA 4.0","1"
"49042433","1","49042625","","2018-03-01 04:03:40","","1","4404","<p>I am trying to use <code>seaborn.barplot</code> to plot data after grouping. My first approach is to generate a new data frame using the following approach:</p>

<pre><code>g_data = g_frame.groupby([""STG"",""GRP""])[""HRE""].mean()
g_data
</code></pre>

<p>Here is the output:</p>

<pre><code>STG   GRP    
S1    Control    0.561871
      OSA        0.589858
S2    Control    0.595950
      OSA        0.629775
S3    Control    0.629906
      OSA        0.674118
S4    Control    0.578875
      OSA        0.568370
S5    Control    0.557712
      OSA        0.569524
Name: HRE, dtype: float64
</code></pre>

<p>Next, I defined a plot function called <code>plot_v1(data)</code> as follows:</p>

<pre><code>def plot_v2(data):

    # Create the bar plot
    ax = sns.barplot(
        x=""STG"", y=""HRE"", hue=""GRP"",
        order=[""S1"", ""S2"", ""S3"", ""S4"", ""S5""],
        hue_order=[""Control"", ""OSA""],
        data=data)

    # Return the figure object and axis
    return plt.gcf(), ax

plot_v2(g_data);
</code></pre>

<p>This throws up an error saying:</p>

<pre><code>149                 if isinstance(input, string_types):
150                     err = ""Could not interpret input '{}'"".format(input)
--&gt; 151                     raise ValueError(err)
152 
153             # Figure out the plotting orientation

ValueError: Could not interpret input 'STG'
</code></pre>

<p>I am not sure what am I doing wrong. When I check the index values, it looks fine.</p>

<pre><code>g_data.index
MultiIndex(levels=[['S1', 'S2', 'S3', 'S4', 'S5'], ['Control', 'OSA']],
       labels=[[0, 0, 1, 1, 2, 2, 3, 3, 4, 4], [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]],
       names=['STG', 'GRP'])
</code></pre>
","2058518","","2058518","","2018-03-01 04:17:57","2018-03-01 04:27:37","Barplot after grouping data using seaborn","<python-3.x><pandas><matplotlib><seaborn><pandas-groupby>","1","2","","","","CC BY-SA 3.0","1"
"56932246","1","56932414","","2019-07-08 09:44:44","","2","4265","<p>I have a pandas dataframe that has been defined as empty and then I would like to add some rows to it after doing some calculations.</p>

<p>I have tried to do the following:</p>

<pre><code>test = pd.DataFrame(columns=['Name', 'Age', 'Gender'])

if #some statement:

test.append(['James', '95', 'M'])

</code></pre>

<p>If I try to print and then append to test shows</p>

<pre><code>print(test)

test.append(['a', 'a', 'a', 'a', 'a', 'a'])

print(test)

&gt;&gt;&gt;

Empty DataFrame
Columns: [Name, Age, Gender]
Index: []
Empty DataFrame
Columns: [Name, Age, Gender]
Index: []

</code></pre>

<p>So clearly the line is not being added to the dataframe.</p>

<p>I want the output to be</p>

<pre><code>Name | Age | Gender
James | 95 | M
</code></pre>
","9940344","","","","","2019-07-08 10:24:28","How to add new line to existing pandas dataframe?","<python><python-3.x><pandas>","6","0","","2019-07-08 10:27:57","","CC BY-SA 4.0","1"
"49203951","1","49204010","","2018-03-10 00:04:40","","3","4230","<p>In my dataset, I have a few rows which contain characters. 
I only need rows which contain all integers. What is the best possible way to do this? Below data set:
e.g I want to remove the rows 2nd and 3rd as they contain 051A, 04A, and 08B respectively.</p>

<pre><code>1   2017    0   321     3   20  42  18
2   051A    0   321     3   5   69  04A
3   460     0   1633    16  38  17  08B
4   1811    0   822     8   13  65  18
</code></pre>
","8637577","","9209546","","2018-03-10 11:07:37","2018-03-10 11:07:37","Python - Pandas Drop Rows with strings","<python><python-3.x><pandas>","6","2","3","","","CC BY-SA 3.0","1"
"48693734","1","48693776","","2018-02-08 19:54:38","","5","3969","<p>I have a function that does group by on a pandas dataframe. The problem is my dataframe can have variable number of columns. I want to aggregate: sum the last column by the first column. The name of the last column is different, but, the name of the first column is fixed.</p>

<p>How could I achieve the group by? I tried using iloc and by getting the column name of the last column using df.columns[-1], but, none of these tricks seem to work.</p>

<p>Are there any better ways to achieve this than changing the last column name to some common value?</p>
","2662206","","","","","2018-02-08 19:57:26","pandas dataframe groupby by column position","<python-3.x><pandas><pandas-groupby>","2","1","","","","CC BY-SA 3.0","1"
"49514662","1","49522300","","2018-03-27 13:47:00","","2","3865","<p>I have a <code>io.BytesIO</code> object, <code>iostream</code>, which is a be2 file read from disk, and I am going to append column headers to the table/<code>iostream</code>,</p>

<pre><code>f = io.BytesIO()
f.write(b'A,B,C,D\n')
f.write(iostream.getvalue())

pd.read_table(f, sep=',', index_col=False, error_bad_lines=False, encoding='utf-8', dtype=type_map)
</code></pre>

<p>but it gave me an error,</p>

<pre><code>pandas.errors.EmptyDataError: No columns to parse from file
</code></pre>

<p>I am wondering how to solve this issue. </p>

<p>Also tried</p>

<pre><code>f = io.StringIO()
f.write('A,B,C,D\n')    
f.write(iostream.getvalue().decode())

pd.read_table(f, sep=',', index_col=False, error_bad_lines=False, encoding='utf-8', dtype=type_map)
</code></pre>

<p>got error</p>

<pre><code>pandas.errors.ParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.
</code></pre>
","766708","","766708","","2018-03-27 15:05:09","2018-03-27 21:21:37","pandas does pd.read_table support io.BytesIO and StringIO?","<python-3.x><pandas><bytesio>","1","0","","","","CC BY-SA 3.0","1"
"32441551","1","32441614","","2015-09-07 15:00:27","","3","3683","<p>I'm trying to create a simple if statement in Pandas.</p>

<p>The excel version is as follows:</p>

<p>=IF(E2=""ABC"",C2,E2)</p>

<p>I'm stuck on how to assign it based on a string or partial string.</p>

<p>Here is what I have.</p>

<pre><code>df['New Value'] = df['E'].map(lambda x: df['C'] if x == 'ABC' else df['E']]
</code></pre>

<p>I know I'm making a mistake here. </p>

<p>As the outcome is the entire dataframe values in each cell.</p>

<p>Any help would be much appreciated!</p>
","5236124","","","","","2015-09-07 15:14:58","Pandas If Statements (excel equivalent)","<python-3.x><if-statement><pandas>","1","0","1","","","CC BY-SA 3.0","1"
"56728584","1","56729507","","2019-06-24 00:09:30","","0","3596","<p>I need to make a frequency dictionary from a pandas series (from the 'amino_acid' column in dataframe below) that also adds an adjacent row for each entry in the dictionary (from 'templates' column). </p>

<pre><code>    templates   amino_acid
0   118       CAWSVGQYSNQPQHF
1   635       CASSLRGNQPQHF
2   468       CASSHGTAYEQYF
3   239       CASSLDRLSSGEQYF
4   51        CSVEDGPRGTQYF
</code></pre>

<p>My current approach of iterating through the dataframe seems to be inefficient and even an anti-pattern according to <a href=""https://stackoverflow.com/a/55557758/7578354"">this post</a>. How can I improve the efficiency/use best practice for doing this?</p>

<p>My current approach:</p>

<pre class=""lang-py prettyprint-override""><code>sequence_counts = {}
seqs = list(zip(df.amino_acid, df.templates))

for seq in seqs:
    if seq[0] not in sequence_counts:
        sequence_counts[seq[0]] = 0
    sequence_counts[seq[0]] += seq[1]
</code></pre>

<p>I've seen people the below way, but can't figure out how to adjust it to add each respective 'templates' entry:</p>

<pre class=""lang-py prettyprint-override""><code>sequence_counts = df['amino_acid'].value_counts().to_dict()
</code></pre>

<p>Any help/feedback would be greatly appreciated! :)</p>
","7578354","","7578354","","2019-06-24 00:36:32","2019-12-27 13:58:47","Building a frequency dictionary from a pandas dataframe without looping","<python><python-3.x><pandas><dataframe><bioinformatics>","2","2","1","","","CC BY-SA 4.0","1"
"41577468","1","41577871","","2017-01-10 20:07:58","","5","3551","<p>How can I replace the values from certain columns in a pandas.DataFrame that occur rarely, i.e. with low frequency (while ignoring NaNs)?  </p>

<p>For example, in the following dataframe, suppose I wanted to replace any values in columns A or B that occur less than three times in their respective column. I want to replace these with ""other"":</p>

<pre><code>import pandas as pd
import numpy as np

df = pd.DataFrame({'A':['ant','ant','cherry', pd.np.nan, 'ant'], 'B':['cat','peach', 'cat', 'cat', 'peach'], 'C':['dog','dog',pd.np.nan, 'emu', 'emu']})
df
   A   |   B   |  C  |
----------------------
ant    | cat   | dog |
ant    | peach | dog |
cherry | cat   | NaN |
NaN    | cat   | emu |
ant    | peach | emu |
</code></pre>

<p>In other words, in columns A and B, I want to replace those values that occur twice or less (but leave NaNs alone).  </p>

<p>So the output I want is:</p>

<pre><code>   A   |   B   |  C  |
----------------------
ant    | cat   | dog |
ant    | other | dog |
other  | cat   | NaN |
NaN    | cat   | emu |
ant    | other | emu |
</code></pre>

<p>This is related to a previously posted question: <a href=""https://stackoverflow.com/questions/32511061/remove-low-frequency-values-from-pandas-dataframe"">Remove low frequency values from pandas.dataframe</a> </p>

<p>but the solution there resulted in an ""AttributeError: 'NoneType' object has no attribute 'any.'""  (I think because I have NaN values?)</p>
","6227378","","-1","","2017-05-23 11:46:05","2017-01-10 20:42:55","Replace low frequency categorical values from pandas.dataframe while ignoring NaNs","<python-3.x><pandas>","3","0","1","","","CC BY-SA 3.0","1"
"50059239","1","50059324","","2018-04-27 09:22:15","","1","3539","<p>I am trying to create a timed backup system for my excel document with <strong>Python</strong> as multiple users will be accessing it.
I want to change the path of the file not to my local directory.</p>

<p>Here's the code;</p>

<pre><code>import pandas as pd
import datetime
import numpy
now = datetime.datetime.now()
ct = now.strftime(""%Y-%m-%d %H.%M"")

table = pd.read_excel(r'Z:\new\Planner_New.xlsx',
                  sheet_name = 'Jan18',                
                  header = 0,
                  index_col = 0,
                  usecols = ""A:AY"",
                  convert_float = True)

writer = pd.ExcelWriter('Planner' + ct + '.xlsx', engine='xlsxwriter')
table.to_excel(writer, sheet_name = ""Jan18"")

workbook  = writer.book
worksheet = writer.sheets['Jan18']

format1 = workbook.add_format({'num_format': '0%'})
worksheet.set_column('H:AY', None, format1)

writer.save()
writer.close()
</code></pre>

<p>I have tried</p>

<pre><code>outpath = (r'Z:\backup')
writer.save(outpath)
writer.close()
</code></pre>

<p>But get back</p>

<blockquote>
  <p>TypeError: save() takes 1 positional argument but 2 were given</p>
</blockquote>
","9709338","","6190043","","2018-04-27 10:45:38","2018-04-27 10:45:38","Issues with saving an Excel.Writer file to new path","<python><python-3.x><pandas><python-requests><pandas.excelwriter>","1","1","","","","CC BY-SA 3.0","1"
"41963193","1","","","2017-01-31 16:56:46","","6","3487","<p>I'm trying to return a tuple of the index (the people names below) and the max value for the '%' column below.
When I create a Dataframe and try </p>

<pre><code>df['%'].max()
</code></pre>

<p>Pandas always returns just the value an not the index.  However, I want to create a tuple from the key value pair of the index and max value in the '%' column.  I'm sure this is a newbie question, thank you for helping me!</p>

<p>Here's some sample data:  </p>

<pre><code>    Points_Scored     Possible_Points    %      Favoriate Food
Jan     60              200              0.3     Pudding
Jane    87              200              0.435   Pizza
Bob     54              200              0.27    Salad
Bubba   42              200              0.21    Salsa
Jack    98              200              0.49    Avacodo
John    45              200              0.225   Bacon
Mike    63              200              0.315   Tacos
Victor  8               200              0.04    Lettuce
</code></pre>
","3225420","","","","","2017-01-31 17:00:35","Return Tuple of Index and .max() Value?","<python><python-3.x><pandas><dataframe><tuples>","1","0","1","","","CC BY-SA 3.0","1"
"49622416","1","","","2018-04-03 04:46:01","","0","3481","<p>When I'm trying to connect to the <strong>MySql</strong> Database server in <strong>Python</strong> on <em>Google Cloud Platform</em>, I'm getting the error. The following is the actual code.</p>

<pre><code>import mysql.connector
import pandas as pd

cnx = mysql.connector.connect(user = 'xxxxxx', password='xxxxxx',
                              host='173.194.104.33',
                              database='xxxxxxxxx')
cursor = cnx.cursor()
cnx.close()
if cur and con:                        
    cur.close() 
    con.close()
sql1 = ""SELECT * FROM ms_trackevaluation_15_16.ms_skill""
cursor.execute(sql1)
rows = cursor.fetchall()
df1 = pd.read_sql(sql1, cnx)
</code></pre>

<p>The following is the error :</p>

<pre><code>InterfaceError                            Traceback (most recent call last)
&lt;ipython-input-2-30094be976fb&gt; in &lt;module&gt;()
      4 cnx = mysql.connector.connect(user = 'xxxxx', password='xxxxx',
      5                               host='173.194.104.33',
----&gt; 6                               database='xxxxx')
      7 cursor = cnx.cursor()
      8 cnx.close()

    /home/scrollstech_shravankumar/anaconda3/lib/python3.5/site-packages/mysql/connector/__init__.py in connect(*args, **kwargs)
        177         return CMySQLConnection(*args, **kwargs)
        178     else:
    --&gt; 179         return MySQLConnection(*args, **kwargs)
        180 Connect = connect  # pylint: disable=C0103
        181 

    /home/scrollstech_shravankumar/anaconda3/lib/python3.5/site-packages/mysql/connector/connection.py in __init__(self, *args, **kwargs)
         93 
         94         if len(kwargs) &gt; 0:
    ---&gt; 95             self.connect(**kwargs)
         96 
         97     def _do_handshake(self):

    /home/scrollstech_shravankumar/anaconda3/lib/python3.5/site-packages/mysql/connector/abstracts.py in connect(self, **kwargs)
        714 
        715         self.disconnect()
    --&gt; 716         self._open_connection()
        717         self._post_connection()
        718 

    /home/scrollstech_shravankumar/anaconda3/lib/python3.5/site-packages/mysql/connector/connection.py in _open_connection(self)
        205         self._socket = self._get_connection()
        206         self._socket.open_connection()
    --&gt; 207         self._do_handshake()
        208         self._do_auth(self._user, self._password,
        209                       self._database, self._client_flags, self._charset_id,

    /home/scrollstech_shravankumar/anaconda3/lib/python3.5/site-packages/mysql/connector/connection.py in _do_handshake(self)
         97     def _do_handshake(self):
         98         """"""Get the handshake from the MySQL server""""""
    ---&gt; 99         packet = self._socket.recv()
        100         if packet[4] == 255:
        101             raise errors.get_exception(packet)

    /home/scrollstech_shravankumar/anaconda3/lib/python3.5/site-packages/mysql/connector/network.py in recv_plain(self)
        241                 chunk = self.sock.recv(4 - packet_len)
        242                 if not chunk:
    --&gt; 243                     raise errors.InterfaceError(errno=2013)
        244                 packet += chunk
        245                 packet_len = len(packet)

    InterfaceError: 2013: Lost connection to MySQL server during query
</code></pre>

<p>Can anyone explain what's wrong with this code and why I'm getting this error.  And please explain how can I resolve this issue ?</p>
","8112792","","2385479","","2018-04-09 00:52:12","2018-04-09 00:52:12","InterfaceError: 2013: Lost connection to MySQL server during query","<mysql><python-3.x><pandas>","1","3","","","","CC BY-SA 3.0","1"
"49247884","1","49250361","","2018-03-13 03:32:54","","2","3442","<p>I am trying to write a program in Python3 that will run a query on a table in Microsoft SQL and put the results into a Pandas DataFrame. </p>

<p>My first try of this was the below code, but for some reason I don't understand the columns do not appear in the order I ran them in the query and the order they appear in and the labels they are given as a result change, stuffing up the rest of my program:</p>

<pre><code> import pandas as pd, pyodbc    

    result_port_mapl = []

    # Use pyodbc to connect to SQL Database
    con_string = 'DRIVER={SQL Server};SERVER='+ &lt;server&gt; +';DATABASE=' + 
&lt;database&gt;


     cnxn = pyodbc.connect(con_string)
    cursor = cnxn.cursor()

    # Run SQL Query
    cursor.execute(""""""
                   SELECT &lt;field1&gt;, &lt;field2&gt;, &lt;field3&gt;
                   FROM result
                   """""")

    # Put data into a list
    for row in cursor.fetchall():
        temp_list = [row[2], row[1], row[0]]
        result_port_mapl.append(temp_list)

    # Make list of results into dataframe with column names
    ## FOR SOME REASON HERE row[1] AND row[0] DO NOT CONSISTENTLY APPEAR IN THE 
    ## SAME ORDER AND SO THEY ARE MISLABELLED
    result_port_map = pd.DataFrame(result_port_mapl, columns={'&lt;field1&gt;', '&lt;field2&gt;', '&lt;field3&gt;'})
</code></pre>

<p>I have also tried the following code</p>

<pre><code>    import pandas as pd, pyodbc

    # Use pyodbc to connect to SQL Database
    con_string = 'DRIVER={SQL Server};SERVER='+ &lt;server&gt; +';DATABASE=' + &lt;database&gt;
    cnxn = pyodbc.connect(con_string)
    cursor = cnxn.cursor()

    # Run SQL Query
    cursor.execute(""""""
                   SELECT &lt;field1&gt;, &lt;field2&gt;, &lt;field3&gt;
                   FROM result
                   """""")

    # Put data into DataFrame
    # This becomes one column with a list in it with the three columns 
    # divided by a comma
    result_port_map = pd.DataFrame(cursor.fetchall())

    # Get column headers
    # This gives the error ""AttributeError: 'pyodbc.Cursor' object has no 
    # attribute 'keys'""
    result_port_map.columns = cursor.keys()
</code></pre>

<p>If anyone could suggest why either of those errors are happening or provide a more efficient way to do it, it would be greatly appreciated.</p>

<p>Thanks</p>
","5565466","","","","","2018-03-13 07:19:47","Querying from Microsoft SQL to a Pandas Dataframe","<python-3.x><pandas><pyodbc>","1","0","","","","CC BY-SA 3.0","1"
"48648773","1","48648917","","2018-02-06 17:32:41","","3","3110","<p>I have a dataframe <code>df</code> like this :   </p>

<pre><code>ID    NAME    AGE
-----------------
M43   ab      32
M32   df      12
M54   gh      34
M43   ab      98
M43   ab      36
M43   cd      32
M32   cd      39
M43   ab      67
</code></pre>

<p>I need to sort the rows based on the <code>ID</code> column.<br>
The output <code>df_grouped</code> should look like :</p>

<pre><code>ID    NAME    AGE
-----------------
M43   ab      32
M43   ab      98
M43   ab      36
M43   cd      32
M43   ab      67
M32   df      12
M32   cd      39
M54   gh      34
</code></pre>

<p>I tried something like :</p>

<pre><code>df_grouped = df.group_by(df.ID)

for id in list(df.ID.unique()):
   grouped_df_list.append(df_grouped.get_group(id))
</code></pre>

<p>Is there any better way to do this ?</p>
","3180352","","9209546","","2018-03-02 03:59:32","2018-03-02 03:59:32","Pandas: Sorting rows based on a value of a column","<python><python-3.x><pandas>","2","10","","","","CC BY-SA 3.0","1"
"57453959","1","","","2019-08-11 22:06:38","","0","3081","<p>I have dataframe which is in below form: </p>

<pre><code>data = [['M',0],['F',0],['M',1], ['M',1],['M',1],['F',1],['M',0], ['M',1],['M',0],['F',1],['M',0], ['M',0]]
df = pd.DataFrame(data,columns=['Gender','label'])
print (df)
  Gender  label
0       M      0
1       F      0
2       M      1
3       M      1
4       M      1
5       F      1
6       M      0
7       M      1
8       M      0
9       F      1
10      M      0
11      M      0
</code></pre>

<p>I am trying to create a stacked bar chart which should percentage as the annotation on the chart.
Code below to create stacked bar chart:</p>

<pre><code>df.groupby('Gender')['label']\
    .value_counts()\
    .unstack(level=1)\
    .plot.bar(stacked=True)
</code></pre>

<p><a href=""https://i.stack.imgur.com/AGpMG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AGpMG.png"" alt=""enter image description here""></a></p>

<p>I am not sure how to get percentages on the chart.</p>

<p>Thanks ina dvance</p>
","3623402","","7758804","","2020-10-05 05:43:20","2020-10-05 05:43:20","How to show percentages on the stacked bar chart in python 3.x","<python-3.x><pandas><stacked-chart>","1","0","","2020-10-05 06:18:20","","CC BY-SA 4.0","1"
"49323280","1","49323303","","2018-03-16 14:36:04","","4","3062","<p>I have a simple dataframe:</p>

<pre><code>df = pd.DataFrame({'id': ['a','a','a','b','b'],'value':[0,15,20,30,0]})
df
  id  value
0  a      0
1  a     15
2  a     20
3  b     30
4  b      0
</code></pre>

<p>And I want a pivot table with the number of values greater than zero.</p>

<p>I tried this:</p>

<pre><code>raw = pd.pivot_table(df, index='id',values='value',aggfunc=lambda x:len(x&gt;0))
</code></pre>

<p>But returned this:</p>

<pre><code>    value
id
a       3
b       2
</code></pre>

<p>What I need:</p>

<pre><code>    value
id
a       2
b       1
</code></pre>

<p>I read lots of solutions with groupby and filter. Is it possible to achieve this only with pivot_table command? If it is not, which is the best approach?</p>

<p>Thanks in advance</p>

<p><strong>UPDATE</strong></p>

<p>Just to make it clearer why I am avoinding filter solution. In my real and complex df, I have other columns, like this:</p>

<pre><code>df = pd.DataFrame({'id': ['a','a','a','b','b'],'value':[0,15,20,30,0],'other':[2,3,4,5,6]})
df
  id  other  value
0  a      2      0
1  a      3     15
2  a      4     20
3  b      5     30
4  b      6      0
</code></pre>

<p>I need to sum the column 'other', but when i filter I got this:</p>

<pre><code>df=df[df['value']&gt;0]
raw = pd.pivot_table(df, index='id',values=['value','other'],aggfunc={'value':len,'other':sum})
    other  value
id
a       7      2
b       5      1
</code></pre>

<p>Instead of:</p>

<pre><code>    other  value
id
a       9      2
b      11      1
</code></pre>
","5838551","","5838551","","2018-03-16 17:17:22","2018-03-16 17:17:22","Pandas Pivot Table Conditional Counting","<python-3.x><pandas>","2","4","","","","CC BY-SA 3.0","1"
"48523762","1","48523821","","2018-01-30 14:27:10","","2","3014","<p>I have the following <code>df</code>,</p>

<pre><code>A
1.0
2.0
3.0
NaN
</code></pre>

<p>I tried to <code>fillna</code> to replace <code>NaN</code> with a string <code>not existed</code>.</p>

<pre><code>df.fillna(value={'A': 'not existed'}, axis=1, inplace=True)
</code></pre>

<p>but I got the following error,</p>

<pre><code>NotImplementedError: Currently only can fill with dict/Series column by column
</code></pre>

<p>if I use <code>replace</code>, it will work,</p>

<pre><code>df['A'].replace(np.nan, 'not existed')
</code></pre>

<p>I am wondering why is that.</p>
","766708","","","","","2018-01-30 14:38:00","pandas fillna Currently only can fill with dict/Series column by column","<python><python-3.x><pandas><dataframe>","2","1","","","","CC BY-SA 3.0","1"
"48709456","1","48868272","","2018-02-09 15:48:49","","2","3002","<p>I am trying to address the following issue. Let's assume a dataframe (loaded from a txt file) with the following structure (and thousands of rows):</p>

<pre><code>foo.head()
</code></pre>

<blockquote>
<pre><code>         X            Y       Z 
 0  125417.5112  536361.8752 -1750.0
 1  127517.7647  533925.8644 -1750.0
 2  128144.1000  533199.4000 -1750.0
 3  128578.8385  532904.9288 -1750.0
 4  125417.5112  536361.8752 -1750.0
 ....
</code></pre>
</blockquote>

<p>The data represents X Y and Z coordinates.</p>

<p>I also have a set of points that define a closed polygon. These are in a numpy array:</p>

<pre><code>polypoints

array([[ 125417.5112,  536361.8752],
       [ 127517.7647,  533925.8644],
       [ 128144.1   ,  533199.4   ],
       ....
       [ 125417.5112,  536361.8752]])
</code></pre>

<p>How can i filter my dataframe to drop the rows that do NOT fall inside the closed polygon?</p>

<p>I have tried defining the polygon using <code>shapely.geometry</code> <code>polygon</code>. By doing:</p>

<pre><code>poly = Polygon(polypoints)
</code></pre>

<p>This works fine. But I am at a loss as to how to continue with this.</p>

<p>Help is much appreciated</p>

<p>----EDIT----
Please see below for updated solution</p>
","5790939","","5790939","","2018-11-13 15:48:10","2018-11-13 15:48:10","How to drop dataframe rows where X and Y coordinates are outside of polygon","<python-3.x><pandas><dataframe><geospatial><point-in-polygon>","3","5","1","","","CC BY-SA 4.0","1"
"41392009","1","41392886","","2016-12-30 06:55:38","","2","2973","<p>I have a frame looks like:      </p>

<pre><code>            2015-12-30  2015-12-31
300100  am    1             3
        pm    3             2
300200  am    5             1
        pm    4             5
300300  am    2             6
        pm    3             7
</code></pre>

<p>and the other frame looks like      </p>

<pre><code>            2016-1-1    2016-1-2    2016-1-3    2016-1-4
300100  am    1           3            5           1
        pm    3           2            4           5
300200  am    2           5            2           6
        pm    5           1            3           7
300300  am    1           6            3           2
        pm    3           7            2           3
300400  am    3           1            1           3
        pm    2           5            5           2
300500  am    1           6            6           1
        pm    5           7            7           5
</code></pre>

<p>Now I want to merge the two frames, and the frame after merge to be looked like this: </p>

<pre><code>             2015-12-30 2015-12-31  2016-1-1    2016-1-2    2016-1-3    2016-1-4
  300100  am    1          3           1           3           5           1
          pm    3          2           3           2           4           5
  300200  am    5          1           2           5           2           6
          pm    4          5           5           1           3           7
  300300  am    2          6           1           6           3           2
          pm    3          7           3           7           2           3
  300400  am                           3           1           1           3
          pm                           2           5           5           2
  300500  am                           1           6           6           1
          pm                           5           7           7           5
</code></pre>

<p>I tried pd.merge(frame1,frame2,right_index=True,left_index=True), but what it returned was not the desired format. Can anyone help? Thanks!</p>
","7329278","","2901002","","2016-12-31 09:28:11","2016-12-31 09:28:11","How to merge two dataframes with MultiIndex?","<python-3.x><pandas><dataframe><concat><multi-index>","2","8","","","","CC BY-SA 3.0","1"
"41211901","1","","","2016-12-18 19:11:19","","4","2769","<p>I have a dataframe as follows but at a much large scale </p>

<pre><code>        Sample  Taxonomy Count
    0   1       A        1 
    1   1       B        2
    2   1       C        5
    3   1       D        7
    4   2       B        1
    6   2       D        3
    7   2       E        4
    8   2       F        5      
</code></pre>

<p>The result I desire is as follows </p>

<pre><code>    Taxonomy  A  B  C  D  E  F  G
    Sample 1: 1  2  5  7  NA NA NA  
    Sample 2: NA 1  NA 3  4  5  NA  
</code></pre>

<p>Tried making taxonomy the index but I still have a copy of each taxonomy for each sample and still when I transpose that with DataFrame.transpose() It does not result in the desired dataframe.</p>
","6269299","","","","","2016-12-18 19:14:07","Pandas DataFrame Transpose index and columns","<python><python-3.x><pandas><dataframe>","1","1","0","","","CC BY-SA 3.0","1"
"49327240","1","49327735","","2018-03-16 18:18:15","","4","2723","<p>How can I rank a DataFrame based on 2 columns?</p>

<p>On below example, <code>col_b</code> would the tie breaker for <code>col_a</code>.</p>

<p><strong>DataFrame:</strong></p>

<pre><code>df = pd.DataFrame({'col_a':[0,0,0,1,1,1], 'col_b':[5,2,8,3,7,4]})

df
   col_a  col_b
0      0      5
1      0      2
2      0      8
3      1      3
4      1      7
5      1      4
</code></pre>

<p><strong>Expected Output:</strong></p>

<pre><code>   col_a  col_b  Rank
0      0      5   2
1      0      2   1
2      0      8   3
3      1      3   4
4      1      7   6
5      1      4   5
</code></pre>
","7217246","","2867928","","2018-03-16 18:56:00","2018-03-16 19:01:45","Rank DataFrame based on multiple columns","<python><python-3.x><pandas><dataframe><ranking>","4","1","","","","CC BY-SA 3.0","1"
"40594210","1","40606608","","2016-11-14 17:10:40","","2","2710","<p>i have this test table in pandas dataframe</p>

<pre><code>   Leaf_category_id  session_id  product_id
0               111           1         987
3               111           4         987
4               111           1         741
1               222           2         654
2               333           3         321
</code></pre>

<p>this is the extension of my previous question, which was answered by @jazrael.
<a href=""https://stackoverflow.com/questions/40587902/function-to-select-from-columns-pandas-df/40587934#40587934"">view answer</a></p>

<p>so after getting the values in product_id column as(just an assumption, little different from the output of my previous question,</p>

<pre><code>   |product_id               |
   ---------------------------
   |111,987,741,34,12        |
   |987,1232                 |
   |654,12,324,465,342,324   |
   |321,741,987              |
   |324,654,862,467,243,754  |
   |6453,123,987,741,34,12   |
</code></pre>

<p>and so on,
i want to create a new column, in which all the values in a row should be made as a bigram with its next one, and the last no. in the row combined with the first one,for example:</p>

<pre><code>   |product_id               |Bigram
   -------------------------------------------------------------------------
   |111,987,741,34,12        |(111,987),**(987,741)**,(741,34),(34,12),(12,111)
   |987,1232                 |(987,1232),(1232,987)
   |654,12,324,465,342,32    |(654,12),(12,324),(324,465),(465,342),(342,32),(32,654)
   |321,741,987              |(321,741),**(741,987)**,(987,321)
   |324,654,862              |(324,654),(654,862),(862,324)
   |123,987,741,34,12        |(123,987),(987,741),(34,12),(12,123)
</code></pre>

<p>ignore the **( i'll tell you later on why i starred that)</p>

<p>the code to achive the bigram is</p>

<pre><code>for i in df.Leaf_category_id.unique(): 
    print (df[df.Leaf_category_id == i].groupby('session_id')['product_id'].apply(lambda x: list(zip(x, x[1:]))).reset_index())
</code></pre>

<p>from this df, i want to consider the bigram column and make one more column named as frequency, which gives me frequency of bigram occured.</p>

<blockquote>
  <p>Note* : (987,741) and (741,987) are to be considered as same and one dublicate entry should be removed and thus frequency of  (987,741) should be 2.
  similar is the case with (34,12) it occurs two times, so frequency should be 2</p>
</blockquote>

<pre><code>   |Bigram
   ---------------
   |(111,987),
   |**(987,741)**
   |(741,34)
   |(34,12)
   |(12,111)
   |**(741,987)**
   |(987,321)
   |(34,12)
   |(12,123)
</code></pre>

<p>Final Result should be.</p>

<pre><code>   |Bigram       | frequency |
   --------------------------
   |(111,987)    |  1 
   |(987,741)    |  2
   |(741,34)     |  1
   |(34,12)      |  2
   |(12,111)     |  1
   |(987,321)    |  1
   |(12,123)     |  1
</code></pre>

<p>i am hoping to find answer here, please help me, i have elaborated it as much as possible.</p>
","6803114","","-1","","2017-05-23 12:24:44","2016-11-15 14:42:38","create a bigram from a column in pandas df","<python><python-2.7><python-3.x><pandas>","2","5","1","","","CC BY-SA 3.0","1"
"41212472","1","46270874","","2016-12-18 20:21:10","","1","2660","<p>I am trying to load the Excel file from the following URL into a dataframe using Python 3.5 and Pandas: </p>

<pre><code>link = ""https://hub.coursera-notebooks.org/user/ejquqxfjajkufidbixxvkx/notebooks/Energy%20Indicators.xls""
</code></pre>

<p>First I tried to download the file manually using urllib.request in order to read it right after:</p>

<pre><code>import urllib.request
urllib.request.urlretrieve (link, ""Energy Indicators.xls"")
</code></pre>

<p>I got the file ""Energy Indicators.xls"", yes, but it is not a valid xls file. It seems more like a html file with the extension changed to xls.</p>

<p>Then I tried to load the file directly using read_csv:</p>

<pre><code>energy = pd.read_csv(link, skiprows = 16, header = 0, skipfooter = 38)
</code></pre>

<p>But I got a traceback error: ""pandas.io.common.CParserError: Error tokenizing data. C error: Expected 1 fields in line 12, saw 2"". If I tried to read it without the arguments skiprows, header, etc. I got another error: ""ValueError: Expected 1 fields in line 41, saw 3"".</p>

<p>Any idea? BTW, I am using Mac OS Sierra and PyCharm Community Edition 2016.3</p>
","5878647","","","","","2017-09-18 02:43:36","using Pandas to download/load xls from URL file","<python-3.x><pandas>","1","6","","","","CC BY-SA 3.0","1"
"41393242","1","41393280","","2016-12-30 08:47:05","","2","2624","<p>I have a Pandas DataFrame that contains two sets of coordinates (lat1, lon1, lat2, lon2). I have a function that computes distance using these coordinates. But some of the rows in the dataframe are invalid. I would like to apply my function only to valid rows and save the result of the function to a 'dist' column (the column already exists in the dataframe). I want something like this SQL:</p>

<pre><code>UPDATE dataframe
SET dist=calculate_dist(lat1, lon1, lat2, lon2)
WHERE lat1 IS NOT NULL AND lat2 IS NOT NULL AND user_id&gt;100;
</code></pre>

<p>How can I achieve this?</p>

<p>I tried using <code>df = df.apply(calculate_dist, axis=1)</code> but with this approach I need to process all rows, not only the rows that match my conditions, and I need to have an if statement inside the calculate_dist function that ignores invalid rows. Is there a better way?</p>

<p>I know that similar questions already appeared on StackOverflow but I could not find any question that utilizes both a function and conditional selection of rows.</p>
","1743367","","2901002","","2016-12-31 09:27:13","2016-12-31 09:27:13","Pandas: Conditionally fill column using a function based on other columns values","<python><python-3.x><pandas><indexing><mask>","1","0","1","","","CC BY-SA 3.0","1"
"41724964","1","41725070","","2017-01-18 16:47:47","","2","2558","<p>I'm reading the columns in a pandas dataframe using a for loop, using a nested if statement to find the minimum and maximum in the datetime range. </p>

<p>I can identify the datetime columns I need, but can't find the correct way to pass the <code>column</code> variable into the <code>dataframe.series.min()</code> and <code>max</code> statement. </p>

<pre><code>import pandas as pd
data = pd.somedata()

for column in data.columns: 

    if data[column].dtype == 'datetime64[ns]':
        data.column.min() 
        data.column.max()
</code></pre>

<p>so when the <code>column</code> variable is passed the loop should return date time values like this: </p>

<pre><code>data.DFLT_DT.min()

Timestamp('2007-01-15 00:00:00')


data.DFLT_DT.max()

Timestamp('2016-10-18 00:00:00')
</code></pre>
","5665505","","5665505","","2017-01-18 16:54:54","2017-01-18 16:58:46","handling dtype in for loop","<python><python-3.x><datetime><pandas>","1","2","","","","CC BY-SA 3.0","1"
"48777426","1","","","2018-02-13 23:36:33","","-1","2393","<p>I'm learning ML from a book in which the writer wrote:
housing.hist(bins=50, figsize=(20,15))
plt.show()
- to draw histogram of the data.
In there, I didn't understand the significance and need of bin attribute and how to decide a value for it.</p>

<p>I went on to pandas documentation website (<a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.hist.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.hist.html</a>) and still did not understand what does the parameter ""bins"" mean.</p>
","9293835","","","","","2018-02-14 00:20:20","What does the parameter ""bins"" signify in dataframe.hist()?","<python><python-3.x><pandas><matplotlib><histogram>","1","1","1","","","CC BY-SA 3.0","1"
"57615642","1","","","2019-08-22 19:02:21","","1","2384","<p>I'm looking to create a temp table and insert a some data into it. I have used pyodbc extensively to pull data but I am not familiar with writing data to SQL from a python environment. I am doing this at work so I dont have the ability to create tables, but I can create temp and global temp tables. My intent is to insert a relatively small dataframe (150rows x 4cols)into a temp table and reference it throughout my session, my program structure makes it so that a global variable in the session will not suffice.I am getting the following error when trying the piece below, what am I doing wrong?</p>

<pre><code>pyodbc.ProgrammingError: ('42S02', ""[42S02] [Microsoft][ODBC SQL Server Driver][SQL Server]Invalid object name 'sqlite_master'. (208) (SQLExecDirectW); [42S02] [Microsoft][ODBC SQL Server Driver][SQL Server]Statement(s) could not be prepared. (8180)"")

import numpy as np
import pandas as pd
import pyodbc


conn = pyodbc.connect('Driver={SQL Server};'
                      'Server=SERVER;'
                      'Database=DATABASE;'
                      'Trusted_Connection=yes;')

cursor = conn.cursor()

temp_creator = '''CREATE TABLE #rankings (Col1 int, Col2 int)'''

cursor.execute(temp_creator)

df_insert = pd.DataFrame({'Col1' : [1, 2, 3], 'Col2':[4,5,6]})
df_insert.to_sql(r'#rankings', conn, if_exists='append')
read_query = '''SELECT * FROM #rankings'''
df_back = pd.read_sql(read_query,conn)
</code></pre>
","8451069","","","","","2019-08-22 20:29:08","Trying to insert pandas dataframe to temporary table","<sql-server><python-3.x><pandas><pyodbc><temp-tables>","1","5","1","","","CC BY-SA 4.0","1"
"57463127","1","57463368","","2019-08-12 14:29:09","","3","2212","<p>I am trying to split a column with comma delimited values into 2 columns but the str.split function returns columns with 0's and 1's instead of the split string values</p>

<p>I have a dataframe with a column 'Full Name' which has a full name with a comma separating last name from first name.</p>

<p>I used the str.split function which works when I execute it for display only.  But: when I try to use the same function to add 2 new columns to the same dataframe with the split data, I get 2 new columns with the first populated with 0's and the second with 1's all the way.</p>

<p>The code that works to display the split data:</p>

<p><code>df2015_2019.iloc[:,0].str.split(',', expand=True)</code></p>

<p>Code that doesn't work to create new columns with split data:</p>

<p><code>df2015_2019['Lname'],df2015_2019['Fname'] =  df2015_2019.iloc[:,0].str.split(',', expand=True)</code></p>

<p>I get a column 'Lname' with all 0's and a column 'Fname' with all 1's</p>
","11759261","","","","","2019-09-11 14:01:47","Splitting a column in dataframe using str.split function","<python><python-3.x><pandas><jupyter>","3","1","","","","CC BY-SA 4.0","1"
"48758159","1","48758222","","2018-02-13 01:27:39","","4","2209","<p>How do I take a normal data frame, like the following:</p>

<pre><code>d = {'col1': [1, 2], 'col2': [3, 4]}
df = pd.DataFrame(data=d)
df

    col1    col2
0   1   3
1   2   4
</code></pre>

<p>and produce a dataframe where the column name is added to the cell in the frame, like the following:</p>

<pre><code>d = {'col1': ['col1=1', 'col1=2'], 'col2': ['col2=3', 'col2=4']}
df = pd.DataFrame(data=d)
df

    col1    col2
0   col1=1  col2=3
1   col1=2  col2=4
</code></pre>

<p>Any help is appreciated.</p>
","1797250","","","","","2018-02-13 02:32:33","How to add column name to cell in pandas dataframe?","<python-3.x><pandas>","3","0","1","","","CC BY-SA 3.0","1"
"33307342","1","33307396","","2015-10-23 16:34:30","","2","2153","<p>Can you help with this error: what am I doing wrong with the df.isin function?</p>

<pre><code>cursor = con.cursor()
cursor.execute(""""""SELECT distinct date FROM raw_finmis_online_activation_temp"""""")
existing_dates = [x[0] for x in cursor.fetchall()]

if df[df['date'].isin(existing_dates)]:
    print ""Yes it's in there""
else:
    print ""N""
</code></pre>

<p>It's giving me this error:</p>

<blockquote>
  <p>ValueError: The truth value of a DataFrame is ambiguous. Use a.empty,
  a.bool(), a.item(), a.any() or a.all().</p>
</blockquote>
","2198720","","7932273","","2019-02-18 13:11:35","2019-02-18 13:11:35","Dataframe.isin() giving this error: The truth value of a DataFrame is ambiguous","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"56799334","1","56799625","","2019-06-27 23:04:08","","2","2127","<p>I am working on a large datasets that looks like this:</p>

<pre><code>Time,   Value
01.01.2018 00:00:00.000,  5.1398
01.01.2018 00:01:00.000,  5.1298
01.01.2018 00:02:00.000,  5.1438
01.01.2018 00:03:00.000,  5.1228
01.01.2018 00:04:00.000,  5.1168
.... , ,,,,
31.12.2018 23:59:59.000,  6.3498
</code></pre>

<p>The data is a <code>minute</code> data from the <code>first</code> day of the year to the <code>last</code> day of the year</p>

<p>I want to use <code>Pandas</code> to find the average of every <code>5</code> days. </p>

<p>For example:</p>

<p>Average from <code>01.01.2018 00:00:00.000</code> to <code>05.01.2018 23:59:59.000</code> is average for <code>05.01.2018</code></p>

<p>The next average will be from <code>02.01.2018 00:00:00.000</code> to <code>6.01.2018 23:59:59.000</code> is average for <code>06.01.2018</code></p>

<p>The next average will be from <code>03.01.2018 00:00:00.000</code> to <code>7.01.2018 23:59:59.000</code> is average for <code>07.01.2018</code></p>

<p>and so on... We are incrementing day by 1 but calculating an average from the day to past 5days, including the current date.</p>

<blockquote>
  <p>For a given day, there are 24hours * 60minutes =  1440 data points. So I need to get the average of 1440 data points * 5 days = 7200 data points.</p>
</blockquote>

<p>The final DataFrame will look like this, time format [DD.MM.YYYY] (without hh:mm:ss) and the <code>Value</code> is the average of 5 data including the current date:</p>

<pre><code>Time,   Value
05.01.2018,  5.1398
06.01.2018,  5.1298
07.01.2018,  5.1438
.... , ,,,,
31.12.2018,  6.3498
</code></pre>

<p>The bottom line is to calculate the average of data from today to the past 5 days and the average value is shown as above.</p>

<p>I tried to iterate through Python loop but I wanted something better than we can do from Pandas.</p>
","9161607","","9161607","","2019-06-27 23:06:58","2019-06-28 16:46:08","Pandas: Calculate average of values for a time frame","<python><python-3.x><pandas>","1","4","1","","","CC BY-SA 4.0","1"
"39620105","1","39620728","","2016-09-21 14:59:33","","2","2125","<p>This is undoubtedly a bit of a ""can't see the wood for the trees"" moment. I've been staring at this code for an hour and can't see what I've done wrong. I know it's staring me in the face but I just can't see it!</p>

<p>I'm trying to convert between two geographical co-ordinate systems using Python.</p>

<p>I have longitude (x-axis) and latitude (y-axis) values and want to convert to OSGB 1936. For a single point, I can do the following:</p>

<pre><code>import numpy as np
import pandas as pd
import shapefile
import pyproj

inProj = pyproj.Proj(init='epsg:4326')
outProj = pyproj.Proj(init='epsg:27700')

x1,y1 = (-2.772048, 53.364265)

x2,y2 = pyproj.transform(inProj,outProj,x1,y1)

print(x1,y1)
print(x2,y2)
</code></pre>

<p>This produces the following:</p>

<pre><code>-2.772048 53.364265
348721.01039783185 385543.95241055806
</code></pre>

<p>Which seems reasonable and suggests that longitude of -2.772048 is converted to a co-ordinate of 348721.0103978.</p>

<p>In fact, I want to do this in a Pandas dataframe. The dataframe contains columns containing longitude and latitude and I want to add two additional columns that contain the converted co-ordinates (called newLong and newLat).</p>

<p>An exemplar dataframe might be:</p>

<pre><code>    latitude  longitude
0  53.364265  -2.772048
1  53.632481  -2.816242
2  53.644596  -2.970592
</code></pre>

<p>And the code I've written is:</p>

<pre><code>import numpy as np
import pandas as pd
import shapefile
import pyproj

inProj = pyproj.Proj(init='epsg:4326')
outProj = pyproj.Proj(init='epsg:27700')

df = pd.DataFrame({'longitude':[-2.772048,-2.816242,-2.970592],'latitude':[53.364265,53.632481,53.644596]})

def convertCoords(row):
    x2,y2 = pyproj.transform(inProj,outProj,row['longitude'],row['latitude'])
    return pd.Series({'newLong':x2,'newLat':y2})

df[['newLong','newLat']] = df.apply(convertCoords,axis=1)

print(df)
</code></pre>

<p>Which produces:</p>

<pre><code>    latitude  longitude        newLong         newLat
0  53.364265  -2.772048  385543.952411  348721.010398
1  53.632481  -2.816242  415416.003113  346121.990302
2  53.644596  -2.970592  416892.024217  335933.971216
</code></pre>

<p>But now it seems that the newLong and newLat values have been mixed up (compared with the results of the single point conversion shown above).</p>

<p>Where have I got my wires crossed to produce this result? (I apologise if it's completely obvious!)</p>
","1718097","","","","","2020-01-20 08:00:39","Converting between projections using pyproj in Pandas dataframe","<python><python-3.x><pandas><gis><proj>","2","0","1","","","CC BY-SA 3.0","1"
"50070428","1","50078783","","2018-04-27 21:17:25","","2","2111","<p>I'm trying to create a fairly complex bokeh circle plot that features several complex interactions (axis changes &amp; year slider &amp; others). I've seemed to gotten it working mostly, and without the use of ColumnDataSource (because the template I borrowed didn't use it). Now I need to create a custom legend (or HoverTool(??)).</p>

<p>I've managed to create a custom legend, but I do not know how to pass the matplotlib color it was assigned into the legend. So currently, it is just a legend showing all the legend items but no color identifiers. How can I pass the right colors into the legend?</p>

<p>Here is the relevant chunk of my code:</p>

<p>dataframe:</p>

<pre><code>FULLNAME | ES | TX | YEAR
Jim        3    12   2008
...
Tim        33    8   2009


import matplotlib
import random
from datetime import date

user_list= list(set((df[""FULLNAME""])))

colordict=dict(matplotlib.colors.cnames.items())
colordict=list(colordict.values())
random_colors=random.sample(colordict,len(user_list))

def yr_slider():
    current=slider.value
    ys=df[y.value][df[""YEAR""]==current].values
    p = figure(plot_width=930, plot_height=325, background_fill_color='snow', x_range=(2016,2019))
    p.circle(x=df[""YEAR""][df[""YEAR""]==current], y=ys, size= 20,
             color=random_colors)
    legend=Legend(items=[(""Jim"",random_colors[1]),
                         (""Tim"",[])
                         ]) ###THIS AINT WORKIN RIGHT###
    p.add_layout(legend,'right')
    print(ys)
    return p

def update(attr, old, new):
    layout2.children[1]=yr_slider()
slider=Slider(start=2017, end=date.today().year, step=1, value=date.today().year, title=""Year"")
slider.on_change('value', update)

y=Select(title='X-Axis', value='ES',options=['TX','ES'])
y.on_change('value',update)

ctrls=widgetbox([y,slider], width=200)
layout2=row(ctrls,yr_slider())

curdoc().add_root(layout2)
</code></pre>

<p>Thanks!</p>
","6480215","","7117003","","2018-12-15 08:58:52","2018-12-15 08:58:52","Bokeh Legend Object - passing assigned color to legend","<python-3.x><pandas><matplotlib><colors><bokeh>","1","0","","","","CC BY-SA 4.0","1"
"57805803","1","","","2019-09-05 12:44:21","","1","2035","<p>I have a CSV file I add the code to process data on it and know how to save the final dataframe to the same CSV by using <code>to_csv</code> method. The problem is I want to add the background color to some of the columns, how can I do it?</p>

<p><img src=""https://i.stack.imgur.com/R2CGFm.png"" alt=""Example data with colored columns""></p>
","11060157","","2336654","","2019-09-05 13:22:48","2019-09-05 13:37:07","How to to add Background color to a specific column of pandas dataframe and save that colored dataframe into that same csv?","<python><python-3.x><pandas><csv><dataframe>","1","4","0","","","CC BY-SA 4.0","1"
"56584075","1","56585145","","2019-06-13 15:38:58","","1","1994","<p>Given the following dataframe</p>

<pre><code>data = [[1, 'Yes','A','No','Yes','No','No','No'],
        [2, 'Yes','A','No','No','Yes','No','No'],
        [3, 'Yes','B','No','No','Yes','No','No'],
        [4, 'No','','','','','',''],
        [5, 'No','','','','','',''],
        [6, 'Yes','C','No','No','Yes','Yes','No'],
        [7, 'Yes','A','No','Yes','No','No','No'],
        [8, 'Yes','A','No','No','Yes','No','No'],
        [9, 'No','','','','','',''],
        [10, 'Yes','B','Yes','Yes','No','No','No']]
df = pd.DataFrame(data,columns=['Cust_ID','OrderMade','OrderType','OrderCategoryA','OrderCategoryB','OrderCategoryC','OrderCategoryD'])


+----+-----------+-------------+-------------+------------------+------------------+------------------+------------------+
|    |   Cust_ID | OrderMade   | OrderType   | OrderCategoryA   | OrderCategoryB   | OrderCategoryC   | OrderCategoryD   |
|----+-----------+-------------+-------------+------------------+------------------+------------------+------------------|
|  0 |         1 | Yes         | A           | No               | Yes              | No               | No               |
|  1 |         2 | Yes         | A           | No               | No               | Yes              | No               |
|  2 |         3 | Yes         | B           | No               | No               | Yes              | No               |
|  3 |         4 | No          |             |                  |                  |                  |                  |
|  4 |         5 | No          |             |                  |                  |                  |                  |
|  5 |         6 | Yes         | C           | No               | No               | Yes              | Yes              |
|  6 |         7 | Yes         | A           | No               | Yes              | No               | No               |
|  7 |         8 | Yes         | A           | No               | No               | Yes              | No               |
|  8 |         9 | No          |             |                  |                  |                  |                  |
|  9 |        10 | Yes         | B           | Yes              | Yes              | No               | No               |
+----+-----------+-------------+-------------+------------------+------------------+------------------+------------------+
</code></pre>

<p>How can I transform this to make rows based on the <code>OrderCategory</code>?</p>

<pre><code>+--------+-----------+----------+----------------+
|Cust_ID | OrderMade |OrderType | OrderCategory  |
|--------+-----------+----------+----------------|
|1       |   Yes     |    A     | OrderCategoryB |
|2       |   Yes     |    A     | OrderCategoryC |
|3       |   Yes     |    B     | OrderCategoryC |
|4       |   No      |          |                |
|5       |   No      |          |                |
|6       |   Yes     |    C     | OrderCategoryC |
|6       |   Yes     |    C     | OrderCategoryD |
|7       |   Yes     |    A     | OrderCategoryB |
|8       |   Yes     |    A     | OrderCategoryC |
|9       |   No      |          |                |
|10      |   Yes     |    B     | OrderCategoryA |
|10      |   Yes     |    B     | OrderCategoryB |
+--------+-----------+----------+----------------+
</code></pre>

<p>I tried to use <code>crosstab</code> to start with one <code>OrderCategory</code>, and planned to duplicate for each category, but this seems inefficient and I wasn't sure how to proceed to get my desired result...</p>

<pre><code>imgCROSS = pd.crosstab(df[""Cust_ID""], df[""OrderCategoryA""])
</code></pre>

<p>Returns...</p>

<pre><code>OrderCategoryA     No  Yes
Cust_ID                   
1               0   1    0
2               0   1    0
3               0   1    0
4               1   0    0
5               1   0    0
6               0   1    0
7               0   1    0
8               0   1    0
9               1   0    0
10              0   0    1
</code></pre>

<p>I also thought I could populate a new empty column called <code>Category</code> and iterate over each row, populating the appropriate category based on the <code>Yes/No</code> value, but this wouldn't work for rows which have multiple categories. Also, the below implementation of this idea returned an empty column.</p>

<pre><code>imgRaw[""Category""] = """"
for index, row in df.iterrows():
    catA = row[""OrderCategoryA""]
    catB = row[""OrderCategoryB""]
    catC = row[""OrderCategoryC""]
    catD = row[""OrderCategoryD""]

    if catA == ""Yes"":
        row[""Category""] = ""OrderCategoryA""
    elif catB == ""Yes"":
        row[""Category""] = ""OrderCategoryB""
    elif catC == ""Yes"":
        row[""Category""] = ""OrderCategoryC""
    elif catD == ""Yes"":
        row[""Category""] = ""OrderCategoryD""
</code></pre>

<p>I know I need to transform the dataframe, probably multiple times before I can get my desired result. Just stuck on how to proceed.</p>
","10540565","","10540565","","2019-06-13 17:09:16","2019-06-13 17:09:16","Pandas duplicate rows based on column value","<python><python-3.x><pandas><dataframe><transformation>","4","0","","","","CC BY-SA 4.0","1"
"41021033","1","41021137","","2016-12-07 15:13:05","","2","1951","<p>This is my code: </p>

<pre><code>import os

file=[]

directory ='/Users/xxxx/Documents/sample/'
for i in os.listdir(directory):
   file.append(i)


Com = list(file)
df=pd.DataFrame(data=Com)
df.to_csv('com.csv', index=False, header=True)

print('done')
</code></pre>

<p>at the moment I am getting all the values for <code>i</code> in one column as a row header. Does anyone know how to make each <code>i</code> value in one row as a column header?</p>
","7079686","","","","","2016-12-07 15:18:58","Pandas writing in csv file as columns not rows-Python","<python-3.x><csv><pandas><dataframe>","1","2","","","","CC BY-SA 3.0","1"
"49243592","1","49243986","","2018-03-12 20:04:54","","0","1932","<p>I have a tsv file with a column containing utf-8 encoded byte strings (e.g., <code>b'La croisi\xc3\xa8re'</code>). I am trying to read this file with the <code>pandas</code> method <code>read_csv</code>, but what I get is a column of strings, not byte strings (e.g., <code>""b'La croisi\xc3\xa8re'""</code>).</p>

<p>How can I read that column as byte strings instead of regular strings in Python 3? I tried to use <code>dtype={'my_bytestr_col': bytes}</code> in <code>read_csv</code> with no luck.</p>

<p>Another way to put it: How can I go from something like <code>""b'La croisi\xc3\xa8re'""</code> to <code>b'La croisi\xc3\xa8re'</code>?</p>
","777706","","","","","2018-03-12 23:59:08","Reading tsv files with byte strings with pandas","<python-3.x><pandas><bytestring>","1","2","","","","CC BY-SA 3.0","1"
"57078803","1","","","2019-07-17 14:56:05","","11","1888","<p>I have a large Pandas dataframe (~15GB, 83m rows) that I am interested in saving as an <code>h5</code> (or <code>feather</code>) file. One column contains long ID strings of numbers, which should have string/object type. But even when I ensure that pandas parses all columns as <code>object</code>:</p>

<pre><code>df = pd.read_csv('data.csv', dtype=object)
print(df.dtypes)  # sanity check
df.to_hdf('df.h5', 'df')

&gt; client_id                object
  event_id                 object
  account_id               object
  session_id               object
  event_timestamp          object
  # etc...
</code></pre>

<p>I get this error:</p>

<pre><code>  File ""foo.py"", line 14, in &lt;module&gt;
    df.to_hdf('df.h5', 'df')
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pandas/core/generic.py"", line 1996, in to_hdf
    return pytables.to_hdf(path_or_buf, key, self, **kwargs)
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pandas/io/pytables.py"", line 279, in to_hdf
    f(store)
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pandas/io/pytables.py"", line 273, in &lt;lambda&gt;
    f = lambda store: store.put(key, value, **kwargs)
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pandas/io/pytables.py"", line 890, in put
    self._write_to_group(key, value, append=append, **kwargs)
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pandas/io/pytables.py"", line 1367, in _write_to_group
    s.write(obj=value, append=append, complib=complib, **kwargs)
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pandas/io/pytables.py"", line 2963, in write
    self.write_array('block%d_values' % i, blk.values, items=blk_items)
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pandas/io/pytables.py"", line 2730, in write_array
    vlarr.append(value)
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/tables/vlarray.py"", line 547, in append
    self._append(nparr, nobjects)
  File ""tables/hdf5extension.pyx"", line 2032, in tables.hdf5extension.VLArray._append
OverflowError: value too large to convert to int
</code></pre>

<p>Apparently it is trying to convert this to an int anyway, and failing.</p>

<p>When running <code>df.to_feather()</code> I have a similar issue:</p>

<pre><code>df.to_feather('df.feather')
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pandas/core/frame.py"", line 1892, in to_feather
    to_feather(self, fname)
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pandas/io/feather_format.py"", line 83, in to_feather
    feather.write_dataframe(df, path)
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pyarrow/feather.py"", line 182, in write_feather
    writer.write(df)
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pyarrow/feather.py"", line 93, in write
    table = Table.from_pandas(df, preserve_index=False)
  File ""pyarrow/table.pxi"", line 1174, in pyarrow.lib.Table.from_pandas
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pyarrow/pandas_compat.py"", line 501, in dataframe_to_arrays
    convert_fields))
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 586, in result_iterator
    yield fs.pop().result()
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 425, in result
    return self.__get_result()
  File ""/usr/lib/python3.6/concurrent/futures/_base.py"", line 384, in __get_result
    raise self._exception
  File ""/usr/lib/python3.6/concurrent/futures/thread.py"", line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pyarrow/pandas_compat.py"", line 487, in convert_column
    raise e
  File ""/shared_directory/projects/env/lib/python3.6/site-packages/pyarrow/pandas_compat.py"", line 481, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
  File ""pyarrow/array.pxi"", line 191, in pyarrow.lib.array
  File ""pyarrow/array.pxi"", line 78, in pyarrow.lib._ndarray_to_array
  File ""pyarrow/error.pxi"", line 85, in pyarrow.lib.check_status
pyarrow.lib.ArrowInvalid: ('Could not convert 1542852887489 with type str: tried to convert to double', 'Conversion failed for column session_id with type object')
</code></pre>

<p>So:</p>

<ol>
<li>Is anything that looks like a number forcibly converted to a number
in storage? </li>
<li>Could the presence of NaNs affect what's happening here?</li>
<li>Is there an alternative storage solution? What would be the best?</li>
</ol>
","6220759","","8557832","","2019-07-28 13:53:34","2019-08-26 12:17:57","OverflowError while saving large Pandas df to hdf","<python-3.x><pandas><hdf><feather>","2","6","1","","","CC BY-SA 4.0","1"
"48595423","1","","","2018-02-03 08:26:32","","3","1809","<p>I am new to using Altair and have crudely combined some code from <a href=""https://stackoverflow.com/questions/22787209/how-to-have-clusters-of-stacked-bars-with-python-pandas"">here</a> and <a href=""https://stackoverflow.com/questions/43797379/how-to-create-a-grouped-bar-chart-in-altair"">here</a> which gives me the plot below but I cannot work out why my columns are shown in a non-sequential order.  My dataframe is as follows:</p>

<pre><code>import pandas as pd

TimeMth = pd.DataFrame([['Jul-2011', '&gt; 1.1 pu',  86],
                       ['Jul-2011', '&lt; 0.94 pu',  18],
                       ['Aug-2011', '&gt; 1.1 pu',  205],
                       ['Aug-2011', '&lt; 0.94 pu', 510],
                       ['Sep-2011', '&gt; 1.1 pu',   53],
                       ['Sep-2011', '&lt; 0.94 pu', 140],
                       ['Oct-2011', '&gt; 1.1 pu',   10],
                       ['Oct-2011', '&lt; 0.94 pu',   0],
                       ['Nov-2011', '&gt; 1.1 pu',   36],
                       ['Nov-2011', '&lt; 0.94 pu',  13],
                       ['Dec-2011', '&gt; 1.1 pu',    7],
                       ['Dec-2011', '&lt; 0.94 pu',   0],
                       ['Jan-2012', '&gt; 1.1 pu',   17],
                       ['Jan-2012', '&lt; 0.94 pu',   0],
                       ['Feb-2012', '&gt; 1.1 pu',    0],
                       ['Feb-2012', '&lt; 0.94 pu',   0],
                       ['Mar-2012', '&gt; 1.1 pu',   17],
                       ['Mar-2012', '&lt; 0.94 pu',   1],
                       ['Apr-2012', '&gt; 1.1 pu',   49],
                       ['Apr-2012', '&lt; 0.94 pu',  79],
                       ['May-2012', '&gt; 1.1 pu',    8],
                       ['May-2012', '&lt; 0.94 pu',   0],
                       ['Jun-2012', '&gt; 1.1 pu',   40],
                       ['Jun-2012', '&lt; 0.94 pu',  12]],
                    columns=['Month','Voltage','No of Violations'])
</code></pre>

<p>which I then attempt to plot with:</p>

<pre><code>import altair as alt

chartTimeMth = alt.Chart(TimeMth).mark_bar().encode(
column=alt.Column('Month',
            axis=alt.Axis(axisWidth=1.0, 
               offset=-8.0, orient='bottom', labelAngle=-45, 
labelAlign='right'),
            scale=alt.Scale(padding=4.0)),
x=alt.X('Voltage',
       axis=False),
y=alt.Y('No of Violations', title='No. of voltage violations', 
axis=alt.Axis(grid=False)),
color=alt.Color('Voltage', scale=alt.Scale(range=['#96ceb4', '#ffcc5c']))
).configure_facet_cell(
strokeWidth=0.0,)\
.configure_scale(bandSize=12)\
.configure_legend(titleFontSize=12, offset=-60, orient='right')

chartTimeMth.display()
</code></pre>

<p>When I use <a href=""https://altair-viz.github.io/API.html?highlight=sort#altair.SortField"" rel=""nofollow noreferrer"">sortField</a> on the columns it gets close to what I need but still not right.  I guess this is something to do with the fact that my column (Month) data is text?  Any ideas on why the above code (with the <em>sortField</em> class omitted) does not plot the columns in index order?
If my <em>Month</em> data was <em>datetimeindex</em>, is there a way to tell Altair to label the x-axis as I have below (ie mmm-yyyy)?</p>

<p><a href=""https://i.stack.imgur.com/a2hWP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a2hWP.png"" alt=""enter image description here""></a></p>
","7519049","","7519049","","2018-02-04 04:21:29","2018-03-05 19:44:18","Sorting Altair column order","<python-3.x><pandas><altair>","1","0","2","","","CC BY-SA 3.0","1"
"41527280","1","41528931","","2017-01-07 22:27:12","","1","1785","<p>I am attempting to find the outliers of seconds with standard deviations. I have two dataframes as below. The outliers I am trying to find are 1.5 standard deviations away from the mean by day of week?  Current code is below the dataframes.  </p>

<p>df1:</p>

<pre><code>name    dateTime              Seconds
joe     2015-02-04 12:12:12   54321.0202
john    2015-01-02 13:13:13   12345.0101
joe     2015-02-04 12:12:12   54321.0202
john    2015-01-02 13:13:13   12345.0101
joe     2015-02-04 12:12:12   54321.0202
john    2015-01-02 13:13:13   12345.0101
joe     2015-02-04 12:12:12   54321.0202
john    2015-01-02 13:13:13   12345.0101
joe     2015-02-04 12:12:12   54321.0202
john    2015-01-02 13:13:13   12345.0101
joe     2015-02-04 12:12:12   54321.0202
joe     2015-01-02 13:13:13   12345.0101
</code></pre>

<p>current output: df2</p>

<pre><code>name   day   standardDev        mean           count
Joe    mon   22326.502700       40900.730647   1886
       tue   9687.486726        51166.213836   159
john   mon   10072.707891       41380.035108   883
       tue   5499.475345        26985.938776   196
</code></pre>

<p>Expected output:</p>

<p>df2 </p>

<pre><code>name   day   standardDev        mean           count     events
Joe    mon   22326.502700       40900.730647   1886      [2015-02-04 12:12:12, 2015-02-04 12:12:13]
       tue   9687.486726        51166.213836   159       [2015-02-04 12:12:12, 2015-02-04 12:12:14]
john   mon   10072.707891       41380.035108   883       [2015-01-02 13:13:13, 2015-01-02 13:13:15]
       tue   5499.475345        26985.938776   196       [2015-01-02 13:13:13, 2015-01-02 13:13:18]
</code></pre>

<p>CODE: </p>

<pre><code>allFiles = glob.glob(folderPath + ""/*.csv"")
list_ = []
for file_ in allFiles:
    df = pd.read_csv(file_, index_col=None, names=['EventTime', ""IpAddress"", ""Hostname"", ""TargetUserName"", ""AuthenticationPackageName"", ""TargetDomainName"", ""EventReceivedTime""])
    df = df.ix[1:]
    list_.append(df)
df = pd.concat(list_)
df['DateTime'] = pd.to_datetime(df['EventTime'])
df['day_of_week'] = df.DateTime.dt.strftime('%a')
df['seconds'] = pd.to_timedelta(df.DateTime.dt.time.astype(str)).dt.seconds
print(df.groupby((['TargetUserName', 'day_of_week'])).agg({'seconds': {'mean': lambda x: (x.mean()), 'std': lambda x: (np.std(x)), 'count': 'count'}}))
</code></pre>
","6916973","","6916973","","2017-01-07 23:06:26","2017-01-08 02:50:48","Find outliers of data","<python><python-3.x><pandas><statistics>","1","9","1","","","CC BY-SA 3.0","1"
"49324988","1","49325042","","2018-03-16 16:00:14","","0","1746","<p>I have a <code>df</code> looks like,</p>

<pre><code>A               B              C    D
2017-10-01      2017-10-11     M    2017-10
2017-10-02      2017-10-03     M    2017-10
2017-11-01      2017-11-04     B    2017-11
2017-11-08      2017-11-09     B    2017-11
2018-01-01      2018-01-03     A    2018-01
</code></pre>

<p>the <code>dtype</code> of <code>A</code> and <code>B</code> are <code>datetime64</code>, <code>C</code> and <code>D</code> are of <code>strings</code>;</p>

<p>I like to <code>groupby</code> <code>C</code> and <code>D</code> and get the differences between <code>B</code> and <code>A</code>,</p>

<pre><code>df.groupby(['C', 'D']).apply(lambda row: row['B'] - row['A'])
</code></pre>

<p>but I don't know how to sum such differences in each group and assign the values to a new column say <code>E</code>, possibly in a new <code>df</code>,</p>

<pre><code>C    D          E
M    2017-10    11
M    2017-10    11
B    2017-11    4
B    2017-11    4
A    2018-01    2
</code></pre>
","766708","","766708","","2018-03-16 16:04:51","2018-03-16 16:06:31","pandas sum the differences between two columns in each group","<python><python-3.x><pandas><dataframe><pandas-groupby>","1","2","1","","","CC BY-SA 3.0","1"
"48883872","1","48883941","","2018-02-20 11:07:21","","3","1727","<p>For the below pandas code in jupyter I am trying to get the data type information .<code>tab</code> in jupyter provides me information that there is two attributes 
It has both <code>dtype</code> and <code>dtypes</code> </p>

<pre><code>import pandas as pd
new_list = [True,False]
new_pd = pd.Series(new_list)
new_pd
</code></pre>

<p><a href=""https://i.stack.imgur.com/RNETr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RNETr.png"" alt=""attributes""></a></p>

<p>As per the documentation both returns data type information
<a href=""https://i.stack.imgur.com/w89yB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w89yB.png"" alt=""dtype""></a></p>

<p><a href=""https://i.stack.imgur.com/3Ndve.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3Ndve.png"" alt=""dtypes""></a></p>

<p>return from both are good and useful</p>

<p><a href=""https://i.stack.imgur.com/Xgsvk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Xgsvk.png"" alt=""result""></a></p>

<p>Question is why there is same duplicate attributes .
Which one to be used in which scenario or its a don't care anyone can be used ?</p>
","7590993","","","","","2019-03-12 09:57:11","What is the difference between pandas dtype vs dtypes","<python><python-3.x><pandas>","2","2","","","","CC BY-SA 3.0","1"
"41044552","1","","","2016-12-08 16:44:02","","1","1716","<p>When importing a csv file I can't seem to set the index. I can't work out if am importing the file correctly, I am doing everything in the interpreter currently, here is what I have:</p>

<pre><code>df = pd.read_csv('E:/test.vbo', sep='\t', encoding='iso-8859-1', skiprows=97)
print(df.head())
</code></pre>

<p>this give the following:</p>

<pre><code>     sats time lat long velocity heading height ...
0                                                                  [data]
1    008 144403.30 003067.21791 000031.98044 010.033 097.16 +00112.43 ...
2    008 144403.40 003067.21777 000031.98036 010.584 098.58 +00113.06 ...
3    008 144403.50 003067.21765 000031.98032 010.809 099.74 +00113.72 ...
4    008 144403.60 003067.21749 000031.98025 011.231 101.05 +00114.34 ...
5    008 144403.70 003067.21728 000031.98021 011.575 102.14 +00114.89 ...
</code></pre>

<p>Which is fine, however, this line:
print(df.set_index('time'))</p>

<p>give an error:</p>

<pre><code>&gt;&gt;&gt; print(df.set_index('time'))
Traceback (most recent call last):
  File ""C:\Users\rob.kinsey\AppData\Local\Continuum\Anaconda3\lib\site-packages\
pandas\indexes\base.py"", line 1945, in get_loc
    return self._engine.get_loc(key)
  File ""pandas\index.pyx"", line 137, in pandas.index.IndexEngine.get_loc (pandas
\index.c:4154)
  File ""pandas\index.pyx"", line 159, in pandas.index.IndexEngine.get_loc (pandas
\index.c:4018)
  File ""pandas\hashtable.pyx"", line 675, in pandas.hashtable.PyObjectHashTable.g
et_item (pandas\hashtable.c:12368)
  File ""pandas\hashtable.pyx"", line 683, in pandas.hashtable.PyObjectHashTable.g
et_item (pandas\hashtable.c:12322)
KeyError: 'time'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""C:\Users\rob.kinsey\AppData\Local\Continuum\Anaconda3\lib\site-packages\
pandas\core\frame.py"", line 2837, in set_index
    level = frame[col]._values
  File ""C:\Users\rob.kinsey\AppData\Local\Continuum\Anaconda3\lib\site-packages\
pandas\core\frame.py"", line 1997, in __getitem__
    return self._getitem_column(key)
  File ""C:\Users\rob.kinsey\AppData\Local\Continuum\Anaconda3\lib\site-packages\
pandas\core\frame.py"", line 2004, in _getitem_column
    return self._get_item_cache(key)
  File ""C:\Users\rob.kinsey\AppData\Local\Continuum\Anaconda3\lib\site-packages\
pandas\core\generic.py"", line 1350, in _get_item_cache
    values = self._data.get(item)
  File ""C:\Users\rob.kinsey\AppData\Local\Continuum\Anaconda3\lib\site-packages\
pandas\core\internals.py"", line 3290, in get
    loc = self.items.get_loc(item)
  File ""C:\Users\rob.kinsey\AppData\Local\Continuum\Anaconda3\lib\site-packages\
pandas\indexes\base.py"", line 1947, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File ""pandas\index.pyx"", line 137, in pandas.index.IndexEngine.get_loc (pandas
\index.c:4154)
  File ""pandas\index.pyx"", line 159, in pandas.index.IndexEngine.get_loc (pandas
\index.c:4018)
  File ""pandas\hashtable.pyx"", line 675, in pandas.hashtable.PyObjectHashTable.g
et_item (pandas\hashtable.c:12368)
  File ""pandas\hashtable.pyx"", line 683, in pandas.hashtable.PyObjectHashTable.g
et_item (pandas\hashtable.c:12322)
KeyError: 'time'
&gt;&gt;&gt;
</code></pre>

<p>What am I missing please?</p>

<pre><code>&gt;&gt;&gt; print(df.columns.tolist())
['sats time lat long velocity heading height vert-vel dgps racceleratorpedal ast
eeringwheel pbrake glateral glongitudinal awingpitch ngearengaged nengine nwheel
fr nwheelrr nwheelfl nwheelrl mengine rdrsavailabledisplayed pwaterpump toil tdc
dc tmotorstator phvac nephsmotor pboost taircharge tcoolant tclutchoil nephspump
demanded tcellmax vbattery paerooil taerooil tmcucoldplate awingpitchdemand tmcu
_igbtmax avifileindex avitime ']
</code></pre>

<p>Solved, here is the correct read_csv line:</p>

<pre><code>&gt;&gt;&gt; df = pd.read_csv('E:/vbox_data/P1GTR__20150922144312_0001.vbo', delim_whitespace=True, encoding='iso-8859-1', header=90)
</code></pre>
","6500048","","6500048","","2016-12-08 17:25:38","2016-12-08 17:25:38","Pandas dataframe set_index KeyError:","<python-3.x><csv><pandas>","0","6","","","","CC BY-SA 3.0","1"
"57130799","1","","","2019-07-21 06:19:30","","1","1711","<p>I have a data frame with a DateTime column, I can get minimum value by using </p>

<pre><code>df['Date'].min()
</code></pre>

<p>How can I get the second, third... smallest values</p>
","11092741","","","","","2019-07-21 06:24:34","Pandas get second minimum value from datetime column","<python-3.x><pandas><dataframe>","2","1","","2019-07-21 06:31:00","","CC BY-SA 4.0","1"
"57503786","1","57504615","","2019-08-15 01:14:21","","0","1669","<p>My dataframe has thousands of rows.<br>
It look like this:</p>

<pre><code>import pandas as pd
import numpy as np
text = ['please send us a dm...','…could you please dm me','dm me plz…','i dmed u yesterday…','dm me asap thx', 'i send a dm to u now', 'thx u r so nice dming u now', 'just sent u a dm']
df = pd.DataFrame({""text"": text})

          text
0   please send us a dm...
1   …could you please dm me
2   dm me plz…
3   i dmed u yesterday…
4   dm me asap thx
5   i send a dm to u now
6   thx u r so nice dming u now
7   just sent u a dm
</code></pre>

<p>I wrote a function to replace abbreviation in column 'text'.</p>

<pre><code>def convert(dataframe, column):
    dataframe[column] = dataframe[column].apply(lambda x: x.replace("" dm "", "" direct message ""))
    dataframe[column] = dataframe[column].apply(lambda x: x.replace("" dming "", "" direct message ""))
    dataframe[column] = dataframe[column].apply(lambda x: x.replace("" dmed "", "" direct message ""))
    dataframe[column] = dataframe[column].apply(lambda x: x.replace("" plz "", "" please ""))
    dataframe[column] = dataframe[column].apply(lambda x: x.replace("" thx "", "" thanks ""))
    dataframe[column] = dataframe[column].apply(lambda x: x.replace("" u "", "" you ""))
    dataframe[column] = dataframe[column].apply(lambda x: x.replace("" asap "", "" as soon as possible ""))
    dataframe[column] = dataframe[column].apply(lambda x: x.replace(""..."", "" ""))
    dataframe[column] = dataframe[column].apply(lambda x: x.replace(""…"", "" ""))   
</code></pre>

<p>However, my code is not working properly, so it can't fully replace all of the abbreviations in my dataframe.</p>

<pre><code>convert(df, 'text')

          text
0   please send us a dm
1   could you please direct message me
2   dm me plz
3   i direct message you yesterday
4   dm me as soon as possible thx
5   i send a direct message to you now
6   thx you r so nice direct message you now
7   just sent you a dm
</code></pre>

<p>The desired final output would look like this:  </p>

<pre><code>          text
0   please send us a direct message
1   could you please direct message me
2   direct message me plz
3   i direct message you yesterday
4   direct message me as soon as possible thanks
5   i send a direct message to you now
6   thanks you r so nice direct message you now
7   just sent you a direct message
</code></pre>

<p>I can't figure out why my code is not working.  </p>
","11876316","","472495","","2019-08-18 12:19:53","2019-08-18 12:19:53","Python - Replace abbreviation in text","<python><python-3.x><pandas><nlp>","3","2","","","","CC BY-SA 4.0","1"
"56690524","1","56695238","","2019-06-20 16:43:45","","2","1665","<h1>How do I set a consistent colorscheme for three <code>axes</code> in the same figure?</h1>

<p>The following should be a wholly reproducible example to run the code and get the same figure I have posted below.</p>

<p>Get the shapefile data from the <a href=""http://geoportal.statistics.gov.uk/datasets/8edafbe3276d4b56aec60991cbddda50_1?selectedAttribute=lad15cd"" rel=""nofollow noreferrer"">Office for National Statistics</a>. Run this in a terminal as a <code>bash</code> file / commands.</p>

<pre class=""lang-sh prettyprint-override""><code>wget --output-document 'LA_authorities_boundaries.zip' 'https://opendata.arcgis.com/datasets/8edafbe3276d4b56aec60991cbddda50_1.zip?outSR=%7B%22latestWkid%22%3A27700%2C%22wkid%22%3A27700%7D&amp;session=850489311.1553456889'

mkdir LA_authorities_boundaries
cd LA_authorities_boundaries
unzip ../LA_authorities_boundaries.zip
</code></pre>

<p>The python code that reads the shapefile and creates a dummy <code>GeoDataFrame</code> for reproducing the behaviour.</p>

<pre class=""lang-py prettyprint-override""><code>import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt

gdf = gpd.read_file(
    'LA_authorities_boundaries/Local_Authority_Districts_December_2015_Full_Extent_Boundaries_in_Great_Britain.shp'
)

# 380 values
df = pd.DataFrame([])
df['AREA_CODE'] = gdf.lad15cd.values
df['central_pop'] = np.random.normal(30, 15, size=(len(gdf.lad15cd.values)))
df['low_pop'] = np.random.normal(10, 15, size=(len(gdf.lad15cd.values)))
df['high_pop'] = np.random.normal(50, 15, size=(len(gdf.lad15cd.values)))

</code></pre>

<p>Join the shapefile from ONS and create a <code>geopandas.GeoDataFrame</code></p>

<pre class=""lang-py prettyprint-override""><code>def join_df_to_shp(pd_df, gpd_gdf):
    """"""""""""
    df_ = pd.merge(pd_df, gpd_gdf[['lad15cd','geometry']], left_on='AREA_CODE', right_on='lad15cd', how='left')

    # DROP the NI counties
    df_ = df_.dropna(subset=['geometry'])

    # convert back to a geopandas object (for ease of plotting etc.)
    crs = {'init': 'epsg:4326'}
    gdf_ = gpd.GeoDataFrame(df_, crs=crs, geometry='geometry')
    # remove the extra area_code column joined from gdf
    gdf_.drop('lad15cd',axis=1, inplace=True)

    return gdf_

pop_gdf = join_df_to_shp(df, gdf)
</code></pre>

<p>Make the plots</p>

<pre class=""lang-py prettyprint-override""><code>fig,(ax1,ax2,ax3,) = plt.subplots(1,3,figsize=(15,6))

pop_gdf.plot(
    column='low_pop', ax=ax1, legend=True,  scheme='quantiles', cmap='OrRd',
)
pop_gdf.plot(
    column='central_pop', ax=ax2, legend=True, scheme='quantiles', cmap='OrRd',
)
pop_gdf.plot(
    column='high_pop', ax=ax3, legend=True,  scheme='quantiles', cmap='OrRd',
)
for ax in (ax1,ax2,ax3,):
    ax.axis('off')
</code></pre>

<p><a href=""https://i.stack.imgur.com/aNinv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/aNinv.png"" alt=""enter image description here""></a></p>

<h1>I want all three <code>ax</code> objects to share the same bins (preferable the <code>central_pop</code> scenario <code>quantiles</code>) so that the legend is consistent for the whole figure. The <code>quantiles</code> from ONE scenario (central) would become the <code>levels</code> for all</h1>

<p>This way I should see darker colors (more red) in the far right <code>ax</code> showing the <code>high_pop</code> scenario.</p>

<p>How can I set the colorscheme bins for the whole figure / each of the <code>ax</code> objects?</p>

<p>The simplest way I can see this working is either
a) Provide a set of bins to the <code>geopandas.plot()</code> function
b) extract the colorscheme / bins from one <code>ax</code> and apply it to another.</p>
","9940782","","9940782","","2019-06-21 09:47:40","2019-06-21 09:47:40","matplotlib geopandas plot chloropleth with set bins for colorscheme","<python><python-3.x><matplotlib><geopandas>","1","4","","","","CC BY-SA 4.0","1"
"48495607","1","","","2018-01-29 06:34:48","","0","1654","<p>I want to make a dataframe a json object.</p>

<p>For example, </p>

<pre><code>function f():
    ...
    ...
    ...
    return dataframe.to_json(orient='records',force_ascii=False)
</code></pre>

<p>but to_json does not have encoding paramter like <code>to_csv</code></p>

<p>How do I encode with utf-8 in the case of <code>to_json</code>?</p>

<p>thank you for reading.</p>
","9219332","","3054161","","2018-01-29 07:10:30","2018-01-29 07:10:30","Does pandas to_json have an encoding parameter?","<python-3.x><pandas><dataframe><to-json>","0","7","","","","CC BY-SA 3.0","1"
"49652693","1","","","2018-04-04 13:46:20","","0","1588","<p>I am working on a excel file with large text data. 2 columns have lot of text data. Like descriptions, job duties. </p>

<p>When i import my file in python df=pd.read_excel(""form1.xlsx""). It shows the columns with text data as NaN. </p>

<p>How do I import all the text in the columns ?
I want to do analysis on job title , description and job duties. Descriptions and Job Title are long text. I have over 150 rows. </p>
","9597083","","","","","2018-04-04 18:21:19","how to read text from excel file in python pandas?","<excel><python-3.x><pandas><import>","2","1","1","","","CC BY-SA 3.0","1"
"41628391","1","","","2017-01-13 06:11:16","","0","1587","<p>How to slice a python pandas data-frame with 1200 rows into 12 equal parts? either in Python2 or Python 3</p>
","7103530","","","","","2017-01-13 08:36:55","How to slice a python pandas dataframe with 1200 rows into 12 equal parts?","<python><python-3.x><pandas><python-2.x>","1","4","1","2017-01-13 09:02:54","","CC BY-SA 3.0","1"
"48645180","1","","","2018-02-06 14:21:03","","3","1577","<p>I have a pandas dataframe (lets say df) which has three columns: </p>

<pre><code>src       dst       weight
a         b         2
c         d         7
b         a         5
d         c         1
d         a         3
a         a         4
b         b         1
</code></pre>

<p>I want to create a directed weighted graph. I have tried the following but i could not get weights into the visualisation.</p>

<pre><code>G = nx.from_pandas_dataframe(df,source='src', target='dst', edge_attr=['weight'], create_using=nx.DiGraph())
nx.draw_circular(G, with_labels=True)
plt.show()  
</code></pre>

<p>Any suggestion how to visualize weight of the edges? Moreover, I would be interested to see weights of both directions between two nodes (if bi-direction connection is present). I would also be interested to visualize nodes which are connected to themselves with a certain weight. For example, in the sample data, node 'a' is connected to node 'a' with a weight of 4, how would you visualize it as a king of closed or loop connection? I am using Networkx library.    </p>
","9316299","","9316299","","2018-02-07 11:37:26","2018-02-11 05:53:46","directed weighted graph from pandas dataframe","<python-3.x><pandas><networkx>","1","4","1","","","CC BY-SA 3.0","1"
"40824201","1","40832063","","2016-11-26 23:24:16","","1","1558","<p>I am new to Python and would like to rebuild this <a href=""https://anaconda.org/jbednar/nyc_taxi/notebook"" rel=""nofollow noreferrer"">example</a>. I have longitude and latitude data about NYC Taxi pick-ups and drop-offs, however, I need to change the data to the Web Mercartor format (this cannot be found in the example above). 
I found a function which can take one pair of longitude and latitude values and change it to Web Mercartor format, which was taken from <a href=""http://www.neercartography.com/latitudelongitude-tofrom-web-mercator/"" rel=""nofollow noreferrer"">here</a>, it looks as follows:</p>

<pre><code>import math
def toWGS84(xLon, yLat):
    # Check if coordinate out of range for Latitude/Longitude
    if (abs(xLon) &lt; 180) and (abs(yLat) &gt; 90):
        return

    # Check if coordinate out of range for Web Mercator
    # 20037508.3427892 is full extent of Web Mercator
    if (abs(xLon) &gt; 20037508.3427892) or (abs(yLat) &gt; 20037508.3427892):
        return

    semimajorAxis = 6378137.0  # WGS84 spheriod semimajor axis

    latitude = (1.5707963267948966 - (2.0 * math.atan(math.exp((-1.0 * yLat) / semimajorAxis)))) * (180/math.pi)
    longitude = ((xLon / semimajorAxis) * 57.295779513082323) - ((math.floor((((xLon / semimajorAxis) * 57.295779513082323) + 180.0) / 360.0)) * 360.0)

    return [longitude, latitude]



def toWebMercator(xLon, yLat):
    # Check if coordinate out of range for Latitude/Longitude
    if (abs(xLon) &gt; 180) and (abs(yLat) &gt; 90):
        return

    semimajorAxis = 6378137.0  # WGS84 spheriod semimajor axis
    east = xLon * 0.017453292519943295
    north = yLat * 0.017453292519943295

    northing = 3189068.5 * math.log((1.0 + math.sin(north)) / (1.0 - math.sin(north)))
    easting = semimajorAxis * east

    return [easting, northing]

def main():
    print(toWebMercator(-105.816001, 40.067633))
    print(toWGS84(-11779383.349100526, 4875775.395628653))

if __name__ == '__main__':
    main()
</code></pre>

<p>How do I apply this data to every pair of long/lat coordinates in my pandas Dataframe and save the output in the same pandasDF? </p>

<pre><code>df.tail()
            |    longitude     |    latitude
____________|__________________|______________
11135465    |    -73.986893    |    40.761093  
1113546     |    -73.979645    |    40.747814  
11135467    |    -74.001244    |    40.743172  
11135468    |    -73.997818    |    40.726055  
...
</code></pre>
","5119500","","","","","2016-11-27 18:08:09","Apply function to every row in Pandas Dataframe","<python><python-3.x><pandas><geolocation>","3","0","","","","CC BY-SA 3.0","1"
"49514425","1","","","2018-03-27 13:37:24","","4","1525","<p>I am following this link to remove outliers, but something is logically wrong here..</p>

<p><a href=""https://stackoverflow.com/questions/35827863/remove-outliers-in-pandas-dataframe-using-percentiles/35828995"">Remove Outliers in Pandas DataFrame using Percentiles</a></p>

<p>I have a dataset with first column as ""id"" and last column as ""label"".</p>

<p>Here is my piece of code I am removing label and id columns and then appending it:</p>

<pre><code>def processing_data(train_data,test_data):
    #computing percentiles.
    low = .05
    high = .95
    filt_df = train_data.loc[:, train_data.columns != 'id']
    filt_df= filt_df.loc[:, filt_df.columns != 'label']
    quant_df = filt_df.quantile([low, high])
    print(quant_df)

    #filtering values based on computed percentiles. To do that use an apply by columns.
    print(""Before removing outlier"",filt_df,filt_df.shape)
    train_data1 = filt_df.apply(lambda x: x[(x&gt;=quant_df.loc[low,x.name]) &amp; (x &lt;=quant_df.loc[high,x.name])], axis=0)
    print(""After removing outlier,"",train_data1,train_data1.shape)
    print(train_data1.isnull().sum())
    train_data1= pd.concat([train_data.loc[:,'id'], train_data1], axis=1)
    train_data=pd.concat([train_data.loc[:,'label'], train_data1], axis=1)
    #train_data.dropna(inplace=True)

    #train_data.fillna(0)
    #test_data.fillna(0)
    #print(train_data)
    #print(np.isnan(train_data).any().sum())
    return train_data,test_data
</code></pre>

<p>Output: All the rows contain some NaN values and when I do 
            train_data.dropna(inplace=True) all the rows are dropped. 
            Strange!!</p>

<p>How can I fix this? When I concat id and label column after outlier treatment, I feel something is fishy there?</p>

<p>Here is the dataset:</p>

<pre><code>id  feature0    feature1    feature2    feature3    feature4    feature249  label
0   25.20824887 -16.7457484 50.86994402 5.593471686 1.188262678   1
1   -86.93144987    0.428227194 2.87483597  -8.064850183    6.056867093     2 
2   42.16093367 7.85701304  151.6127571 9.639675583 5.570138511             0
3   20.66694385 8.680641918 -56.44917913    -9.814779803    -2.382979151    1
4   35.9466789  4.57373573  -28.16021186    -6.91297056 4.879375409         0
</code></pre>
","9298540","","9209546","","2018-03-27 17:56:00","2018-06-27 00:27:43","Removing outliers from pandas data frame using percentile","<python><python-3.x><pandas>","1","9","1","","","CC BY-SA 3.0","1"
"50421457","1","50505361","","2018-05-19 03:23:52","","1","1494","<p>Having issues with inserting pandas dataframe to MySQL db.Have listed the sample data and code used along with errors that follow.
How to insert data with pandas dataframe to mysql?</p>

<p><strong>Packages used</strong></p>

<pre><code>Pandas - 0.22.0
sqlalchemy - 1.2.1
</code></pre>

<p><strong>Dataframe used:</strong></p>

<pre><code>Out[135]: 
  P_ID  S_ID            Action                        Timestamp  \
0     Harold           1  Extensions 2017-11-07 03:17:27.342295+00:00   
1     Harold           1  Extensions 2017-11-07 03:17:27.447234+00:00   
2     Harold           1  Extensions 2017-11-07 03:17:27.552406+00:00   
3     Harold           1  Extensions 2017-11-07 03:17:27.657676+00:00   
4     Harold           1  Extensions 2017-11-07 03:17:27.762737+00:00   

       Value  
0 -0.096083  
1 -0.003894  
2 -0.004779  
3  0.131210  
4  0.161990  
</code></pre>

<p><strong>dtypes :</strong></p>

<pre><code>P_ID                 object
S_ID                 int64
Action               object
Timestamp            datetime64[ns, UTC]
Value                float64
dtype: object
</code></pre>

<p>The following code snippets were used with the errors followed </p>

<p>Also pd.to_datetime() did not seem to have any effect. </p>

<p><strong>Code used (1) without index</strong></p>

<pre><code>engine = create_engine('mysql+pymysql://xxxx:3306/xxxx')
test.to_sql(name='table1', con=engine, if_exists = 'append',index=False)
conn.close()
</code></pre>

<p><strong>Error :</strong></p>

<blockquote>
  <p>ValueError: Cannot cast DatetimeIndex to dtype datetime64[us]</p>
</blockquote>

<p><strong>Code used (2) with timestamp column indexed</strong></p>

<pre><code>engine = create_engine('mysql+pymysql://xxxx:3306/xxxx')
test.to_sql(name='table1', con=engine, if_exists = 'append',index=True,
                   index_label='Timestamp',
                   dtype={'Timestamp':typeTIMESTAMP(timezone=True)})
conn.close()
</code></pre>

<p><strong>Error :</strong></p>

<blockquote>
  <p>ValueError: duplicate name in index/columns: cannot insert Timestamp, already exists</p>
</blockquote>
","5465463","","4420967","","2018-05-19 06:41:55","2018-12-08 13:28:06","issues with pandas dataframe insert to mysql with timestamp","<python><mysql><python-3.x><pandas><sqlalchemy>","3","0","","","","CC BY-SA 4.0","1"
"48543032","1","48549278","","2018-01-31 13:10:59","","0","1479","<p>I have a huge csv-file and one column has rows with summaries in different languages. My goal is to sort out those paragraphs that are not written in english.
I don't mind if some words get sorted wrong.</p>

<p>My current code is working but as I'm still a beginner I fear ..it's not really up to speed. Meaning it takes very long this way and as I have about 80k rows I guess I'd still be sitting here next week waiting.
I've checked for solutions but didn't find anything that worked for me, since the langdetections used seemed to be for a small amount of data.</p>

<pre><code>import langdetect
import nltk
import pandas as pd
from nltk.corpus import stopwords
from nltk.corpus import gutenberg
from nltk.tokenize import word_tokenize


test = pd.read_csv(""file.csv"", sep='\t',header=0,index_col=False, quoting=csv.QUOTE_NONE, usecols = (""TI"",""AB"",""PY"",""DI""),dtype = str)

stop_e = stopwords.words('english')
worte = gutenberg.words()

for line in test[""AB""]:
    if type(line) == str:
        tokens = word_tokenize(line)
    for token in tokens:
        if token.isalpha()and token not in stop_e and not in worte:
</code></pre>

<p>After this I'm currently just printing stuff to check if my code is working so far. </p>

<p>Edit. This is faster already, since I skip rows that are purely english. But as was pointed out in the comments: I'm still deleting by word, as I don't know how to remove entire paragraphs.</p>

<pre><code>for line in alle[""AB""]:
    if type(line) == str:
        if detect(line) == 'en':
            pass
        else:
            tokens = word_tokenize(line)
            for token in tokens:
                if token.isalpha()and token not in stop_e and token not in worte: 
#del word
</code></pre>

<p>Do you have any ideas for improvement? I guess my problem is that every word is checked with the whole Gutenberg-corpus..but is there a faster way to do this? </p>

<p>Using <code>from nltk.corpus import words</code> as a corpus instead of Gutenberg seems to be a bit faster but not significantly.</p>

<p>Sample of my dataframe. The summaries in AB are all english here but I want to throw out any german/spanish/others that made it into the csv.</p>

<pre><code>TI  AB  PY  DI
83009   Disability and inclusive education in times of...   When communities fall into decline, disabled p...   2014    10.1080/01425692.2014.919845
83010   Transforming marginalised adult learners' view...   Adult learners on Access to Higher Education c...   2014    10.1080/01425692.2014.919842
83011   Home education, school, Travellers and educati...   The difficulties Traveller pupils experience i...   2014    10.1080/01425692.2014.919840
83012   Promoting online deliberation quality: cogniti...   This research aims to contribute to the theory...   2014    10.1080/1369118X.2014.899610
83013   Living in an age of online incivility: examini...   Communication scholars have examined the poten...   2014    10.1080/1369118X.2014.899609
</code></pre>
","9262250","","9262250","","2018-01-31 14:43:15","2019-11-15 14:02:44","Fast way of checking for language in csv","<python-3.x><pandas><csv><nltk>","1","8","1","","","CC BY-SA 3.0","1"
"48618902","1","48619218","","2018-02-05 09:04:36","","0","1477","<p>I am trying to make a dataframe with Historical data of daily No. of stock Advancing and declining with their respective volumes of Nifty 50 index.
Being new to python I am having trouble handling pandas dataframe and conditions.</p>

<p>Below is the code that I wrote, but it has a lot of issues:</p>

<ol>
<li><p>df.index = data.index error:ValueError: Length mismatch: Expected axis has 0 elements, new values have 248 elements</p></li>
<li><p>if I comment out the above line where I set the index of the empty dataframe, the code runs and gives an empty Dataframe at the end.</p>

<pre><code>#setting default dates
end_date = date.today()
start_date = end_date - timedelta(365)

#Deriving the names of 50 stocks in Nifty 50 Index
nifty_50 = pd.read_html('https://en.wikipedia.org/wiki/NIFTY_50')

nifty50_symbols = nifty_50[1][1]

df = pd.DataFrame(columns = {'Advances','Declines','Adv_Volume','Dec_Volume'})


for x in nifty50_symbols:
    data = nsepy.get_history(symbol = x, start=start_date, end=end_date)
    sclose = data['Close']
    sopen = data['Open']
    svol = data['Volume']


 ##    df.index = data.index

 ##   for i in df.index: --- since df.index was commented out it's value was nill
    for i in data.index:
        if sclose &gt; sopen:
            df['Advances'] = df['Advances'] + 1
            df['Adv_Volume'] = df['Adv_Volume'] + svol

        elif sopen &gt; sclose:
            df['Declines'] = df['Declines'] + 1
            df['Dec_Volume'] = df['Dec_Volume'] + svol


print(df.tail())
</code></pre></li>
</ol>

<p>Output :</p>

<pre><code>Empty DataFrame
Columns: [Dec_Volume, Declines, Advances, Adv_Volume]
Index: []
</code></pre>

<p>EDIT: Found the reason why the code was giving an empty dataframe, because df.index was nill, so the if statement was never triggered.When I changed that part into data.index if statement was triggered. But Now I do not know how do I use the IF statements, since it is giving the error:</p>

<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
</code></pre>

<p>EDIT2: Updated Code with the help of Akshay Nevrekar: Still getting an empty dataframe at the end. Also I have to set index of DF as the dates in data.index, so that I can later relate the Advances/declines to their respective dates.</p>

<pre><code>#setting default dates
end_date = date.today()
start_date = end_date - timedelta(365)

#Deriving the names of 50 stocks in Nifty 50 Index
nifty_50 = pd.read_html('https://en.wikipedia.org/wiki/NIFTY_50')

nifty50_symbols = nifty_50[1][1]

df = pd.DataFrame(columns = {'Advances','Declines','Adv_Volume','Dec_Volume'})


for x in nifty50_symbols:
    data = ns.get_history(symbol = x, start=start_date, end=end_date)
##    sclose = data['Close']
##    sopen = data['Open']
##    svol = data['Volume']

##    df.index = data.index

    for i in data.index:
        sclose=data.loc[i]['Close']
        sopen=data.loc[i]['Open']
        svol = data.loc[i]['Volume']

        if sclose &gt; sopen :
            df['Advances'] = df['Advances'] + 1
            df['Adv_Volume'] = df['Adv_Volume'] + svol

        elif sopen &gt; sclose :
            df['Declines'] = df['Declines'] + 1
            df['Dec_Volume'] = df['Dec_Volume'] + svol



print(df
</code></pre>

<p>)</p>
","9298373","","9298373","","2018-02-05 09:46:55","2018-02-05 09:50:03","Code returns empty dataframe without index","<python><python-3.x><pandas>","1","4","","","","CC BY-SA 3.0","1"
"57803412","1","57803722","","2019-09-05 10:26:05","","0","1470","<p>I have written a small program calling <code>SentimentIntensityAnalyzer</code> function from <code>nltk.sentiment.vader</code> library in Python. I want to analyze comments mentioned in column c2 &amp; provide sentiment score in the new adjacent column. There are 10,000 comments &amp; my comments are in <code>remarks.txt</code> file. I have created <code>get_sentiment()</code> function but facing issues passing each row of the data frame as argument &amp; calling it using for loop to provide sentiment score &amp; store it in adjacent column. </p>

<p>I tried these codes:</p>

<pre><code>df['add'] = df.apply(lambda row: get_sentiment(row)) 
</code></pre>

<p>Couldn't find the solution anywhere on the internet. Codes are as follows:</p>

<pre><code>import nltk
import pandas as pd
import re
from nltk.sentiment.vader import SentimentIntensityAnalyzer
remarks = pd.read_csv('/Users/ZKDN0YU/Desktop/comments/Comments.txt', 
sep='\t')
remarks.head(50)
df = pd.DataFrame(remarks)
print(df)
def get_sentiment(remarks):
 sentiment_score = sid.polarity_scores(row)
 positive_meter = round((sentiment_score['pos'] * 10), 2)
 negative_meter = round((sentiment_score['neg'] * 10), 2)
 return positive_meter, negative_meter

for index, row in df.iterrows():
 df['add'] = df.apply(lambda row: get_sentiment(row)) 
 print(row['c1'], row['c2'],""Positive"",positive_meter,""Negative"", 
 negative_meter)
</code></pre>

<p>Getting following error while running above codes:</p>

<pre><code>File ""&lt;ipython-input-9-7223b4fb6bd7&gt;"", line 10, in get_sentiment
 sentiment_score = sid.polarity_scores(row)

NameError: (""name 'sid' is not defined"", 'occurred at index c1')
</code></pre>
","12024595","","5770501","","2019-09-05 10:38:36","2019-09-05 14:02:06","Applying SentimentIntensityAnalyzer function on each row of the dataframe & providing the sentiment score","<python><python-3.x><pandas>","1","3","","","","CC BY-SA 4.0","1"
"41434214","1","41434647","","2017-01-02 22:52:41","","3","1412","<p>I am using requests library to wrap into a function an api:</p>

<pre><code>import pandas as pd
import requests, json

def foo(text):
    payload = {'key': '00ac1ef82687c7533d54be2e9', 'of': 'json', \
               'nko': text, \
               'woei': 'm', \
               'nvn': 'es'}

    r = requests.get('http://api.example.com/foo', params=payload)
    data = json.loads(r.text)
    return data
</code></pre>

<p>Then, I would like to apply the above function to the following dataframe:</p>

<p>df:</p>

<pre><code>    colA
0   lore lipsum dolor done
1   lore lipsum
2   done lore
3   dolor lone lipsum
</code></pre>

<p>Thus, I tried the following:</p>

<pre><code>df['new_col'] = df['colA'].apply(foo)
df
</code></pre>

<p>However, I got the following exception:</p>

<blockquote>
  <p>/usr/local/lib/python3.5/site-packages/pandas/core/series.py in
  apply(self, func, convert_dtype, args, **kwds)    2287     2288<br>
  if is_extension_type(self.dtype):
  -> 2289                 mapped = self._values.map(f)    2290             else:    2291                 values = self.asobject</p>
  
  <p>/usr/local/lib/python3.5/site-packages/pandas/core/categorical.py in
  map(self, mapper)
      950             return self.from_codes(self._codes.copy(),
      951                                    categories=new_categories,
  --> 952                                    ordered=self.ordered)
      953         except ValueError:
      954             return np.take(new_categories, self._codes)</p>
  
  <p>/usr/local/lib/python3.5/site-packages/pandas/core/categorical.py in
  from_codes(cls, codes, categories, ordered, name)
      466                 ""codes need to be convertible to an arrays of integers"")
      467 
  --> 468         categories = cls._validate_categories(categories)
      469 
      470         if len(codes) and (codes.max() >= len(categories) or codes.min() &lt; -1):</p>
  
  <p>/usr/local/lib/python3.5/site-packages/pandas/core/categorical.py in
  _validate_categories(cls, categories, fastpath)
      571             # categories must be unique
      572 
  --> 573             if not categories.is_unique:
      574                 raise ValueError('Categorical categories must be unique')
      575 </p>
  
  <p>pandas/src/properties.pyx in pandas.lib.cache_readonly.<strong>get</strong>
  (pandas/lib.c:43685)()</p>
  
  <p>/usr/local/lib/python3.5/site-packages/pandas/indexes/base.py in
  is_unique(self)    1068     def is_unique(self):    1069         """"""
  return if the index has unique values """"""
  -> 1070         return self._engine.is_unique    1071     1072     @property</p>
  
  <p>pandas/index.pyx in pandas.index.IndexEngine.is_unique.<strong>get</strong>
  (pandas/index.c:4883)()</p>
  
  <p>pandas/index.pyx in pandas.index.IndexEngine.initialize
  (pandas/index.c:5828)()</p>
  
  <p>pandas/src/hashtable_class_helper.pxi in
  pandas.hashtable.PyObjectHashTable.map_locations
  (pandas/hashtable.c:13788)()</p>
  
  <p>TypeError: unhashable type: 'dict'</p>
</blockquote>

<p>Therefore, my question is how can I apply correctly <code>foo</code> function to <code>df</code> column?</p>
","4140027","","6622587","","2017-01-03 01:07:29","2017-01-03 01:07:29","Unhashable type: 'dict' while applying a function with pandas?","<python><python-3.x><pandas><python-requests>","1","3","","","","CC BY-SA 3.0","1"
"57804145","1","57807389","","2019-09-05 11:08:25","","3","1372","<p>I am researching prescription habits and have large dataframes of sold products.</p>

<p>I am trying to transform purchases of medications into courses of the drugs by calculating how long the product would have lasted and adding a 5 day fudge factor for compliance, starting delays, etc to calculate an end date for the purchase.</p>

<p>I then want to combine prescriptions with overlapping date windows but I'm struggling to find an efficient way to do this. I was hoping a groupby would be possible but I can't figure out how to do this.</p>

<p>I know how to iterate over the dataframe to create a new dataframe with the relevant information, but it is a slow operation and I am hoping I can find a more elegant solution.</p>

<pre class=""lang-py prettyprint-override""><code>ID      start       end         ingredient  days    dose    end
1000    2018-10-03  2018-10-18  Metron...   10.0    125.00 
1000    2018-10-13  2018-10-25  Metron...   7.0     125.00 
1001    2018-03-08  2018-03-20  Cefalexin   7.0     150.00
1001    2018-09-17  2018-10-05  Cefalexin   13.0    150.00
1002    2018-05-18  2018-05-30  Amoxiclav   7.0     75.00
1002    2018-05-25  2018-06-06  Amoxiclav   7.0     100.00 
1003    2018-07-01  2018-07-16  Amoxiclav   10.0    50.00
1003    2018-07-15  2018-07-30  Amoxiclav   10.0    50.00 
1003    2018-07-25  2018-08-09  Amoxiclav   10.0    50.00 
</code></pre>

<p>My expected result is as follows:</p>

<pre class=""lang-py prettyprint-override""><code>ID      start       end         ingredient  days    dose
1000    2018-10-03  2018-10-25  Metron...   17.0    125.00
1001    2018-03-08  2018-03-20  Cefalexin   7.0     150.00
1001    2018-09-17  2018-10-05  Cefalexin   13.0    150.00
1002    2018-05-18  2018-05-30  Amoxiclav   7.0     75.00
1002    2018-05-25  2018-06-06  Amoxiclav   7.0     100.00 
1003    2018-07-01  2018-08-05  Amoxiclav   30.0    50.00
</code></pre>

<p><code>1000</code>'s second purchase was exactly 10 days in so the end date is the same as their second end date.</p>

<p><code>1001</code> did not overlap so remains as they are.</p>

<p><code>1002</code> overlaps on start and end dates but had a change in their dose so should not be combined.</p>

<p><code>1003</code> had 30 days worth in total. The start date of their final purchase is later than the end date of the first. Their end date should be 35 days after they first made a purchase. This is a negotiable criterion and an end date matching the final purchase's end date would be acceptable.</p>

<p>Am I barking up the wrong tree here? Must this be done iteratively?</p>
","8633424","","","","","2019-09-05 14:11:11","Combining rows with overlapping time periods in a pandas dataframe","<python-3.x><pandas><dataframe>","1","2","3","","","CC BY-SA 4.0","1"
"49419104","1","","","2018-03-22 01:29:33","","2","1363","<p>I am trying to add two columns and create a new one. This new column should become the first column in the dataframe or the output csv file. </p>

<pre><code>column_1 column_2
84       test
65       test
</code></pre>

<p>Output should be </p>

<pre><code>column         column_1 column_2
trial_84_test   84      test
trial_65_test   65      test
</code></pre>

<p>I tried below given methods but they did not work:</p>

<pre><code>sum = str(data['column_1']) + data['column_2']

data['column']=data.apply(lambda x:'%s_%s_%s' % ('trial' + data['column_1'] + data['column_2']),axis=1)
</code></pre>

<p>Help is surely appreciated. </p>
","5428772","","9209546","","2019-08-19 18:16:48","2019-08-19 18:16:48","Adding two columns in Python","<python><python-3.x><string><pandas><dataframe>","3","0","","","","CC BY-SA 3.0","1"
"41729084","1","41729710","","2017-01-18 20:43:16","","2","1307","<h2>Problem</h2>

<p>I am trying to use python to read my csv file and put it into Microsoft SQL Server 2016 as a new table. Simply put, I don't want to create a table on SQL and import the csv, I want to write a script in python that can read the csv and create a new table in SQL for me. </p>

<p><strong>UPDATE</strong> </p>

<p>I may have to rethink my approach. I corrected the driver, but I am getting the following error from <code>to_sql</code>. I am thinking that there is something wrong with my authentication scheme. Sadly, the <code>to_sql</code> documentation and <code>sql_alchemy</code> is not shedding much light. Starting to consider alternatives. </p>

<pre><code>sqlalchemy.exc.DBAPIError was unhandled by user code
Message: (pyodbc.Error) ('08001', '[08001] [Microsoft][SQL Server Native Client 11.0]Named Pipes Provider: Could not open a connection to SQL Server [53].  (53) (SQLDriverConnect)')
</code></pre>

<h2>Code</h2>

<pre><code>import pandas as pd 
import sqlalchemy

#Read the file 
data = pd.read_csv(file.csv)

#Connect to database and write the table 
 server = ""DERPSERVER""
 database = ""HERPDB""
 username = ""DBUser"" 
 password = ""password""
 tablename = ""HerpDerpTable""
 driver = ""SQL+Server+Native+Client+11.0"" 

 #Connect to SQL using SQL Server Driver 
 print(""Connect to SQL Server"")
 cnxn = sqlalchemy.create_engine(""mssql+pyodbc://""+username+"":""+password+""@""+server +""/""+database + ""?driver=""+driver)
</code></pre>

<p><strong>UPDATE</strong></p>

<p>I rewrote the string as follows, but it doesn't work: </p>

<pre><code>sqlalchemy.create_engine('mssql+pymssql://'+username+':'+ password + '@' + server + '/' + database + '/?charset=utf8')


data.to_sql(tablename, cnxn);
</code></pre>

<h2>Attempts</h2>

<p>These are some important things to note in my approach. Pay special attention to the second bullet point I share below. I think my connection string for <code>create_engine</code> is somehow or maybe wrong, but don't know what is wrong because I followed the documentation. </p>

<ul>
<li>I believe I am in a DSN-less situation. Thus, was attempting to connect by  other means as described by the documentation. </li>
<li>I was using this <a href=""http://docs.sqlalchemy.org/en/rel_0_9/dialects/mssql.html#dialect-mssql-pyodbc-connect"" rel=""nofollow noreferrer"">link</a> to help me create the connection string part in create_engine. </li>
<li>I tried <code>to_sql</code> to write the to the database, but think my connection string might still be messed up? I consulted this <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html#pandas.DataFrame.to_sql"" rel=""nofollow noreferrer"">question</a> on stackoverflow.</li>
<li><strong>Update</strong> I added the driver specification as MaxU and the documentation for sqlalchemy specified. However, I am getting an error saying my data source name was not found and no default driver is specified with <code>to_sql</code>. Do I need to feed <code>to_sql</code> the driver as well? If so, where is the documentation or a sample code that shows me where I am going wrong? </li>
</ul>

<p>I am making good effort to pick up python and to use it as a scripting language because of future goals and needs. I would appreciate any assistance, help, mentorship rendered. </p>
","4005362","","4005362","","2017-01-21 20:52:52","2017-01-21 20:52:52","Error when trying to create new database table in SQL Server 2016 from csv file while using python 3.5 with pandas and sqlalchemy","<sql-server><python-3.x><csv><pandas><sqlalchemy>","1","2","3","","","CC BY-SA 3.0","1"
"49656386","1","49656539","","2018-04-04 16:50:32","","0","1296","<p>When I run this code:</p>

<pre><code>from sklearn.tree import DecisionTreeRegressor 
melbourne_model = DecisionTreeRegressor() 
melbourne_model.fit(X, y)
</code></pre>

<p>I get this output:</p>

<p><code>ValueError: Input contains NaN, infinity or a value too large for dtype('float32').</code></p>

<p>This error points to the line where it says <code>melbourne_model.fit(X, y)</code>.<br>
I want the code to fit the model with <code>X</code> and <code>y</code> so I can make future predictions of houses in Melbourne based on a few variables I input such as year built, land area, rooms/bedrooms, location, etc. Right now I can't do that because of this error. </p>

<p>I think it is because the <code>X</code> and <code>y</code> aren't NumPy Arrays and when I use <code>np.asarray()</code> and put what I want to turn into a NumPy Array, it doesn't work. I know this because when I write <code>type(X)</code> or <code>type(y)</code>, I get <code>pandas.core.series.Series</code>.</p>

<p>The whole code to my file:</p>

<pre><code>import pandas as pd
import numpy as np
melbourne_file_path = 'melb_data.csv\\melb_data.csv'
melbourne_data = pd.read_csv(melbourne_file_path)
np.asarray(melbourne_data.Price)
y = melbourne_data.Price
melbourne_predictors = ['Rooms', 'Bathroom', 'Landsize', 'BuildingArea', 
                    'YearBuilt', 'Lattitude', 'Longtitude']
np.asarray(melbourne_data[melbourne_predictors])
X = melbourne_data[melbourne_predictors]
from sklearn.tree import DecisionTreeRegressor
melbourne_model = DecisionTreeRegressor()
melbourne_model.fit(X, y)
</code></pre>

<p>I am using Jupyter Notebook as part of Anaconda. </p>

<p>The CSV file I am using can be downloaded <a href=""https://www.kaggle.com/dansbecker/melbourne-housing-snapshot/data"" rel=""nofollow noreferrer"">here</a>.
Once you do the download the folder you need to extract the files and the csv is inside the folder. You can make your own <code>melbourne_file_path</code> based on where the file is for you.</p>
","8760970","","","","","2018-04-04 16:59:28","I have a ValueError while using Jupyter Notebook and need help to find out why I get this error and how to fix it","<python><python-3.x><pandas><machine-learning><data-science>","1","3","","","","CC BY-SA 3.0","1"
"41175215","1","41175301","","2016-12-15 23:50:23","","1","1266","<p>I want to sample 2 rows from ""only"" the class=1 in the ""labels"" column.</p>

<p>In my code you will see that:</p>

<p>1) I sample ALL rows from class=1 (4 rows)</p>

<p>2) Then I sample 2 rows from the previous dataframe</p>

<p>But I am sure there must be a better way to do this.</p>

<pre><code># Creation of the dataframe
df = pd.DataFrame(np.random.rand(12, 5))
label=np.array([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3])
df['label'] = label


# Sampling
df1=df.loc[df['label'] == 1] #Extract ALL samples with class=1
df2 = pd.concat(g.sample(2) for idx, g in df1.groupby('label')) #Extract 2 samples from df1
df2
</code></pre>

<p><a href=""https://i.stack.imgur.com/soYpM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/soYpM.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/orWTk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/orWTk.png"" alt=""enter image description here""></a></p>
","6106842","","2336654","","2016-12-15 23:58:03","2016-12-15 23:59:50","How to sample a # of rows from a specific class in python?","<python-3.x><pandas><numpy>","1","0","","","","CC BY-SA 3.0","1"
"50049216","1","50063221","","2018-04-26 17:52:39","","2","1240","<p>I need to send a dataframe from a backend to a frontend and so first need to convert it either to an object that is JSON serialisable or directly to JSON. The problem being that I have some dataframes that don't have unique cols. I've looked into the <code>orient</code> parameter, <code>to_json()</code>, <code>to_dict()</code> and <code>from_dict()</code> methods but still can't get it to work...</p>

<p><strong>The goal is to be able to convert the df to something json serializable and then back to its initial self.</strong></p>

<p>I'm also having a hard time copy-pasting it using pd.read_clipboard so I've included a sample df causing problems as an image (sorry!).</p>

<p><a href=""https://i.stack.imgur.com/O8wMD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/O8wMD.png"" alt=""enter image description here""></a></p>
","8397886","","","","","2018-04-27 13:19:05","python - Convert pandas dataframe to json or dict and then back to df with non-unique columns","<python><python-3.x><pandas><dataframe><to-json>","1","0","1","","","CC BY-SA 3.0","1"
"33773681","1","","","2015-11-18 07:00:25","","0","1221","<p>I have a csv file that contains data in the following format.</p>

<pre><code>Layer   relative_time   Ht    BSs    Vge    Temp    Message
57986   2:52:46       0.00m   87    15.4    None    CMSG 
20729   0:23:02       45.06m  82    11.6    None    BMSG 
20729   0:44:17       45.06m  81    11.6    None    AMSG 
</code></pre>

<p>I want to get read in this csv file and calculate the average <code>BSs</code> for every hour. My csv file is quite huge about 2000 values. However the values are not evenly distributed across every hour. For e.g. </p>

<p>I have <code>237 samples from hour 3 and only 4 samples from hour 6</code>. Also I should mention that the <code>BSs</code> can be collected from multiple sources.The value always ranges from <code>20-100</code>. Because of this it is giving a skewed result. For each hour I am calculating the sum of <code>BSs</code> for that hour divided by the number of samples in that hour. 
The primary purpose is to understand how <code>BSs</code> evolves over time.</p>

<p>But what is the common approach to this problem. Is this where people apply normalization? It would be great if someone could explain how to apply normalization in such a situation. </p>

<p>The code I am using for my processing is shown below. I believe the code below is correct.</p>

<pre class=""lang-py prettyprint-override""><code>#This 24x2 matrix will contain no of values recorded per hour per hour
hours_no_values = [[0 for i in range(24)] for j in range(2)]

#This 24x2 matrix will contain mean bss stats per hour
mean_bss_stats = [[0 for i in range(24)] for j in range(2)]


with open(PREFINAL_OUTPUT_FILE) as fin, open(FINAL_OUTPUT_FILE, ""w"",newline='') as f:
    reader = csv.reader(fin, delimiter="","")
    writer = csv.writer(f)
    header = next(reader)  # &lt;--- Pop header out
    writer.writerow([header[0],header[1],header[2],header[3],header[4],header[5],header[6]]) # &lt;--- Write header
    sortedlist = sorted(reader, key=lambda row: datetime.datetime.strptime(row[1],""%H:%M:%S""), reverse=True)
    print(sortedlist)
    for item in sortedlist:
        rel_time = datetime.datetime.strptime(item[1], ""%H:%M:%S"")
        if rel_time.hour not in hours_no_values[0]:
            print('item[6] {}'.format(item[6]))
            if 'MAN' in item[6]:
                print('Hour found {}'.format(rel_time.hour))
                hours_no_values[0][rel_time.hour] = rel_time.hour
                mean_bss_stats[0][rel_time.hour] = rel_time.hour

                mean_bss_stats[1][rel_time.hour] += int(item[3])

                hours_no_values[1][rel_time.hour] +=1
            else:
                pass
        else:
            if 'MAN' in item[6]:
                print('Hour Previous {}'.format(rel_time.hour))
                mean_bss_stats[1][rel_time.hour] += int(item[3])

                hours_no_values[1][rel_time.hour] +=1
            else:
                pass

    for i in range(0,24):
        if(hours_no_values[1][i] != 0):
            mean_bss_stats[1][i] = mean_bss_stats[1][i]/hours_no_values[1][i]    
        else:
            mean_bss_stats[1][i] = 0


    pprint.pprint('mean bss stats {} \n hour_no_values {} \n'.format(mean_bss_stats,hours_no_values))
</code></pre>

<p>The number of value per each hour are as follows for hours starting from <code>0 to 23</code>.</p>

<pre><code>[31, 117, 85, 237, 3, 67, 11, 4, 57, 0, 5, 21, 2, 5, 10, 8, 29, 7, 14, 3, 1, 1, 0, 0]
</code></pre>
","316082","","923794","","2015-11-18 08:10:56","2015-11-18 12:04:42","Calculating the average of a column in csv per hour","<csv><python-3.x><numpy><pandas><average>","1","2","","","","CC BY-SA 3.0","1"
"41834274","1","41837194","","2017-01-24 17:12:15","","1","1200","<p>I have a data frame as below.</p>

<pre><code>ID  Word       Synonyms
------------------------
1   drove      drive
2   office     downtown
3   everyday   daily
4   day        daily
5   work       downtown
</code></pre>

<p>I'm reading a sentence and would like to replace words in that sentence with their synonyms as defined above. Here is my code:</p>

<pre><code>import nltk
import pandas as pd
import string

sdf = pd.read_excel('C:\synonyms.xlsx')
sd = sdf.apply(lambda x: x.astype(str).str.lower())
words = 'i drove to office everyday in my car'

#######

def tokenize(text):
    text = ''.join([ch for ch in text if ch not in string.punctuation])
    tokens = nltk.word_tokenize(text)
    synonym = synonyms(tokens)
    return synonym

def synonyms(words):
    for word in words:
        if(sd[sd['Word'] == word].index.tolist()):
            idx = sd[sd['Word'] == word].index.tolist()
            word = sd.loc[idx]['Synonyms'].item()
        else:
            word
    return word

print(tokenize(words))
</code></pre>

<p>The code above tokenizes the input sentence. I would like to achieve the following output:</p>

<p><strong>In</strong>: <code>i drove to office everyday in my car</code><br>
<strong>Out</strong>: <code>i drive to downtown daily in my car</code></p>

<p>But the output I get is</p>

<p><strong>Out</strong>: <code>car</code></p>

<p>If I skip the <code>synonyms</code> function, then my output has no issues and is split into individual words. I am trying to understand what I'm doing wrong in the <code>synonyms</code> function. Also, please advise if there is a better solution to this problem.</p>
","3988268","","3988268","","2017-01-24 20:01:29","2017-01-24 21:20:23","Replace words by checking from pandas dataframe","<python><python-3.x><pandas><dataframe>","2","5","","","","CC BY-SA 3.0","1"
"56700175","1","","","2019-06-21 09:03:34","","0","1197","<p>I am displaying a pandas df in <strong>Jupyter lab 0.35.5</strong> with,</p>

<pre><code>pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)` 
</code></pre>

<p>but I cannot see the righthand columns, there is no scrollbar (see image). How can I get them to display?</p>

<p>TIA!</p>

<p><a href=""https://i.stack.imgur.com/Z5X1t.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z5X1t.png"" alt=""enter image description here""></a></p>
","1773592","","11607986","","2019-06-21 12:48:53","2019-06-21 12:48:53","Why can't I see all the columns in Jupyter notebook?","<python-3.x><pandas><jupyter-notebook><jupyter-lab>","1","2","","","","CC BY-SA 4.0","1"
"49171273","1","49171333","","2018-03-08 10:49:14","","0","1171","<p>I have a Column 'Date' which has values in the form YYYYMM,
The Column date is of type float.</p>

<p>I wish to convert it in to date type as YYYY-MM.</p>

<p>When I try the below, it gives the error float is not sliceable.</p>

<pre><code>df['Date'] = pd.to_datetime(df['Date'], format='%Y%m').dt.strftime('%Y%m')
</code></pre>

<p>Input Data</p>

<pre><code> Date(Float)
    201101.0
    201812.0
</code></pre>

<p>Output Required</p>

<pre><code>Date(Date Type)
2011-01
2018-12
</code></pre>
","848510","","5276797","","2018-03-08 10:52:25","2018-03-08 10:52:49","Pandas - Float to Date conversion YYYY-MM","<python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"41957703","1","41957914","","2017-01-31 12:32:32","","1","1166","<p>I have a Series containing <code>datetime64[ns]</code> elements called <code>series</code>, and would like to increment the months. I thought the following would work fine, but it doesn't:</p>

<pre><code>    series.dt.month += 1
</code></pre>

<p>The error is </p>

<pre><code>    ValueError: modifications to a property of a datetimelike object are not supported. Change values on the original.
</code></pre>

<p>Is there a simple way to achieve this without needing to redefine things?</p>
","6204900","","","","","2017-01-31 12:42:18","Increment attributes of a datetime Series in pandas","<python><python-3.x><pandas><datetime><python-datetime>","1","0","1","","","CC BY-SA 3.0","1"
"57194543","1","57194607","","2019-07-25 04:52:11","","3","1148","<p>I want to create function which return me the difference between two dates excluding weekends and holidays ?</p>

<p>Eg:- Difference between 01/07/2019 and 08/07/2019 should return me 5 days excluding (weekend on 6/7/107 and 7/07/2019).</p>

<p>What should be best possible way to achieve this ???</p>
","11829611","","11607986","","2019-07-25 05:03:28","2019-07-25 05:14:56","Calculate difference between two dates excluding weekends in python?","<python><python-3.x><pandas><datetime>","1","0","3","","","CC BY-SA 4.0","1"
"57693731","1","57693869","","2019-08-28 13:34:23","","3","1146","<p>I have bag of words as elements in a list format. I am trying to search if each and every single of these words appear in the pandas data frame ONLY if it 'startswith' the element in the list. I have tried 'startswith' and 'contains' to compare. </p>

<p>Code: </p>

<pre><code>import pandas as pd
# list of words to search for
searchwords = ['harry','harry potter','secret garden']

# Data
l1 = [1, 2, 3,4,5]
l2 = ['Harry Potter is a great book',
      'Harry Potter is very famous',
      'I enjoyed reading Harry Potter series',
      'LOTR is also a great book along',
      'Have you read Secret Garden as well?'
]
df = pd.DataFrame({'id':l1,'text':l2})
df['text'] = df['text'].str.lower()

# Preview df:
    id  text
0   1   harry potter is a great book
1   2   harry potter is very famous
2   3   i enjoyed reading harry potter series
3   4   lotr is also a great book along
4   5   have you read secret garden as well?
</code></pre>

<p>Try #1:</p>

<pre><code>When I run this command it picks it up and gives me the results through out the text column. Not what I am looking for. I just used to check if I am doing things right for an example reasons for my understanding.
df[df['text'].str.contains('|'.join(searchwords))]
</code></pre>

<p>Try #2: 
    When I run this command it returns nothing. Why is that? I am doing something wrong? When I search 'harry' as single it works, but not when I pass in the list of elements.  </p>

<pre><code>df[df['text'].str.startswith('harry')] # works with single string.
df[df['text'].str.startswith('|'.join(searchwords))] # returns nothing! 
</code></pre>
","4150078","","9153298","","2019-08-28 13:49:53","2019-08-28 13:49:53","Pandas: search list of keywords in the text column and tag it","<python><python-3.x><pandas>","3","0","","","","CC BY-SA 4.0","1"
"49567723","1","49567763","","2018-03-30 02:44:56","","2","1142","<p>I have two different DataFrame named df1 and df2, with same id column, but some ids have the same count and some ids have the different counts, so I want to get the data for same ids with the different count values, and both DataFrames have the different indexes</p>

<p>following is my df1</p>

<pre><code>    id  valueA
0   255 1141
1   91  1130
2   347 830
3   30  757
4   68  736
5   159 715
6   32  713
7   110 683
8   225 638
9   257 616
</code></pre>

<p>my df2 is </p>

<pre><code>    id  valueB
0   255 1231
1   91  1170
2   5247  954
3   347 870
4   30  757
5   68  736
6   159 734
7   32  713
8   110 683
9   225 644
10  257 616
11  917 585
12  211 575
13  25  530
</code></pre>

<p>how can I do that?</p>
","2298012","","","","","2018-03-30 03:25:55","How can I compare different values with same ids of different dataframe pandas","<python-3.x><pandas>","1","0","1","","","CC BY-SA 3.0","1"
"49173429","1","49173463","","2018-03-08 12:44:06","","1","1127","<p>I had two date columns in the data frame, which was of float type, So I converted it in to date format YYYYMM. Now I have to find the difference of months between
them. I tried the below, but I goves error.</p>

<pre><code>df['Date_1'] = pd.to_datetime(df['Date_1'], format = '%Y%m%d').dt.strftime('%Y%m') #Convert float to YYYYMM Format
df['Date_2'] = pd.to_datetime(df['Date_2'], format='%Y%m.0').dt.strftime('%Y%m') #Convert float to YYYYMM Format
df['diff'] = df['Date_1'] - df['Date_2'] #Gives error
</code></pre>
","848510","","","","","2018-03-08 12:52:12","Month difference YYYYMM Pandas","<python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"40644841","1","","","2016-11-17 00:36:19","","2","1126","<p>I am currently moving some data from a numpy array to a Pandas DataFrame so that I can refer to columns by their name, rather than their index. The problem that I have is that I would like to allow for multiple names to refer to the same column.</p>

<pre><code>data = np.array([[1, 2, 3],
                 [4, 5, 6],
                 [7, 8, 9]])
df = pd.DataFrame(data, columns=('Sensor1231', 'Sensor4221', 'Sensor4673'))
</code></pre>

<p>Sensor4221, for example, is an accelerometer on the 5th level of a structure. I want to add an additional label so (eg. AccLevel5) so that I can refer to the column without having to remember an obscure sensor number.</p>

<p>Therefore, both of the following will provide the same output.</p>

<pre><code>Accel = df['Sensor4221']
</code></pre>

<p>and</p>

<pre><code>Accel = df['AccLevel5']
</code></pre>

<p>both give:</p>

<pre><code>2
5
8
</code></pre>
","7170240","","","","","2016-11-17 00:47:11","Multiple column names in Pandas DataFrame","<python-3.x><pandas><numpy><dataframe>","1","2","","","","CC BY-SA 3.0","1"
"49326591","1","","","2018-03-16 17:38:13","","0","1123","<p>My question is similar to <a href=""https://stackoverflow.com/questions/12329853/how-to-rearrange-pandas-column-sequence"">this question</a>. In answers of that question i saw few ways to do what i am trying to do below. But answers are old and i feel that there might be a fast and an efficient way to do what i am looking for. </p>

<pre><code>import pandas as pd

df =pd.DataFrame({'a':[1,2,3,4],'b':[2,4,6,8]})
df['x']=df.a + df.b
df['y']=df.a - df.b
df
   a  b   x  y
0  1  2   3 -1
1  2  4   6 -2
2  3  6   9 -3
3  4  8  12 -4
</code></pre>

<p>Now I want to move x and y columns to beginning. I can always do below. </p>

<pre><code>df = df[['x','y','a','b']]
df
    x  y  a  b
0   3 -1  1  2
1   6 -2  2  4
2   9 -3  3  6
3  12 -4  4  8
</code></pre>

<p>But I don't want that solution. I want an efficient solution that would move columns x and y to beginning without mentioning names of other columns as my dataframe is going to change and i might not know names of all the columns</p>
","9237237","","","","","2018-03-16 18:45:47","python pandas move few columns to beginning without mentioning other columns","<python-3.x><pandas><multiple-columns>","2","0","","","","CC BY-SA 3.0","1"
"49171911","1","49171951","","2018-03-08 11:22:47","","2","1080","<p>This is my pandas dataframe</p>

<pre><code>                     time    energy
0     2018-01-01 00:15:00    0.0000
1     2018-01-01 00:30:00    0.0000
2     2018-01-01 00:45:00    0.0000
3     2018-01-01 01:00:00    0.0000
4     2018-01-01 01:15:00    0.0000
5     2018-01-01 01:30:00    0.0000
6     2018-01-01 01:45:00    0.0000
7     2018-01-01 02:00:00    0.0000
8     2018-01-01 02:15:00    0.0000
9     2018-01-01 02:30:00    0.0000
10    2018-01-01 02:45:00    0.0000
11    2018-01-01 03:00:00    0.0000
12    2018-01-01 03:15:00    0.0000
13    2018-01-01 03:30:00    0.0000
14    2018-01-01 03:45:00    0.0000
15    2018-01-01 04:00:00    0.0000
16    2018-01-01 04:15:00    0.0000
17    2018-01-01 04:30:00    0.0000
18    2018-01-01 04:45:00    0.0000
19    2018-01-01 05:00:00    0.0000
20    2018-01-01 05:15:00    0.0000
21    2018-01-01 05:30:00    0.9392
22    2018-01-01 05:45:00    2.8788
23    2018-01-01 06:00:00    5.5768
24    2018-01-01 06:15:00    8.6660
25    2018-01-01 06:30:00   15.8648
26    2018-01-01 06:45:00   24.1760
27    2018-01-01 07:00:00   38.5324
28    2018-01-01 07:15:00   49.9292
29    2018-01-01 07:30:00   64.3788
</code></pre>

<p>I would like to select the values from <strong>energy column</strong> using a specific Time range <strong>01:15:00 - 05:30:00</strong> and sum those values. To select datas from column I need both hour and minute values. I know how to select data from column using hour and minute separately..</p>

<pre><code>import panadas as pd
from datetime import datetime as dt
energy_data = pd.read_csv(""/home/mayukh/Downloads/Northam_january2018/output1.csv"", index_col=None)
#Using Hour 
sum = energy_data[((energy_data.time.dt.hour &lt; 1) &amp; (energy_data.time.dt.hour &gt;= 5))]['energy'].sum()
#using Minute
sum = energy_data[((energy_data.time.dt.minute &lt; 15) &amp; (energy_data.time.dt.minute &gt;= 30))]['energy'].sum()
</code></pre>

<p>but I don't know how to use both hour and minute together to select data. Please tell me the way how can I will proceed.</p>
","7725857","","7758804","","2019-10-29 17:58:43","2019-10-29 17:58:43","How to select column for a specific time range from pandas dataframe in python3?","<python><python-3.x><pandas><python-datetime>","2","0","","","","CC BY-SA 4.0","1"
"41241572","1","41241815","","2016-12-20 11:43:56","","1","1063","<p>The goal is to create a new column from df with a 1 if the value from column '% Renewable' is at or above the median, and a 0 if the value is below the median.</p>

<pre><code>df = pd.DataFrame({'% Renewable': [np.NaN, 12, np.NaN, 11, 17, 62, 18, 15, np.NaN, 2, np.NaN, np.NaN, 6, np.NaN, 70]},
index=['China', 'United States', 'Japan', 'United Kingdom', 'Russian Federation', 'Canada', 'Germany', 'India', 'France', 'South Korea', 'Italy', 'Spain', 'Iran', 'Australia', 'Brazil'])
</code></pre>

<p>I got the median:</p>

<pre><code>median = df['% Renewable'].median()
</code></pre>

<p>But now what? Should I use get_dummies function? Or cut perhaps?</p>
","5878647","","","","","2016-12-20 11:57:04","Python Pandas: Generate dummy variable from numeric variable according to a threshold","<python-3.x><pandas><dummy-variable>","1","0","","","","CC BY-SA 3.0","1"
"57023470","1","57023572","","2019-07-13 23:17:20","","1","1053","<p>In this question, my goal is to preserve the <strong>last</strong> <strong>trailing</strong> <code>zeros</code> when exporting the <code>DataFrame</code> to <code>CSV</code></p>

<p>My <code>dataset</code> looks like this:</p>

<pre><code>EST_TIME    Open    High
2017-01-01  1.0482  1.1200    
2017-01-02  1.0483  1.1230
2017-01-03  1.0485  1.0521
2017-01-04  1.0480  1.6483
2017-01-05  1.0480  1.7401
...., ...., ....
2017-12-31  1.0486  1.8480
</code></pre>

<p>I import and create a DataFrame and save to CSV by doing this:</p>

<pre><code>df_file = '2017.csv'
df.to_csv(df_file, index=False)
files.download(df_file)
</code></pre>

<p>When I view the CSV, I see this:</p>

<pre><code>EST_TIME    Open    High
2017-01-01  1.0482  1.12   
2017-01-02  1.0483  1.123
2017-01-03  1.0485  1.0521
2017-01-04  1.048   1.6483
2017-01-05  1.048   1.7401
...., ...., ....
2017-12-31  1.0486  1.848
</code></pre>

<p>All the zeros at the end are gone. I want to preserve the trailing zeros when I save the CSV and I want it at 4 decimal place.</p>

<p>Could you please let me know how can I achieve this?</p>
","9161607","","","","","2019-07-14 02:20:48","Pandas how to keep the LAST trailing zeros when exporting DataFrame into CSV","<python><python-3.x><pandas><dataframe><export-to-csv>","2","1","1","","","CC BY-SA 4.0","1"
"56976680","1","56977060","","2019-07-10 18:31:21","","2","1045","<p>I'm working on an anomaly detection task using KMeans. <br>
Pandas dataframe that i'm using has a single feature and it is like the following one:</p>

<pre><code>df = array([[12534.],
           [12014.],
           [12158.],
           [11935.],
           ...,
           [ 5120.],
           [ 4828.],
           [ 4443.]])
</code></pre>

<p>I'm able to fit and to predict values with the following instructions:</p>

<pre class=""lang-py prettyprint-override""><code>km = KMeans(n_clusters=2)
km.fit(df)
km.predict(df)
</code></pre>

<p>In order to identify anomalies, I would like to calculate the distance between centroid and each single point, but with a dataframe with a single feature i'm not sure that it is the correct approach.</p>

<p>I found examples which used euclidean distance to calculate the distance. An example is the following one:</p>

<pre class=""lang-py prettyprint-override""><code>def k_mean_distance(data, cx, cy, i_centroid, cluster_labels):
    distances = [np.sqrt((x - cx) ** 2 + (y - cy) ** 2) for (x, y) in data[cluster_labels == i_centroid]]
    return distances

centroids = self.km.cluster_centers_
distances = []
for i, (cx, cy) in enumerate(centroids):
    mean_distance = k_mean_distance(day_df, cx, cy, i, clusters)
    distances.append({'x': cx, 'y': cy, 'distance': mean_distance})
</code></pre>

<p>This code doesn't work for me because centroids are like the following one in my case, since i have a single feature dataframe:</p>

<pre><code>array([[11899.90692187],
       [ 5406.54143126]])
</code></pre>

<p>In this case, what is the correct approach to find the distance between centroid and points? Is it possible?</p>

<p>Thank you and sorry for the trivial question, i'm still learning</p>
","4614675","","","","","2019-07-10 19:00:17","Find distance between centroid and points in a single feature dataframe - KMeans","<python><python-3.x><pandas><machine-learning><k-means>","2","0","","","","CC BY-SA 4.0","1"
"56978974","1","","","2019-07-10 21:35:52","","3","1028","<p>I've got a pandas dataframe like this:</p>

<pre><code>     id  foo  
 0   A   col1 
 1   A   col2  
 2   B   col1  
 3   B   col3  
 4   D   col4  
 5   C   col2  
</code></pre>

<p>I'd like to create four additional columns based on unique values in <code>foo</code> column. <code>col1</code>,<code>col2</code>, <code>col3</code>, <code>col4</code></p>

<pre><code>     id  foo   col1 col2 col3 col4
 0   A   col1   75   20   5    0
 1   A   col2   20   80   0    0
 2   B   col1   82   10   8    0
 3   B   col3   5    4   80   11
 4   D   col4   0    5   10   85
 5   C   col2   12   78   5    5
</code></pre>

<p>The logic for creating the columns is as follows:</p>

<p>if <code>foo</code> = <code>col1</code> then <code>col1</code> contains a random number between <code>75-100</code> and the other columns (<code>col2</code>, <code>col3</code>, <code>col4</code>) contains random numbers, such that the total for each row is <code>100</code></p>

<p>I can manually create a new column and assign a random number, but I'm unsure how to include the logic of sum for each row of 100.</p>

<p>Appreciate any help!</p>
","5236124","","","","","2019-07-10 22:39:26","pandas fill column with random numbers with a total for each row","<python-3.x><pandas>","4","0","","","","CC BY-SA 4.0","1"
"49376077","1","49376307","","2018-03-20 03:52:57","","3","1017","<p>New to Python here.  I hope my question isn't entirely redundant - if it is, let me know and chalk it up to my inexperience with StackOverflow.  </p>

<p>In any case, I'm working with the Titanic dataset from kaggle.com, and I'm looking to use a set of conditional statements to replace NaN 'values' throughout the Age column of the dataframe.  Ultimately, I'd like to generate results based on the following conditions: 
1) if age==NaN, and Title==(X or Y or Z), generate a random number in the 0-18 range
2) if age==NaN, and Title==(A or B or C), generate a random number in the 19-80 range</p>

<p>Note: 'Title' is a column with the title of individual listed (i.e. Mr., Mrs., Lord, etc.)</p>

<p>I found a similar situation <a href=""https://stackoverflow.com/questions/48476827/how-to-replace-nan-values-in-pandas-conditionally?answertab=active#tab-top"">here</a>, but I haven't been able to adapt it to my case as it doesn't approach conditionality at all.  </p>

<p>Here is my most recent attempt (per. the replies as this update)  </p>

<p><strong>Attempt 1</strong></p>

<pre><code>import random

mask_young = (df.Age.isnull()) &amp; (df.Title.isin(Title_Young)) 
df.loc[mask_young, 'Age'] = df.loc[mask_young, 'Age'].apply(lambda x: np.random.randint(0,18))

mask_old = (df.Age.isnull()) &amp; (df.Title.isin(Title_Old)) 
df.loc[mask_old, 'Age'] = df.loc[mask_old, 'Age'].apply(lambda x: np.random.randint(18,65))

mask_all = (df.Age.isnull()) &amp; (df.Title.isin(Title_All)) 
df.loc[mask_all, 'Age'] = df.loc[mask_all, 'Age'].apply(lambda x: np.random.randint(0,65))
</code></pre>

<p><em>Result is no error, but no correction to NaN values in 'Age' column</em></p>
","9297580","","9297580","","2018-03-21 02:27:29","2018-03-21 02:27:29","Conditional replacement of NaN","<python><python-3.x><pandas><dataframe><conditional>","1","2","","","","CC BY-SA 3.0","1"
"41724947","1","","","2017-01-18 16:47:01","","1","1008","<p>I keep running into this problem when trying to take a slice of a pandas DataFrame.  It has four columns as shown below and we'll call it <code>all_data</code>:</p>

<p><a href=""https://i.stack.imgur.com/wdvuQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wdvuQ.png"" alt=""enter image description here""></a></p>

<p>I can plot it nicely with the code below:</p>

<pre><code>generations = ['Boomers+', 'Gen X', 'Millennials', 'Students', 'Gen Z']

fig = plt.figure(figsize = (15,15))

for i,generations in enumerate(generations):
    ax = plt.subplot(2,3,i+1)
    ix = all_data['CustomerGroups'] == generations

    kmf.fit(T[ix], C[ix], label=generations)


    kmf.plot(ax=ax, legend=False)
    plt.title(generations, size = 16)
    plt.xlabel('Timeline in years')
    plt.xlim(0,5)
    if  i ==0:
        plt.ylabel('Frac Remaining After $n$ Yrs')
    if  i ==3:
        plt.ylabel('Frac Remaining After $n$ Yrs')

fig.tight_layout()
#fig.suptitle('Survivability of Checking Accts By Generation', size = 16)
fig.subplots_adjust(top=0.88, hspace = .4)
plt.show()
</code></pre>

<p>However, I would like to do a couple of things that seem similar.  The <code>CustomerGroups</code> column has <code>NaN</code> in it which is the reason for the manual array of <code>generations</code>.</p>

<p>It seems like every operation to slice the dataframe and remove the NaN gives me an error <code>Unalignable boolean Series key provided</code> when I attempt to plot is using the same code and just changing the dataframe.</p>

<p>Likewise, the <code>Channel</code> column can be either <code>Online</code> or <code>Branch</code>.  Again, any way I try to split the <code>all_data</code> either by the <code>Channel</code> column to create new dataframes from either <code>Online</code> or <code>Branch</code> criteria, I get the same error regarding a boolean index.</p>

<p>I have tried many options from other posts here <code>(reset_index, pd.notnull,)</code> etc. , but it keeps creating the index problem when I go to plot it with the same code.</p>

<p>What is the best method to create subsets from the <code>all_data</code> that does not created the <code>Unalignable boolean Series key</code> error?</p>

<p>This worked in the past:</p>

<pre><code>#create the slice using the .copy
online = checkingRed[['Channel', 'State', 'CustYears', 'Observed',
'CustomerGroups', 'ProductYears']].copy()

#remove the branch from the data set
online = online.loc[online['Channel'] != 'Branch'] #use .loc for cleaner slice

#reset the index so that it is unique
online['index'] = np.arange(len(online))
online = online.set_index('index')
</code></pre>
","4950753","","4950753","","2017-01-18 18:28:41","2017-01-18 18:28:41","Unalignable boolean Series key provided","<python-3.x><pandas><indexing><slice>","0","6","0","","","CC BY-SA 3.0","1"
"33115107","1","","","2015-10-14 01:17:53","","1","1005","<p>I am trying to randomly select a certain percentage of rows and columns in my dataframe and fit these features into a logistic regression over 10 iterations.  My dependent variable is whether a team won (1) or lost (0).</p>

<p>If I have a df that looks something like this (data is made up):</p>

<pre><code> Won    Field    Injuries   Weather    Fouls    Players
  1       2         3         1          2         8
  0       3         2         0          1         5
  1       4         5         3          2         6
  1       3         2         1          4         5 
  0       2         3         0          1         6
  1       4         2         0          2         8
  ...
</code></pre>

<p>For example, let's say I want to select 50% (but this could change). I want to randomly select 50% (or the closest amount to 50% if its an odd number) of the columns (field,injuries,weather,fouls,players) and 50% of the rows in those columns to place in my model.</p>

<p>Here is my current code which right now runs by selecting <strong>all</strong> of the columns and rows and fitting it into my model but I would like to dictate a random percentage:</p>

<pre><code>z = []
For i in range(10):
    train_cols = df.columns[1:] 
    logit = sm.Logit(df['Won'], df[train_cols])
    result = logit.fit()
    exp = np.exp(result.params)
    z.append([i, exp])
</code></pre>
","3682157","","3682157","","2015-10-14 01:23:36","2015-10-14 23:50:14","Randomly select a percentage of columns and rows in a dataframe (Pandas, Python 3)","<python-3.x><pandas><scikit-learn>","0","4","1","","","CC BY-SA 3.0","1"
"49826378","1","49826958","","2018-04-13 23:30:25","","2","986","<p>I have two tables in pandas:</p>

<p>df1: Containing User IDs and IP_Addresses for 150K users.</p>

<pre><code>|---------------|---------------|  
|    User_ID    |   IP_Address  |
|---------------|---------------|  
|      U1       |   732758368.8 |
|      U2       |   350311387.9 |
|      U3       |   2621473820  |
|---------------|---------------|
</code></pre>

<p>df2: Containing IP Address range and country it belongs to, 139K records</p>

<pre><code>|---------------|-----------------|------------------|  
|    Country    | Lower_Bound_IP  |  Upper_Bound_IP  |
|---------------|-----------------|------------------|  
|   Australia   |   1023787008    |    1023791103    |
|   USA         |   3638734848    |    3638738943    |
|   Australia   |   3224798976    |    3224799231    |
|   Poland      |   1539721728    |    1539721983    |
|---------------|-----------------|------------------|
</code></pre>

<p>My objective is to create a country column in df1 such that IP_Address of df1 lies between the range of Lower_Bound_IP and Upper_Bound_IP of that country in df2.</p>

<pre><code>|---------------|---------------|---------------|   
|    User_ID    |   IP_Address  |    Country    |
|---------------|---------------|---------------|   
|      U1       |   732758368.8 |   Indonesia   |
|      U2       |   350311387.9 |   Australia   |
|      U3       |   2621473820  |   Albania     |
|---------------|---------------|---------------|
</code></pre>

<p>My first approach was to do a cross join (cartesian product) of the two tables and then filter to the relevant records. However, a cross join using pandas.merge() is not feasible, since it will create 21 billion records. The code crashes every time. Could you please suggest an alternative solution which is feasible?</p>
","5498704","","5241032","","2018-04-14 00:28:01","2019-08-06 12:11:07","Joining Two Tables in Python Based on Condition","<python><python-3.x><pandas><join><merge>","1","2","1","2018-04-14 01:19:20","","CC BY-SA 3.0","1"
"41728800","1","45685074","","2017-01-18 20:24:16","","0","970","<p>What I am trying to do is to get bootstrap confidence limits by row regardless of the number of rows and make a new dataframe from the output.I currently can do this for the entire dataframe, but not by row. The data I have in my actual program looks similar to what I have below:</p>

<pre><code>    0   1   2
0   1   2   3
1   4   1   4
2   1   2   3
3   4   1   4
</code></pre>

<p>I want the new dataframe to look something like this with the lower and upper confidence limits:</p>

<pre><code>    0   1   
0   1   2   
1   1   5.5 
2   1   4.5 
3   1   4.2 
</code></pre>

<p>The current generated output looks like this:</p>

<pre><code>     0   1
 0  2.0 2.75
</code></pre>

<p>The python 3 code below generates a mock dataframe and generates the bootstrap confidence limits for the entire dataframe. The result is a new dataframe with just 2 values, a upper and a lower confidence limit rather than 4 sets of 2(one for each row). </p>

<pre><code>import pandas as pd
import numpy as np
import scikits.bootstrap as sci

zz = pd.DataFrame([[[1,2],[2,3],[3,6]],[[4,2],[1,4],[4,6]],
               [[1,2],[2,3],[3,6]],[[4,2],[1,4],[4,6]]])
print(zz)

x= zz.dtypes
print(x)

a = pd.DataFrame(np.array(zz.values.tolist())[:, :, 0],zz.index, zz.columns)
print(a)
b = sci.ci(a)
b = pd.DataFrame(b)
b = b.T
print(b)
</code></pre>

<p>Thank you for any help.</p>
","5950072","","","","","2017-08-15 00:56:51","Python Pandas: bootstrap confidence limits by row rather than entire dataframe","<python-3.x><pandas><statistics-bootstrap><scikits>","2","0","","","","CC BY-SA 3.0","1"
"49627084","1","","","2018-04-03 09:51:10","","1","958","<p>I'm trying to read tweets which are stored as json files. I'm using pandas to load the data. But found some strange behaviour in the <code>read_json</code> function. It I'm providing an <a href=""https://stackoverflow.com/help/mcve"">mcve</a> below:</p>

<pre><code>json_content=""""""
{ 
    ""1"": {
        ""tid"": ""9999999999999998"", 
    }, 
    ""2"": {
        ""tid"": ""9999999999999999"", 
    },
    ""3"": {
        ""tid"": ""10000000000000001"", 
    },
    ""4"": {
        ""tid"": ""10000000000000002"", 
    }
}
""""""
df=pd.read_json(json_content,
                orient='index', # read as transposed
                convert_axes=False, # don't convert keys to dates
        )
print(df.info())
print(df)
</code></pre>

<p>Which outputs the following on my PC:</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Index: 4 entries, 1 to 4
Data columns (total 1 columns):
tid    4 non-null int64
dtypes: int64(1)
memory usage: 64.0+ bytes
None
                 tid
1   9999999999999998
2  10000000000000000
3  10000000000000000
4  10000000000000002
</code></pre>

<blockquote>
  <p>Which is not storing the correct values for <code>tid</code> column, why is this
  happening?</p>
</blockquote>

<p><strong>Note:</strong> There shouldn't be <a href=""https://github.com/pandas-dev/pandas/issues/18842"" rel=""nofollow noreferrer"">an overflow case</a>. The <code>tid</code> column is being stored as int64, which has a limit about 10 times higher than the tid I originally tested with(see below):</p>

<pre><code>import sys
# original problem 
tid_0 = 956677215197970432 
print(sys.maxsize,tid_0,sys.maxsize/tid_0)    # &lt; 1 if overflow possible
# minimal case
tid = 10000000000000001 
print(sys.maxsize,tid,sys.maxsize/tid)    # &lt; 1 if overflow possible

#Output
9223372036854775807 956677215197970432 9
9223372036854775807 10000000000000001 922
</code></pre>

<p><strong>Update</strong> : </p>

<blockquote>
  <p>It is reading correctly on explictly specifying the argument
  <code>dtype=int</code>, but I don't understand why. What changes when we specify
  the dtype?</p>
</blockquote>
","6242649","","6242649","","2018-04-04 07:56:55","2018-04-04 07:56:55","pandas read_json reads large integers as strings incorrectly","<python><json><python-3.x><pandas>","1","1","","","","CC BY-SA 3.0","1"
"49284197","1","","","2018-03-14 17:28:04","","0","951","<p>I'm trying to find the values of p,d,q and the seasonal values of P,D,Q using statsmodels as ""sm"" in python.</p>

<p>The data set i'm using is a csv file that contains time series data over three years recording the energy consumption. The file was split into a smaller data frame in order to work with it. Here is what df_test.head() looks like.</p>

<pre><code>                      time  total_consumption
122400 2015-05-01 00:01:00            106.391
122401 2015-05-01 00:11:00            120.371
122402 2015-05-01 00:21:00            109.292
122403 2015-05-01 00:31:00             99.838
122404 2015-05-01 00:41:00             97.387
</code></pre>

<p>Here is my code so far.</p>

<pre><code>#Importing the timeserie data set from local file
df = pd.read_csv(r""C:\Users\path\Name of the file.csv"")

#Rename the columns, put time as index and assign datetime to the column time
df.columns = [""time"",""total_consumption""]
df['time'] = pd.to_datetime(df.time)
df.set_index('time')

#Select test df (there is data from the 2015-05-01 2015-06-01)
df_test = df.loc[(df['time'] &gt;= '2015-05-01') &amp; (df['time'] &lt;= '2015-05-14')]

#Find minimal AIC value for the ARIMA model integers
p = range(0,2)
d = range(0,2)
q = range(0,2)

pdq = list(itertools.product(p,d,q))

seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p,d,q))]

warnings.filterwarnings(""ignore"")

for param in pdq:
    for param_seasonal in seasonal_pdq:
        try:
            mod = sm.tsa.statespace.SARIMAX(df_test,
                                            order=param,
                                            seasonal_order=param_seasonal,
                                            enforce_stationarity=False,
                                            enforce_invertibility=False)

            results = mod.fit()
            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))

        except:
            continue
</code></pre>

<p>When I try to run the code as it is, the program doesn't even acknowledge the ""for"" loop. But when I take out the</p>

<pre><code>try:
except:
    continue
</code></pre>

<p>the program gives me this error message</p>

<pre><code>ValueError: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).
</code></pre>

<p>How could I remedy that and is there a way to automate the process directly output the parameters with the lowest AIC value (without having to look for it throught all the possibilities).</p>

<p>Thanks !</p>
","9423193","","9423193","","2018-03-15 13:23:04","2018-03-15 13:23:04","Find SARIMAX AIC and pdq values, statsmodels","<python-3.x><pandas><time-series><statsmodels>","0","7","","","","CC BY-SA 3.0","1"
"49042340","1","49053162","","2018-03-01 03:50:30","","2","925","<p>Prior to attempting the fit I have thoroughly cleaned my data frame and ensured that the entire data frame has no inf or NaN values and is composed of entirely non-null float64 values.  However, I still redundantly verified this using np.isinf(), df.isnull().sum() and df.info() methods.  All my research showed that others with the same issue had NaN, inf, or object data type in their data frame.  This is not so in my case. Lastly, I found a vaguely similar <a href=""https://stackoverflow.com/questions/43335489/error-in-fit-transform-input-contains-nan-infinity-or-a-value-too-large-for-dt"">case</a> which found a resolution using this code: </p>

<pre><code>df = df.apply(lambda x: pd.to_numeric(x,errors='ignore'))
</code></pre>

<p>This did not help in my situation.  How can I resolve this ValueError exception?</p>

<pre><code>from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
import pandas as pd
import numpy as np

# Read csv file and assign column names
headers=['symboling','normalized_losses','make','fuel_type','aspiration','num_of_doors',
         'body_style','drive_wheels','engine_location','wheel_base','length','width',
        'height','curb_weight','engine_type','num_of_cylinders','engine_size','fuel_system',
        'bore','stroke','compression_ratio','horsepower','peak_rpm','city_mpg','highway_mpg',
        'price']
cars = pd.read_csv('imports-85.data.txt', names=headers)

# Select only the columns with continuous values from - https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.names
continuous_values_cols = ['normalized_losses', 'wheel_base', 'length', 'width', 'height', 'curb_weight', 
                          'bore', 'stroke', 'compression_ratio', 'horsepower', 'peak_rpm', 'city_mpg', 'highway_mpg', 'price']
numeric_cars = cars[continuous_values_cols].copy()

# Clean Data Set by Convert missing values (?) with np.NaN then set the type to float
numeric_cars.replace(to_replace='?', value=np.nan, inplace=True)
numeric_cars = numeric_cars.astype('float')

# Because the column we're trying to predict is 'price', any row were price is NaN will be removed.""
numeric_cars.dropna(subset=['price'], inplace=True)

# All remaining NaN's will be filled with the mean of its respective column
numeric_cars = numeric_cars.fillna(numeric_cars.mean())

# Create training feature list and k value list
test_features = numeric_cars.columns.tolist()
predictive_feature = 'price'
test_features.remove(predictive_feature)
k_values = [x for x in range(10) if x/2 != round(x/2)]

# Normalize columns
numeric_cars_normalized = numeric_cars[test_features].copy()
numeric_cars_normalized = numeric_cars_normalized/ numeric_cars.max()
numeric_cars_normalized[predictive_feature] = numeric_cars[predictive_feature].copy()


def knn_train_test(df, train_columns, predict_feature, k_value):

    # Randomly resorts the DataFrame to mitigate sampling bias
    np.random.seed(1)
    df = df.loc[np.random.permutation(len(df))]

    # Split the DataFrame into ~75% train / 25% test data sets
    split_integer = round(len(df) * 0.75)
    train_df = df.iloc[0:split_integer]
    test_df = df.iloc[split_integer:]

    train_features = train_df[train_columns]
    train_target = train_df[predict_feature]

    # Trains the model
    knn = KNeighborsRegressor(n_neighbors=k_value)
    knn.fit(train_features, train_target)

    # Test the model &amp; return calculate mean square error
    predictions = knn.predict(test_df[train_columns])
    print(""predictions"")
    mse = mean_squared_error(y_true=test_df[predict_feature], y_pred=predictions)
    return mse


# instantiate mse dict
mse_dict = {}

# test each feature and do so with a range of k values
# in an effot to determine the optimal training feature and k value
for feature in test_features:

    mse = [knn_train_test(numeric_cars_normalized,feature, predictive_feature, k) for k in k_values]
    mse_dict[feature] = mse

print(mse_dict)
</code></pre>

<p>Here's the full error trace back:</p>

<pre><code>C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
  DeprecationWarning)
Traceback (most recent call last):
  File ""C:\DATAQUEST\06_MachineLearning\01_ML_Fundamentals\06_GuidedProject_PredictingCarPrices\PredictingCarPrices.py"", line 76, in &lt;module&gt;
    mse = [knn_train_test(numeric_cars_normalized,feature, predictive_feature, k) for k in k_values]
  File ""C:\DATAQUEST\06_MachineLearning\01_ML_Fundamentals\06_GuidedProject_PredictingCarPrices\PredictingCarPrices.py"", line 76, in &lt;listcomp&gt;
    mse = [knn_train_test(numeric_cars_normalized,feature, predictive_feature, k) for k in k_values]
  File ""C:\DATAQUEST\06_MachineLearning\01_ML_Fundamentals\06_GuidedProject_PredictingCarPrices\PredictingCarPrices.py"", line 60, in knn_train_test
    knn.fit(train_features, train_target)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\neighbors\base.py"", line 741, in fit
    X, y = check_X_y(X, y, ""csr"", multi_output=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py"", line 521, in check_X_y
    ensure_min_features, warn_on_dtype, estimator)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py"", line 407, in check_array
    _assert_all_finite(array)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\sklearn\utils\validation.py"", line 58, in _assert_all_finite
    "" or a value too large for %r."" % X.dtype)
ValueError: Input contains NaN, infinity or a value too large for dtype('float64').
</code></pre>

<p>Here's the code and output I used to verify that there are no NaN or inf values in my DataFrame:</p>

<pre><code># Verify data for NaN and inf
print(len(numeric_cars_normalized))
# 201

print(numeric_cars_normalized.info())
# &lt;class 'pandas.core.frame.DataFrame'&gt;
# Int64Index: 201 entries, 0 to 204
# Data columns (total 14 columns):
# bore                 201 non-null float64
# city_mpg             201 non-null float64
# compression_ratio    201 non-null float64
# curb_weight          201 non-null float64
# height               201 non-null float64
# highway_mpg          201 non-null float64
# horsepower           201 non-null float64
# length               201 non-null float64
# normalized_losses    201 non-null float64
# peak_rpm             201 non-null float64
# price                201 non-null float64
# stroke               201 non-null float64
# wheel_base           201 non-null float64
# width                201 non-null float64
# dtypes: float64(14)
# memory usage: 23.6 KB
# None

print(numeric_cars_normalized.isnull().sum())
# bore                 0
# city_mpg             0
# compression_ratio    0
# curb_weight          0
# height               0
# highway_mpg          0
# horsepower           0
# length               0
# normalized_losses    0
# peak_rpm             0
# price                0
# stroke               0
# wheel_base           0
# width                0
# dtype: int64

# The loop below, essentially does the same as the above
# verification, but using different methods
# the purpose is to prove there's no nan or inf in my data set
index = []
NaN_counter = []
inf_counter = []
for col in numeric_cars_normalized.columns:
    index.append(col)
    # inf counter
    col_isinf = np.isinf(numeric_cars_normalized[col])
    if col_isinf.value_counts().index[0] == False:
        inf_counter.append(col_isinf.value_counts()[0])

    # nan counter    
    col_isnan = np.isnan(numeric_cars_normalized[col])
    if col_isnan.value_counts().index[0] == False:
        NaN_counter.append(col_isnan.value_counts()[0])

data_check = {'NOT_NaN_count': NaN_counter, 'NOT_inf_count': inf_counter}
data_verification = pd.DataFrame(data=data_check, index=index)
print(data_verification)

#                    NOT_NaN_count  NOT_inf_count
# bore                         201            201
# city_mpg                     201            201
# compression_ratio            201            201
# curb_weight                  201            201
# height                       201            201
# highway_mpg                  201            201
# horsepower                   201            201
# length                       201            201
# normalized_losses            201            201
# peak_rpm                     201            201
# price                        201            201
# stroke                       201            201
# wheel_base                   201            201
# width                        201            201
</code></pre>

<p>I may have found the problem, but still not sure how to fix it.</p>

<pre><code># Here's a another methodology for extra redudnant data checking
index = []
NaN_counter = []
inf_counter = []

for col in numeric_cars_normalized.columns:
    index.append(col)
    inf_counter.append(np.any(np.isfinite(numeric_cars_normalized[col])))
    NaN_counter.append(np.any(np.isnan(numeric_cars_normalized[col])))

data_check = {'Any_NaN': NaN_counter, 'Any_inf': inf_counter}
data_verification = pd.DataFrame(data=data_check, index=index)
print(data_verification)

                   Any_NaN  Any_inf
# bore                 False     True
# city_mpg             False     True
# compression_ratio    False     True
# curb_weight          False     True
# height               False     True
# highway_mpg          False     True
# horsepower           False     True
# length               False     True
# normalized_losses    False     True
# peak_rpm             False     True
# price                False     True
# stroke               False     True
# wheel_base           False     True
# width                False     True
</code></pre>

<p>So clearly I have inf in my DataSet, but I'm not sure why or how to fix it.</p>
","8328781","","8328781","","2018-03-01 14:19:17","2018-03-01 15:39:29","ValueError: Input contains NaN, infinity or a value too large for dtype('float64') using fit from KNeighborsRegressor","<python><python-3.x><pandas><numpy><scikit-learn>","1","2","","","","CC BY-SA 3.0","1"
"40663510","1","40663593","","2016-11-17 19:32:41","","1","917","<p>I am running Python 3.5.2 and Pandas 0.19.1. I use <code>read_fwf()</code> to read in a large data file that was originally formatted in FORTRAN. It has columns that look like this: </p>

<pre><code>SiC4+  e-    C2     c-SiC2     1.500e-07 -5.000e-01  0.000e+00 2.00e+00 0.00e+00 logn  8     10    280  3   746 1  1
SiC4+  e-    C      l-SiC3     1.500e-07 -5.000e-01  0.000e+00 2.00e+00 0.00e+00 logn  8     10    280  3   747 1  1
O      e-    O-                1.500e-15  0.000e+00  0.000e+00 2.00e+00 0.00e+00 logn  8     10    280  3   744 1  1
S      e-    S-                5.000e-15  0.000e+00  0.000e+00 2.00e+00 0.00e+00 logn  8     10    280  3   745 1  1
</code></pre>

<p>To read this in, I'm using this code: </p>

<pre><code>convert = lambda x: int(species[x]) if x!='' else None
reactions = pd.read_fwf('data.dat',sep='\s+',converters{0:convert,1:convert,2:convert,3:convert})
reactions.fillna(0,inplace=True)
</code></pre>

<p>The converters take the first 4 columns' chemical names and replace them with index numbers (from another file), and any missing data is replaced with index number zero. This works fine. </p>

<p>What doesn't work is the 6th column and the 15th column.   </p>

<pre><code>116      76        7       30    1.500000e-07   0.5    0.0    2.0  0.0  logn   8   10  280     3  46  1  1 
116      76        1       41    1.500000e-07   0.5    0.0    2.0  0.0  logn   8   10  280     3  47  1  1  
  4      76       74        0    1.500000e-15   0.0    0.0    2.0  0.0  logn   8   10  280     3  44  1  1 
  5      76       75        0    5.000000e-15   0.0    0.0    2.0  0.0  logn   8   10  280     3  45  1  1   
</code></pre>

<p>What is going on here? The 6th column loses it's negative sign, and the 15th column is missing its leading '7'. I can't find a reason for why this is happening, and it doesn't make sense. Other columns in the file that have leading negative signs are left untouched. </p>

<p><strong>Update</strong></p>

<p>The solution below is not incorrect, but for it to work for me required a very important change to the file header. The first 7 columns of my file looks like this (with headers): </p>

<pre><code>Input1    Input2   Output1    Output2    alpha      beta       gamma     
NC3       CRP      C2         CN         2.000e+03  0.000e+00  0.000e+00
C2N2      CRP      CN         CN         2.000e+03  0.000e+00  0.000e+00 
NC7       CRP      C6         CN         2.000e+03 -1.000e+00  0.000e+00
</code></pre>

<p><code>read_fwf()</code> read in the headers and the spaces in between, and must have presumed that the column marked beta was spaced 2 characters away from the end of the column marked alpha, completely ignoring the negative sign on some of the values in beta. </p>

<p>I changed the header position for all columns that this could be a problem for, and the problem was fixed.  </p>

<pre><code>Input1    Input2   Output1    Output2    alpha     beta       gamma     
NC3       CRP      C2         CN         2.000e+03  0.000e+00  0.000e+00
C2N2      CRP      CN         CN         2.000e+03  0.000e+00  0.000e+00 
NC7       CRP      C6         CN         2.000e+03 -1.000e+00  0.000e+00
</code></pre>

<p>Notice that the file header for beta (and gamma) are pulled one space to the left. This starts the column early enough for <code>read_fwf()</code> to include the negative sign. </p>
","5445694","","5445694","","2016-11-18 16:33:26","2016-11-18 16:56:33","Pandas read_fwf ignoring values","<python-3.x><pandas>","1","2","","","","CC BY-SA 3.0","1"
"48903312","1","48903665","","2018-02-21 10:15:49","","0","910","<p>I have the following table of results in a <em>Pandas DataFrame</em>. Each player has been assigned an ID number:</p>

<pre><code>+----------------+----------------+-------------+-------------+
| Home Player ID | Away Player ID | Home Points | Away Points |
+----------------+----------------+-------------+-------------+
|              1 |              2 |           3 |           0 |
|              3 |              4 |           1 |           1 |
|              2 |              3 |           3 |           0 |
|              4 |              1 |           3 |           0 |
|              2 |              4 |           1 |           1 |
|              3 |              1 |           1 |           1 |
|              2 |              1 |           0 |           3 |
|              4 |              3 |           1 |           1 |
|              3 |              2 |           0 |           3 |
|              1 |              4 |           0 |           3 |
|              4 |              2 |           1 |           1 |
|              1 |              3 |           1 |           1 |
+----------------+----------------+-------------+-------------+
</code></pre>

<p>The aim is to create a 4x4 <em>numpy</em> matrix (dimensions equal to the number of players) and fill the matrix with the points they earned from games between the respective players.</p>

<p>The matrix should end up like this:</p>

<pre><code>+--------+---+---+---+---+
| Matrix | 1 | 2 | 3 | 4 |
+--------+---+---+---+---+
|      1 | 0 | 3 | 1 | 0 |
|      2 | 0 | 0 | 3 | 1 |
|      3 | 1 | 0 | 0 | 1 |
|      4 | 3 | 1 | 1 | 0 |
+--------+---+---+---+---+
</code></pre>

<p>The left hand column is the ID number of the home players, with the column headers the IDs of the away players.</p>

<p>For example, when the Home Player ID = 1 and the Away Player ID = 2, Player 1 earned 3 points, so the entry for the Matrix(1,2) <em>(or 0,1 because of the zero indexing)</em> would equal 3.</p>

<p>I can just about manage to do this with two for loops, but it seems quite inefficient. Is there a better way to achieve this?</p>

<p>Would really appreciate any advice!</p>
","8848555","","","","","2018-02-21 10:32:13","Creating a Results Grid from a Pandas DataFrame","<python-3.x><pandas><numpy>","1","0","","","","CC BY-SA 3.0","1"
"49174367","1","49182944","","2018-03-08 13:33:13","","1","899","<p>I have a <code>hdf5</code> file that contains a table where the column <code>time</code> is in datetime64[ns] format.</p>

<p>I want to get all the rows that are older than <code>thresh</code>. How can I do that? This is what I've tried:</p>

<pre><code>thresh = pd.datetime.strptime('2018-03-08 14:19:41','%Y-%m-%d %H:%M:%S').timestamp()
hdf = pd.read_hdf(STORE, 'gh1', where = 'time&gt;thresh' )
</code></pre>

<p>I get the following error:</p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-80-fa444735d0a9&gt;"", line 1, in &lt;module&gt;
    runfile('/home/joao/github/control_panel/controlpanel/controlpanel/reading_test.py', wdir='/home/joao/github/control_panel/controlpanel/controlpanel')

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/spyder/utils/site/sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""/home/joao/github/control_panel/controlpanel/controlpanel/reading_test.py"", line 15, in &lt;module&gt;
    hdf = pd.read_hdf(STORE, 'gh1', where = 'time&gt;thresh' )

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/pandas/io/pytables.py"", line 370, in read_hdf
    return store.select(key, auto_close=auto_close, **kwargs)

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/pandas/io/pytables.py"", line 717, in select
    return it.get_result()

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/pandas/io/pytables.py"", line 1457, in get_result
    results = self.func(self.start, self.stop, where)

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/pandas/io/pytables.py"", line 710, in func
    columns=columns, **kwargs)

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/pandas/io/pytables.py"", line 4141, in read
    if not self.read_axes(where=where, **kwargs):

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/pandas/io/pytables.py"", line 3340, in read_axes
    self.selection = Selection(self, where=where, **kwargs)

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/pandas/io/pytables.py"", line 4706, in __init__
    self.condition, self.filter = self.terms.evaluate()

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/pandas/core/computation/pytables.py"", line 556, in evaluate
    self.condition = self.terms.prune(ConditionBinOp)

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/pandas/core/computation/pytables.py"", line 118, in prune
    res = pr(left.value, right.value)

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/pandas/core/computation/pytables.py"", line 113, in pr
    encoding=self.encoding).evaluate()

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/pandas/core/computation/pytables.py"", line 327, in evaluate
    values = [self.convert_value(v) for v in rhs]

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/pandas/core/computation/pytables.py"", line 327, in &lt;listcomp&gt;
    values = [self.convert_value(v) for v in rhs]

  File ""/home/joao/anaconda3/lib/python3.6/site-packages/pandas/core/computation/pytables.py"", line 185, in convert_value
    v = pd.Timestamp(v)

  File ""pandas/_libs/tslib.pyx"", line 390, in pandas._libs.tslib.Timestamp.__new__

  File ""pandas/_libs/tslib.pyx"", line 1549, in pandas._libs.tslib.convert_to_tsobject

  File ""pandas/_libs/tslib.pyx"", line 1735, in pandas._libs.tslib.convert_str_to_tsobject

ValueError: could not convert string to Timestamp
</code></pre>
","2213825","","","","","2018-03-08 21:39:58","Query hdf5 datetime column","<python><python-3.x><pandas><dataframe><hdf5>","1","2","","","","CC BY-SA 3.0","1"
"42281691","1","","","2017-02-16 18:15:25","","0","895","<p>Am having a pandas series &amp; i need to show it's elements in columnar fashion embedded in an email body with HTML.Hence tried below snippet. It worked for me initially with two columns, but now i have three columns to display. Here is code-</p>

<pre><code>a = max_compare.rename_axis('Metric').reset_index(name='Yesterday').to_html()
</code></pre>

<p>Here max_compare is series with below output-</p>

<pre><code>&lt;table border=""1"" class=""dataframe""&gt;
  &lt;thead&gt;
    &lt;tr style=""text-align: right;""&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Metric&lt;/th&gt;
      &lt;th&gt;Yesterday&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;GSV&lt;/td&gt;
      &lt;td&gt;4424180.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Units&lt;/td&gt;
      &lt;td&gt;7463.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</code></pre>

<p>Now i am having another element to be shown in column. Tried below code snippet, but got exception.</p>

<pre><code>a =max_compare.rename_axis('Metric').reset_index(name=['Yesterday'],[L30 average]).to_html()
</code></pre>

<p>Current Output in HTML
Output- <a href=""https://i.stack.imgur.com/Mhu3m.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Mhu3m.png"" alt=""Added Image for current &amp; expected output in HTML table format""></a> </p>
","7523859","","7523859","","2017-02-17 02:06:51","2017-02-18 07:48:07","How to add multiple columns in reset_index from a pandas- series","<python-3.x><pandas>","1","0","1","","","CC BY-SA 3.0","1"
"56891456","1","","","2019-07-04 16:05:52","","5","893","<p>Assuming <code>df['time']</code> is from type <code>timedelta64[ns]</code> and <code>df['a']</code> as well as <code>df['b']</code> are from type <code>float64</code>, the two series can be plotted like this:</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
fig, axs = plt.subplots(2, sharex=True)

time_values = pd.to_datetime(df['time'])
axs[0].plot(time_values, df['a'])
axs[1].plot(time_values, df['b'])
plt.show()
</code></pre>

<p>This works.. But gives the following Warning:</p>

<pre><code>FutureWarning: Passing timedelta64-dtype data is deprecated, will raise a TypeError in a future version
</code></pre>

<p>So what should be used instead of <code>pd.to_datetime</code> to display <code>timedelta64[ns]</code> as human-readable time in <code>matplotlib</code>?</p>
","940592","","940592","","2019-07-06 07:29:55","2020-01-28 09:28:15","timedelta64[ns] -> FutureWarning: Passing timedelta64-dtype data is deprecated, will raise a TypeError in a future version","<python><python-3.x><pandas><matplotlib><plot>","1","2","","","","CC BY-SA 4.0","1"
"40559769","1","40559808","","2016-11-12 05:05:01","","0","890","<p>I want to classify this Dataframe(named:loandata) and import each sub-dataframe into many csv files. The first step, I try to convert one of the results, but unfortunately failed and got this error:</p>

<pre><code>import pandas as pd
import csv
import os

#readfile
loandata=pd.DataFrame(pd.read_table('/Users/lixuefei/Desktop/Sample Dataset/test.txt',header = None,index_col=2))

#classify
volume_type=list(set(loandata[3]))
system_type=list(set(loandata[4]))
area_name=list(set(loandata[5]))

df=pd.DataFrame(loandata[(loandata[3]==volume_type[0])&amp; (loandata[4]==system_type[0])&amp;(loandata[5]==area_name[0])])
#set the file path
path='/Users/lixuefei/Desktop/Sample Dataset'
filename=volume_type[0]+system_type[0]+area_name[0]
filetype=csv

if not df.empty:
    df.to_csv(os.path.join(path,filename+filetype),header=None)
else:
    print(""Empty"")
</code></pre>

<p>and this is the Error:</p>

<pre class=""lang-none prettyprint-override""><code>/Users/lixuefei/anaconda/bin/python3.5/Users/lixuefei/PycharmProjects/project/project.11.09.py
   Traceback (most recent call last):
     File ""/Users/lixuefei/PycharmProjects/project/project.11.09.py"", line 25, in &lt;module&gt;
       df.to_csv(os.path.join(path,filename+filetype),header=None)
   TypeError: Can't convert 'module' object to str implicitly
</code></pre>
","7148763","","355230","","2016-11-12 06:05:15","2016-11-12 06:05:15","TypeError: Can't convert 'module' object to str implicitly","<python><python-3.x><csv><pandas><dataframe>","1","1","","","","CC BY-SA 3.0","1"
"57022698","1","57022825","","2019-07-13 20:45:14","","1","889","<p><strong>Goal</strong></p>

<p>Apply <code>deid_notes</code> function to <code>df</code></p>

<p><strong>Background</strong></p>

<p>I have a <code>df</code> that resembles this sample <code>df</code></p>

<pre><code>import pandas as pd
df = pd.DataFrame({'Text' : ['there are many different types of crayons', 
                                   'i like a lot of sports cares', 
                                   'the middle east has many camels '], 

                      'P_ID': [1,2,3], 
                      'Word' : ['crayons', 'cars', 'camels'],
                      'P_Name' : ['John', 'Mary', 'Jacob'],
                      'N_ID' : ['A1', 'A2', 'A3']

                     })

#rearrange columns
df = df[['Text','N_ID', 'P_ID', 'P_Name', 'Word']]
df

    Text                  N_ID P_ID P_Name  Word
0   many types of crayons   A1  1    John   crayons
1   i like sports cars      A2  2    Mary   cars
2   has many camels         A3  3    Jacob  camels
</code></pre>

<p>I use the following function to deidentify certain words within the <code>Text</code> column using NeuroNER <a href=""http://neuroner.com/"" rel=""nofollow noreferrer"">http://neuroner.com/</a></p>

<pre><code>def deid_notes(text):

    #use predict function from neuorNER to tag words to be deidentified 
    ner_list = n1.predict(text)      

    #n1.predict wont work in this toy example because neuroNER package needs to be installed (and installation is difficult) 
    #but the output resembles this: [{'start': 1, 'end:' 11, 'id': 1, 'tagged word': crayon}]

    #use start and end position of tagged words to deidentify and replace with **BLOCK**
    if len(ner_list) &gt; 0:
        parts_to_take = [(0, ner_list[0]['start'])] + [(first[""end""]+1, second[""start""]) for first, second in zip(ner_list, ner_list[1:])] + [(ner_list[-1]['end'], len(text)-1)] 
        parts = [text[start:end] for start, end in parts_to_take] 
        deid = '**BLOCK**'.join(parts)

    #if n1.predict does not identify any words to be deidentified, place NaN 
    else:
        deid='NaN'

    return pd.Series(deid, index='Deid')
</code></pre>

<p><strong>Problem</strong></p>

<p>I apply the <code>deid_notes</code> function to my <code>df</code> using the following code</p>

<pre><code>fx = lambda x: deid_notes(x.Text,axis=1)
df.join(df.apply(fx))
</code></pre>

<p>But I get the following error </p>

<pre><code>AttributeError: (""'Series' object has no attribute 'Text'"", 'occurred at index Text')
</code></pre>

<p><strong>Question</strong></p>

<p>How do I get the <code>deid_notes</code> function to work on my <code>df</code>?</p>
","6598999","","6598999","","2019-08-25 15:32:36","2019-08-25 15:32:36","Use lambda, apply, and join function on a pandas dataframe","<python-3.x><pandas><join><lambda><apply>","1","4","","","","CC BY-SA 4.0","1"
"56691690","1","","","2019-06-20 18:10:45","","0","882","<p>I have just changed laptop (from Windows to Mac) and after reinstalling anaconda/spyder I have tried to run the same code that reads a csv file as a pandas dataframe.</p>

<p>I have been using: pd.read_csv (path file)</p>

<pre><code>pd.read_csv(""/User/Documents/etc...csv"")
</code></pre>

<p>but the code gets stuck. It runs for several minutes and afterwards the console displays:</p>

<pre><code>""Errno 5"" Input/output error: '/User/Documents/....'.
</code></pre>

<p>Do you have an idea of which reason could be behind this?</p>
","5535239","","","","","2019-06-21 05:39:18","I cannot read csv file as pandas dataframe (anymore)","<python-3.x><pandas><csv>","3","0","","","","CC BY-SA 4.0","1"
"49574502","1","49574541","","2018-03-30 12:32:46","","4","881","<p>I need to apply if else condition and for loop in single line.I need to update both 'RL' and ""RM"" at a time and update other values as 'Others'.How to do it??.IS it possible??</p>

<pre><code>train['MSZoning']=['RL' if x=='RL' else 'Others' for x in train['MSZoning']]
</code></pre>
","9444212","","","","","2018-03-30 13:07:42","IF else and for loop in one line","<python-3.x><pandas><scikit-learn><sklearn-pandas>","2","5","","","","CC BY-SA 3.0","1"
"56890101","1","56891431","","2019-07-04 14:32:50","","0","880","<p>I have a Pandas DataFrame that was generated based on a dictionary (that had some dictionaries inside it).</p>

<p>When I print the dictionary it gives me something like this illustration below:</p>

<p><strong>Current Dataframe Table</strong>
<a href=""https://i.stack.imgur.com/F83qg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/F83qg.png"" alt=""enter image description here""></a></p>

<p>I need to, somehow, to transform those dictionaries into columns that are going to be children of the month (Jan, Feb, etc.) column. Like this:</p>

<p><strong>Dataframe Table that I need</strong>
<a href=""https://i.stack.imgur.com/MzdeD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MzdeD.png"" alt=""enter image description here""></a></p>

<p>**UPDATE - Adding the dictionary **</p>

<p><code>data={'2007': {'Jan': {'working_days': 23,'weekend': 4,'holydays': 4,'days': 31},'Feb': {'working_days': 20,'weekend': 6,'holydays': 2,'days': 28},'Mar': {'working_days': 20,'weekend': 6,'holydays': 2,'days': 28}},'2008': {'Jan': {'working_days': 23,'weekend': 4,'holydays': 4,'days': 31},'Feb': {'working_days': 20,'weekend': 6,'holydays': 2,'days': 28},'Mar': {'working_days': 20,'weekend': 6,'holydays': 2,'days': 28}},'2009': {'Jan': {'working_days': 23,'weekend': 4,'holydays': 4,'days': 31},'Feb': {'working_days': 20,'weekend': 6,'holydays': 2,'days': 28},'Mar': {'working_days': 20,'weekend': 6,'holydays': 2,'days': 28}}}</code></p>

<p>How can I do it?</p>

<p>Thanks in advance!</p>
","5864214","","5864214","","2019-07-05 09:45:22","2019-07-05 09:45:22","Dictionary inside Pandas Dataframe","<python><python-3.x><pandas>","1","5","","","","CC BY-SA 4.0","1"
"44058082","1","44058454","","2017-05-18 21:29:53","","0","875","<p>I have time series data with a column which can take a value A, B, or C.</p>

<p>An example of my data looks like this:</p>

<pre><code>date,category
2017-01-01,A
2017-01-15,B
2017-01-20,A
2017-02-02,C
2017-02-03,A
2017-02-05,C
2017-02-08,C
</code></pre>

<p>I want to group my data by month and store both the sum of the count of A and the count of B in column <code>a_or_b_count</code> and the count of <code>C</code> in <code>c_count</code>.</p>

<p>I've tried several things, but the closest I've been able to do is to preprocess the data with the following function:</p>

<pre><code>def preprocess(df):
    # Remove everything more granular than day by splitting the stringified version of the date.
    df['date'] = pd.to_datetime(df['date'].apply(lambda t: t.replace('\ufeff', '')), format=""%Y-%m-%d"")
    # Set the time column as the index and drop redundant time column now that time is indexed. Do this op in-place.
    df = df.set_index(df.date)
    df.drop('date', inplace=True, axis=1)
    # Group all events by (year, month) and count category by values.
    counted_events = df.groupby([(df.index.year), (df.index.month)], as_index=True).category.value_counts()
    counted_events.index.names = [""year"", ""month"", ""category""]
    return counted_events
</code></pre>

<p>which gives me the following:</p>

<pre><code>year  month  category
2017  1      A           2
             B           1
      2      C           3
             A           1
</code></pre>

<p>The process to sum up all A's and B's would be quite manual since category becomes a part of the index in this case.</p>

<p>I'm an absolute pandas menace, so I'm likely making this much harder than it actually is. Can anyone give tips for how to achieve this grouping in pandas?</p>
","2883245","","2883245","","2017-05-18 21:35:01","2017-05-18 22:04:36","How can I count categorical columns by month in Pandas?","<python-3.x><pandas><group-by><time-series>","1","0","","","","CC BY-SA 3.0","1"
"40646032","1","40646119","","2016-11-17 03:03:42","","3","873","<p>i have a table in pandas df</p>

<pre><code>bigram         frequency
(123,3245)       2
(676,35346)      84
(93,32)          9
</code></pre>

<p>and so on, till 50 rows.</p>

<p>what i am looking for is, split the <strong>bigram</strong> column into two different columns removing the brackets and comma like,</p>

<pre><code>col1     col2      frequency
123       3245        2
676       35346       84
93        32          9
</code></pre>

<p>is there any way to split if after comma,and removing brackets.</p>
","6803114","","","","","2016-11-17 07:38:53","Split Column containing 2 values into different column in pandas df","<python><regex><python-2.7><python-3.x><pandas>","3","2","","","","CC BY-SA 3.0","1"
"41438146","1","","","2017-01-03 07:06:21","","1","853","<p>I have a folder of PDF files (Drawings) that need to be renamed. </p>

<p>The file names are all in the format ""x-xxxxx-xxx-xx-xx-xxx-xx-xxx-xxx-xxxxxx"" with the x's being numbers or letters.</p>

<p>I have an excel sheet that lists these files in Column 'A' then in Column 'B' i have the new name that is required for each row in the format ""yy-yyyyyy-yyy"" with the y's being numbers or letters.</p>

<p>What I would like to do is use Python (preferably Python 3) to rename the files so that the 'y' name is first with the 'x' name appended after with a separator between, </p>

<p>i.e. 
""yy-yyyyyy-yyy -- x-xxxxx-xxx-xx-xx-xxx-xx-xxx-xxx-xxxxxx""</p>

<p>I'm guessing that Pandas would be used to interact with the Excel file?</p>

<p>Can anyone help?</p>

<p>I'm relatively new to Python so a breakdown and explanation of the steps needed would be appreciated.</p>

<p><strong>EDIT:</strong> I have tried the following code that was from an old post by <a href=""https://stackoverflow.com/questions/30223601/renaming-multiple-files-on-a-folder-python-of-a-excel-spreadsheet"">Jesse Lopez</a> which is similar.</p>

<pre><code>import pandas
import shutil

IdMapping = pandas.io.excel.read_excel('IdMapping.xlsx', 'sheet1')
for r in master:
    key1 = r.split('.')[3]
    key2 = r.split('.')[4][-2:]
    old = 'ASSET MOVEMENT %s %s' % (key1, key2)
    new = r
    shutil.move(old, new)
</code></pre>

<p>I understand the first parts but the two lines that refer to key1 and key2 I don't understand. I'm assuming that the square brackets are 'slicing' the string into sections but not sure of the terminology. After that I believe that a new string is put together and them the file renamed with the shutil command.</p>

<p><strong>EDIT:</strong> Thanks for the help lbellomo, I have tried to use it and got as far as loading the data into the datatframe (i am using spyder so I can see that the data is read). But on the iterrows line I get an error saying incorrect syntax and a carrot symbol pointing to the space after the colon on df.iterrows():     I have no space after and cant figure out why i'm getting the error.</p>
","7179203","","-1","","2017-05-23 12:09:00","2017-01-06 09:21:18","Rename files with Python, using name map from Excel","<python><python-3.x><pandas>","1","1","","","","CC BY-SA 3.0","1"
"57558338","1","57558444","","2019-08-19 14:01:46","","1","850","<p>Lets suppose I have a dataframe and would like to set datatypes to all columns just like then I call <code>read_csv</code> method. For simplicity same error <code>TypeError: object of type 'type' has no len()</code>  gives this piece of code:</p>

<pre><code>df = pd.DataFrame([1,2,2,3], columns = ['num'], dtype={'num':int})
</code></pre>

<p>What is wrong here and how to make it work?</p>

<p>Full error stack:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-42-e8a84bf74364&gt; in &lt;module&gt;()
----&gt; 1 df = pd.DataFrame([1,2,2,3], columns = ['num'], dtype={'num':int})

C:\Anaconda3\lib\site-packages\pandas\core\frame.py in __init__(self, data, index, columns, dtype, copy)
    264             data = {}
    265         if dtype is not None:
--&gt; 266             dtype = self._validate_dtype(dtype)
    267 
    268         if isinstance(data, DataFrame):

C:\Anaconda3\lib\site-packages\pandas\core\generic.py in _validate_dtype(self, dtype)
    145 
    146         if dtype is not None:
--&gt; 147             dtype = pandas_dtype(dtype)
    148 
    149             # a compound dtype

C:\Anaconda3\lib\site-packages\pandas\core\dtypes\common.py in pandas_dtype(dtype)
   1895 
   1896     try:
-&gt; 1897         npdtype = np.dtype(dtype)
   1898     except (TypeError, ValueError):
   1899         raise

C:\Anaconda3\lib\site-packages\numpy\core\_internal.py in _usefields(adict, align)
     60         names = None
     61     if names is None:
---&gt; 62         names, formats, offsets, titles = _makenames_list(adict, align)
     63     else:
     64         formats = []

C:\Anaconda3\lib\site-packages\numpy\core\_internal.py in _makenames_list(adict, align)
     28     for fname in fnames:
     29         obj = adict[fname]
---&gt; 30         n = len(obj)
     31         if not isinstance(obj, tuple) or n not in [2, 3]:
     32             raise ValueError(""entry not a 2- or 3- tuple"")

TypeError: object of type 'type' has no len()
</code></pre>
","1739325","","","","","2019-10-02 05:24:53","Setting dtypes from pd.DataFrame gives TypeError: object of type 'type' has no len()","<python><python-3.x><pandas><types><runtime-error>","1","1","","2019-08-19 14:16:47","","CC BY-SA 4.0","1"
"41440068","1","41440095","","2017-01-03 09:17:37","","1","837","<p>I have a following dataframe:</p>

<pre><code>In [25]: df1
Out[25]: 
          a         b
0  0.752072  0.813426
1  0.868841  0.354665
2  0.944651  0.745505
3  0.485834  0.163747
4  0.001487  0.820176
5  0.904039  0.136355
6  0.572265  0.250570
7  0.514955  0.868373
8  0.195440  0.484160
9  0.506443  0.523912
</code></pre>

<p>Now I want to create another column <code>df1['c']</code> whose values would be maximum among <code>df1['a']</code> and <code>df1['b']</code>. Thus, I would like to have this as an output:</p>

<pre><code>In [25]: df1
Out[25]: 
          a         b        c
0  0.752072  0.813426 0.813426
1  0.868841  0.354665 0.868841
2  0.944651  0.745505 0.944651
3  0.485834  0.163747 0.485834
4  0.001487  0.820176 0.820176
</code></pre>

<p>I tried :</p>

<pre><code>In [23]: df1['c'] = np.where(max(df1['a'], df1['b'], df1['a'], df1['b'])
</code></pre>

<p>However, this throws a syntax error. I don't see any way in which I can do this in pandas. My actual dataframe is way too complex and so I would like to have a generic solution for this. Any ideas?</p>
","2396502","","","","","2017-01-03 09:23:52","Comparing two columns in pandas dataframe to create a third one","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"48492623","1","","","2018-01-28 23:49:32","","0","826","<p>With CSV file in Pandas, how do I count how many items in a column are more than a specified value?</p>
","9166243","","","","","2018-01-29 00:06:06","Counting items in columns (Python Pandas)","<python-3.x><pandas><count>","3","0","","","","CC BY-SA 3.0","1"
"41205001","1","41205215","","2016-12-18 02:43:37","","11","826","<p>Thank you for your time.</p>

<p>I am writing some code that is checking for correlation between multiple sets of data. It works great when I am using the original data (which I am honestly unsure of which format it is in at that point), but after I run the data through some equations using the Decimal module, the data set will not show up when tested for correlation.</p>

<p>I feel really stupid and new lol, I am sure it's a very easy fix.</p>

<p>Here is a small program I wrote to demonstrate what I mean.</p>

<pre><code>from decimal import Decimal
import numpy as np
import pandas as pd

a = [Decimal(2.3), Decimal(1.5), Decimal(5.7), Decimal(4.6), Decimal(5.5), Decimal(1.5)]
b = [Decimal(2.1), Decimal(1.2), Decimal(5.3), Decimal(4.4), Decimal(5.3), Decimal(1.7)]

h = [2.3,1.5,5.7,4.6,5.5,1.5]
j = [2.1,1.2,5.3,4.4,5.3,1.7]

corr_data1 = pd.DataFrame({'A': a, 'B': b}) 

corr_data2 = corr_data1.corr()
print(corr_data2)

corr_data3 = pd.DataFrame({'H': h, 'J': j})

corr_data4 = corr_data3.corr()
print(corr_data4)
</code></pre>

<p>The data for both lists A &amp; B as well as H &amp; F are exactly the same, with the only difference of A &amp; B being decimal formated numbers, where as H &amp; F are not.</p>

<p>When the program is run, A &amp; B returns:</p>

<pre><code>Empty DataFrame
Columns: []
Index: []
</code></pre>

<p>and H &amp; J returns:</p>

<pre><code>          H         J
H  1.000000  0.995657
J  0.995657  1.000000
</code></pre>

<p>How do I make it so I can utilize the data after I've ran it through my equations?</p>

<p>Sorry for the stupid question and thank you for your time. I hope you are all well, happy holidays!</p>
","7311804","","240490","","2016-12-18 17:48:54","2016-12-18 17:48:54","How do I check for correlation using Decimal numbers/data with python 3","<python><python-3.x><pandas><numpy><decimal>","3","3","","","","CC BY-SA 3.0","1"
"57693073","1","","","2019-08-28 13:01:21","","2","824","<p>I have some parameters that I am plotting against 'pandas.core.indexes.datetimes.DatetimeIndex' as x-axis.
When I plot, I see the day and hour, minute, seconds. How do I get rid of the day? 
Here is a minimal form of the problem:</p>

<pre><code>time_ci=
DatetimeIndex(['2019-07-12 13:25:00', '2019-07-12 13:25:10',
           '2019-07-12 13:25:20', '2019-07-12 13:25:30',
           '2019-07-12 13:25:40', '2019-07-12 13:25:50'],
          dtype='datetime64[ns]', freq=None)



 h2o_g_= masked_array(
data=[[1.7723087072372437],
    [1.7723088264465332],
    [1.7723087072372437],
    [1.7723088264465332],
    [1.7723089456558228],
    [1.7723088264465332]],
mask=[[False],
    [False],
    [False],
    [False],
    [False],
    [False]],
fill_value=1e+20,
dtype=float32)
</code></pre>

<p>This is the script for the plot:</p>

<pre><code>fig2, ax = plt.subplots(figsize=(12,10))
plt.plot(time_ci,h2o_g)
</code></pre>

<p>This is what I get:</p>

<p><a href=""https://i.stack.imgur.com/HQ8gA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/HQ8gA.png"" alt=""The plot with datetime, the 12 is the date and i do not want it there.""></a></p>

<p>The 12 is the date (that I don't want to be there).</p>

<p>I have already tried these methods:</p>

<pre><code>import matplotlib.dates as md
ax=plt.gca()
xfmt = md.DateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(xfmt)
</code></pre>

<p>and also:</p>

<pre><code>import matplotlib.dates as mdates
h_fmt = mdates.IndexDateFormatter('%H:%M:%S')
ax.xaxis.set_major_formatter(h_fmt)
</code></pre>

<p>but none of them worked.</p>

<p>Also, the seconds are not there either.</p>
","6787264","","6787264","","2019-08-28 15:37:08","2019-08-28 15:37:08","How to have only hour, minutes, and seconds for x-axis in matplotlib (and not the date)","<python><python-3.x><pandas><datetime><matplotlib>","0","9","","","","CC BY-SA 4.0","1"
"56851679","1","56851755","","2019-07-02 11:42:35","","5","816","<p>I have a Pandas column that contains results from a survey, which are either free text or numbers from 1-5. I am retrieving these from an API in JSON format and convert them into a DataFrame. Each row represents one question with the answer of a participant like this:</p>

<pre><code>Memberid | Question | Answer
       1   Q1             3
       1   Q2             2
       1   Q3         Test Text
       2   Q1             3
       2   Q2             2
       2   Q3         Test Text
</code></pre>

<p>The column that has the results stores all of them as string for now, so when exporting them to excel the numbers are stored as text. </p>

<p>My goal is to have a separate column for the text answers and leave the field they were originally in empty, so that we have separate columns for the text results and the numeric results for calculation purposes.</p>

<pre><code>Memberid | Question | Numeric Answers | Freetext answers
       1   Q1             3
       1   Q2             2
       1   Q3                             Test Text
       2   Q1             3
       2   Q2             2
       2   Q3                             Test Text
</code></pre>

<p>I am generating this df from lists like this: </p>

<pre><code>d = {'Memberid':memberid, 'Question':title, 'Answer':results}
df = pd.DataFrame(d)
</code></pre>

<p>So the first thing I tried was to convert the numeric values in the column from string to numbers via this:</p>

<p><code>df[""Answer""] = pd.to_numeric(df['Answer'], errors='ignore')</code></p>

<p>Idea was that if it works I can simply do a for loop to check if a value in the answer column is string and then move that value into a new column.</p>

<p>The issue is, that the errors command does not work as intended for me. When I leave it on ignore, nothing gets converted. When I change it to coerce, the numbers get converted from str to numeric, but the fields where there freetext answers are now empty in Excel.</p>
","8150532","","8895292","","2019-07-02 13:55:56","2019-07-23 08:35:38","How to separate Pandas column that contains values stored as text and numbers into two seperate columns","<python><python-3.x><pandas>","5","0","1","","","CC BY-SA 4.0","1"
"56910377","1","","","2019-07-06 00:29:36","","1","815","<p>I am loading Geopandas into a Watson Studio Notebook.</p>
<pre><code>!conda install --channel conda-forge geopandas geoplot geopy --yes
</code></pre>
<p>Output seems to indicate it works</p>
<p>!conda install --channel conda-forge geopandas geoplot geopy --yes</p>
<pre><code>Solving environment: done

## Package Plan ##

  environment location: /opt/conda/envs/Python36

  added / updated specs: 
    - geopandas
    - geoplot
    - geopy


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------

    geopandas-0.5.0            |             py_3         891 KB  conda-forge
    
    ------------------------------------------------------------
                                           Total:       357.8 MB

The following NEW packages will be INSTALLED:


    geopandas:            0.5.0-py_3               conda-forge
    geoplot:              0.2.4-py_0               conda-forge
    geopy:                1.20.0-py_0              conda-forge


Downloading and Extracting Packages


geopandas-0.5.0      | 891 KB    | ##################################### | 100% 

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
</code></pre>
<p>But when I try to import it</p>
<pre><code>import geopandas
from shapely.geometry import Point # Shapely for converting latitude/longtitude to geometry
from geopandas import GeoDataFrame
import geoplot
import geopy
import geoplot.crs as gcrs
import matplotlib.pyplot as plt
#from scipy import stats
import seaborn as sns
%matplotlib inline
</code></pre>
<p>I get</p>
<blockquote>
<hr />
<p>ModuleNotFoundError                       Traceback (most recent call last)</p>
<p> in </p>
<p>----&gt; 1 import geopandas</p>
</blockquote>

<blockquote>
<p>ModuleNotFoundError: No module named 'geopandas'</p>
</blockquote>
<p>And neither</p>
<pre><code>!conda list
</code></pre>
<p>nor the Watson Studio Software configuration details shows that it was loaded.  Same with geopy and geoplot.</p>
<p>Strange thing is this worked last week. Seems to be the same problem with Python 3.5.</p>
<p>I also tried loading Geopandas with pip but same results.  It seems unlikely both the conda and pip scripts would be in error.</p>
<p>Can anyone suggest a way of investigating the cause.  I have a case open with IBM but thought I would ask here as well.  Thanks in advance.</p>
","8072669","","-1","","2020-06-20 09:12:55","2019-07-08 14:52:33","Conda seems to load Geopandas but it is not added to the IBM Watson Studio Notebook Python 3.6","<python-3.x><loading><conda><geopandas><watson-studio>","1","0","1","","","CC BY-SA 4.0","1"
"48646739","1","","","2018-02-06 15:41:50","","2","802","<p>I am trying to split a source column of a dataframe in several columns based on its content, and then fill this newly generated columns with a boolean 1 or 0 in the following way:</p>

<p>Original dataframe:</p>

<pre><code>ID   source_column
A    value 1
B    NaN
C    value 2
D    value 3
E    value 2
</code></pre>

<p>Generating the following output:</p>

<pre><code>ID   source_column    value 1    value 2    value 3
A    value 1          1          0          0
B    NaN              0          0          0
C    value 2          0          1          0
D    value 3          0          0          1
E    value 2          0          1          0
</code></pre>

<p>I thought about manually create each different column, and then with a function for each column and .apply, filling the newly column with a 1 or a 0, but this is highly ineffective.</p>

<p>Is there a quick and efficient way for this?</p>
","6049160","","8544123","","2018-02-06 18:43:30","2018-02-06 18:43:30","Python Pandas: create a new column for each different value of a source column (with boolean output as column values)","<python><python-3.x><pandas><dataframe><dummy-variable>","4","1","","","","CC BY-SA 3.0","1"
"49373363","1","49375561","","2018-03-19 22:21:23","","0","790","<p>I am getting a <code>MemoryError: cannot allocate memory for array</code> when using <code>df.duplicated()</code> to check for duplicates in a data frame in Python 3.6.4. </p>

<p>The df has about 150,000 rows and 208 columns and there are no issues with loading the data into a df (using chunks per below).</p>

<pre><code>myList = []
for chunks in pd.read_csv(filename, header=0, low_memory=False, chunksize=20000):
        myList.append(chunks)

dfMain = pd.concat(myList, axis=0)
dfMain.index.name = 'Index'

print (dfMain.shape)
Out: (151982, 208)
</code></pre>

<p>Everything is fine up until this point. </p>

<pre><code>   #Marks duplicated rows with TRUE or FALSE and put into a new df
    dfDup1 = pd.DataFrame(dfMain.duplicated(keep=False)) #set to False to view all duplicates
</code></pre>

<p>This is where the error occurs: <code>MemoryError: cannot allocate memory for array</code> and the script stops.</p>

<p>Unfortunately, reducing the number of columns is not an option, I need to check for duplicates across all of variables (although I did drop 150 variables to test but the problem persisted). And I do need to export the duplicated values to df/csv, so can't use <code>drop_duplicates()</code> at this stage.</p>

<p>The computer has plenty of RAM (64 gigs), but Python/pandas is using only a fraction of it.</p>

<p>Any help would be appreciated. </p>
","9513001","","","","","2018-03-20 02:41:58","Python/Pandas - df.duplicated() MemoryError: cannot allocate memory for array","<python><python-3.x><pandas><memory>","1","6","","","","CC BY-SA 3.0","1"
"57331686","1","57331797","","2019-08-02 18:20:56","","0","788","<p>I am trying to make a line plot in which on the x-axis I have strings that correspond to categories, however, I can't seem to sort the x-axis from smallest to largest because I can't seem to force sort of the axis. </p>

<pre><code>d = {'Average_Age': [22, 34, 49, 61], 'Salary_Category': 
   ['[1200, 2000[', '[2000, 6500[', '[6500, 10000[', '[11000, 15000]']}

df = pd.DataFrame(data=d)
df

test = sns.lineplot(x= 'Salary_Category' , y= 'Average_Age', data = df)
plt.title('Average Salaries per Age Category')
plt.xlabel('Salary Category')
plt.ylabel('Average Age')
plt.xticks(rotation=45)
</code></pre>

<p>So this is the plot that I get,  which doesn't make sense since the. salary categories are mixed up. 
   <a href=""https://i.stack.imgur.com/jpVFm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jpVFm.png"" alt=""enter image description here""></a></p>

<p>How could I fix this?</p>
","11053294","","","","","2019-08-02 18:31:17","Force a x-axis order in a line plot with strings","<python-3.x><pandas><seaborn>","1","1","","","","CC BY-SA 4.0","1"
"57501013","1","57502212","","2019-08-14 19:39:22","","0","787","<p>Using Visual Studio Code.</p>

<p>Python 3 installed on local. </p>

<p>Working files stored on a server. </p>

<p>I am using pandas <code>ExcelWriter</code> to modify excel file. </p>

<p>If <code>ExcelWriter</code> points to local path then it works fine,</p>

<pre><code>writer = pd.ExcelWriter('C:\\Users\\username\\Documents\\TestingFolder\\ExpirationList.xlsx') 
</code></pre>

<p>but if I point it to a server location then it gives me an error:</p>

<pre><code>FileNotFoundError: [Errno 2] No such file or directory: '\\01.02.03.04\\username\\Documents\\Documents\\Python\\""Expiration List report email send out""\\ExpirationList.xlsx' 
</code></pre>

<p><strong>sample code:</strong></p>

<pre><code> import pandas as pd 
    import pyodbc 
    import smtplib 
    from email.mime.multipart import MIMEMultipart 
    from email.mime.text import MIMEText 
    from email.mime.base import MIMEBase 
    from email import encoders 
    import datetime
    import calendar


    query = """"""select top 5 PolicyNumber from tblQuotes where PolicyNumber is not null""""""
    conn = pyodbc.connect('DRIVER={SQL Server};server=servername;DATABASE=dbName;Trusted_Connection=yes;')
    df = pd.read_sql_query(query, conn)

    writer = pd.ExcelWriter('C:\\Users\\username\\Documents\\TestingFolder\\ExpirationList.xlsx') 

    #Does not work
    writer = pd.ExcelWriter('\\01.02.03.04\\username\\Documents\\Documents\\Python\\""Expiration List report email send out""\\ExpirationList.xlsx')  

    df.to_excel(writer, sheet_name='Sheet1', startrow=4, header=False, index=False)
    writer.save()
</code></pre>

<p>What would be the way to use server location ?</p>
","6043544","","","","","2019-08-14 21:23:46","""No such file or directory"" when using ExcelWriter pointing to a server","<python><python-3.x><pandas><pandas.excelwriter>","2","3","","","","CC BY-SA 4.0","1"
"57508626","1","57509112","","2019-08-15 10:57:35","","1","767","<p>I have a dataframe with 8 rows and 6028 columns. I want to create a heatmap of the 8 rows for the first column (eventually I will create an animation so the map updates reading through each column)</p>

<p>This is a snippet of the dataframe:</p>

<pre><code>                       value                    
percentage_time         0.00      0.15      0.16
region                                          
Anterior Distal     0.111212  0.119385  0.116270
Anterior Proximal   0.150269  0.153613  0.168188
Lateral Distal      0.130440  0.137157  0.136494
Lateral Proximal    0.171977  0.182251  0.181090
Medial Distal       0.077468  0.082064  0.082553
Medial Proximal     0.194924  0.198803  0.199339
Posterior Distal    0.164124  0.171221  0.166328
Posterior Proximal  0.131310  0.145706  0.136094
</code></pre>

<p>I have used the following code but it gives me one plot with the indices stacked and all the data in the dataframe:</p>

<pre><code>sns.heatmap(region_pressure_data)
</code></pre>

<p><a href=""https://i.stack.imgur.com/GRT1e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GRT1e.png"" alt=""enter image description here""></a></p>

<p>When I try to use the following code to get just the first column, I get the following:</p>

<pre><code>sns.heatmap(region_pressure_data.ix[:,0:1])
</code></pre>

<p><a href=""https://i.stack.imgur.com/WjHIu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WjHIu.png"" alt=""enter image description here""></a></p>

<p>Ideally, I would like 1 map of 8 regions, with 2 rows (proximal and distal) and 4 columns (anterior, lateral, posterior, medial), displaying the data of one column.</p>

<p>I'd appreciate any advice on progressing with this method or if there is a better way to approach the challenge.</p>

<p>Thanks in advance.</p>
","11196704","","","","","2019-08-15 11:41:27","Create custom heatmap from pandas dataframe","<python><python-3.x><pandas><heatmap>","2","0","","","","CC BY-SA 4.0","1"
"56617033","1","56617375","","2019-06-16 07:41:20","","0","747","<p>Hope someone can enlighten me on this question. I am using Python 3.7 with pandas and MySQL 8.0 CE (using sqlalchemy and MySQLdb).</p>

<p>I am unsure how to insert <strong>specific columns</strong> from a single dataframe and insert it into <strong>multiple MySQL tables</strong> within the same db.</p>

<p>Original dataframe (df.head):</p>

<pre><code>      date      time    price   discount
0   11/6/2019   7:10     4.56     0.25
1   11/6/2019   7:15     5.01     0.26
2   11/6/2019   7:20     4.87     0.25
3   11/6/2019   7:25     4.54     0.23
</code></pre>

<p>When I use my code (given below), it sends to 'summary' MySQL table:</p>

<pre><code>  date      time    price   discount
11/6/2019   7:10     4.56     0.25
11/6/2019   7:15     5.01     0.26
11/6/2019   7:20     4.87     0.25
11/6/2019   7:25     4.54     0.23
</code></pre>

<p>End Goal - Insert price and discount in separate MySQL table within the same db for example:</p>

<p>MySQL table 1 - 'price'</p>

<pre><code>  date      time    price   
11/6/2019   7:10     4.56  
11/6/2019   7:15     5.01 
11/6/2019   7:20     4.87 
11/6/2019   7:25     4.54 
</code></pre>

<p>MySQL table 2 - 'discount'</p>

<pre><code>  date      time   discount
11/6/2019   7:10     0.25
11/6/2019   7:15     0.26
11/6/2019   7:20     0.25
11/6/2019   7:25     0.23
</code></pre>

<p>This is my original .py code that works if I need to insert the entire dataframe into a single table called 'summary':</p>

<pre><code>import pandas as pd
import numpy as np
import MySQLdb
from sqlalchemy import create_engine

df = pd.read_csv('pricing_1.csv')
engine = create_engine('mysql+mysqldb://root:python@localhost:3306/testdb2', echo = False)

df.to_sql(name = 'summary', con=engine, if_exists = 'append', index = True)
</code></pre>

<p>I have tried to Google before posting this question here but unfortunately couldn't find anything useful - perhaps it has to do with my understanding of how MySQL works?</p>

<p>Really appreciate for someone to guide me with some URL articles or theory! Thanks!</p>
","8526986","","8526986","","2019-06-16 08:00:08","2019-06-16 08:47:37","Insert Specific Dataframe Columns into Multiple MySQL Tables","<mysql><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57378383","1","","","2019-08-06 14:30:36","","1","747","<p>I am trying to color specific points on the seaborn:stripplot based on a defined value. For example, </p>

<p><code>value=(df['x']=0.0</code> </p>

<p>I know you can do this with regplot, etc using:</p>

<p><code>df['color']= np.where( value==True , ""#9b59b6"")</code> and the 
<code>scatter_kws={'facecolors':df['color']}</code> </p>

<p>Is there a way to do it for the <code>pair grid</code> and <code>stripplot</code>? Specifically, to color a specified value in <code>t1</code> or <code>t2</code> below? </p>

<p>I have also tried passing the <code>match</code> var in <code>hue</code>. However, this produced image #2 below and is not what I am looking for. </p>

<p>Here is the <code>df</code>: </p>

<pre><code>par        t1         t2    found   
30000.0   0.50       0.45     yes   
10000.0   0.30       0.12     yes   
3000.0    0.40       0.00     no    

</code></pre>

<p>Here is my code: </p>

<pre><code># Import dependencies
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv(""data.csv"")

# Make the PairGrid
g = sns.PairGrid(df.sort_values(""par"", ascending=True),
                 x_vars=df.columns[1:3], y_vars=[""par""], 
                 height=5, aspect=.65)

# Draw a dot plot using the stripplot function
g.map(sns.stripplot, size=16, orient=""h"", 
      linewidth=1, edgecolor=""gray"", palette=""ch:2.5,-.2,dark=.3"")

sns.set_style(""darkgrid"")

# Use the same x axis limits on all columns and add better labels
g.set(xlim=(-0.1, 1.1), xlabel=""% AF"", ylabel="""")

# Use semantically meaningful titles for the columns
titles = [""Test 1"", ""Test 2""]

for ax, title in zip(g.axes.flat, titles):

    # Set a different title for each axes
    ax.set(title=title)

    # Make the grid horizontal instead of vertical
    ax.xaxis.grid(False)
    ax.yaxis.grid(True)

sns.despine(left=True, bottom=True)
</code></pre>

<p>I am trying to color the point with the <code>value=0.0</code> a different color: </p>

<p><a href=""https://i.stack.imgur.com/lBSA8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lBSA8.png"" alt=""enter image description here""></a></p>

<p>Passing the <code>match</code> var into <code>hue</code> produces the below and removes the 3rd <code>par=3000</code> value and collapses the plot. I can categorize the outliers I want highlighted a different color. However, the outlier value is removed from the y-axis and plot collapsed.. </p>

<p><a href=""https://i.stack.imgur.com/XttEF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XttEF.png"" alt=""enter image description here""></a></p>
","1698774","","1698774","","2019-08-06 14:47:40","2019-08-06 19:22:39","seaborn:stripplot how to color individual point based on value","<python-3.x><pandas><matplotlib><seaborn>","1","0","","","","CC BY-SA 4.0","1"
"57942068","1","57943080","","2019-09-15 07:32:36","","1","741","<p>There is a match index function in Excel that i use to match if the elements are present in the required column </p>

<pre><code>=iferror(INDEX($B$2:$F$8,MATCH($J4,$B$2:$B$8,0),MATCH(K$3,$B$1:$F$1,0)),0)
</code></pre>

<p>This is the function i am using right now and it is yielding me good results but I want to implement it in python.</p>

<pre><code>brand   N   Z   None
Honor   63  96  190     
Tecno   0   695 763     
</code></pre>

<p>from this table I want</p>

<pre><code>  brand L   N   Z
  Honor 0   63  96
  Tecno 0   0   695
</code></pre>

<p>It should compare both the column and index and give the appropriate value</p>

<p>i have tried the lookup function in python but that gives me the</p>

<pre><code>ValueError: Row labels must have same size as column labels
</code></pre>
","11911984","","11911984","","2019-09-16 06:36:09","2019-09-16 06:36:09","Match index function from excel in pandas","<python><python-3.x><pandas>","2","3","1","","","CC BY-SA 4.0","1"
"56851249","1","56853030","","2019-07-02 11:16:28","","4","741","<p>I have a 6 level deep Parent / child hierarchy df which is as follows</p>

<pre><code>Hierarchy Name,Hierarchy Node ID,Hierarchy Level,Hierarchy Node Desc,Node Higher

0,L1,1,1,Top level,#
1,L110,1072,2,Level 2,1
2,L1100,992,3,Level 3 A,1072
3,L1101,994,3,Level 3 B,1072
4,L1102,997,3,Level 3 C,1072
5,L1103,1013,4,Level 4 1,992
6,L1104,1014,5,Level 5 A,1013
</code></pre>

<p>I want to flatten this into the following dataframe for all the paths from the bottom level to the top e.g.</p>

<pre><code>NodeID, NodeDesc, Lvl1, lvl1desc, lvl2, lvl2desc, ...lvl5, lvl5desc

1,Top Level, 1072, Level 2, 992, Level 3 A, 1013, Level 4 1, 1014, Level 5 A
</code></pre>

<p>My method which works is as follows,</p>

<p>Step 1 add a column of parent and child</p>

<pre><code>df2['Dictionery'] = list(zip(df2['Hierarchy Node ID'], df2['Node ID of the 
Highe']))
ancestry = df2['Dictionery']
</code></pre>

<p>step 2 get the path of all relationships, I found this code online for printing out the full path of a parent /child tree</p>

<pre><code>l=[]
parents = set()
children = {}
for c,p,cd in ancestry:
    parents.add(p)
    children[c] = p
# recursively determine parents until child has no parent
def ancestors(p):
    return (ancestors(children[p]) if p in children else []) + [p]

# for each child that has no children print the geneology
for k in (set(children.keys()) - parents):
   l.append('/'.join(ancestors(k)))
</code></pre>

<p>Add the path to a dataframe</p>

<pre><code>df3 = pd.DataFrame(l, columns = ['Path']) 
</code></pre>

<p>Split the path column into each of the level node ids</p>

<pre><code>new = df3[""Path""].str.split(""/"", expand = True) 
df3[""Level1""]= new[0] 
df3[""Level2""]= new[1]
df3[""Level3""]= new[2] 
df3[""Level4""]= new[3] 
df3[""Level5""]= new[4] 
df3[""Level6""]= new[5] 
df3[""Level7""]= new[6]
df3.fillna(value=0, inplace=True)   
</code></pre>

<p>Which gives the following df3</p>

<pre><code>path,  Level1,  Level2 , Level3, Level4, Level5, Level 6

0   #/1/1071/1249/1504/1505/1546, #, 1, 1071 , 1249, 1504, 1505 , 1546

1   #/1/1071/1249/1250/1269/1275, #, 1, 1071, 1249, 1250, 1269, 1275
</code></pre>

<p>I then created a dictionary from original df to map the node id and description e.g.</p>

<pre><code>{'Hierarchy Node Desc': {0: '0.0',
  1: 'Top Level',
  1072: 'Level 2',
  992: 'Level 3 A',
  994: 'Level 3 B',
  997: 'Level 3 C',
  1013: 'Level 4 1',
...}}
</code></pre>

<p>I then map new columns for the description in  df3 for each Level using the dictionary</p>

<p>e.g.</p>

<pre><code>df['Level2desc'] = df['Level2'].map(dict)
</code></pre>

<p>This gives me the flat hierarchy I am after but it just seems a lot of work to get to it and I expect there is a much simpler / more efficient way to do it.</p>

<p>Any advice to do this in simpler way?</p>
","10776453","","7117003","","2019-07-02 11:56:31","2019-07-02 12:57:10","How to flatten a hierarchy with Pandas","<python><python-3.x><pandas><hierarchical-data>","1","0","1","","","CC BY-SA 4.0","1"
"48645850","1","","","2018-02-06 14:56:41","","0","729","<p>I trying reads data for dataset with help of geopandas, but interpreter wright: </p>

<blockquote>
  <p>File
  ""/home/divinitytoffee/PycharmProjects/Radar/venv/lib/python3.5/site-packages/geopandas/datasets/<strong>init</strong>.py"",
  line 33, in get_path raise alueError(msg) ValueError: The dataset
  'resource/RAVL_vLuki/rd0a0h.00d' is not available</p>
</blockquote>

<pre><code>import geopandas as gpd
import fiona.ogrext
import pandas as pd

gpd_data = gpd.gpd.read_file(gpd.datasets.get_path('resource/RAVL_vLuki/rd0a0h.00d'))
</code></pre>

<p>Actually the question, how to fix?
Data are presented in the form of a <code>*.00d</code></p>
","7091445","","6673446","","2018-02-06 16:34:32","2019-12-21 08:30:46","Reading datasets for geopandas","<python><python-3.x><geopandas>","2","3","","","","CC BY-SA 3.0","1"
"57457723","1","57462023","","2019-08-12 08:18:24","","1","725","<p>Given a df</p>

<pre><code>session_id  article session_type
  1         a       req
  1         b       req
  1         null    action
  2         home    req
  2         h       req
  2         j       req
  2         home    req
  3         home    req
  3         home    req
  3         r       req
  3         home    req
</code></pre>

<p>I would like to aggregate to one column as a dict of:
1. unique articles
2.unique session_type
3. count all home that are not consecutive</p>

<p>Output:</p>

<pre><code>sess_id agg_col
1      {unique_articles:2,unique_promotion_session:2,non_consectutive_home:0}
2      {unique_articles:2,unique_promotion_session:1,non_consectutive_home:2}
3      {unique_articles:1,unique_promotion_session:1,non_consectutive_home:1}
</code></pre>

<p>Thanks.</p>
","10377244","","10377244","","2019-08-12 13:05:32","2019-08-12 13:26:15","Pandas Group by on multiple columns to one column","<python-3.x><pandas><pandas-groupby>","1","6","","","","CC BY-SA 4.0","1"
"48539952","1","48540715","","2018-01-31 10:31:20","","1","719","<p>I have csv file with 744 rows and 186 columns with the following format:</p>

<pre><code> Label   1          1         0         0         1
 TaxID   P_ERR161   P_ERR162  P_ERR163  P_ERR164  P_ERR165 
 333046  0.05       0         22.33     0.06      7.32
 1049    0.03       0         0.04      0.01      0.02
 337090  0.01       0         9.79      45.88     3.99   
 288681  3.6        0         1.03      251.01    8.11
</code></pre>

<p>I need to group the row values based on the label 0 and 1 into two separate dataframes. I have seen other posts but unable to find the solution to my problem. I need this for statistical analysis like t test, wilcoxon rank sum test.</p>

<p>I have tried this :</p>

<pre><code>df = pd.read_csv('final_out_transposed.csv')
case = df.where(df.Label == 1).dropna()['SRA ID']
ctrl = df.where(df.Label == 2).dropna()['SRA ID']
</code></pre>

<p>But this isn't helping me. Any help will be really useful.</p>

<p>Thanks in advance. </p>
","8682510","","8682510","","2018-01-31 10:49:45","2018-01-31 12:57:04","python group by based on column headers","<python-3.x><pandas><csv><dataframe><pandas-groupby>","1","5","","","","CC BY-SA 3.0","1"
"49626378","1","49626658","","2018-04-03 09:17:52","","1","716","<p>There's a few posts like this already, but after following them I still encounter some issues. </p>

<pre><code>trade = client.get_my_trades(symbol=ticker)  # fetching all trades made on the ticker IOSTBTC
print(trade)
json_trade = json.dumps(trade, indent=4)  # converting and indenting for easier readability.
print(json_trade+""\n"")
json_normalised = json_normalize(trade)  # normalising with pandas for spreadsheet use
print(""Normalised JSON\n"", json_normalised)

json_normalised = DataFrame(pandas.read_json(""logs.xlsx""))
json_normalised_str = str(json_normalised)
logs = open(""logs.xlsx"", ""w"")  # creating file to write to
logs.write(json_normalised_str)  # writing data to file, oldest first
</code></pre>

<p>This code runs fine, with no erros. However, when I check the logs.xlsx, all the data is in a single column, with spaces in between where they should be separated by columns.</p>

<p>For example, here's some of the JSON data:</p>

<pre><code>[{'id': 3084149, 'orderId': 7071890, 'price': '0.00000312', 'qty': '400.00000000', 'commission': '0.00041327', 'commissionAsset': 'BNB', 'time': 1522223234240, 'isBuyer': True, 'isMaker': True, 'isBestMatch': True}, {'id': 3084468, 'orderId': 7073272, 'price': '0.00000314', 'qty': '400.00000000', 'commission': '0.00041694', 'commissionAsset': 'BNB', 'time': 1522223910252, 'isBuyer': False, 'isMaker': True, 'isBestMatch': True}]
</code></pre>

<p>What I want is for 'id' and 'orderId' and 'price' (et cetera) to have it's own column. With the above data I would have two rows of information. But instead, this is what I recieve when I use this data: <a href=""https://i.stack.imgur.com/EDOJ3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EDOJ3.png"" alt=""spreadsheet screenshot""></a></p>

<p>What can I do?</p>
","8858356","","8858356","","2018-04-03 09:28:26","2020-04-07 16:48:30","Making a spreadsheet out of JSON data using DataFrame from pandas","<python><python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 3.0","1"
"49373537","1","49414489","","2018-03-19 22:37:57","","2","712","<p>I have two columns ""ID"" and ""division"" as shown below. </p>

<pre><code>df = pd.DataFrame(np.array([['111', 'AAA'],['222','AAA'],['333','BBB'],['444','CCC'],['444','AAA'],['222','BBB'],['111','BBB']]),columns=['ID','division'])

    ID  division
0   111 AAA
1   222 AAA
2   333 BBB
3   444 CCC
4   444 AAA
5   222 BBB
6   111 BBB
</code></pre>

<p>The expected output is as shown below where I need to pivot on the same column but the count is dependent on ""division"". This should be presented in a heatmap.</p>

<pre><code>    df = pd.DataFrame(np.array([['0','2','1','1'],['2','0','1','1'],['1','1','0','0'],['1','1','0','0']]),columns=['111','222','333','444'],index=['111','222','333','444'])

    111 222 333 444
111 0   2   1   1
222 2   0   1   1
333 1   1   0   0
444 1   1   0   0
</code></pre>

<p>So, technically I am doing an overlap between ID's with respect to division.</p>

<p>Example: 
The highlighted box in red where the overlap between 111 and 222 ID's is 2(AAA and BBB). where as the overlap between 111 and 444 is 1 (AAA highlighted in the black box).</p>

<p><a href=""https://i.stack.imgur.com/Wp8wO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Wp8wO.png"" alt=""enter image description here""></a></p>

<p>I could do this in excel in 2 steps.Not sure if below one helps.
Step1:=<code>SUM(COUNTIFS($B$2:$B$8,$B2,$A$2:$A$8,$G2),COUNTIFS($B$2:$B$8,$B2,$A$2:$A$8,H$1))-1</code>
Step2:=<code>IF($G12=H$1,0,SUMIFS(H$2:H$8,$G$2:$G$8,$G12))</code></p>

<p>But is there any way that we can do it in Python using dataframes.
Appreciate your help</p>

<p><strong>Case-2</strong></p>

<pre><code>if df = pd.DataFrame(np.array([['111', 'AAA','4'],['222','AAA','5'],['333','BBB','6'],
                            ['444','CCC','3'],['444','AAA','2'], ['222','BBB','2'],
                            ['111','BBB','7']]),columns=['ID','division','count'])

   ID   division count
0   111  AAA      4
1   222  AAA      5
2   333  BBB      6
3   444  CCC      3
4   444  AAA      2
5   222  BBB      2
6   111  BBB      7
</code></pre>

<p>Expected output would be</p>

<pre><code>df_result = pd.DataFrame(np.array([['0','18','13','6'],['18','0','8','7'],['13','8','0','0'],['6','7','0','0']]),columns=['111','222','333','444'],index=['111','222','333','444'])

   111 222  333 444
111 0   18  13  6
222 18  0   8   7
333 13  8   0   0
444 6   7   0   0
</code></pre>

<p>Calculation: Here there is an overlap between 111 and 222 with respect to divisions AAA and BBB hence the sum would be 4+5+2+7=18</p>
","8194358","","4829258","","2018-03-21 20:21:41","2018-03-22 12:53:45","Python dataframe: pivot on same column","<python><python-3.x><pandas><dataframe>","1","2","0","","","CC BY-SA 3.0","1"
"48780369","1","","","2018-02-14 05:40:47","","1","709","<p>I have a csv in which I have two columns representing start date: <code>st_dt</code> and end date: 'end_dt` , I have to subtract these columns to get the number of weeks. I tried iterating through columns using pandas, but it seems my output is wrong.</p>

<pre><code> st_dt                 end_dt
---------------------------------------
20100315           20100431
</code></pre>
","4950830","","7932273","","2018-02-14 06:19:56","2018-02-14 06:40:23","Python Subtracting two columns with date data, from csv to get number of weeks , months?","<python-3.x><pandas>","1","5","1","","","CC BY-SA 3.0","1"
"49326927","1","49327103","","2018-03-16 17:59:42","","2","699","<p>This is probably a very basic question but I haven't been able to find the answer so here goes...</p>

<p>Question:</p>

<p>Is there an say way to sort the values alphabetically while also removing any duplicate instances?</p>

<p>Here's what I have:</p>

<pre><code>data = ['Car | Book | Apple','','Book | Car | Apple | Apple']
df = pd.DataFrame(data,columns=['Labels']
print(df)

    Labels
0   Car | Book | Apple
1   
2   Book | Car | Apple | Apple
</code></pre>

<p>Desired Output:</p>

<pre><code>    Labels
0   Apple | Book | Car
1   
2   Apple | Book | Car
</code></pre>

<p>Thanks!</p>
","9448079","","9209546","","2018-03-17 15:05:46","2018-03-17 15:05:46","How to sort values within Pandas DF Column and remove duplicates","<python><python-3.x><pandas><sorting>","3","0","","","","CC BY-SA 3.0","1"
"57702718","1","57711340","","2019-08-29 03:47:59","","1","693","<pre><code>Name = [list(['Amy', 'A', 'Angu']),
          list(['Jon', 'Johnson']),
          list(['Bob', 'Barker'])]

Other =  [list(['Amy', 'Any', 'Anguish']),
        list(['Jon', 'Jan']),
        list(['Baker', 'barker'])]

import pandas as pd
df = pd.DataFrame({'Other' : Other,  
                  'ID': ['E123','E456','E789'], 
                  'Other_ID': ['A123','A456','A789'],
                  'Name' : Name,

                 })

    ID              Name    Other               Other_ID
0   E123    [Amy, A, Angu]  [Amy, Any, Anguish] A123
1   E456    [Jon, Johnson]  [Jon, Jan]          A456
2   E789    [Bob, Barker]   [Baker, barker]     A789
</code></pre>

<p>I have the df as seen above. I want to make columns <code>ID</code>, <code>Name</code> and <code>Other</code> into a dictionary with they key being <code>ID</code>. I tried this according to <a href=""https://stackoverflow.com/questions/18012505/python-pandas-dataframe-columns-convert-to-dict-key-and-value"">python pandas dataframe columns convert to dict key and value</a></p>

<pre><code>todict = dict(zip(df.ID, df.Name))
</code></pre>

<p>Which is close to what I want </p>

<pre><code>{'E123': ['Amy', 'A', 'Angu'],
 'E456': ['Jon', 'Johnson'],
 'E789': ['Bob', 'Barker']}
</code></pre>

<p>But I would like to get this output that includes values from <code>Other</code> column</p>

<pre><code>{'E123': ['Amy', 'A', 'Angu','Amy', 'Any','Anguish'],
 'E456': ['Jon', 'Johnson','Jon','Jan'],
'E789':  ['Bob', 'Barker','Baker','barker']
}
</code></pre>

<p>And If I put the third column <code>Other</code> it gives me errors</p>

<pre><code>todict = dict(zip(df.ID, df.Name, df.Other))
</code></pre>

<p>How do I get the output I want?</p>
","","user11962395","","user11962395","2019-08-29 04:13:07","2019-08-29 13:32:07","turn three columns into dictionary python","<python-3.x><pandas><list><dictionary><zip>","1","0","","","","CC BY-SA 4.0","1"
"52593475","1","","","2018-10-01 14:37:31","","0","684","<p>I am looking to create new columns in a pandas dataframe based on other column value using apply.  I receive this error and I don't understand why: </p>

<pre><code>File ""C:\dev\Anaconda3\lib\site-packages\pandas\core\frame.py"", line 2448, in _setitem_array
    raise ValueError('Columns must be same length as key')
ValueError: Columns must be same length as key
</code></pre>

<p>Am I misunderstanding the apply function?  Can you update/create multiple columns using a single apply call?</p>

<p>Here is my sample data:</p>

<pre><code>import pandas as pd

x = pd.DataFrame({'VP': ['Brian', 'Sarah', 'Sarah', 'Brian', 'Sarah'],
                  'Director': ['Jim', 'Ian', 'Ian', 'Jim', 'Jerry'],
                  'Requester': ['Kelly', 'Dave', 'Jordan', 'Matt', 'Rob'],
                  'VP from Query': ['Jordan', 'Justin', 'Sarah', 'Brian', 'Sarah'],
                  'Director from Query': ['Other', 'Other', 'Ian', 'Jim', 'Jerry'],
                  'Requester from Query': ['Kelly', 'Dave', 'Jordan', 'Matt', 'Rob']
                  })
x = x[['VP', 'Director', 'Requester', 'VP from Query', 'Director from Query', 'Requester from Query']]


def set_suggested_hierarchy(row):
    if row['VP'] != row['VP from Query']:
        return row[['VP', 'Director']]
    else:
        return row[['VP from Query', 'Director from Query']]


x[['Suggested VP', 'Suggested Director']] = x.apply(lambda row: set_suggested_hierarchy(row), axis=1)
</code></pre>

<p>Thank you so much</p>
","6770704","","","","","2018-10-01 14:50:51","Create new columns in pandas dataframe using apply","<python-3.x><pandas><dataframe><lambda><apply>","3","0","","","","CC BY-SA 4.0","1"
"49327485","1","","","2018-03-16 18:34:28","","1","680","<p>I wanted to know is there a way to open an excel workbook through <strong>PANDAS</strong> update some values in the original file keeping all data validations, formulas and formatting <strong>as it was in the original file</strong> and save it?</p>

<p>I can see similar question was posted here, but didn't got an answer and omits special fields:</p>

<p><a href=""https://stackoverflow.com/questions/48573622/pandas-to-change-the-colum-value-withoutchanging-the-format"">pandas to change the colum value withoutchanging the format</a></p>

<p>I do understand this can be done through writer as described here:</p>

<p><a href=""https://stackoverflow.com/questions/25479159/changing-formats-of-contents-in-columns-in-an-existing-excel-workbook"">Changing formats of contents in columns in an existing Excel workbook</a></p>

<p>but this would mean I need to redesign whole workbook in python with formatting, data validations countifs, etc which is just inhumane for the workbook I have to deal with. </p>

<p>Can this be achieved?</p>
","7273886","","","","","2018-03-16 18:34:28","Pandas - How to change values in the original excel workbook without changing the formatting","<python-3.x><pandas><jupyter-notebook><xlrd>","0","1","","","","CC BY-SA 3.0","1"
"49134216","1","49148171","","2018-03-06 15:18:34","","2","672","<p>I have a very simple sample of data/labels, the problem I'm having is that the decision tree generated (pdf) is repeating the class name:</p>

<pre><code>from sklearn import tree
from sklearn.externals.six import StringIO  
import pydotplus

features_names = ['weight', 'texture']
features = [[140, 1], [130, 1], [150, 0], [110, 0]]
labels = ['apple', 'apple', 'orange', 'orange']

clf = tree.DecisionTreeClassifier()
clf.fit(features, labels)

dot_data = StringIO()
tree.export_graphviz(clf, out_file=dot_data, 
                         feature_names=features_names,  
                         class_names=labels,  
                         filled=True, rounded=True,  
                         special_characters=True,
                         impurity=False)

graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) 
graph.write_pdf(""apples_oranges.pdf"")
</code></pre>

<p>The resulting pdf looks like:</p>

<p><a href=""https://i.stack.imgur.com/fV0s7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fV0s7.png"" alt=""enter image description here""></a></p>

<p>So, the problem is pretty obvious, it's apple for both possibilities. What am I doing wrong?</p>

<p>From the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html"" rel=""nofollow noreferrer"">DOCS</a>: </p>

<blockquote>
  <p>list of strings, bool or None, optional (default=None)<br>
  Names of each of the target classes in ascending numerical order. Only relevant for classification and not supported for multi-output. If True, shows a symbolic representation of the class name.</p>
</blockquote>

<p>""...ascending numerical order"" this doesn't make much sense for me, if I change the <code>kwarg</code> to:</p>

<pre><code>class_names=sorted(labels)
</code></pre>

<p>The result is the same (obvious in this case).</p>
","7801160","","7801160","","2018-03-06 15:38:43","2018-03-07 09:20:25","decision tree repeating class names","<python><python-3.x><decision-tree><sklearn-pandas>","1","3","1","","","CC BY-SA 3.0","1"
"41207249","1","41207275","","2016-12-18 09:59:10","","2","665","<p>I am trying to analyze some data related to air-routes using pandas. So I have two dataframes:</p>

<pre><code>print(airports.head())

           IATA/FAA           Country
Airport ID                           
1               GKA  Papua New Guinea
2               MAG  Papua New Guinea
3               HGU  Papua New Guinea
4               LAE  Papua New Guinea
5               POM  Papua New Guinea

print(routes.head())

        Source airport Destination airport
Airline                                   
2B                 AER                 KZN
2B                 ASF                 KZN
2B                 ASF                 MRV
2B                 CEK                 KZN
2B                 CEK                 OVB
</code></pre>

<p>Now I want to add two more columns to the dataframe <code>routes</code>: ""SA country"" which represents the country of the source airport and ""DA country"" which represents the country of the destination airport. For a given <code>IATA/FAA</code>, the country can somehow be extracted from the dataframe <code>airports</code>. However, I am not able to understand that ""somehow"". Any ideas?</p>
","2396502","","2396502","","2016-12-18 10:22:33","2016-12-18 10:22:33","How to assign to values to column in pandas dataframe based on columns in other dataframe?","<python><python-3.x><pandas><multiple-columns>","1","0","1","","","CC BY-SA 3.0","1"
"50065450","1","50066006","","2018-04-27 15:14:50","","1","661","<p>I am trying to read multiple excel files. Each time one excel file is read I would like to append it to the other excel file. At the end, I should end up with one dataframe which has the content of all excel files.</p>

<p>How can I do that in a for loop?</p>

<p>Here is my attempt:</p>

<pre><code>for i in range(1,10):
    temp = pd.read_excel(path[i])
    temp_final=temp
</code></pre>

<p>The idea here is to have temp_final containing the content of all excel files. Something similar to <code>temp_final=[excelfile1, excelfile2]</code> <code>pd.concat(temp_final)</code></p>

<p>I would welcome any idea on how I can finish this <code>for</code> loop. Many Thanks</p>
","9478743","","9209546","","2018-04-27 15:48:19","2019-06-08 17:57:45","Reading multiple excel files using a loop and append","<python><python-3.x><pandas><for-loop><dataframe>","3","0","","","","CC BY-SA 3.0","1"
"57608526","1","","","2019-08-22 11:40:17","","2","657","<p>I am using the <code>recordlinkage</code> library with <code>pandas</code>. In the first step, I created indexes, the parameters are:</p>

<pre><code>indexer = recordlinkage.Index()
indexer.block(fr.iloc[:, 2])
pairs = indexer.index(fr)
</code></pre>

<p>Notice, that the page of project indicates the following usage:</p>

<pre><code> indexer = recordlinkage.Index()
 indexer.block('orignal_link')
 candidate_links = indexer.index(dfA, dfB)
</code></pre>

<p>I replaced the column label with the same position (<code>.iloc</code>). It couldn't find any column name. However, when I specifically asked for the column names, I got the following output:</p>

<pre><code>Index(['_id', 'doi', 'orignal_link', 'title', 'authors', 'affiliation', 'citation', 'abstract', 'paper', 'references'], dtype='object')
</code></pre>

<p>Anyway, after replacement the produced error is following: </p>

<pre><code>KeyError: ""None of [Index([('https://aip.scitation.org/doi/full/10.1063/1.5097416', 'https://aip.scitation.org/doi/full/10.1063/1.5110298', 'https://aip.scitation.org/doi/full/10.1063/1.5096407', 'https://aip.scitation.org/doi/full/10.1063/1.5093609', 'https://aip.scitation.org/doi/full/10.1063/1.5094748', 'https://aip.scitation.org/doi/full/10.1063/1.5098007', 'https://aip.scitation.org/doi/full/10.1063/1.5095979', 'https://aip.scitation.org/doi/full/10.1063/1.5109249', 'https://iopscience.iop.org/article/10.1088/1367-2630/12/7/073006/meta')], dtype='object')] are in the [columns]""
</code></pre>

<p>If it's not finding values, how can it print them out?</p>

<p>Any help?</p>

<p>Thank you</p>
","11566849","","7122272","","2019-08-22 16:09:19","2019-08-22 16:09:19","KeyError: ""None of [Index(...')] are in the [columns]""","<python-3.x><pandas>","0","0","","","","CC BY-SA 4.0","1"
"48799212","1","","","2018-02-15 02:33:51","","1","656","<p>I have a .csv file that contains columns: NAME, DATE, INFO, STATS, MORE_INFO</p>

<p>I want to import the .csv file (either with Python 3.6 code, or Pandas)</p>

<p>Then I need to filter the columns: NAME, DATE, INFO</p>

<p>Here's where I am having trouble, I need to take the 'DATE' column of my .csv file which lists a date as 1/16/2016 and separate it so I can select only the MONTH for my filtered .csv output file.</p>

<p>My final columns to read like NAME, MONTH, INFO</p>

<p>The entire 'DATE' column is split by two years - 2016 and 2017. I need to take the NAME, MONTH, INFO for 2016 and save it to a new .csv file, and the same thing for 2017 so each year is separated by grouped months.</p>

<pre><code>averageData = pd.read_csv('Data.csv', sep = ',', parse_dates=True)
df1 = averageData.loc[:, [""DATE"", ""NAME"", ""INFO""]]
df1[""DATE""] = pd.to_datetime(df1[""DATE""])
stripdate = datetime.datetime.strptime(['01-01-2016'], '%Y-%m-%d')
</code></pre>

<p>The variable stripdate where datetime.datetime.strptime() is supposed to rearrange the date in the right format is where I am misunderstanding something. How do I just group everything by MONTH of the dates listed under the 'DATE' column from my .csv file. Then further group them by year in new .csv files?</p>
","9297791","","","","","2018-02-15 05:56:53","Split Date from .CSV file: 1/16/2016 to Year: 2016, Day: 16, Month: 1","<python-3.x><pandas><datetime><split><strptime>","1","0","","","","CC BY-SA 3.0","1"
"40620072","1","40620204","","2016-11-15 21:34:14","","2","652","<p>I am using <code>apply</code> on my data frame <code>my_df</code> like below:</p>

<pre><code>my_df['column_C'] = my_df.apply(lambda x : 'hello' if x['column_B'] is None else x['column_B'] )
</code></pre>

<p>I want:</p>

<pre><code>  if x['column_B'] = None -&gt; return 'hello'
  if x['column_B'] != None -&gt; return x['column_B']
</code></pre>

<p>Then I got the following errors:</p>

<pre><code>&lt;ipython-input-31-aa087c9a635e&gt; in &lt;lambda&gt;(x)
----&gt; 1 my_df['column_C'] = my_df.apply(lambda x : 'hello' if x['column_B'] is None else x['column_B'] )

/usr/local/lib/python3.4/dist-packages/pandas/core/series.py in __getitem__(self, key)
    599         key = com._apply_if_callable(key, self)
    600         try:
--&gt; 601             result = self.index.get_value(self, key)
    602 
    603             if not is_scalar(result):

/usr/local/lib/python3.4/dist-packages/pandas/indexes/base.py in get_value(self, series, key)
   2187             # python 3
   2188             if is_scalar(key):  # pragma: no cover
-&gt; 2189                 raise IndexError(key)
   2190             raise InvalidIndexError(key)
   2191 

IndexError: ('column_B', 'occurred at index column_A')
</code></pre>

<p>Does anyone know what I did wrong here?</p>
","3993270","","","","","2016-11-15 22:12:58","python: pandas apply function: InvalidIndexError","<python-3.x><pandas><apply>","1","2","","","","CC BY-SA 3.0","1"
"50070789","1","","","2018-04-27 21:56:26","","1","644","<p>Hi there all machine learning and python experts. I seek your help.</p>

<p>I have recently seen into google ml crash course. They recommended to get familiar with pandas before starting the crash course.</p>

<p>So, I have installed pip v8.1.1 using python v3.5.2 and pandas from pip. And also installed pytest 3.5.1 using pip.</p>

<p>But when I tried to use the recommended test suite given in the documentation of pandas <a href=""http://pandas.pydata.org/pandas-docs/stable/install.html"" rel=""nofollow noreferrer"">installation page</a> , I got errors.</p>

<pre><code>user@host:~$ python3
Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; pd.test()
running: pytest --skip-slow --skip-network /home/ashraf/.local/lib/python3.5/site-packages/pandas
==================================================================== test session starts ====================================================================
platform linux -- Python 3.5.2, pytest-3.5.1, py-1.5.3, pluggy-0.6.0
rootdir: /home/user, inifile:
collected 0 items / 1 errors                                                                                                                                

========================================================================== ERRORS ===========================================================================
_____________________________________________________________________ ERROR collecting  _____________________________________________________________________
.local/lib/python3.5/site-packages/_pytest/config.py:334: in _getconftestmodules
    return self._path2confmods[path]
E   KeyError: local('/home/user/.local/lib/python3.5/site-packages/pandas/tests/io')

During handling of the above exception, another exception occurred:
.local/lib/python3.5/site-packages/_pytest/config.py:365: in _importconftest
    return self._conftestpath2mod[conftestpath]
E   KeyError: local('/home/user/.local/lib/python3.5/site-packages/pandas/tests/io/conftest.py')

During handling of the above exception, another exception occurred:
.local/lib/python3.5/site-packages/_pytest/config.py:371: in _importconftest
    mod = conftestpath.pyimport()
.local/lib/python3.5/site-packages/py/_path/local.py:668: in pyimport
    __import__(modname)
.local/lib/python3.5/site-packages/_pytest/assertion/rewrite.py:213: in load_module
    py.builtin.exec_(co, mod.__dict__)
.local/lib/python3.5/site-packages/pandas/tests/io/conftest.py:3: in &lt;module&gt;
    import moto
E   ImportError: No module named 'moto'

During handling of the above exception, another exception occurred:
.local/lib/python3.5/site-packages/py/_path/common.py:377: in visit
    for x in Visitor(fil, rec, ignore, bf, sort).gen(self):
.local/lib/python3.5/site-packages/py/_path/common.py:429: in gen
    for p in self.gen(subdir):
.local/lib/python3.5/site-packages/py/_path/common.py:418: in gen
    dirs = self.optsort([p for p in entries
.local/lib/python3.5/site-packages/py/_path/common.py:419: in &lt;listcomp&gt;
    if p.check(dir=1) and (rec is None or rec(p))])
.local/lib/python3.5/site-packages/_pytest/main.py:434: in _recurse
    ihook = self.gethookproxy(path)
.local/lib/python3.5/site-packages/_pytest/main.py:338: in gethookproxy
    my_conftestmodules = pm._getconftestmodules(fspath)
.local/lib/python3.5/site-packages/_pytest/config.py:348: in _getconftestmodules
    mod = self._importconftest(conftestpath)
.local/lib/python3.5/site-packages/_pytest/config.py:376: in _importconftest
    raise ConftestImportFailure(conftestpath, sys.exc_info())
E   _pytest.config.ConftestImportFailure: ImportError(""No module named 'moto'"",)
E     File ""/home/user/.local/lib/python3.5/site-packages/_pytest/assertion/rewrite.py"", line 213, in load_module
E       py.builtin.exec_(co, mod.__dict__)
E     File ""/home/user/.local/lib/python3.5/site-packages/pandas/tests/io/conftest.py"", line 3, in &lt;module&gt;
E       import moto
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 errors during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
================================================================== 1 error in 0.59 seconds ==================================================================
</code></pre>

<p>Now, how may I fix it, what may I be doing wrong or missing any package? Please, help me in it.</p>
","3270433","","","","","2018-04-28 15:28:51","Keyerror when trying to testsuite pytest with pandas","<python><python-3.x><pandas><machine-learning><pytest>","1","7","","","","CC BY-SA 3.0","1"
"57509968","1","57510055","","2019-08-15 12:53:44","","4","642","<p>I have a list of lists of tuples, where every tuple is of equal length, and I need to convert the tuples to a Pandas dataframe in such a way that the columns of the dataframe are equal to the length of the tuples, and each tuple item is a row entry across the columns.</p>

<p>I have consulted other questions on this topic (e.g., <a href=""https://stackoverflow.com/questions/44393344/convert-a-list-of-lists-of-tuples-to-pandas-dataframe"">Convert a list of lists of tuples to pandas dataframe</a>, <a href=""https://stackoverflow.com/questions/50467297/list-of-list-of-tuples-to-pandas-dataframe"">List of list of tuples to pandas dataframe</a>, <a href=""https://stackoverflow.com/questions/22530507/split-list-of-tuples-in-lists-of-list-of-tuples"">split list of tuples in lists of list of tuples</a>) unsuccessfully.</p>

<p>The closest I get is with list comprehension from a different question on Stack Overflow:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd

tupList = [[('commentID', 'commentText', 'date'), ('123456', 'blahblahblah', '2019')], [('45678', 'hello world', '2018'), ('0', 'text', '2017')]]

# Trying list comprehension from previous stack question:
pd.DataFrame([[y for y in x] for x in tupList])

</code></pre>

<p>But this yields the unintended result:</p>

<pre class=""lang-py prettyprint-override""><code>    0                                 1
0   (commentID, commentText, date)    (123456, blahblahblah, 2019)
1   (45678, hello world, 2018)        (0, text, 2017)
</code></pre>

<p>When the expected result is as follows:</p>

<pre><code>      0            1                 2
0     commentID    commentText       date
1     123456       blahblahblah      2019
2     45678        hello world       2018
3     0            text              2017
</code></pre>

<p>In sum: I need columns equal to the length of each tuple (in the example, 3), where each item within the tuple is a row entry across the columns.</p>

<p>Thanks!</p>
","10963911","","","","","2019-08-15 13:04:40","List of LISTS of tuples to Pandas dataframe?","<python><python-3.x><pandas><tuples><list-comprehension>","4","0","","","","CC BY-SA 4.0","1"
"48572965","1","48573211","","2018-02-01 22:44:00","","1","640","<p>I have a pandas dataframe that contains the results of computation and need to:</p>

<ul>
<li>take the maximum value of a column and for that value find the maximum value of another column</li>
<li>take the minimum value of a column and for that value find the maximum value of another column</li>
</ul>

<p>Is there a more efficient way to do it?</p>

<p>Setup
</p>

<pre><code>metrictuple = namedtuple('metrics', 'prob m1 m2')
l1 =[metrictuple(0.1, 0.4, 0.04),metrictuple(0.2, 0.4, 0.04),metrictuple(0.4, 0.4, 0.1),metrictuple(0.7, 0.2, 0.3),metrictuple(1.0, 0.1, 0.5)]
df = pd.DataFrame(l1)
# df
#   prob   m1    m2
#0   0.1  0.4  0.04
#1   0.2  0.4  0.04
#2   0.4  0.4  0.10
#3   0.7  0.2  0.30
#4   1.0  0.1  0.50

tmp = df.loc[(df.m1.max() == df.m1), ['prob','m1']]
res1 = tmp.loc[tmp.prob.max() == tmp.prob, :].to_records(index=False)[0]
#(0.4, 0.4)
tmp = df.loc[(df.m2.min() == df.m2), ['prob','m2']]
res2 = tmp.loc[tmp.prob.max() == tmp.prob, :].to_records(index=False)[0]
#(0.2, 0.04)
</code></pre>
","7426965","","9209546","","2018-03-02 03:48:53","2018-03-02 03:48:53","Efficiently combine min/max on different columns of a pandas dataframe","<python><python-3.x><pandas>","1","3","","","","CC BY-SA 3.0","1"
"56607878","1","56608109","","2019-06-15 06:19:07","","0","640","<p>In addition on my previous question <a href=""https://stackoverflow.com/questions/55099272/search-for-value-in-all-dataframe-columns-except-first-column-and-add-new-co"">Search for value in all DataFrame columns (except first column !) and add new column with matching column name</a> (where I used a static keyword)</p>

<p>I'd like to check if the string in the first column is contained in one of the another columns in the same row and then add a new column with the matching column name(s). All columns names of all matched values!</p>

<p>Now i'm using this with a static keyword:</p>

<pre><code>keyword='123'
f = lambda row: row.apply(str).str.replace(""."","""").str.contains(keyword ,na=False, flags=re.IGNORECASE)
df1 = df.iloc[:,1:].apply(f, axis=1)

df.insert(loc=1, column='Matching_Columns', value=df1.dot(df.columns[1:] + ', ').str.strip(', '))
</code></pre>

<p>Sample:</p>

<p>Input:</p>

<pre><code>key | col_B | col_C | col_D | col_E
------------------------------------
123 | abcd  | 12345 | fght  | 7890
567 | tdfe  | 6353  | 0567  | 56789
</code></pre>

<p>Output:</p>

<pre><code>key | match       | col_B | col_C | col_D | col_E
-------------------------------------------------
123 | col_C       | abcd  | 12345 | fght  | 7890
567 | col_D,col_E | tdfe  | 6353  | 0567  | 56789
</code></pre>

<p>Any help much appreciated!</p>
","439621","","439621","","2019-06-15 06:54:07","2019-06-15 09:33:39","Check if string in one column is contained in string of another column in the same row and add new column with matching column name","<python-3.x><pandas><dataframe>","3","2","","","","CC BY-SA 4.0","1"
"56888869","1","56889040","","2019-07-04 13:19:32","","0","639","<p>I am trying to create a pivot table plot where the x-axis is custom sorted.
My code:</p>

<pre><code>import pandas as pd
data_dict = {
    'x' : [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],
    'y' : [9,10,11,12,13,14,15,1,2,3,4,5,6,7,8]
}
res_table = pd.DataFrame(data_dict)

df_pivot = pd.pivot_table(res_table, 
                    index='x', values='y', 
                    aggfunc='sum')

ax = df_pivot.plot(kind='line', marker='*', title='y data')

ax.set(xlabel='x', ylabel='y')
ax.legend(bbox_to_anchor=(1,1), loc=""upper left"")
ax.figure
</code></pre>

<p>This code gives this plot:</p>

<p><a href=""https://i.stack.imgur.com/Fr9Yr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Fr9Yr.png"" alt=""graph""></a></p>

<p>Now I want to change the X-axis so that it start with 8 to 15 and then follows with 1 to 7. So that the graph becomes a straight line.</p>

<p>I tried this code: (<a href=""https://stackoverflow.com/questions/37147822/pandas-pivot-table-nested-sorting"">Pandas pivot table Nested Sorting</a>)</p>

<pre><code>order = [8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7]
pd.concat([res_table.loc[res_table['x'] == val, :].set_index(['x']) for val in order])
</code></pre>

<p>But that doesn't seem to change anything.</p>

<p>*Edit: Of course I can add an extra column with the correct index and plot the graph using that but I would prefer the graph to show the real x value.</p>

<p>**Edit: I celebrated a bit to early.
My real code has extra columns to plot multiple of these lines on one graph</p>

<pre><code>data_dict = {
    'x' : [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],
    'y' : [9,10,11,12,13,14,15,1,2,3,4,5,6,7,8,29,30,31,32,33,34,35,21,22,23,24,25,26,27,28],
    'z' : [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2]
}
res_table = pd.DataFrame(data_dict)
df_pivot = pd.pivot_table(res_table, 
                    index='x', values='y', 
                    aggfunc='sum', columns='z')

cut_val = 8
tmp_df = pd.concat((df_pivot.loc[cut_val:], 
                     df_pivot.loc[:cut_val-1])).reset_index()
print(tmp_df)
fig, ax = plt.subplots()

ax.plot(tmp_df.index, tmp_df.y, '-o')
ticks = ax.get_xticks()
ax.set_xticklabels(tmp_df.loc[ticks, 'x'])
ax.figure
</code></pre>

<p>The code stops at this line with a key error on tmp_df.y:</p>

<pre><code>ax.plot(tmp_df.index, tmp_df.y, '-o')
</code></pre>

<p>Without the columns argument the code works fine</p>

<p>In the original code the graph looks like this
<a href=""https://i.stack.imgur.com/pq3Pu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pq3Pu.png"" alt=""enter image description here""></a></p>
","2290132","","2290132","","2019-07-04 14:58:57","2019-07-04 15:01:58","Python Pivot tables custom sort x-axis","<python-3.x><pandas><pivot-table>","1","2","","","","CC BY-SA 4.0","1"
"49518229","1","49520642","","2018-03-27 16:37:45","","1","639","<p>Migrating from python 2.7 to python 3.x, This code breaks Source:  </p>

<p><a href=""https://stackoverflow.com/questions/38931566/pandas-style-background-gradient-both-rows-and-columns"">pandas style background gradient both rows and columns</a></p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import colors

def background_gradient(s, m=None, M=None, cmap='PuBu', low=0, high=0):
    print(s.shape)
    if m is None:
        m = s.min().min()
    if M is None:
        M = s.max().max()
    rng = M - m
    norm = colors.Normalize(m - (rng * low),
                            M + (rng * high))
    normed = s.apply(norm)
    cm = plt.cm.get_cmap(cmap)
    c = normed.applymap(lambda x: colors.rgb2hex(cm(x)))
    ret = c.applymap(lambda x: 'background-color: %s' % x)
    return ret

df = pd.DataFrame([[3,2,10.3,4],[20,1,3.5,2],[5,4,6.9,1]])
df.style.apply(background_gradient, axis=None)
</code></pre>

<p>This is last line of stack trace:</p>

<pre><code>TypeError: (""float() argument must be a string or a number, not 'SingleBlockManager'"", 'occurred at index 0')
</code></pre>
","428862","","428862","","2018-03-27 16:44:32","2018-03-27 19:05:19","Python 3 break pandas style for gradients, 'SingleBlockManager' is cause","<python><python-3.x><pandas><matplotlib>","1","4","","","","CC BY-SA 3.0","1"
"56951030","1","56953678","","2019-07-09 10:57:44","","1","617","<p>I have a pandas dataframe and I want to iterate over rows of this dataframe, get slices of data, based on a value in a column. </p>

<p>To say it more brief, I have a dataframe like below: </p>

<pre><code>districts = [['dist','name','sale','purchase'],['dis1','avelin',2300, 1400],['dis2','matri', 4300, 2500], ['dis1', 'texi', 1500, 1700],['dis2','timi', 2300, 1400]]
</code></pre>

<p>I'd like to iterate over all rows and extract dataframes based on 'dist' column.<br>
the output should look like below:</p>

<pre><code>dis1 = [[2300, 1400], [1500,1700]]
dis2 = [[4300,2500],[2300,1400]]  
</code></pre>
","8794465","","8928024","","2019-07-09 11:16:15","2019-07-09 13:26:09","how to slice a dataframe based on a specific values in a column in an iterative way and create a new dataframe?","<python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"56656349","1","","","2019-06-18 20:08:03","","0","609","<p>I have two dataframes, df1 and df2, which have the exact same columns and most of the time the same values for each key.</p>

<blockquote>
<pre><code>Country   A   B   C   D   E   F   G   H   Key
Argentina xylo    262 4632    0   0   26.12   2   0   Argentinaxylo
Argentina phone   6860    155811  48  0   4375.87 202 0   Argentinaphone
Argentina land    507 1803728 2   117 7165.810566 3   154 Argentinaland
Australia xylo    7650    139472  69  0   16858.42    184 0   Australiaxylo
Australia mink    1284    2342788 1   0   39287.71    53  0   Australiamink


Country   A   B   C   D   E   F   G   H   Key
Argentina xylo    262 4632    0   0   26.12   2   0   Argentinaxylo
Argentina phone   6860    155811  48  0   4375.87 202 0   Argentinaphone
Argentina land    507 1803728 2   117 7165.810566 3   154 Argentinaland
Australia xylo    7650    139472  69  0   16858.42    184 0   Australiaxylo
Australia mink    1284    2342788 1   0   39287.71    53  0   Australiamink
</code></pre>
</blockquote>

<p>I want a snippet that compares the keys (key = column Country + column A) in each dataframe against each other and calculates the percent difference for each column B-H, if there is any. If there isn't, output nothing.</p>
","5957744","","5957744","","2019-06-18 20:10:56","2019-06-19 05:49:35","Compare two dataframes based off of key","<python><python-3.x><pandas><dataframe><analytics>","1","2","","","","CC BY-SA 4.0","1"
"49412811","1","","","2018-03-21 17:20:49","","0","608","<p>I try to make this code work, but it have error:</p>

<pre><code>for myname in mn:
    mydf = df.loc[df.Teacher_name == myname]
    mydf.to_excel(writer, sheet_name = myname)
writer.save()

  File ""&lt;stdin&gt;"", line 4
    writer.save()
         ^
SyntaxError: invalid syntax
</code></pre>

<p>import=</p>

<pre><code>import pandas as pd
</code></pre>

<p>writer = </p>

<pre><code>writer = pd.ExcelWriter(""MyData.xlsx"", engine='xlsxwriter')
</code></pre>

<p>mn = </p>

<pre><code>mn = df['name'].unique().tolist()

mn = ['Teacher A', 'Teacher B', 'Teacher C']
</code></pre>

<p>To make df was used this query:</p>

<pre><code>df = pd.read_csv('XXX.csv', names=['Teacher_name', 'Studen_name','Extra'])
</code></pre>

<p>df otput:</p>

<pre><code>        Teacher_name      Student_name    Extra                                                    
1       Teacher A         Student 1       xxxx
2       Teacher A         Student 2       xxxx
3       Teacher B         Student 3       xxxx
4       Teacher C         Student 4       xxxx
</code></pre>
","2037081","","2037081","","2018-03-21 17:34:47","2018-03-21 17:34:47","Python (Pandas) - Syntax Error - Invalid Syntax - writer.save()","<python><python-3.x><pandas>","0","13","","","","CC BY-SA 3.0","1"
"57074037","1","57074195","","2019-07-17 10:43:55","","1","607","<p>I want to save a dataframe consisting of values with decimal value separated by dots to an excel file. In the excel file the values should be shown with a comma.
I know that I can include decimal = ',' when using dataframe.to_csv. But is there a comparable solution for dataframe.to_excel?</p>

<p>Here is my code. Df1 is a dataframe:</p>

<pre><code>df1 = pandas.DataFrame(topics1, index=topicnumber, columns= wordnumber)
with pandas.ExcelWriter(topics1_speicherpfad_excel) as writer:
     df1.to_excel(writer, sheet_name = ""Topics"")
</code></pre>
","9751594","","","","","2019-07-17 10:52:08","Convert decimal value to comma for pandas to excel","<python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"57228605","1","57228702","","2019-07-27 02:04:11","","1","605","<p>I have a dataframe with two columns as:</p>

<pre><code>CHILD   PARENT
1       2
2       3
3       4
10      11
11      12
</code></pre>

<p>I need to create a dictionary where I keep the top parent as key and all its descendents as set of values, like:</p>

<pre><code>4: [1,2,3]
12: [10,11]
</code></pre>

<p>I have been able to extract 12 and 4 as top parents from this dataframe, by code from following link: </p>

<p><a href=""https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe/36685531"">extract column value based on another column pandas dataframe</a></p>

<p>Now, I have no idea how to do this in python. In java I can do this by following a dfs. Any suggestions?</p>
","6843546","","11470719","","2019-07-27 12:49:14","2019-07-27 12:49:14","Creating dictionary of parent child pairs in pandas dataframe","<python><python-3.x><pandas>","2","2","1","","","CC BY-SA 4.0","1"
"57294793","1","","","2019-07-31 15:57:44","","0","604","<p>I'm trying to load a monthly mastercard statement into a csv file for my business using python. then create a program to send an in-house itemized statement to send to each user based on their spending for the month. How would I write multiple .csv files based on their card number.</p>

<p>I'm new to programming so i'm not entirely sure how to write to multiple csv files. I can do a single file but i'm not sure how to take the next step.</p>

<pre><code>import pandas as pd
import csv
df = pd.read_csv(""/users/user/documents/data.csv"")
df['Total'] = df.groupby('Card Number')['Posted Amount'].transform('sum')

body = df.groupby(['Card Number','Cardholder Name','Email','Total','Merchant Name'])['Posted Amount'].agg('sum')
</code></pre>

<p>when i hit print(body) the following information is displayed </p>

<pre><code>cardnumber,name,email,total,merchant_name,posted_amount
0001,user1,user1@email.com,1531,merchant 1,1530
                                merchant 2,1531
0002,user2,user2@tmail.com,500,merchant 5,200
                               merchant 6,250
                               merchant 7,50 
</code></pre>

<p>How can i write multiple csv files to be separated by each cardnumber or name? I'm relatively new to all of this, so if you need any more information let me know.</p>
","11799274","","","","","2019-07-31 16:55:18","Writing Multiple CSV files from dataframe","<python-3.x><pandas><csv>","1","0","","","","CC BY-SA 4.0","1"
"57048617","1","57048628","","2019-07-16 00:24:25","","0","600","<p>I want to find all the strings in my dataframe and I want to replace them with NaN values so that I can drop all associated NaN values with the function df.dropna(). For example, if I have the following data set:</p>

<pre><code>x = np.array([1,2,np.NaN,4,5,6,7,8,9,10])
z = np.array([1,2,np.NaN,4,5,np.NaN,7,8,9,""My Name is Jeff""])
y = np.array([""Hello World"",2,3,4,5,6,7,8,9,10])
</code></pre>

<p>I should first be able to dynamically replace all strings with np.nan so my output should be: </p>

<pre><code>x = np.array([1,2,np.NaN,4,5,6,7,8,9,10])
z = np.array([1,2,np.NaN,4,5,np.NaN,7,8,9,np.NaN])
y = np.array([np.NaN,2,3,4,5,6,7,8,9,10])
</code></pre>

<p>and then running df.dropna() (Assume that x,y,z reside in a data frame and not just separate variables) should allow me to have:</p>

<pre><code>x = np.array([2,4,5,7,8,9])
z = np.array([2,4,5,7,8,9])
y = np.array([2,4,5,7,8,9])
</code></pre>
","9712785","","2535611","","2019-07-16 00:41:42","2019-07-16 09:41:16","How do I replace all string values with NaN (Dynamically)?","<python><python-3.x><pandas><numpy><dataframe>","4","1","0","","","CC BY-SA 4.0","1"
"41174535","1","","","2016-12-15 22:42:11","","2","598","<p>I am reading a CSV file saved from an MS SQL query. When I read file using pandas, the last column has long string which is causing NaN values. I understand this could be from windows end of line returns and/or encoding issues. </p>

<p>I am a newbie and don't really understand if this is an encoding problem and how to change the read statement.  When I use standard Python <code>import csv</code> and save as utf-8 I don't have any issues re-opening and working with a txt file. Also don't find anything particularly helpful from <a href=""https://github.com/pandas-dev/pandas/issues/3501"" rel=""nofollow noreferrer"">this issue</a></p>

<pre><code>Location = r'newpositive.csv'
df = pd.read_csv(Location)
</code></pre>
","5285095","","240490","","2016-12-16 08:43:42","2016-12-16 08:43:42","pandas read_csv getting NaN values, using Python 3","<sql-server><python-3.x><csv><pandas>","0","3","","","","CC BY-SA 3.0","1"
"40987319","1","40987390","","2016-12-06 03:25:20","","3","596","<p>I have a dataframe which has more than 10 million raws composed of about 30 columns.</p>

<p>The first column is ID</p>

<pre><code>ID   C
1    1
1    2
1    3
1    2
1    3
2    1
2    5
2    9
2    0
2    1
</code></pre>

<p>I would like to extract only the first four rows of each ID(they are the newest inputs as it is already sorted)</p>

<p>I am currently using the below code, but unfortunately it is so slow as it takes about two hours to process about 5% of the data and it may take a day or so to process the whole data.</p>

<pre><code>df1 = pd.DataFrame() # an empty dataframe
for i in df.ID:   # df is the dataframe which contains the data
    df2 = df[df[""ID""]== i] 
    df2 = df2[0:4] # take the first four rows
    df_f = df1.append(df2) 
</code></pre>

<p>Are there an effecient way to do the same thing in a shorter time.</p>
","6511941","","","","","2016-12-06 03:44:47","How to subset a dataset in pandas dataframe?","<python><python-3.x><pandas>","1","2","1","","","CC BY-SA 3.0","1"
"56853636","1","56854240","","2019-07-02 13:30:02","","1","589","<p>For a pandas DataFrame with groups I want to keep all rows until the first occurence of a specific value (and discard all other rows).</p>

<p>MWE:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'A' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar', 'tmp'],
                   'B' : [0, 1, 0, 0, 0, 1, 0],
                   'C' : [2.0, 5., 8., 1., 2., 9., 7.]})
</code></pre>

<p>gives</p>

<pre><code>    A    B  C
0   foo  0  2.0
1   foo  1  5.0
2   foo  0  8.0
3   bar  0  1.0
4   bar  0  2.0
5   bar  1  9.0
6   tmp  0  7.0
</code></pre>

<p>and I want to keep all rows for each group (<code>A</code> is the grouping variable) until <code>B == 1</code> (including this row). So, my desired output is</p>

<pre><code>    A    B  C
0   foo  0  2.0
1   foo  1  5.0
3   bar  0  1.0
4   bar  0  2.0
5   bar  1  9.0
6   tmp  0  7.0
</code></pre>

<p><strong>How can I keep all rows of a grouped DataFrage meeting a certain criteria?</strong></p>

<p>I found <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.filter.html"" rel=""nofollow noreferrer"">how to drop specific groups not meeting a certain criteria (and keeping all other rows of all other groups)</a>, but not how to drop specific rows for all groups. The farest I got was to get the indices of the rows in each group, I want to keep: </p>

<pre><code>df.groupby('A').apply(lambda x: x['B'].cumsum().searchsorted(1))
</code></pre>

<p>resulting in</p>

<pre><code>A
bar    2
foo    1
tmp    1
</code></pre>

<p>Which isn't sufficient, as it does not return the actual data (and it might be better, if for <code>tmp</code> the result was <code>0</code>)</p>
","6256241","","","","","2019-07-02 14:01:34","How can I keep all rows of a grouped pandas DataFrage meeting a certain criteria?","<python-3.x><search><indexing><pandas-groupby><rows>","1","0","","","","CC BY-SA 4.0","1"
"56768072","1","56768135","","2019-06-26 08:05:34","","1","586","<p>have a df : </p>

<p>how to insert a column with increasing values to the whole table</p>

<p>df : </p>

<pre><code>   0 
1  a 
2  b
3  c 
</code></pre>

<p>expected ouput : </p>

<pre><code>        0         1
    1   a       pa001
    2   b       pa002
    3   c       pa003
</code></pre>

<p>......
for another part have a df and how to insert random intergers in range </p>

<p>like  <code>randint(3.5,4.5)</code>  to whole table</p>

<p>df : </p>

<pre><code>   0 
1  a 
2  b
3  c
</code></pre>

<p>output : </p>

<pre><code>   0   |   1
1  a   |  3.5
2  b   |  3.6
3  c   |  3.7
</code></pre>
","11124889","","","","","2019-06-26 08:29:16","How to add column in dataframe with increasing value consisting aplha-numeric and another column with random numbers","<python><python-3.x><pandas><dataframe>","1","2","","","","CC BY-SA 4.0","1"
"48881766","1","48882246","","2018-02-20 09:26:09","","3","581","<p>I have a DataFrame Pandas, which I'd like to group by data the most possible with combinations of columns A, B, C and D.</p>

<p>Let's say it has this form:</p>

<pre><code>      A   B   C   D   E   F   G        
0     Y   X   Y   Z   1   2   7
1     Y   X   Y   Z   3   4   8 
2     X   Y   U   V   1   1   1
3     X   Y   V   U   1   2   0
4     X   Z   Z   Z   1   8   1
</code></pre>

<p>First, I try to group by at higher level, so I try to group by ['A', 'B', 'C', 'D']. For the rows that haven't been regrouped, I try lower combinations like ['A', 'B', 'C'], ['A', 'B', 'D'], and so on. Finally I am using no combination but just grouping by ['A'] then ['B'] then ['C'] then ['D']. At this point, I have grouped data with each possible aggregation key implying A, B, C and D.</p>

<p>With this method, the desired output would be:</p>

<pre><code>      A   B   C   D           
0     Y   X   Y   Z    
1     X   Y     
2     X   
</code></pre>

<p>Is it possible to do these operations easily?</p>
","6033816","","6033816","","2018-02-20 10:13:53","2018-02-20 15:55:36","Pandas - Aggregate by each possible combination of keys","<python><python-3.x><pandas><dataframe>","1","2","2","","","CC BY-SA 3.0","1"
"57228543","1","","","2019-07-27 01:45:52","","0","580","<p>I'm looking for a way chart, using Plotly, data with date on X axis and a value from a table on the Y axis. Then I want to be able to group the data by week, day, month etc dynamically (i.e with a widget) like in the Plotly Histogram binning example <a href=""https://plot.ly/python/aggregations/#histogram-binning"" rel=""nofollow noreferrer"">https://plot.ly/python/aggregations/#histogram-binning</a></p>

<p>Having the same or similar functionality would be acceptable as long as I can have the chart do this dynamically i.e without needing to create a whole new plot</p>

<p>I've tried using the Histogram binning as outlined in the documentations however that doesn't work as histogram relies on counting a rows from a table rather than reading a value straight from a table and using that as the histogram height. </p>

<p>The same functionality doesn't seem to work with bar charts however</p>

<p>My data is arranged like this in a python pandas dataframe</p>

<pre><code>    Date           Count
0   2018-01-23     28418
1   2018-08-01     25403
</code></pre>

<p>The code itself right now is:</p>

<pre><code>data = [dict(
  x = final['Date'],
  y = final['Cage Poll [cases]'],
  autobinx = False,
  autobiny = True,
  marker = dict(color = 'rgb(220, 20, 127)'),
  name = 'test',
  type = 'histogram',
  normed = 'True',
  xbins = dict(
    #end = '2016-12-31 12:00',
    size = 'M1',
    #start = '1983-12-31 12:00'
  )
)]

layout = dict(
  paper_bgcolor = 'rgb(240, 240, 240)',
  plot_bgcolor = 'rgb(240, 240, 240)',
  title = '&lt;b&gt;Data sampled from daily reports&lt;/b&gt;',
  showlegend = True,
  xaxis = dict(
    title = 'Date',
    type = 'date'
  ),
  yaxis = dict(
    title = 'Count',
    type = 'linear'
  ),
  updatemenus = [dict(
        x = 0.1,
        y = 1.15,
        xref = 'paper',
        yref = 'paper',
        yanchor = 'top',
        active = 1,
        showactive = True,
        buttons = [
        dict(
            args = ['xbins.size', 'D1'],
            label = 'Day',
            method = 'restyle',
        ), dict(
            args = ['xbins.size', 'D7'],
            label = 'Week',
            method = 'restyle',
        ), dict(
            args = ['xbins.size', 'M1'],
            label = 'Month',
            method = 'restyle',
        ), dict(
            args = ['xbins.size', 'M3'],
            label = 'Quarter',
            method = 'restyle',
        ), dict(
            args = ['xbins.size', 'M6'],
            label = 'Half Year',
            method = 'restyle',
        ), dict(
            args = ['xbins.size', 'M12'],
            label = 'Year',
            method = 'restyle',
        )]
  )]
)
fig_dict = dict(data=data, layout=layout)
iplot(fig_dict, validate=False)
</code></pre>
","11843843","","11843843","","2019-07-27 03:30:27","2019-07-27 13:34:40","In Python Plotly is there a bar chart equivilant to histogram binning?","<python><python-3.x><pandas><plotly>","1","0","","","","CC BY-SA 4.0","1"
"48778836","1","","","2018-02-14 02:39:06","","1","577","<p>This error keeps popping up when I try to create a DataFrame out of a .xls file. Here's my code: </p>

<pre><code>energy = pd.read_excel('Energy indicators.xls', sheetname='Energy', header=16, skip_footer=246, index_col=2, na_values='...')
</code></pre>

<p>Which results in the error: 
TypeError: object of type 'NoneType' has no len() </p>

<p>Does anyone know what I'm doing wrong here? </p>
","9349156","","","","","2018-02-14 04:19:05","TypeError: object of type 'NoneType' has no len() when using pd.read_excel","<python-3.x><pandas>","0","2","","2018-02-14 04:18:16","","CC BY-SA 3.0","1"
"40569542","1","","","2016-11-13 01:20:36","","1","575","<p>I am trying to build a voting classifier with multiple pipelines as input. I am pretty new at this. Following is the code I am using:</p>

<pre><code>clf1 = MultinomialNB(alpha= 0.99, fit_prior= True)
clf2 = Pipeline([('vect', CountVectorizer(max_features=5000,ngram_range=(1,2))),
                    ('tfidf', TfidfTransformer(use_idf= True)),
                    ('clf', SGDClassifier(alpha=0.001,learning_rate='optimal',loss= 'epsilon_insensitive'
                                          ,penalty= 'l2',n_iter = 100, random_state=42))])
clf3 = Pipeline([('vect', CountVectorizer(max_features=3500)),
                    ('tfidf', TfidfTransformer(use_idf=False)),
                    ('clf', SVC(random_state= 42,kernel=""linear"",degree=1,decision_function_shape=None))])
clf4 = Pipeline([('vect', CountVectorizer(max_features = 4000)),
                    ('tfidf', TfidfTransformer(use_idf=False)),
                    ('clf', RandomForestClassifier(random_state = 42,criterion=""entropy""))])
eclf = VotingClassifier(estimators=[('mnb', clf1), ('sgd', clf2), ('svm', clf3), ('rf',clf4)], voting='hard')
eclf = eclf.fit(train_data,train_label)

p = eclf.predict(test_data)
np.mean(p==test_class)
</code></pre>

<p>The code basically builds 4 classifiers- Multnomial Naive Bayes, SGD Classifier, SVM with linear kernel and random forest classifier. When I try to fit my data it gives me following error:</p>

<p><code>could not convert string to float: ""training string here""</code></p>

<p>If I try to call fit on individual classifiers, the mode runs fine. Can someone please help with this?</p>
","1610216","","","","","2016-11-13 01:20:36","Giving multiple pipelines as input to Voting Classifier - sklearn","<python-3.x><machine-learning><scikit-learn><sklearn-pandas>","0","1","","","","CC BY-SA 3.0","1"
"56669137","1","56670731","","2019-06-19 13:41:55","","2","574","<p>I want to run a function using <code>concurrent</code> in Python. This is the function that I have :</p>

<pre><code>import concurrent.futures
import pandas as pd
import time

def putIndf(file):
    listSel = getline(file)
    datFram = savetoDataFrame(listSel)
    return datFram #datatype : dataframe

def main():
    newData = pd.DataFrame()
    with concurrent.futures.ProcessPoolExecutor(max_workers=30) as executor:
        for i,file in zip(fileList, executor.map(dp.putIndf, fileList)):
            df = newData.append(file, ignore_index=True)
    return df

if __name__ == '__main__':
    main()
</code></pre>

<p>I want to join dataframe to be one dataframe <code>newData</code>, but the result is only the last dataframe from that function</p>
","10907221","","","","","2020-09-26 02:23:07","How to append dataframe to an empty dataframe using concurrent","<python><python-3.x><pandas><dataframe><concurrent.futures>","1","2","","","","CC BY-SA 4.0","1"
"57228272","1","57228419","","2019-07-27 00:30:13","","1","574","<p>I get the following error message when I try to use <code>process.extract</code> from the <a href=""https://pypi.org/project/fuzzywuzzy/"" rel=""nofollow noreferrer"">fuzzywuzzy</a> library on a column in a pandas DataFrame:</p>

<blockquote>
  <p>TypeError: ('expected string or bytes-like object', 'occurred at index 0')</p>
</blockquote>

<p><strong>Background</strong></p>

<p>I have the following sample <code>df</code>:</p>

<pre><code>from fuzzywuzzy import fuzz 
from fuzzywuzzy import process
import pandas as pd
import nltk 

name_list = ['John D Doe', 'Jane L Doe', 'Jack Doe']
text_list = [' Reason for Visit: John D Doe is a Jon has male pattern baldness',
       'Jane is related to John and Jan L Doe is his sister  ',
            'Jack Doe is thier son and jac is five']
df = pd.DataFrame(
    {'Names': name_list,
     'Text': text_list,
     'P_ID': [1,2,3]

    })
#tokenize
df['Token_Names'] = df.apply(lambda row: nltk.word_tokenize(row['Names']), axis=1)
df['Token_Text'] = df.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)

#df
    Names        Text                         P_ID  Token_Names     Token_Text
0   John D Doe  Reason for Visit: John D Doe    1   [John, D, Doe]  [Reason, for, Visit, :, John, D, Doe, is, a, J...
1   Jane L Doe  Jane is related to John         2   [Jane, L, Doe]  [Jane, is, related, to, John, and
2   Jack Doe    Jack Doe is thier son           3   [Jack, Doe]     [Jack, Doe, is, thier, son, and, jac, is, five]
</code></pre>

<p><strong>Problem</strong></p>

<p>I create the following function </p>

<pre><code>def get_alt_names(token_name, token_text):
    if len(token_name) &gt; 1:

          extract = process.extract(token_name,token_text, limit = 3, scorer = fuzz.ratio)
    return extract
</code></pre>

<p>And I use <code>lambda</code> and <code>apply</code>    </p>

<pre><code> #use apply with extract
 df['Alt_Names'] = df.apply(lambda x: get_alt_names(x.Token_Names, x.Token_Text) , axis =1)
</code></pre>

<p>But I get the following error:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-12-6dcc99fa91b0&gt; in &lt;module&gt;()
      1 #use apply with extract
----&gt; 2 df['Alt_Names'] = df.apply(lambda x: get_alt_names(x.Token_Names, x.Token_Text) , axis =1)

C:\Anaconda\lib\site-packages\pandas\core\frame.py in apply(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)
   6002                          args=args,
   6003                          kwds=kwds)
-&gt; 6004         return op.get_result()
   6005 
   6006     def applymap(self, func):

C:\Anaconda\lib\site-packages\pandas\core\apply.py in get_result(self)
    140             return self.apply_raw()
    141 
--&gt; 142         return self.apply_standard()
    143 
    144     def apply_empty_result(self):

C:\Anaconda\lib\site-packages\pandas\core\apply.py in apply_standard(self)
    246 
    247         # compute the result using the series generator
--&gt; 248         self.apply_series_generator()
    249 
    250         # wrap results

C:\Anaconda\lib\site-packages\pandas\core\apply.py in apply_series_generator(self)
    275             try:
    276                 for i, v in enumerate(series_gen):
--&gt; 277                     results[i] = self.f(v)
    278                     keys.append(v.name)
    279             except Exception as e:

&lt;ipython-input-12-6dcc99fa91b0&gt; in &lt;lambda&gt;(x)
      1 #use apply with extract
----&gt; 2 df['Alt_Names'] = df.apply(lambda x: get_alt_names(x.Token_Names, x.Token_Text) , axis =1)

&lt;ipython-input-10-360a3b67e5d2&gt; in get_alt_names(token_name, token_text)
      5     #if len(token_name) inside token_names_unlisted &gt; 1:
      6     if len(token_name) &gt; 1:
----&gt; 7         extract = process.extract(token_name,token_text, limit = 3, scorer = fuzz.ratio)
      8         return extract

C:\Anaconda\lib\site-packages\fuzzywuzzy\process.py in extract(query, choices, processor, scorer, limit)
    166     """"""
    167     sl = extractWithoutOrder(query, choices, processor, scorer)
--&gt; 168     return heapq.nlargest(limit, sl, key=lambda i: i[1]) if limit is not None else \
    169         sorted(sl, key=lambda i: i[1], reverse=True)
    170 

C:\Anaconda\lib\heapq.py in nlargest(n, iterable, key)
    567     # General case, slowest method
    568     it = iter(iterable)
--&gt; 569     result = [(key(elem), i, elem) for i, elem in zip(range(0, -n, -1), it)]
    570     if not result:
    571         return result

C:\Anaconda\lib\heapq.py in &lt;listcomp&gt;(.0)
    567     # General case, slowest method
    568     it = iter(iterable)
--&gt; 569     result = [(key(elem), i, elem) for i, elem in zip(range(0, -n, -1), it)]
    570     if not result:
    571         return result

C:\Anaconda\lib\site-packages\fuzzywuzzy\process.py in extractWithoutOrder(query, choices, processor, scorer, score_cutoff)
     76 
     77     # Run the processor on the input query.
---&gt; 78     processed_query = processor(query)
     79 
     80     if len(processed_query) == 0:

C:\Anaconda\lib\site-packages\fuzzywuzzy\utils.py in full_process(s, force_ascii)
     93         s = asciidammit(s)
     94     # Keep only Letters and Numbers (see Unicode docs).
---&gt; 95     string_out = StringProcessor.replace_non_letters_non_numbers_with_whitespace(s)
     96     # Force into lowercase.
     97     string_out = StringProcessor.to_lower_case(string_out)

C:\Anaconda\lib\site-packages\fuzzywuzzy\string_processing.py in replace_non_letters_non_numbers_with_whitespace(cls, a_string)
     24         numbers with a single white space.
     25         """"""
---&gt; 26         return cls.regex.sub("" "", a_string)
     27 
     28     strip = staticmethod(string.strip)

TypeError: ('expected string or bytes-like object', 'occurred at index 0')
</code></pre>

<p>I think this is because my input is a list</p>

<p><strong>Desired Output</strong></p>

<p>I would expect the output to looking something like below (a list of list, maybe?)</p>

<pre><code> Other_Columns_Here    Alt_Names
0                 [('John', 100), ('Jon', 86), ('Reason', 40)][('D', 100), ('Doe', 50), ('baldness', 22)][('Doe', 100), ('D', 50), ('baldness', 36)]
1                 [('Jane', 100), ('Jan', 86), ('and', 57)] [('L', 100), ('related', 25), ('Jane', 0)][('Doe', 100), ('to', 40), ('and', 33)]
2                 [('Doe', 100), ('to', 40), ('and', 33)] [('Doe', 100), ('son', 33), ('and', 33)]
</code></pre>

<p><strong>Question</strong></p>

<p>How do I fix my error?</p>
","6598999","","3657941","","2019-07-27 06:19:28","2019-07-27 06:19:28","TypeError: ('expected string or bytes-like object', 'occurred at index 0') when calling process.extract","<python><python-3.x><pandas><nltk><fuzzywuzzy>","1","4","","","","CC BY-SA 4.0","1"
"57605034","1","57607086","","2019-08-22 08:23:35","","1","572","<p>I want to create a class of holidays of Christmas and the day after, but I encounter a problem. There is one out of three special cases I cannot quite solve it.</p>

<p>Case1: Christmas is Friday, then the Holidays are Friday(12-25) and Saturday(12-26)(solved). This case happens in 2020</p>

<p>Case2: Christmas is Saturday, then the Holidays are Saturday(12-25) and Monday(12-27)(solved). This case happens in 2021</p>

<p>Case3: Christmas is Sunday, then the Holidays are Monday(12-26) and Tuesday(12-27)(unsolved). This case happens in 2016</p>

<p>Case 3 could be solved if <code>observance= monday_to_tuesday</code> was available, but sadly it isn't, which is why I am here asking for help, since the <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html"" rel=""nofollow noreferrer"">pandas tseries user guide</a> doesn't say anything about creating a custom observance or anything about the situation where the observance is from any weekday.</p>

<p>The class I am working on is the following:</p>

<hr>

<pre><code>from pandas.tseries.holiday import Holiday, sunday_to_monday, AbstractHolidayCalendar

import pandas as pd
import datetime as dt

class MyHolidays(AbstractHolidayCalendar):
    rules = [Holiday('Christmas',month=12,day=25, observance= sunday_to_monday),
             Holiday('Boxingday',month=12,day=26, observance= sunday_to_monday)]

cal= MyHolidays()

if __name__ == '__main__':
    myholidays =cal.holidays(start = dt.datetime(2016,1,1), end=dt.datetime(2021,12,31))
    temp_s = pd.Series(myholidays)
    print(temp_s)
</code></pre>

<p>Is there anyway to solve my problem? Thank you in advance</p>
","5831856","","","","","2019-08-22 10:32:58","Custom holiday observance in Pandas tseries Holiday","<python><python-3.x><pandas><datetime><time-series>","1","0","","","","CC BY-SA 4.0","1"
"33792915","1","","","2015-11-19 00:04:15","","1","570","<p>I have some data in a csv file as show below(only partial data is shown here).</p>

<pre><code>SourceID   local_date   local_time  Vge    BSs              PC         hour Type
7208       8/01/2015    11:00:19    15.4    87             +BC_MSG      11  MAIN
11060      8/01/2015    11:01:56    14.9    67             +AB_MSG      11  MAIN
3737       8/01/2015    11:02:09    15.4    88             +AB_MSG      11  MAIN
9683       8/01/2015    11:07:19    14.9    69             +AB_MSG      11  MAIN
9276       8/01/2015    11:07:52    15.4    88             +AB_MSG      11  MAIN
7754       8/01/2015    11:09:26    14.7    62             +AF_MSG      11  MAIN
11111      8/01/2015    11:10:06    15.2    80             +AF_MSG      11  MAIN
9276       8/01/2015    11:10:52    15.4    88             +AB_MSG      11  MAIN
11111      8/01/2015    11:12:56    15.2    80             +AB_MSG      11  MAIN
6148       8/01/2015    11:15:29    15      70             +AB_MSG      11  MAIN
11111      8/01/2015    11:15:56    15.2    80             +AB_MSG      11  MAIN
9866       8/01/2015    11:16:28    4.102   80             +SUB_MSG     11  SUB
9866       8/01/2015    11:16:38    15.1    78             +AH_MSG      11  MAIN
9866       8/01/2015    11:16:38    4.086   78             +SUB_MSG     11  SUB
20729      8/01/2015    11:23:21    11.6    82             +AB_MSG      11  MAIN
9276       8/01/2015    11:25:52    15.4    88             +AB_MSG      11  MAIN
11111      8/01/2015    11:34:16    15.2    80             +AF_MSG      11  MAIN
20190      8/01/2015    11:36:09    11.2    55             +AF_MSG      11  MAIN
7208       8/01/2015    11:37:09    15.3    85             +AB_MSG      11  MAIN
7208       8/01/2015    11:38:39    15.3    86             +AB_MSG      11  MAIN
7754       8/01/2015    11:39:16    14.7    61             +AB_MSG      11  MAIN
8968       8/01/2015    11:39:39    15.5    91             +AB_MSG      11  MAIN
3737       8/01/2015    11:41:09    15.4    88             +AB_MSG      11  MAIN
9683       8/01/2015    11:41:39    14.9    69             +AF_MSG      11  MAIN
20729      8/01/2015    11:44:36    11.6    81             +AB_MSG      11  MAIN
9704       8/01/2015    11:45:20    14.9    68             +AF_MSG      11  MAIN
11111      8/01/2015    11:46:06    4.111   87             +SUB_MSG     11  PAN
</code></pre>

<p>I have the following python program that uses pandas to process this input</p>

<pre><code>import sys
import csv
import operator
import os
from glob import glob
import fileinput
from relativeDates import *
import datetime
import math
import pprint
import numpy as np
import pandas as pd
from io import StringIO

COLLECTION = 'NEW'
BATTERY = r'C:\MyFolder\Analysis\\{}'.format(COLLECTION)
INPUT_FILE = Pandas + r'\in.csv'
OUTPUT_FILE = Pandas + r'\out.csv'


with open(INPUT_FILE) as fin:
    df = pd.read_csv(INPUT_FILE,
                  usecols=[""SourceID"", ""local_date"",""local_time"",""Vge"",'BSs','PC'],
                  header=0)


    #df.set_index(['SourceID','local_date','local_time','Vge','BSs','PC'],inplace=True)
    df.drop_duplicates(inplace=True)
    #df.reset_index(inplace=True)

    hour_list = []
    gb = df['local_time'].groupby(df['local_date'])
    for i in list(gb)[0][1]:
             hour_list.append(i.split(':')[0])
    for j in list(gb)[1][1]:
            hour_list.append(str(int(j.split(':')[0])+ 24))

    df['hour'] = pd.Series(hour_list,index=df.index)


    df.set_index(['SourceID','local_date','local_time','Vge'],inplace=True)

    #gb = df['hour'].groupby(df['PC'])
    #print(list(gb))
    gb = df['PC']
    class_list = []
    for msg in df['PC']:
        if 'SUB' in msg:
            class_list.append('SUB')
        else:
            class_list.append('MAIN')

    df['Type'] = pd.Series(class_list,index=df.index)


    print(df.groupby(['hour','Type'])['BSs'].aggregate(np.mean))
    gb = df['Type'].groupby(df['hour'])
    #print(list(gb))

    #print(list(df.groupby(['hour','Type']).count()))

    df.to_csv(OUTPUT_FILE)
</code></pre>

<p>I want to get an average of <code>BSs</code> field over time. This is what I am attempting to do in <code>print(df.groupby(['hour','Type'])['BSs'].aggregate(np.mean))</code> above.</p>

<p>However few things needs to be considered.</p>

<ol>
<li><code>Vge</code> values can be classified into <code>2 types</code> based on <code>Type</code> field.</li>
<li>The number of <code>Vge</code> values that we get can vary from hour to hour widely.</li>
<li>The whole data set is for 24 hours.</li>
<li>The <code>Vge</code> values can be recieved from a number of <code>SourceID</code>s.</li>
<li>The <code>Vge</code> values can vary little bit among <code>SourceID</code> but should somewhat similar during the same time interval (same hour)</li>
</ol>

<p>In such a situation calculation of simple mean as above <code>print(df.groupby(['hour','Type'])['BSs'].aggregate(np.mean))</code> won't be sufficient as the number of samples during different time periods (hours) are different. </p>

<p>What function should be used in such a situation?</p>
","316082","","2901002","","2015-11-19 06:58:28","2015-11-19 06:58:28","pandas mean calculation over a column in a csv","<python><python-3.x><numpy><pandas><data-analysis>","0","1","","","","CC BY-SA 3.0","1"
"49458203","1","49471501","","2018-03-23 20:57:50","","1","566","<p>I am running a <code>multiprocessing</code> pool in a for loop over a chuck of data. It runs fine for two iterations and hangs on the third. If I reduce the size of each chuck it hangs later on perhaps the forth or fifth iteration. In the program where I discovered the problem I am running a more extensive function but this works to reproduce the error. </p>

<p>Is there a proper way to terminate a pool after it is finished? So that I can start it again.</p>

<pre><code>import pandas as pd
import numpy as np
from multiprocess import Pool

df = pd.read_csv('paths.csv')

def do_something(user):
    v = df[df['userId'] == user]
    return v

if __name__ == '__main__':
    users = df['userId'].unique()
    n_chunks = round(len(users)/40)
    subsets = [users[i:i+n_chunks] for i in range(0, len(users), n_chunks)]
    chunk_counter = 0
    for user_subset in subsets:
        chunk_counter += 1
        print(f'Beginning to process chunk {chunk_counter}...')
        with Pool() as pool:
            frames = pool.map(do_something, user_subset)
            pool.close()
            pool.terminate()

        print(f'Completed processing chunk {chunk_counter}.')
</code></pre>
","2737477","","","","","2018-03-26 10:09:39","python multiprocessing hangs after a few iterations","<python><python-3.x><pandas><multiprocessing>","1","4","","","","CC BY-SA 3.0","1"
"57079268","1","57079508","","2019-07-17 15:20:30","","1","559","<p>I created a scatter plot in seaborn using seaborn.relplot, but am having trouble putting the legend all in one graph.</p>

<p>When I do this simple way, everything works fine:</p>

<pre><code>import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

df2 = df[df.ln_amt_000s &lt; 700]
sns.relplot(x='ln_amt_000s', y='hud_med_fm_inc', hue='outcome', size='outcome', legend='brief', ax=ax, data=df2)
</code></pre>

<p>The result is a scatter plot as desired, with the legend on the right hand side.</p>

<p><a href=""https://i.stack.imgur.com/nYyRS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nYyRS.png"" alt=""enter image description here""></a></p>

<p>However, when I try to generate a matplotlib figure and axes objects ahead of time to specify the figure dimensions I run into problems:</p>

<pre><code>a4_dims = (10, 10) # generating a matplotlib figure and axes objects ahead of time to specify figure dimensions
df2 = df[df.ln_amt_000s &lt; 700]
fig, ax = plt.subplots(figsize = a4_dims)
sns.relplot(x='ln_amt_000s', y='hud_med_fm_inc', hue='outcome', size='outcome', legend='brief', ax=ax, data=df2)
</code></pre>

<p>The result is two graphs -- one that has the scatter plots as expected but missing the legend, and another one below it that is all blank except for the legend on the right hand side.
<a href=""https://i.stack.imgur.com/SbClJ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SbClJ.png"" alt=""enter image description here""></a></p>

<p>How do I fix this such? My desired result is one graph where I can specify the figure dimensions and have the legend at the bottom in two rows, below the x-axis (if that is too difficult, or not supported, then the default legend position to the right on the same graph would work too)? I know the problem lies with ""ax=ax"", and in the way I am specifying the dimensions as matplotlib figure, but I'd like to know specifically why this causes a problem so I can learn from this.</p>

<p>Thank you for your time.</p>
","6447174","","1552748","","2019-07-17 15:49:53","2019-07-17 16:38:23","Using ""hue"" for a Seaborn visual: how to get legend in one graph?","<python-3.x><pandas><matplotlib><seaborn>","1","0","","","","CC BY-SA 4.0","1"
"57941911","1","57942054","","2019-09-15 07:10:20","","0","557","<p>I got a data set (Excel) with hundreds of entries. In one string column there is most of the information. The information is divided by '_' and typed in by humans. Therefore, it is not possible to work with index positions.</p>

<p>To create a usable data basis it's mandatory to extract information from this column in another column. </p>

<p>The search pattern = '<em>*v*</em>' is alone not enough. But combined with the condition that the first item has to be a digit it works.   </p>

<p>I tried to get it to work with iterrows, iteritems, str.strip, str.extract and many more. But the best solution I received with a for-loop. </p>

<pre><code>     pattern = '_*v*_'
    test = []


    for i in df['col']:
'#Split the string in substrings
        i = i.split('_')
        for c in i:
            if c.find('x') == 1:
                if c[0].isdigit():
                   # print(c)
                    test.append(c)
                else:
'#To be able to fix a few rows manually
                    test.append(0)
[4]: test =[22v3, 33v55, 4v2]



#Input

+-----------+-----------+
|    col    | targetcol |
+-----------+-----------+
| as_22v3   |           |
| 33v55_bdd |           |
| Ave_4v2   |           |
+-----------+-----------+

#Output

+-----------+-----------+--+
|    col    | targetcol |  |
+-----------+-----------+--+
| as_22v3   | 22v3      |  |
| 33v55_bdd | 33v55     |  |
| Ave_4v2   | 4v2       |  |
+-----------+-----------+--+
</code></pre>

<p>My code does work, but only for the first few rows. It stops after 36 values and I can't figure out why. There is no error message besides of course that it is not possible to assign the list to a DataFrame series since it has not the same size.</p>
","11808710","","10961238","","2019-09-15 15:46:51","2019-09-16 07:48:17","Extract a substring new column based on a substring based on conditions ideally with Pandas","<python-3.x><pandas><for-loop>","2","0","","","","CC BY-SA 4.0","1"
"32553207","1","","","2015-09-13 18:38:47","","1","543","<p>I've some twitter feed loaded in Pandas Series that I want to store in HDF5 format. Here's a sample of it:</p>

<pre><code>    &gt;&gt;&gt; feeds[80:90]

    80    BØR MAN STARTE en tweet med store bokstaver? F...
    81    @NRKSigrid @audunlysbakken Har du husket Per S...
    82    Lurer på om IS har fått med seg kaoset ved Eur...
    83    synes han hørte på P3 at Opoku uttales Opoko. ...
    84    De statsbærende partiene Ap og Høyre må ta sky...
    85    April 2014. Blir MDG det nye arbeider @partiet...
    86                       MDG: Hasj for kjøtt. #valg2015
    87               Grønt skifte.. https://t.co/OuM8quaMz0
    88                    Kinderegg https://t.co/AsECmw2sV9
    89    MDG for honning, frukt og grønt. https://t.co/...
    Name: feeds, dtype: object
</code></pre>

<p>Whenever I try to load the above data from a saved HDF5 file, some values are missing and are replaced by <code>''</code>... And the same values reappear when I change the indexing. For example, while storing rows with index <code>84-85</code>:</p>

<pre><code>    &gt;&gt;&gt; store = pd.HDFStore('feed.hd5')
    &gt;&gt;&gt; store.append('feed', feeds[84:86], min_itemsize=200, encoding='utf-8')
    &gt;&gt;&gt; store.close()
</code></pre>

<p>when I read the file, the value of <code>84th</code> row is now missing:</p>

<pre><code>    &gt;&gt;&gt; pd.read_hdf('feed.hd5', 'feed')

    84                                                     
    85    April 2014. Blir MDG det nye arbeider @partiet...
    Name: feeds, dtype: object
</code></pre>

<p>I get the same output as above if I do this way too:</p>

<pre><code>    &gt;&gt;&gt; feeds[84:86].to_hdf('feed.hd5', 'feed', format='table', data_columns=True)
    &gt;&gt;&gt; pd.read_hdf('feed.hd5', 'feed')
</code></pre>

<p>But If I change the index to, say, <code>[84:87]</code> from <code>[84:86]</code>, the <code>84th</code> row is now loaded. </p>

<pre><code>    &gt;&gt;&gt; feeds[84:87].to_hdf('feed.hd5', 'feed', format='table', data_columns=True)
    &gt;&gt;&gt; res = pd.read_hdf('feed.hd5', 'feed')
    &gt;&gt;&gt; res

    84    De statsbærende partiene Ap og Høyre må ta sky...
    85    April 2014. Blir MDG det nye arbeider @partiet...
    86                       MDG: Hasj for kjøtt. #valg2015
    Name: feeds, dtype: object
</code></pre>

<p>But now, the loaded string is missing some characters when compared with the original tweet. here's that <code>84th</code> row valued tweet:</p>

<pre><code>    &gt;&gt;&gt; # Original tweet (Length: 140)
    &gt;&gt;&gt; print (feeds[84])

    De statsbærende partiene Ap og Høyre må ta skylda for Miljøpartiets fremgang. Velgerne har sett at SV og V ikke vinner frem i miljøspørsmål.

    &gt;&gt;&gt; # Loaded tweet (Length: 134)
    &gt;&gt;&gt; print (res[84])

    De statsbærende partiene Ap og Høyre må ta skylda for Miljøpartiets fremgang. Velgerne har sett at SV og V ikke vinner frem i miljøspø
</code></pre>

<p>I plan to use Python 3.3.x mainly for this unicode column support in PyTables (Am I wrong?) but could not store all the data successfully, yet. Can anyone explain this and let me know how can I avoid it ?</p>

<p>I am using <code>OS: Mac OS X Yosemite, Pandas: 0.16.2, Python: 3.3.5, PyTables: 3.2.0</code></p>

<p>UPDATE: I confirmed with HDFView (<a href=""http://www.hdfgroup.org/products/java/hdfview/"" rel=""nofollow"">http://www.hdfgroup.org/products/java/hdfview/</a>) that the data is indeed getting stored always (although with some last characters missing) but I am unable to load it successfully every time though.</p>

<p>Thanks.</p>
","3282392","","3282392","","2015-09-15 16:07:15","2015-09-16 21:53:35","Values missing when loaded from Pandas HDF5 file","<python><python-3.x><pandas><hdf5><pytables>","2","0","1","","","CC BY-SA 3.0","1"
"41943584","1","41943940","","2017-01-30 19:06:53","","0","543","<p>I have a nested dictionary of sets of dates, d</p>

<pre><code>d= {""A"": {'a': {datetime1, datetime2}, 'b': {datetime3}, 'c':{datetime4},
    ""B"": {'a': datetime5, datetime1, datetime3}}
</code></pre>

<p>I want a pandas DataFrame df</p>

<pre><code>          dates   
A    a    datetime1
          datetime2
     b    datetime3
          ...
</code></pre>

<p>This might be a duplicate question of
<a href=""https://stackoverflow.com/questions/24988131/nested-dictionary-to-multiindex-dataframe-where-dictionary-keys-are-column-label"">Nested dictionary to multiindex dataframe where dictionary keys are column labels</a></p>

<p>However, I couldn't get the advise given in that question to work here, so I dare to repost the question. (However, I have previously successfully used methods in that question on other nested dictionaries). So, doing something like </p>

<pre><code>df = pd.DataFrame.from_dict({(i, j): d[i][j]
                                     for i in d.keys()
                                     for j in d[i].keys()},
                                     orient='index')
</code></pre>

<p>creates a mess of thousands of integers as columns (one for each date maybe?), and tuple (i,j) as a single index (instead of two levels of indexes, i and j). Is the problem simply because here I have only one column in the dataframe? Can't I have a multiindexed series? Or am I missing something very obvious?</p>
","6592934","","-1","","2017-05-23 12:16:48","2017-01-30 20:47:51","multiindex dataframe from nested dictionary of sets","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"49247971","1","49248009","","2018-03-13 03:41:30","","1","540","<p>I am trying to merge two dataframe together. df2 has more sample points than df. I want to merge them base on the index of df in a way that for each timestamp for the closest non missing value to timestamp be the value.</p>

<p>my original data set is categorical that is why I made the column as strings.  </p>

<pre><code>from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import random
    ##Generate the Data
np.random.seed(12) 
date_today = datetime.now()
ndays = 5
df = pd.DataFrame({'date': [date_today + timedelta(days=x) for x in range(ndays)], 
                       'test': pd.Series(np.random.randn(ndays)),     'test2':pd.Series(np.random.randn(ndays))})
df = df.set_index('date').sort_index()
df = df.mask(np.random.random(df.shape) &lt; .7)
print(df)

df2 = pd.DataFrame({'date': [date_today + timedelta(days=(abs(np.random.randn(1))*0.25)[0]*x) for x in range(ndays*2)], 
                       'test3': pd.Series(np.random.randn(ndays*2))})
df2 = df2.set_index('date').sort_index() 

df2 = df2.mask(np.random.random(df2.shape) &lt; .3)
df['test']=df['test'].astype(str)
df['test2']=df['test2'].astype(str)
df2['test3']=df2['test3'].astype(str)



print(df2)
df2.reindex(df.index, method='bfill')
</code></pre>

<p>current output: </p>

<pre><code>                                test3
date    
2018-03-12 22:31:52.177918  -1.6817565103951275
2018-03-13 22:31:52.177918  nan
2018-03-14 22:31:52.177918  nan
2018-03-15 22:31:52.177918  nan
2018-03-16 22:31:52.177918  nan
</code></pre>

<p>Desired out put: </p>

<pre><code>                                test3
date    
2018-03-12 22:31:52.177918  -1.6817565103951275
2018-03-13 22:31:52.177918   0.214975948415751
2018-03-14 22:31:52.177918  nan
2018-03-15 22:31:52.177918  nan
2018-03-16 22:31:52.177918  nan
</code></pre>

<p>Thanks in advance,</p>
","9264149","","6567270","","2018-03-13 15:52:21","2018-03-13 15:52:21","Pandas: merge two dataframes with different index and missing values","<python-3.x><pandas><indexing><pandas-groupby>","1","0","","","","CC BY-SA 3.0","1"
"57195156","1","57195326","","2019-07-25 05:53:04","","1","540","<p>I have a list column in dataframe as mentioned below.</p>

<pre><code> df=pd.DataFrame({'a':[""a,b,c""]})
</code></pre>

<p>df:</p>

<pre><code>a
0  a,b,c
</code></pre>

<pre><code>df.a.astype(str).values.tolist()
</code></pre>

<p><code>['a,b,c']</code></p>

<p>But I want the output to be [""a"",""b"",""c""] in this format.
Can someone help me with the code.</p>
","4689708","","11607986","","2019-07-25 05:54:19","2019-07-25 06:06:13","How to convert a dataframe column to list and values in the list to be enclosed with double quotes","<python><python-3.x><pandas><list>","1","11","","","","CC BY-SA 4.0","1"
"48511651","1","","","2018-01-29 23:26:11","","-1","531","<h2>from pandas import Dataframe</h2>

<p>ImportError                               Traceback (most recent call last)
 in ()
----> 1 from pandas import Dataframe</p>

<p>ImportError: cannot import name 'Dataframe'</p>

<p>I understand there are workarounds but I need to do this for an assignment. I am using Jupiter Python ver 3.6.</p>

<p>Thsnks in Advance</p>
","9285810","","","","","2018-01-30 00:24:47","Cannot from pandas import Dataframe","<python-3.x><pandas>","1","1","","","","CC BY-SA 3.0","1"
"48836433","1","","","2018-02-16 23:44:13","","2","530","<p>I would like to take user input and store it into a dataframe. User is to input make, model, type, and rating. Then I would like to print the dataframe table of the user inputs. So this is what I have so far</p>

<p>taking user input</p>

<pre><code>t = int(input(""Enter the number of car instances: ""))
ca = cb = cc = cd = ce = cf = 0.0
carList = {'make': [],
           'model': [],
           'type': [],
           'rating': []}
df = pd.DataFrame(carList)

ca_sedan = ca_coupe = ca_suv = 0.0
cb_sedan = cb_coupe = cb_suv = 0.0
cc_sedan = cc_coupe = cc_suv = 0.0
cd_sedan = cd_coupe = cd_suv = 0.0
ce_sedan = ce_coupe = ce_suv = 0.0
cf_sedan = cf_coupe = cf_suv = 0.0

for _ in range(t):
    k = map(str, input(""Enter the make,model,type,rating: "").split(','))
    if k[3] == 'A':
        ca += 1
        if k[2] == 'sedan': ca_sedan += 1
        if k[2] == 'coupe': ca_coupe += 1
        if k[2] == 'SUV': ca_suv += 1
    if k[3] == 'B':
        cb += 1
        if k[2] == 'sedan': cb_sedan += 1
        if k[2] == 'coupe': cb_coupe += 1
        if k[2] == 'SUV': cb_suv += 1
    if k[3] == 'C':
        cc += 1
        if k[2] == 'sedan': cc_sedan += 1
        if k[2] == 'coupe': cc_coupe += 1
        if k[2] == 'SUV': cc_suv += 1
    if k[3] == 'D':
        cd += 1
        if k[2] == 'sedan': cd_sedan += 1
        if k[2] == 'coupe': cd_coupe += 1
        if k[2] == 'SUV': cd_suv += 1
    if k[3] == 'E':
        ce += 1
        if k[2] == 'sedan': ce_sedan += 1
        if k[2] == 'coupe': ce_coupe += 1
        if k[2] == 'SUV': ce_suv += 1
    if k[3] == 'F':
        cf += 1
        if k[2] == 'sedan': cf_sedan += 1
        if k[2] == 'coupe': cf_coupe += 1
        if k[2] == 'SUV': cf_suv += 1
    carList.append(Car(k[0], k[1], k[2], k[3]))
</code></pre>
","8738671","","5079316","","2018-02-17 11:28:18","2018-02-17 11:28:18","Storing user input into dataframe","<python-3.x><python-2.7><pandas>","0","4","","","","CC BY-SA 3.0","1"
"41947609","1","41947696","","2017-01-31 00:05:28","","1","526","<p>I am looking to do a particular operation on a pandas <code>DataFrame</code>using python3. I want to collapse a <em>NxK</em> <code>DataFrame</code> into a <em>NKx3</em> <code>DataFrame</code> which consists of three columns: the entry, the column and the index from the original <code>DataFrame</code>. Here is an example:</p>

<pre><code>          'a' 'b' 'c'
    'e'    1   2   3
    'f'    4   5   6
</code></pre>

<p>Desired output:</p>

<pre><code>         0   1   2
    0    1  'a' 'e'
    1    4  'a' 'f'
    2    2  'b' 'e'
    3    5  'b' 'f'
    4    3  'c' 'e'
    5    6  'c' 'f'
</code></pre>

<p>I am looking for a pythonic elegant way to achieve this, but as I am dealing with very large dataframes, the highest priority is efficiency.</p>
","6204900","","2336654","","2017-04-21 00:56:11","2017-04-21 00:56:11","Collapse with column and index in pandas","<python><python-3.x><pandas><numpy><dataframe>","1","0","1","","","CC BY-SA 3.0","1"
"48677856","1","48677994","","2018-02-08 04:58:48","","3","525","<p>I am trying to merge multiple CSV file into one CSV file. </p>

<p>The CSV files are like</p>

<pre><code> Energy_and_Power_Day1.csv,     
 Energy_and_Power_Day2.csv, 
 Energy_and_Power_Day3.csv,      
  ....................., 
 Energy_and_Power_Day31.csv
</code></pre>

<p>I have used a small python script to concatenate the multiple CSV file.The script is doing it's job but it is not concatenate the files in serial manner.
It should take  <code>Energy_and_Power_Day1.csv</code> then  <code>Energy_and_Power_Day2.csv</code> then  <code>Energy_and_Power_Day3.csv</code> like this way.. but instead of this it takes randomly not in serially.
This is my code</p>

<pre><code>import pandas as pd
import csv
import glob
import os

os.chdir(""/home/mayukh/Downloads/Northam_bill_data"")
results = pd.DataFrame([])
filelist = glob.glob(""Energy_and_Power_Day*.csv"")
#dfList=[]
for filename in filelist:
  print(filename)  
  namedf = pd.read_csv(filename, skiprows=0, index_col=0)
  results = results.append(namedf)

results.to_csv('Combinefile.csv')
</code></pre>

<p>The script is giving this output from <code>print(filename)</code> and combine these csv files in this manner.</p>

<pre><code>Energy_and_Power_Day1.csv
Energy_and_Power_Day16.csv
Energy_and_Power_Day23.csv
Energy_and_Power_Day22.csv
Energy_and_Power_Day11.csv
Energy_and_Power_Day21.csv
Energy_and_Power_Day31.csv
Energy_and_Power_Day17.csv
Energy_and_Power_Day25.csv
Energy_and_Power_Day28.csv
Energy_and_Power_Day9.csv
Energy_and_Power_Day19.csv
Energy_and_Power_Day7.csv
Energy_and_Power_Day15.csv
Energy_and_Power_Day20.csv
Energy_and_Power_Day24.csv
Energy_and_Power_Day4.csv
Energy_and_Power_Day6.csv
Energy_and_Power_Day14.csv
Energy_and_Power_Day13.csv
Energy_and_Power_Day27.csv
Energy_and_Power_Day3.csv
Energy_and_Power_Day18.csv
Energy_and_Power_Day8.csv
Energy_and_Power_Day30.csv
Energy_and_Power_Day12.csv
Energy_and_Power_Day29.csv
Energy_and_Power_Day10.csv
Energy_and_Power_Day5.csv
Energy_and_Power_Day2.csv
Energy_and_Power_Day26.csv
</code></pre>

<p>My question is how or which way I can combine those CSV files serially?</p>
","7725857","","","","","2018-02-08 06:12:17","How to combine multiple csv into one file in serial manner using python?","<python><python-3.x><pandas><csv>","1","0","","","","CC BY-SA 3.0","1"
"49133841","1","","","2018-03-06 15:00:13","","0","520","<p>How can I aggregate only specific columns in pandas?</p>

<pre><code>import numpy as np

df = pd.DataFrame({'A': [1, 1, 2, 2],
                    'B': [1, 2, 3, 4],
               'C': np.random.randn(4)})

df.groupby('A').agg(['max']) #works as expected, but gives values for col B &amp; C

df.groupby('A').agg({'Bmax':'B'.max}) #FAILS
</code></pre>
","1634753","","6567270","","2018-03-07 01:05:23","2020-03-02 18:38:11","Pandas aggregate specific column","<python><python-3.x><pandas><numpy>","1","3","","","","CC BY-SA 3.0","1"
"57703538","1","","","2019-08-29 05:29:42","","1","519","<p>I've this dataframe in pandas</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>        key                              date  story_point  Story point
0   SOF-158  2019-06-04 09:51:01.143000+02:00          3.0          3.0
1   SOF-152  2019-05-24 09:10:23.483000+02:00          3.0          3.0
2   SOF-151  2019-05-24 09:10:14.978000+02:00          3.0          3.0
3   SOF-150  2019-05-24 09:10:23.346000+02:00          3.0          3.0
4   SOF-149  2019-05-24 09:10:23.024000+02:00          3.0          3.0
5   SOF-148  2019-05-24 09:10:23.190000+02:00          3.0          3.0
6   SOF-146  2019-05-24 09:10:22.840000+02:00          5.0          5.0
7   SOF-142  2019-04-15 10:50:03.946000+02:00          2.0          2.0
8   SOF-141  2019-03-29 10:54:08.677000+01:00          2.0          2.0
9   SOF-139  2019-04-15 10:44:56.033000+02:00          3.0          3.0
10  SOF-138  2019-04-15 10:48:53.874000+02:00          3.0          3.0
11  SOF-129  2019-03-28 11:56:17.221000+01:00          5.0          5.0
12  SOF-128  2019-03-29 11:34:47.552000+01:00          1.0          1.0
13  SOF-106  2019-03-25 10:15:43.231000+01:00          5.0          5.0
14  SOF-105  2019-03-25 10:15:43.252000+01:00          3.0          3.0
15  SOF-103  2019-03-29 11:55:45.984000+01:00          8.0          8.0
16  SOF-102  2019-03-25 10:15:43.210000+01:00          8.0          8.0
17  SOF-101  2019-03-25 10:15:43.179000+01:00          8.0          8.0
18  SOF-100  2019-03-29 12:08:16.525000+01:00         13.0         13.0
19   SOF-99  2019-03-19 12:48:58.168000+01:00          1.0          1.0
20   SOF-98  2019-03-19 12:47:28.172000+01:00         13.0         13.0
21   SOF-91  2019-03-08 11:53:19.456000+01:00          3.0          3.0
22   SOF-89  2019-04-05 09:32:39.517000+02:00          8.0          8.0
23   SOF-88  2019-03-25 10:15:42.927000+01:00          5.0          5.0
24   SOF-87  2019-04-05 09:32:25.519000+02:00          8.0          8.0</code></pre>
</div>
</div>
</p>

<p>At certain point I need to groupby by week, so I used resample.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>weekly_summary[""story_point""] = df.story_point.resample('W').sum()</code></pre>
</div>
</div>
</p>

<p>But I have this error, and I can't figure out why</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>Traceback (most recent call last):
  File ""main.py"", line 98, in &lt;module&gt;
    main()
  File ""main.py"", line 44, in main
    analyze_project(project)
  File ""main.py"", line 70, in analyze_project
    weekly_summary[""story_point""] = df.story_point.resample('W').sum()
  File ""/Users/xxxx/anaconda/envs/xxx/lib/python3.6/site-packages/pandas/core/generic.py"", line 8449, in resample
    level=level,
  File ""/Users/xxx/anaconda/envs/xxx/lib/python3.6/site-packages/pandas/core/resample.py"", line 1306, in resample
    return tg._get_resampler(obj, kind=kind)
  File ""/Users/xxx/anaconda/envs/xxxx/lib/python3.6/site-packages/pandas/core/resample.py"", line 1443, in _get_resampler
    ""but got an instance of %r"" % type(ax).__name__
TypeError: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'RangeIndex'</code></pre>
</div>
</div>
</p>
","1228481","","","","","2019-08-29 05:40:15","TypeError: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex, but got an instance of 'RangeIndex' and I can't figure out why","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"49243204","1","49243294","","2018-03-12 19:36:40","","1","515","<p>I have several columns in a DataFrame that I would like to combine into one column:    </p>

<pre><code>from functools import reduce # python 3.x
na=pd.np.nan
df1=pd.DataFrame({'a':[na,'B',na],'b':['A',na,na],'c':[na,na,'C']})
print(df1)
     a    b    c
0  NaN    A  NaN
1    B  NaN  NaN
2  NaN  NaN    C
</code></pre>

<p>The output I am trying to get is supposed to look like (column name doesn't matter): </p>

<pre><code>  a
0 A
1 B
2 C
</code></pre>

<p>I get <code>ValueError: cannot index with vector containing NA / NaN values</code> when I run this line of code: </p>

<pre><code>reduce(lambda c1,c2: df1[c1].fillna(df1[c2]),df1.loc[:,'a':'c'])
</code></pre>

<p>However, it seems to work when I change the <code>sequence</code> argument of <code>reduce</code> to just two columns <code>df1.loc[:,'a':'b']</code>:</p>

<pre><code>reduce(lambda c1,c2: df1[c1].fillna(df1[c2]),df1.loc[:,'a':'b'])
0      A
1      B
2    NaN
Name: a, dtype: object
</code></pre>

<p>I've also tried to use the DataFrame/Series <code>.combine</code> method, but that produces the same error. <strong>I would like to try to get this working in case I ever want to fill non-nan values</strong>: </p>

<pre><code>reduce(lambda c1,c2: df1[c1].combine(df1[c2],(lambda x,y: y if x==pd.np.nan else x)),df1.loc[:,'a':'c'])
</code></pre>

<p>I don't think this is working like I am hoping though, because when I again restrict to just two columns I get this output: </p>

<pre><code>reduce(lambda c1,c2: df1[c1].combine(df1[c2],(lambda x,y: y if x==pd.np.nan else x)),df1.loc[:,'a':'b'])
0    NaN
1      B
2    NaN
dtype: object
</code></pre>
","6382434","","","","","2018-03-12 19:48:26","Reduce multiple columns into one using pandas","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"48520478","1","48520603","","2018-01-30 11:36:46","","1","505","<p>I have the following <code>df</code>,</p>

<pre><code>A         B
3 days    NaT
NaT       1 days
4 days    3 days
NaT       NaT
</code></pre>

<p>the <code>dtype</code> of <code>A</code> and <code>B</code> is <code>timedelta64[ns]</code>, I am tring to get <code>days</code> from each <code>timedelta</code> of the two columns, so first I tried to remove all the rows with <code>A</code> and <code>B</code> happened to be all <code>NaT</code>,</p>

<pre><code>daydelta = df.dropna(subset=['A', 'B'], how='all')
</code></pre>

<p>and then get <code>days</code> on each column value,</p>

<pre><code>daydelta[['A', 'B']] = daydelta[['A', 'B']].applymap(lambda x: int(Timedelta(x).days))
</code></pre>

<p>but it failed since there is no <code>days</code> attribute in <code>NaT</code>. I am wondering how to get <code>days</code> from <code>timedelta</code> value, while replacing <code>NaT</code> with a string <code>timedelta value does not exist</code>.  </p>
","766708","","","","","2018-01-30 11:42:14","pandas replace NaT with strings in columns when trying to get timedelta object days","<python><python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 3.0","1"
"57047074","1","","","2019-07-15 20:54:32","","0","501","<p>I am trying to clean huge amount of tweets, more than 15 million. 
SO first I collected many txt files and concat them together. 
When I try to clean the tweets by running the python file from terminal I get:</p>

<blockquote>
  <p>Killed: 9</p>
</blockquote>

<p>And when I run it in Jupyter notebook I get:</p>

<blockquote>
  <p>The kernel appears to have died. It will restart automatically.</p>
</blockquote>

<p>I saw some solutions suggesting to update NumPy and  tensorflow. I did and nothing happened. The same error.</p>

<p>I tried using only one of the dataFrames before the concat and it works fine so it seems that the problem is with the size of my DataFrame. </p>

<p>An example code of my cleaning process for the tweets where the error occurs:</p>

<pre><code>    # remove mentions
   df['clean_tweet'] = np.vectorize(remove_pattern)(str(df['tweet']),""@[\w]*"")
</code></pre>

<p>What can I do to fix this? I tried every possible solution I could nothing worked. 
I want to create new column that will have the clean tweets. 
I tried also just adjusting the same column but the same error.</p>
","8358457","","8358457","","2019-07-15 21:01:22","2019-07-15 21:01:22","how to fix killed:9 when running python?","<python-3.x><pandas><dataframe><tensorflow>","0","4","","","","CC BY-SA 4.0","1"
"57192086","1","57192350","","2019-07-24 22:34:01","","1","495","<p>I have a dataframe where <code>parameters</code> column is JSON and contains multiple actual rows and columns:</p>

<pre><code>input_data = pandas.DataFrame({'id':['0001','0002','0003'],
                               'parameters':[""{'product':['book','cat','fish'],'person':['me','you']}"",
                                             ""'{'product':['book','cat'],'person':['me','you','us']}'"",
                                             ""'{'product':['apple','snake','rabbit','octopus'],'person':['them','you','us','we','they']}'""]})
</code></pre>

<p>... from which I'd like to extract the following data frames:</p>

<pre><code>product_data = pandas.DataFrame({'id':['0001','0001','0001','0002','0002','0003','0003','0003','0003'],
                                'product':['book','cat','fish','book','cat','apple','snake','rabbit','octopus']})


person_data = pandas.DataFrame({'id':['0001','0001','0002','0002','0002','0003','0003','0003','0003','0003'],
                                'person':['me','you','me','you','us','them','you','us','we','they']})
</code></pre>

<p>Below is how I've utilized Regular Expressions to get me there. I doubt this is the best way to do it but here it goes:</p>

<pre><code>for i in input_data.id.tolist():
    s = ''.join(input_data[input_data.id == i]['parameters'])
    product_string = re.search(r""product':(.*?),'person"", str(s)).group(1)
    product_data = pandas.DataFrame(product_string[1:-1].split(','))
    person_string = re.search(r""person':(.*?)}"", str(s)).group(1)
    person_data = pandas.DataFrame(person_string[1:-1].split(','))
    print(""........"")
    print(product_data)
    print(""........"")
    print(person_data)
</code></pre>

<p>I'd like to learn a faster, more elegant, or wholesome solution that may capture unexpected nuances.</p>
","3116753","","202229","","2019-07-24 23:30:15","2019-07-24 23:30:15","Extracting portions of JSON string column containing multiple rows and columns in pandas","<python><json><regex><python-3.x><pandas>","2","4","","","","CC BY-SA 4.0","1"
"48553028","1","48553062","","2018-01-31 23:18:04","","2","487","<p>I have a list of lists, where the outer list is length 100k, and each inner list is length 2, with two bool entries, for example <code>[True, False]</code>. I'm trying to put each of the first entries in one column of a dataframe and each of the second entries into another. I have code that works, but it is very slow (takes about 12 seconds):</p>

<pre><code>populate_df[[""col1"", ""col2""]] = pd.Series(list_of_lists).apply(pd.Series)
</code></pre>

<p>Can somebody please recommend a way that performs better?</p>
","2089899","","9209546","","2018-03-02 03:50:53","2018-03-02 03:50:53","Performance unpacking list of lists into pandas dataframe","<python><python-3.x><pandas><indexing>","2","0","","","","CC BY-SA 3.0","1"
"56646636","1","56647172","","2019-06-18 10:05:35","","2","486","<p>I'm trying to create a pandas dataframe from a class object in python.</p>

<p>The class object is the output of postman python script I got from the following tutorial: <a href=""https://developer.cisco.com/meraki/build/meraki-postman-collection-getting-started/"" rel=""nofollow noreferrer"">https://developer.cisco.com/meraki/build/meraki-postman-collection-getting-started/</a></p>

<p>I wish to take the output of this </p>

<pre><code>print(response.text)

</code></pre>

<p>which gives:</p>

<pre><code>[{""id"":578149602163689207,""name"":""Axel Network Test""},{""id"":578149602163688579,""name"":""Your org""},{""id"":578149602163688880,""name"":""Your org""},{""id"":578149602163688885,""name"":""Your org""},{""id"":578149602163689038,""name"":""Tory's Test Lab""},.......
</code></pre>

<p>I want to put this into a pandas dataframe with and ID column and name column.</p>

<pre><code>import requests
import pandas as pd

url = ""https://api.meraki.com/api/v0/organizations""

headers = {
    'X-Cisco-Meraki-API-Key': ""xxxxxxxxxxxxxxxxxxxxxxxxxxx"",
    'User-Agent': ""PostmanRuntime/7.15.0"",
    'Accept': ""*/*"",
    'Cache-Control': ""no-cache"",
    'Postman-Token': ""7d29cb4e-b022-4954-8fc8-95e5361d15ba,1a3ec8cb-5da8-4983-956d-aab45ed00ca1"",
    'accept-encoding': ""gzip, deflate"",
    'referer': ""https://api.meraki.com/api/v0/organizations"",
    'Connection': ""keep-alive"",
    'cache-control': ""no-cache""
    }

response = requests.request(""GET"", url, headers=headers)

</code></pre>

<p>I tired to write </p>

<pre><code>df = pd.DataFrame(response, columns=['id', 'name']) 
</code></pre>

<p>but this produces many errors.</p>

<p>See error log: <a href=""https://pastebin.com/4BKFYng1"" rel=""nofollow noreferrer"">https://pastebin.com/4BKFYng1</a></p>

<p>How can I achieve what I want?</p>
","9940344","","8212173","","2019-06-18 10:23:39","2019-06-18 11:04:07","How to transform a class in python into a pandas dataframe?","<python><python-3.x><pandas><api><postman>","4","5","1","","","CC BY-SA 4.0","1"
"49766652","1","","","2018-04-11 04:44:34","","1","485","<p>I am trying to calculate the difference between rows based on multiple columns. The data set is very large and I am pasting dummy data below that describes the problem:</p>

<p><a href=""https://i.stack.imgur.com/xcPgd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xcPgd.png"" alt=""enter image description here""></a></p>

<p>if I want to calculate the daily difference in weight at a pet+name level. So far I have only come up with the solution of concatenating these columns and creating multiindex based on the new column and the date column. But I think there should be a better way. In the real dataset I have more than 3 columns I am using calculate row difference. </p>

<pre><code>df['pet_name']=df.pet + df.name

df.set_index(['pet_name','date'],inplace = True)
df.sort_index(inplace=True)

df['diffs']=np.nan

for idx in t.index.levels[0]:
    df.diffs[idx] = df.weight[idx].diff()
</code></pre>
","9517856","","","","","2018-04-11 06:03:54","DataFrame difference between rows based on multiple columns","<python-3.x><pandas><dataframe>","2","0","","","","CC BY-SA 3.0","1"
"40528637","1","40528697","","2016-11-10 13:19:19","","2","481","<p>I have written some code that replaces values in a DataFrame with values from another frame using a dictionary, and it is working, but i am using this on some large files, where the dictionary can get very long. A few thousand pairs. When I then uses this code it runs very slow, and it have also been going out of memory on  a few ocations. </p>

<p>I am somewhat convinced that my method of doing this is far from optimal, and that there must be some faster ways to do this. I have created a simple example that does what I want, but that is slow for large amounts of data. Hope someone have a simpler way to do this.</p>

<pre><code>import pandas as pd

#Frame with data where I want to replace the 'id' with the name from df2
df1 = pd.DataFrame({'id' : [1, 2, 3, 4, 5, 3, 5, 9], 'values' : [12, 32, 42,    51, 23, 14, 111, 134]})

#Frame containing names linked to ids
df2 = pd.DataFrame({'id' : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'name' : ['id1',   'id2', 'id3', 'id4', 'id5', 'id6', 'id7', 'id8', 'id9', 'id10']})

#My current ""slow"" way of doing this.

#Starts by creating a dictionary from df2
#Need to create dictionaries from the domain and banners tables to link ids
df2_dict = dict(zip(df2['id'], df2['name']))

#and then uses the dict to replace the ids with name in df1
df1.replace({'id' : df2_dict}, inplace=True)
</code></pre>
","4727118","","","","","2016-11-10 13:36:51","Better way to replace values in DataFrame from large dictionary","<python-3.x><pandas><dictionary><replace>","1","0","","","","CC BY-SA 3.0","1"
"56702605","1","56702860","","2019-06-21 11:31:47","","4","479","<p>I am having trouble in plotting a bar graph on this Dataset. </p>

<pre><code>+------+------------+--------+
| Year | Discipline | Takers |
+------+------------+--------+
| 2010 | BSCS       |   213  |
| 2010 | BSIS       |   612  |
| 2010 | BSIT       |   796  |
| 2011 | BSCS       |   567  |
| 2011 | BSIS       |   768  |
| 2011 | BSIT       |   504  |
| 2012 | BSCS       |   549  |
| 2012 | BSIS       |   595  |
| 2012 | BSIT       |   586  |
+------+------------+--------+
</code></pre>

<p>I'm trying to plot a bar chart with 3 bars representing the number of takers for each year. This is the algorithm I did.</p>

<pre><code>import matplotlib.pyplot as plt
import pandas as pd

Y = df_group['Takers']
Z = df_group['Year']

df = pd.DataFrame(df_group['Takers'], index = df_group['Discipline'])
df.plot.bar(figsize=(20,10)).legend([""2010"", ""2011"",""2012""])

plt.show()
</code></pre>

<p>I'm expecting to show something like this graph</p>

<p>With the same legends</p>

<p><a href=""https://i.stack.imgur.com/nibol.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nibol.png"" alt=""enter image description here""></a></p>
","11585521","","5841306","","2019-06-21 11:41:11","2019-06-21 12:21:39","How do I plot a Bar graph when comparing the rows?","<python><python-3.x><pandas><matplotlib><jupyter-notebook>","3","0","","2019-06-21 12:12:20","","CC BY-SA 4.0","1"
"49139129","1","","","2018-03-06 20:06:47","","0","478","<p>I'm trying to import a csv file with a two nested JSON Obejcts inside using Jupiter Notebook.</p>

<p>I'm getting this error.</p>

<blockquote>
  <p>ParserError: Error tokenizing data. C error: Expected 29 fields in line 3, saw 35</p>
</blockquote>

<p>The problem is that Pandas doesn't recognise the JSON Object and just uses the CSV delimiters which is a comma.</p>

<p>Here's a sample row of the CSV file:</p>

<pre><code>309,DVD10_Welt.mxf,16947519284,00:37:32:24,0_yd3ugljx,""{""Type"":""Source"",""Content-Type"":""Beitrag""}"",97,""Welt"",NULL,NULL,NULL,""{""ContentType"":""Beitrag"",""Description"":""Sie beobachten jeden."",""Keywords"":[""wissensthek"",""zukunft"",""\u00dcberwachung"",""roboter"",""technik"",""internet"",""dvd"",""wissen""],""ProductionDate"":""2013-07-10T00:30:06.000Z"",""TitleIntern"":null}""
</code></pre>

<p>This is my Line in Jupyter:</p>

<pre><code>df = pd.read_csv(csv_file)
df
</code></pre>

<p>Can someone pls give me a hint?</p>

<p>Thanks Manuel</p>
","9230814","","","","","2018-03-06 21:00:02","Import CSV with nested JSON into Pandas DataFrame","<json><python-3.x><pandas><csv><import>","1","0","","","","CC BY-SA 3.0","1"
"41575306","1","41575576","","2017-01-10 17:56:28","","2","472","<p>I have a DataFrame:  </p>

<pre><code>import numpy as np import pandas as pd

a = pd.DataFrame(np.random.rand(10,6)) cols = [['A', 'B', 'C'],
['AA','BB']] a.columns = pd.MultiIndex.from_product(
    cols,
    names= ['first lvl', 'second level'])
</code></pre>

<p>which gives a MultiIndex df as per below: </p>

<pre><code>first lvl            A                   B                   C          
second level        AA        BB        AA        BB        AA        BB
0             0.991608  0.469706  0.338347  0.254777  0.739046  0.980094
1             0.039133  0.959985  0.718216  0.746632  0.341260  0.264836
2             0.164068  0.158672  0.175882  0.211732  0.146807  0.678957
3             0.324433  0.343780  0.269040  0.432309  0.469457  0.247455
4             0.932380  0.314262  0.439924  0.037954  0.641936  0.011523
5             0.608288  0.308212  0.680107  0.988747  0.349255  0.775298
6             0.082478  0.859175  0.546415  0.471169  0.013312  0.824054
7             0.244569  0.049261  0.194941  0.350334  0.203621  0.408066
8             0.132751  0.092825  0.237527  0.383277  0.288257  0.764209
9             0.417155  0.578300  0.325731  0.504903  0.718891  0.861813
</code></pre>

<p>I would like for iterate through columns A, B and C and do a <code>np.polyfit(AA, BB, deg=1)</code>. </p>

<p>Is there an elegant and simple way to do other than with: </p>

<pre><code>cols = np.unique(a.columns.get_level_values(0)) beta =
[np.polyfit(a[col]['AA'], a[col]['BB'], deg= 1) for col in cols]
</code></pre>
","5553761","","2285236","","2017-01-10 18:03:47","2017-01-10 18:21:16","Elegant way to iterate into a multilevel pandas DataFrame","<python><python-3.x><pandas><numpy><multi-index>","2","0","1","","","CC BY-SA 3.0","1"
"57744355","1","57744637","","2019-09-01 08:22:10","","2","471","<p>Given this list of dictionaries</p>

<pre><code>[{'Empire:FourKingdoms:': {'US': '208', 'FR': '96', 'DE': '42', 'GB': '149'}}, 
 {'BigFarmMobileHarvest:': {'US': '211', 'FR': '101', 'DE': '64', 'GB': '261'}}, 
 {'AgeofLords:': {'US': '00', 'JP': '00', 'FR': '00', 'DE': '00', 'GB': '00'}}, 
 {'BattlePiratesHQ:': {'US': '00', 'JP': '00', 'FR': '00', 'DE': '00', 'GB': '00'}},
 {'CallofWar:': {'US': '00', 'JP': '00', 'FR': '00', 'DE': '00', 'GB': '00'}}, 
 {'Empire:AgeofKnights:': {'US': '00', 'JP': '00', 'FR': '00', 'DE': '00', 'GB': '00'}}, 
 {'Empire:MillenniumWars:': {'US': '00', 'JP': '00', 'FR': '00', 'DE': '00', 'GB': '00'}}, 
 {'eRepublik:': {'US': '00', 'JP': '00', 'FR': '00', 'DE': '00', 'GB': '00'}}, 
 {'GameofEmperors:': {'US': '00', 'JP': '00', 'FR': '00', 'DE': '00', 'GB': '00'}}, 
 {'GameofTrenches:': {'US': '00', 'JP': '00', 'FR': '00', 'DE': '00', 'GB': '00'}}]
</code></pre>

<p>and this list of row names:</p>

<pre><code>['Name', 'country', '30/08/2019']
</code></pre>

<p>How could I arrive at this DataFrame:</p>

<pre><code>        Name:    Empire:FourKingdoms  BigFarmMobileHarvest  AgeofLords     ...
0    Country:    US  FR  DE  GB       US  FR  DE  GB        US JP FR DE GB
1 30/08/2019:    208 96  42  149      211 101 64  261       00 00 00 00 00 ...
</code></pre>

<p>each Country and 30/08/2019 value would have its own cell in the DataFrame. But they should be placed under each Game. 
Not sure if this is possible when dicts are different lengths.</p>

<p>My initial idea was to get the dicts out of the list, convert to DataFrame (somehow) in the desired way, and add the row names later. I'm thinking some transposing has to find place. </p>

<p>Another idea is to make dict keys column names and go from there.</p>

<p>Eventually, this would have to be printed to an excel sheet. </p>

<p>I looked at previous <a href=""https://stackoverflow.com/questions/20638006/convert-list-of-dictionaries-to-a-pandas-dataframe"">questions</a>, but not sure if it could apply in my case. </p>
","11739577","","","","","2019-11-03 21:17:45","convert list of uneven dictionaries to pandas dataframe","<python-3.x><pandas><list><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"49651442","1","","","2018-04-04 12:47:21","","1","470","<blockquote>
  <p>df.to_sql(name='hourly', con=engine, if_exists='append', index=False)</p>
</blockquote>

<p>It inserts data not only to table 'hourly', but also to table 'margin' - I execute this particular line only.
It's Postgresql 10.
While Creating table 'hourly', I inherited column names and dtypes from table 'margin'.</p>

<p>Is it something wrong with the db itself or is it Python code?</p>
","8980884","","8980884","","2018-04-04 12:59:51","2018-04-04 13:05:18","Pandas Dataframe.to_sql wrongly inserting into more than one table (postgresql)","<python-3.x><postgresql><pandas>","2","3","","","","CC BY-SA 3.0","1"
"57195275","1","","","2019-07-25 06:02:17","","0","465","<p><a href=""https://i.stack.imgur.com/GEQEh.png"" rel=""nofollow noreferrer"">Initial ""ImportDate"" datatype</a> <a href=""https://i.stack.imgur.com/kcJ7h.png"" rel=""nofollow noreferrer"">Initial Pandas Dataframe interested in ""ImportDate</a></p>

<p>Problem statement - </p>

<p>I want to extract the data where ""ImportDate"" last till ""1-1-2019"". For eg - start_date to 1-1-2019. I tried converting ""object"" into ""datetime64[ns] and wrote the code as </p>

<pre><code>df[df['ImportDate'].between(4/26/2018, 1/1/2019)]
</code></pre>

<p>But resulted in an error while extracting the data: </p>

<pre><code>""'&gt;=' not supported between instances of 'str' and 'float""
</code></pre>

<p>Can anyone help me how to deal with my problem statement?</p>
","11287101","","9726075","","2019-07-25 06:51:31","2019-07-25 07:14:37","How to extract the data from pandas dataframe between two dates?","<python-3.x><pandas><datetime>","2","4","","","","CC BY-SA 4.0","1"
"48933464","1","48936900","","2018-02-22 17:13:54","","0","455","<p>I am attempting to read a list of urls and domains from csv and have a <code>Scrapy</code> spider iterate through the list of domains and starting urls with the goal of having all urls within that domain exported to a csv file through my pipeline.</p>

<pre><code>import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from NONPROF.items import NonprofItem
from scrapy.http import Request
import pandas as pd


file_path = 'C:/csv'
open_list = pd.read_csv(file_path)
urlorgs = open_list.http.tolist()

open_list2 = pd.read_csv(file_path)
domainorgs = open_list2.domain.tolist()



class Nonprof(CrawlSpider):
        name = ""responselist""
    for domain in domainorgs:
        allowed_domains = [domain]
    for url in urlorgs:
        start_urls = [url]

        rules = [
            Rule(LinkExtractor(
                allow=['.*']),
                 callback='parse_item',
                 follow=True)
            ]

        def parse_item (self, response):
            item = NonprofItem()
            item['responseurl'] = response.url
            yield item
</code></pre>

<p>When I run the spider it will either give me an indention error, or when I make adjustments to indention then it will only recognize the last domain in the list.</p>

<p>Any recommendations on how to accomplish this are appreciated.  </p>
","8980454","","","","","2018-02-23 01:39:20","Scrapy iterate through starting urls and domains","<python-3.x><pandas><scrapy><scrapy-spider>","2","1","","","","CC BY-SA 3.0","1"
"40987479","1","","","2016-12-06 03:46:24","","0","452","<p>I'm new to pandas and Python. I have a pivot table like:</p>

<pre><code>grp3 = DataFrame(hrsub.pivot_table('left',index = 'sales',columns = 'sat_levels', aggfunc = 'count'))
</code></pre>

<p>output:</p>

<pre><code>sat_levels  
    low high

IT  199 74

RandD   85  36

accounting  161 43

hr  159 56

management  66  25

marketing   149 54

product_mng 129 69

sales   727 287

support 383 172

technical   492 205
</code></pre>

<p>I want to take the ratio of two columns via</p>

<pre><code>hrsub['low']/hrsub['high']
</code></pre>

<p>But it errors out saying that data should be 1-dimensional. What should I be doing instead?</p>
","6529152","","344286","","2016-12-06 03:53:09","2018-06-12 09:55:12","Python Pivot Table Python","<python-3.x><pandas><pivot><data-analysis>","1","3","","","","CC BY-SA 3.0","1"
"57261855","1","57261876","","2019-07-29 21:54:48","","0","451","<p>I want to subtract the current column value from the previous column value of the same row (axis=1) except the first column value</p>

<pre><code>My Dataframe:

   A   B  C  D
0  5  11  4  5
1  3   2  3  4
2  6   4  8  2
3  4   3  5  8

Expected Dataframe:

   A   B  C  D
0  5   6 -2  7
1  3  -1  4  0
2  6  -2  10 12
3  4  -1  6  2
</code></pre>
","11380679","","","","","2019-07-29 22:34:49","subtract current column value from the previous column of the same row in pandas dataframe","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"49131704","1","49291850","","2018-03-06 13:12:06","","8","451","<p>I have a lot of data that I'd like to structure in a Pandas dataframe. However, I need a multi-index format for this. The Pandas MultiIndex feature has always confused me and also this time I can't get my head around it.</p>

<p>I built the structure as I want it as a dict, but because my actual data is much larger, I want to use Pandas instead. The code below is the <code>dict</code> variant. Note that the original data has a lot more labels and more rows as well.</p>

<p>The idea is that the original data contains rows of a task with index <code>Task_n</code> that has been performed by a participant with index <code>Participant_n</code>. Each <em>row</em> is a segment. Even though the original data does not have this distinction, I want to add this to my dataframe. In other words:</p>

<pre><code>Participant_n | Task_n | val | dur
----------------------------------
            1 |      1 |  12 |   2
            1 |      1 |   3 |   4
            1 |      1 |   4 |  12
            1 |      2 |  11 |  11
            1 |      2 |  34 |   4
</code></pre>

<p>The above example contains <em>one</em> participants, <em>two</em> tasks, with respectively <em>three</em> and <em>two</em> segments (rows).</p>

<p>In Python, with a <code>dict</code> structure this looks like this:</p>

<pre><code>import pandas as pd

cols = ['Participant_n', 'Task_n', 'val', 'dur']

data = [[1,1,25,83],
        [1,1,4,68],
        [1,1,9,987],
        [1,2,98,98],
        [1,2,84,4],
        [2,1,9,21],
        [2,2,15,6],
        [2,2,185,6],
        [2,2,18,4],
        [2,3,8,12],
        [3,1,7,78],
        [3,1,12,88],
        [3,2,12,48]]

d = pd.DataFrame(data, columns=cols)

part_d = {}
for row in d.itertuples():
    participant_n = row.Participant_n
    participant = ""participant"" + str(participant_n)
    task = ""task"" + str(row.Task_n)

    if participant in part_d:
        part_d[participant]['all_sum']['val'] += int(row.val)
        part_d[participant]['all_sum']['dur'] += int(row.dur)
    else:
        part_d[participant] = {
            'prof': 0 if participant_n &lt; 20 else 1,
            'all_sum': {
                'val': int(row.val),
                'dur': int(row.dur),
            }
        }

    if task in part_d[participant]:
        # Get already existing keys
        k = list(part_d[participant][task].keys())

        k_int = []
        # Only get the ints (i.e. not all_sum etc.)
        for n in k:
            # Get digit from e.g. seg1
            n = n[3:]
            try:
                k_int.append(int(n))
            except ValueError:
                pass

        # Increment max by 1
        i = max(k_int) + 1
        part_d[participant][task][f""seg{i}""] = {
            'val': int(row.val),
            'dur': int(row.dur),
        }
        part_d[participant][task]['task_sum']['val'] += int(row.val)
        part_d[participant][task]['task_sum']['dur'] += int(row.dur)
    else:
        part_d[participant][task] = {
            'seg1': {
                'val': int(row.val),
                'dur': int(row.dur),
            },
            'task_sum': {
                'val': int(row.val),
                'dur': int(row.dur),
            }
        }

print(part_d)
</code></pre>

<p>In the end result here I have some additional variables such as: task_sum (the sum over the task of a participant), all_sum (sum of all a participant's actions), and also <code>prof</code> which is an arbitrary boolean flag. The resulting dict looks like this (not beautified to save space. If you want to inspect, open in text editor as JSON or Python dict and beautify):</p>

<pre><code>{'participant1': {'prof': 0, 'all_sum': {'val': 220, 'dur': 1240}, 'task1': {'seg1': {'val': 25, 'dur': 83}, 'task_sum': {'val': 38, 'dur': 1138}, 'seg2': {'val': 4, 'dur': 68}, 'seg3': {'val': 9, 'dur': 987}}, 'task2': {'seg1': {'val': 98, 'dur': 98}, 'task_sum': {'val': 182, 'dur': 102}, 'seg2': {'val': 84, 'dur': 4}}}, 'participant2': {'prof': 0, 'all_sum': {'val': 235, 'dur': 49}, 'task1': {'seg1': {'val': 9, 'dur': 21}, 'task_sum': {'val': 9, 'dur': 21}}, 'task2': {'seg1': {'val': 15, 'dur': 6}, 'task_sum': {'val': 218, 'dur': 16}, 'seg2': {'val': 185, 'dur': 6}, 'seg3': {'val': 18, 'dur': 4}}, 'task3': {'seg1': {'val': 8, 'dur': 12}, 'task_sum': {'val': 8, 'dur': 12}}}, 'participant3': {'prof': 0, 'all_sum': {'val': 31, 'dur': 214}, 'task1': {'seg1': {'val': 7, 'dur': 78}, 'task_sum': {'val': 19, 'dur': 166}, 'seg2': {'val': 12, 'dur': 88}}, 'task2': {'seg1': {'val': 12, 'dur': 48}, 'task_sum': {'val': 12, 'dur': 48}}}}
</code></pre>

<p>Instead of a dictionary, I would like this to end up in a <code>pd.DataFrame</code> with multiple indexes that looks like the representation below, or similar. (For simplicity's sake, instead of <code>task1</code> or <code>seg1</code> I just used the indices.)</p>

<pre><code>Participant   Prof all_sum      Task    Task_sum     Seg   val   dur
                   val    dur           val    dur
====================================================================
participant1  0    220   1240      1     38   1138     1    25    83
                                                       2     4    68
                                                       3     9   987
                                   2    182    102     1    98    98
                                                       2    84     4
--------------------------------------------------------------------
participant2  0    235     49      1      9     21     1     9    21
                                   2    218     16     1    15     6
                                                       2   185     6
                                                       3    18     4
                                   3      8     12     1     8    12
--------------------------------------------------------------------
participant3  0     31    214      1     19    166     1     7    78
                                                       2    12    88
                                   2     12     48     1    12    48
</code></pre>

<p><strong>Is this a structure that is possible in Pandas? If not, which reasonable alternatives are?</strong></p>

<p>Again I have to emphasise that in reality there is a lot more data and possibly more sub-levels. The solution thus has to be flexible, <em>and</em> efficient. If it makes things a lot simpler, I am willing to only have multi-index on one axis, and change the header to:</p>

<pre><code>Participant  Prof  all_sum_val  all_sum_dur  Task  Task_sum_val  Task_sum_dur  Seg   
</code></pre>

<p>The main issue I am having is that I do not understand how I can build a multi index df if I don't know the dimensions in advance. I don't know in advance how many tasks or segments there will be. So I am pretty sure I can keep the loop construct from my initial <code>dict</code> approach and I guess I'd then have to append/concat to an initial empty DataFrame, but the question is then what the structure has to look like. It can't be a simple Series, because that does not take multi index in account. So how?</p>

<p>For the people who have read this far and want to try their hand at this, I think that my original code can be re-used for the most part (loop and variable assignment), but instead of a dict it have to be accessors to the DataFrame. That an import aspect: data should be easily readable with getters/setters, just as a regular DataFrame is. E.g. it should be easy to get the duration value for participant two, task 2, segment 2, and so on. But also, getting a subset of the data (e.g. where <code>prof === 0</code>) should be without problems.</p>
","1150683","","1150683","","2018-03-10 12:21:54","2018-03-15 04:53:42","Convert dict constructor to Pandas MultiIndex dataframe","<python><python-3.x><pandas><dictionary><dataframe>","2","1","1","","","CC BY-SA 3.0","1"
"57266867","1","57286675","","2019-07-30 07:56:44","","0","450","<p>Is there a simple way to <strong>manually</strong> iterate through <strong>existing</strong> <code>pandas</code> <code>groupby</code> objects?</p>

<pre><code>import pandas as pd

df = pd.DataFrame({'x': [0, 1, 2, 3, 4], 'category': ['A', 'A', 'B', 'B', 'B']})
grouped = df.groupby('category')
</code></pre>

<p>In the application a <code>for name, group in grouped:</code> loops follows. For manual-testing I would like to do something like <code>group = grouped[0]</code> and run the code within the for-loop. Unfortunately this does not work. The best thing I could find (<a href=""https://stackoverflow.com/questions/55971179/iterate-over-a-subset-of-a-pandas-groupby-object"">here</a>) was </p>

<pre><code>group = df[grouped.ngroup()==0]
</code></pre>

<p>which relies on the original DataFrame and not soley on the groupby-Object and is therefore not optimal imo.</p>
","6256241","","6256241","","2019-07-31 07:00:43","2019-07-31 08:45:40","Is there a simple way to manually iterate through existing pandas groupby objects?","<python-3.x><pandas><pandas-groupby><iterable><manual-testing>","1","12","","","","CC BY-SA 4.0","1"
"32282097","1","32286749","","2015-08-29 02:20:23","","3","450","<p>I have a Pandas data frame as follow:</p>

<pre><code>import pandas as pd
data = pd.DataFrame([[1, 1, 100], [1, 2, 101], [1, 3, 102], 
                     [2, 1, 103], [2, 2, 104], [2, 3, 105], 
                     [3, 1, 106] ,[3, 2, 107], [3, 3, 108]],
                    columns=['row', 'column', 'cell value'])
</code></pre>

<p>Each row of data represents the value and location (by 'row' and 'column') of a cell. What I am hoping to do is to calculate the mean cell value of the adjacent cells for each cell. For example, for cell (row==2 column==2), I need the mean cell value calculated from:</p>

<ul>
<li>cell value (row==1,column==2)</li>
<li>cell value (row==3,column==2)</li>
<li>cell value (row==2,column==1)</li>
<li>cell value (row==2,column==3)</li>
</ul>

<p>I need to apply this calculation to every cell.</p>

<p>I have the following defined functions:</p>

<p>This function extracts the cell value for a particular cell:</p>

<pre><code>def val(r,c):
    return float(data['cell value'][(data['row'] == r) &amp; (data['column'] == c)])
</code></pre>

<p>This function extracts the adjacent cell values:</p>

<pre><code>def adjval(r,c): 
    adj = []
    if r != data['row'].max(): 
        adj.append(thick(r + 1, c))
    if r!=1: 
        adj.append(thick(r - 1, c))
    if c!=data['column'].max(): 
        adj.append(thick(r, c + 1))
    if c!=1: 
        adj.append(thick(r, c - 1))
    return adj
</code></pre>

<p>But then I am struggling to find a way to apply this function to each cell in the dataframe. I tried <code>iterrows</code> but it was very slow as the actual dataset is very large.</p>

<p>Any advice on how I can proceed forward will be greatly appreciated.</p>
","5278870","","2411802","","2015-08-29 18:08:17","2015-08-29 18:08:17","Calculate conditional means in Pandas","<python-3.x><numpy><pandas>","1","3","","","","CC BY-SA 3.0","1"
"48721630","1","","","2018-02-10 14:07:40","","-1","445","<p>I try to build 3-dimensional DataFrame as follow:</p>

<pre><code>import numpy as np
import pandas as pd

ii = [days for days in np.arange(10, 30, 10)]
jj = [days for days in np.arange(20, 50, 10)]
kk = [days for days in np.arange(50, 200, 50)]
all_sharpes = pd.DataFrame()

for i in ii:
    for j in jj:
        for k in kk:
            all_sharpes[i][j][k] = i+j+k

all_sharpes
</code></pre>

<p>But I got errors as follow:</p>

<pre><code>pandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:4154)()

pandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:4018)()

pandas/hashtable.pyx in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)()

pandas/hashtable.pyx in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)()

KeyError: 10
</code></pre>

<p>Besides, I want to get its heap-map(). </p>
","9342619","","9342619","","2018-02-12 08:47:37","2018-02-12 08:47:37","Building a 3 dimension DataFrame and its heat_map() in Python","<python-3.x><pandas><dataframe>","1","2","","","","CC BY-SA 3.0","1"
"49626793","1","49626845","","2018-04-03 09:36:49","","1","438","<p>I have a dataframe below</p>

<pre><code>df=pd.DataFrame(np.random.randn(6,3),index=list(""ABCDEF""),columns=list(""XYZ""))
df.reset_index(inplace=True)
df
</code></pre>

<p>I want to have a new column named ""Q"". The values under column ""Q"" shall be calculated based on the labels under index column with the following three conditions:</p>

<pre><code>conditions=[(df[""index""]== ""A""|""B""|""C""|""D""),(df[""index""]== ""E""),(df[""index""]== ""F"")]
returned_value=[df[""X""]+df[""Y""],df[""Y""]*2,df[""Z""]]
</code></pre>

<p>So I was thinking using</p>

<pre><code>df[""Q""]=np.select(conditions, returned_value)
</code></pre>

<p>I however got the error after defining the conditions. I first used or, and got another error, and then changed to |, but got the following. Any hints on how can I achieve what I want?</p>

<pre><code>TypeError: unsupported operand type(s) for |: 'str' and 'str'
</code></pre>
","8618545","","","","","2018-04-03 09:39:30","Conditional operation based on column label in pandas dataframe","<python-3.x><pandas><conditional><operations>","1","0","","","","CC BY-SA 3.0","1"
"57196967","1","57197305","","2019-07-25 07:56:12","","1","437","<p>I need to help with normalize columns in pandas DataFrame.
This is input</p>

<pre class=""lang-py prettyprint-override""><code>df = {0:[{'Code' : 1,'Category' : 'X'},
         {'Code' : 2,'Category' : 'Y','snapshots' : [{'Address': {'City': 'City B'}}] },
         {'Code' : 3,'Category' : 'Z','snapshots' : [{'Address': {'City': 'City C'}}] }
         ]
        }
df = pd.DataFrame(df)
</code></pre>

<p>My code:</p>

<pre class=""lang-py prettyprint-override""><code>df_1 = pd_json.json_normalize(df[0],
                              meta=[""Code""],
                              record_path=[""snapshots""],
                              record_prefix=""snapshots."",
                              errors=""ignore""
                              )
df_2 = (df_1.drop('snapshots.Address', 1)
                    .assign(**df_1[""snapshots.Address""].dropna()
                    .apply(pd.Series).add_prefix('snapshots.Address.')))
</code></pre>

<p>Error:</p>

<blockquote>
  <p>Traceback (most recent call last):</p>
  
  <p>File """", line 5, in 
      errors=""ignore""</p>
  
  <p>File
  ""C:\Users\my_user\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\io\json\normalize.py"",
  line 267, in json_normalize
      _recursive_extract(data, record_path, {}, level=0)</p>
  
  <p>File
  ""C:\Users\my_user\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\io\json\normalize.py"",
  line 244, in _recursive_extract
      recs = _pull_field(obj, path[0])</p>
  
  <p>File
  ""C:\Users\my_user\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\io\json\normalize.py"",
  line 189, in _pull_field
      result = result[spec]</p>
  
  <p>KeyError: 'snapshots'</p>
</blockquote>

<p>There is missing ""snapshots"" in DataFrame for ""Code=1"". 
My idea is add ""snapshots"" to places in DataFrame, where it is missing. But I do not know how to do it. </p>

<p>Expected result:</p>

<pre class=""lang-py prettyprint-override""><code>df_2
Out[617]: 
   Code snapshots.Address.City
0     1                    NaN
1     2                 City B
2     3                 City C
</code></pre>
","11609437","","8708364","","2019-07-25 08:29:49","2019-07-25 08:29:49","python json.normalize error if some subrecords do not exist","<python><python-3.x><pandas><dataframe><nested>","1","0","","","","CC BY-SA 4.0","1"
"57860460","1","57860961","","2019-09-09 19:55:52","","0","437","<p>I have the following code for example. </p>

<pre><code>df = pd.DataFrame(dtype=""category"")
df[""Gender""]=np.random.randint(2, size=100)
df[""Q1""] = np.random.randint(3, size=100)
df[""Q2""] = np.random.randint(3, size=100)
df[""Q3""] = np.random.randint(3, size=100)
df[[""Gender"", ""Q1"", ""Q2"", ""Q3""]] = df[[""Gender"", ""Q1"", ""Q2"", ""Q3""]].astype('category')
pd.pivot_table(data=df,index=[""Gender""])
</code></pre>

<p>I want to have a pivot table with percentages over gender for all the other columns. Infact, like the follwing.</p>

<p><a href=""https://i.stack.imgur.com/hniAu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hniAu.png"" alt=""enter image description here""></a></p>

<p>How to achieve this?</p>

<p>The above code gives an error saying that 
<code>No numeric types to aggregate</code></p>

<p>I dont have any numerical columns. I just want to find the frequency in each category under male and female and find the percentage of them over male and female respectively. </p>
","5620862","","","","","2019-09-09 20:43:29","How to have a cross tabulation for categorical data in Pandas (Python)?","<python-3.x><pandas><pivot-table><crosstab>","1","0","","","","CC BY-SA 4.0","1"
"41177176","1","41177209","","2016-12-16 04:12:58","","1","435","<p>Forgive me if the answer is simplistic. I am a beginner of Pandas. Basically, I want to retrieve the label index of a row of my pandas dataframe. I know the integer index of it.</p>

<p>For example, suppose that I want to get the label index of the last row of my pandas dataframe df. I tried:</p>

<pre><code>df.iloc[-1].index
</code></pre>

<p>But that retrieved the column headers of my dataframe, rather than the label index of the last row. How can I get that label index?</p>
","3008221","","","","","2016-12-16 04:17:21","How can I retrieve the label index of a pandas dataframe row given its integer index?","<python><python-2.7><python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 3.0","1"
"57049254","1","57049402","","2019-07-16 02:20:07","","-1","433","<p>My dataset <code>df</code> looks like this. It is a <code>minute</code> based dataset.</p>

<pre><code>time, Open, High
2017-01-01 00:00:00, 1.2432, 1.1234
2017-01-01 00:01:00, 1.2432, 1.1234
2017-01-01 00:02:00, 1.2332, 1.1234
2017-01-01 00:03:00, 1.2132, 1.1234
...., ...., ....
2017-12-31 23:59:00, 1.2132, 1.1234
</code></pre>

<p>I want to find the hourly <code>rolling mean</code> for <code>Open</code> column but it should be flexible so that I can also find hourly <code>rolling mean</code> for other columns.</p>

<p>What did I do?</p>

<p>I am able to find the <code>daily rolling average</code> like given below:</p>

<pre><code># Pandas code to find the rolling mean for a single day

df
.assign(1davg=df.rolling(window=1*24*60)['Open'].mean()) 
.groupby(df['time'].dt.date) 
.last() 
</code></pre>

<p>Please note that changing this(<code>window=1*24*60</code> to <code>window=60</code>) line of code does not work because I already tried it.</p>

<p>The new <code>output</code> should look like this:</p>

<pre><code>time,                 Open,  High,   Open_hour_avg
2017-01-01 00:00:00, 1.2432, 1.1234,   1.2532
2017-01-01 01:00:00, 1.2432, 1.1234,   1.2632    
2017-01-01 02:00:00, 1.2332, 1.1234,   1.2332
2017-01-01 03:00:00, 1.2132, 1.1234,   1.2432
...., ...., ...., ....
2017-12-31 23:00:00, 1.2132, 1.1234,   1.2232
</code></pre>

<p>here,</p>

<p><code>2017-01-01 00:00:00, 1.2432, 1.1234,   1.2532</code> is the <code>minute</code> average data for <code>midnight</code> </p>

<p>and <code>2017-01-01 01:00:00, 1.2432, 1.1234, 1.2632</code> is the <code>minute</code> average data for <code>1 AM</code> </p>
","9161607","","","","","2019-07-16 02:46:35","Pandas find hourly rolling average","<python-3.x><pandas><dataframe><datetime><python-datetime>","2","4","1","","","CC BY-SA 4.0","1"
"48777128","1","48777528","","2018-02-13 23:04:35","","2","429","<p>Consider the two dataframes <code>df_a</code> and <code>df_b</code>:</p>

<pre><code>&gt;&gt;&gt; df_a = pd.DataFrame.from_dict({1: [1,2,3], 2: [""a"", ""b"", ""c""], 3:[4,5,6]})
&gt;&gt;&gt; df_a.index = pd.Index([0,1,3])
&gt;&gt;&gt; print(df_a)

   1  2  3
0  1  a  4
1  2  b  5
3  3  c  6

&gt;&gt;&gt; df_b = pd.DataFrame.from_dict({2: [""d"", ""e"", ""f"", ""g""]})
&gt;&gt;&gt; print(df_b)

   2
0  d
1  e
2  f
3  g
</code></pre>

<p>And the following code:</p>

<pre><code>&gt;&gt;&gt; df_a = pd.concat([df_a, df_b])
&gt;&gt;&gt; df_c = df_a.loc[~df_a.index.duplicated(keep='last'),df_b.columns]
&gt;&gt;&gt; df_d = df_a.loc[~df_a.index.duplicated(keep='first'), ~df_a.columns.isin(df_b.columns)]
&gt;&gt;&gt; df_e = df_d.merge(df_c, ""outer"", left_index=True, right_index=True)
&gt;&gt;&gt; df_e.sort_index(axis=1, inplace=True)
</code></pre>

<p>Which produces the desired dataframe (<code>df_e</code>):</p>

<pre><code>&gt;&gt;&gt; print(df_e)
     1  2    3
0  1.0  d  4.0
1  2.0  e  5.0
2  NaN  f  NaN
3  3.0  g  6.0
</code></pre>

<p>Is there a more efficient way to get to <code>df_e</code>? I have tried various methods of using <code>pd.concat</code>, <code>pd.merge</code> and <code>pd.update</code>, but my efforts have resulted in one or more of these undesirable consequences:</p>

<ol>
<li>It disrupts the index of <code>df_a</code> (i.e. the values do not have the same index - some sort of index creation happens 'under the hood').</li>
<li>Columns get renamed.</li>
<li>NaNs appear in places where <code>df_a</code> values should be.</li>
</ol>

<p>Basically, the operation I want to perform is:</p>

<ol>
<li>Update <code>df_a</code> with values of <code>df_b</code>.</li>
<li>If values exist in <code>df_b</code> that do not have corresponding index/columns, expand <code>df_a</code> appropriately to include these values (keeping the index/columns in the appropriate order).</li>
</ol>

<p>EDIT: Provided better example that isn't naturally sorted.</p>
","2728074","","9209546","","2018-03-02 04:00:34","2018-03-02 04:00:34","pandas: Better way to update and merge dataframes","<python><python-3.x><pandas><merge><updates>","3","0","","","","CC BY-SA 3.0","1"
"32303374","1","","","2015-08-31 02:53:17","","0","426","<p>I'm using pyinstaller on one of my scripts but I'm getting an error on the line of code I wrote:</p>

<pre><code>from statsmodels.stats.diagnostic import kstest_normal
</code></pre>

<p>The command I used was </p>

<pre><code>pyinstaller fitness_of_statistical_tests.py --hidden-import=scipy.linalg.cython_blas --hidden-import=scipy.linalg.cython_lapack
</code></pre>

<p>The full error I get is:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Python34\lib\site-packages\pandas\__init__.py"", line 7, in &lt;module&gt;
    from pandas import hashtable, tslib, lib
ImportError: cannot import name 'lib'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""&lt;string&gt;"", line 9, in &lt;module&gt;
  File ""C:\Python34\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 311, in load_module
    exec(bytecode, module.__dict__)
  File ""D:\SEDS\gahndiwashingtonmethod\fitness_of_statistical_tests_view.py"", line 11, in &lt;module&gt;
    from statsmodels.stats.diagnostic import kstest_normal
  File ""C:\Python34\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 311, in load_module
    exec(bytecode, module.__dict__)
  File ""C:\Python34\lib\site-packages\statsmodels\__init__.py"", line 8, in &lt;module&gt;
    from .tools.sm_exceptions import (ConvergenceWarning, CacheWriteWarning,
  File ""C:\Python34\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 311, in load_module
    exec(bytecode, module.__dict__)
  File ""C:\Python34\lib\site-packages\statsmodels\tools\__init__.py"", line 1, in &lt;module&gt;
    from .tools import add_constant, categorical
  File ""C:\Python34\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 311, in load_module
    exec(bytecode, module.__dict__)
  File ""C:\Python34\lib\site-packages\statsmodels\tools\tools.py"", line 11, in &lt;module&gt;
    from statsmodels.datasets import webuse
  File ""C:\Python34\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 311, in load_module
    exec(bytecode, module.__dict__)
  File ""C:\Python34\lib\site-packages\statsmodels\datasets\__init__.py"", line 5, in &lt;module&gt;
    from . import (anes96, cancer, committee, ccard, copper, cpunish, elnino,
  File ""C:\Python34\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 311, in load_module
    exec(bytecode, module.__dict__)
  File ""C:\Python34\lib\site-packages\statsmodels\datasets\anes96\__init__.py"", line 1, in &lt;module&gt;
    from .data import *
  File ""C:\Python34\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 311, in load_module
    exec(bytecode, module.__dict__)
  File ""C:\Python34\lib\site-packages\statsmodels\datasets\anes96\data.py"", line 90, in &lt;module&gt;
    from statsmodels.datasets import utils as du
  File ""C:\Python34\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 311, in load_module
    exec(bytecode, module.__dict__)
  File ""C:\Python34\lib\site-packages\statsmodels\datasets\utils.py"", line 13, in &lt;module&gt;
    from pandas import read_csv, DataFrame, Index
  File ""C:\Python34\lib\site-packages\PyInstaller\loader\pyimod03_importers.py"", line 311, in load_module
    exec(bytecode, module.__dict__)
  File ""C:\Python34\lib\site-packages\pandas\__init__.py"", line 13, in &lt;module&gt;
    ""extensions first."".format(module))
ImportError: C extension: 'lib' not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace' to build the C extensions first.
fitness_of_statistical_tests returned -1
</code></pre>

<p>All of these imports work in the python interpreter, but it's just when I run the exe from PyInstaller that all this stuff breaks.</p>
","3761494","","2166798","","2015-08-31 13:16:36","2015-08-31 13:16:36","PyInstaller statsmodels.stats.diagnostic import kstest_normal","<python-3.x><pandas><pyinstaller><statsmodels>","0","2","","","","CC BY-SA 3.0","1"
"49413824","1","","","2018-03-21 18:15:21","","0","424","<p>I currently have an excel file with, for minimally viable example, say 3 sheets. I want to change 2 of those sheets to be based on new values coming from 2 pandas dataframes (1 dataframe for each sheet). </p>

<p>This is the code I currently have:</p>

<pre><code>from openpyxl.writer.excel import ExcelWriter
from openpyxl import load_workbook

path = r""Libraries\Documents\Current_Standings.xlsx""
book = load_workbook('Current_Standings.xlsx')
writer = pd.ExcelWriter(path, 'Current_Standings.xlsx', 
engine='openpyxl')
writer.book = writer
Blank_Propensity_Scores.to_excel(writer, sheet_name = 
'Blank_Propensity.xlsx')
Leads_by_Rep.to_excel(writer,sheet_name = 'Leads_by_Rep.xlsx')
writer.save()
</code></pre>

<p>when I run this I get the following error message, not sure why, because every stack overflow answer I have looked at has only 1 item for openpyxl:</p>

<pre><code>TypeError: __new__() got multiple values for argument 'engine'
</code></pre>

<p>I also tried playing around with getting rid of the engine='openpyxl' argument but when I do that I get the following error message instead:</p>

<pre><code>ValueError: No Excel writer 'Current_Standings.xlsx'
</code></pre>
","7620499","","","","","2018-03-22 19:27:19","Modifying an existing excel workbook's multiple worksheets based on pandas dataframe","<python><python-3.x><pandas><openpyxl><pandas.excelwriter>","1","7","1","","","CC BY-SA 3.0","1"
"48959301","1","48959495","","2018-02-24 04:15:08","","1","424","<p>Using: Python 3.6, Pandas 0.22</p>

<p>I am trying to take the final line of the DataFrame where I am calculating the average and write it to a new .csv file in a specific format.</p>

<pre><code>df2 = pd.read_csv(""Data.csv"")

gname = df2.groupby(['NAME'])

for NAME,  NAME_df2 in gname:
    df2['DATE'] = pd.to_datetime(df2['DATE'])
    df2.groupby([df2.DATE.dt.month,'NAME'])['SNOW'].mean().sort_values().to_csv('avg.csv')
</code></pre>

<p>Here is my desired output for the avg.csv file:</p>

<pre><code>NAME   MONTH    AVERAGE
    GRAND RAPIDS GERALD R FORD INTERNATIONAL AIRPORT, MI US January, 0.006451613
</code></pre>

<p>In my head, the logic should be:</p>

<pre><code>df2.groupby([df2.DATE.dt.month,'NAME'])['SNOW'].mean().sort_values().to_csv('avg.csv', columns = 'NAME', 'MONTH', 'AVERAGE')
</code></pre>

<p>I have tried to create the months and column header 'AVERAGE' in variables to write to the new file, but that does not work. I have been searching through documentation to test more things, but cannot find anything relevant to this issue.</p>

<p>Or another attempt to create the new column, but this does not work for linking them with the 'NAME' column:</p>

<pre><code>df2 = df2.convert_objects(convert_numeric=True)
df['MONTH']='?'
</code></pre>
","7637930","","13302","","2018-02-28 19:23:57","2018-02-28 19:23:57","Python Pandas Write Calculations to New Csv File","<python><python-3.x><pandas>","1","1","","","","CC BY-SA 3.0","1"
"49623249","1","49623739","","2018-04-03 06:02:09","","1","421","<p>I am new to data science. I want to check which elements from one data frame exist in another data frame, e.g.</p>

<pre><code>df1 = [1,2,8,6]
df2 = [5,2,6,9]

# for 1 output should be False

# for 2 output should be True

# for 6 output should be True
</code></pre>

<p>etc.</p>

<p>Note: I have matrix not vector. </p>

<p>I have tried using the following code:</p>

<pre><code>import pandas as pd
import numpy as np

    priority_dataframe = pd.read_excel(prioritylist_file_path, sheet_name='Sheet1', index=None)

    priority_dict = {column: np.array(priority_dataframe[column].dropna(axis=0, how='all').str.lower()) for column in
                         priority_dataframe.columns}
    keys_found_per_sheet = []
    if file_path.lower().endswith(('.csv')):
        file_dataframe = pd.read_csv(file_path)
    else:
        file_dataframe = pd.read_excel(file_path, sheet_name=sheet, index=None)

    file_cell_array = list()
    for column in file_dataframe.columns:
        for file_cell in np.array(file_dataframe[column].dropna(axis=0, how='all')):
            if isinstance(file_cell, str) == 'str':
                file_cell_array.append(file_cell)
            else:
                file_cell_array.append(str(file_cell))

    converted_file_cell_array = np.array(file_cell_array)

    for key, values in priority_dict.items():
        for priority_cell in values:
            if priority_cell in converted_file_cell_array[:]:
                keys_found_per_sheet.append(key)
                break
</code></pre>

<p>I am doing something wrong in <code>if priority_cell in converted_file_cell_array[:]</code> ?</p>

<p>Is there any other efficient way to do that?</p>
","4447920","","202229","","2019-08-13 09:33:24","2019-08-13 09:33:24","Compare values from two pandas data frames, order-independent","<python><python-3.x><pandas><unordered>","2","12","","","","CC BY-SA 4.0","1"
"57659624","1","57659962","","2019-08-26 14:12:05","","1","417","<p><strong>BACKGROUND:</strong> Large excel mapping file with about 100 columns and 200 rows converted to .csv. Then stored as dataframe. General format of df as below. </p>

<p>Starts with a named column (e.g. Sales) and following two columns need to be renamed. This pattern needs to be repeated for all columns in excel file.</p>

<p><strong>Essentially</strong>: Link the subsequent 2 columns to the ""parent"" one preceding them. </p>

<pre><code> Sales Unnamed: 2  Unnamed: 3  Validation Unnamed: 5 Unnamed: 6
0       Commented  No comment             Commented  No comment                                   
1     x                                             x                        
2                            x          x                                                
3                x                                             x 
</code></pre>

<p><strong>APPROACH FOR SOLUTION:</strong> I assume it would be possible to begin with an index (e.g. index of Sales column 1 = x) and then rename the following two columns as (x+1) and (x+2). 
Then take in the text for the next named column (e.g. Validation) and so on.</p>

<p>I know the <code>rename()</code> function for dataframes.</p>

<p>BUT, not sure how to apply the iteratively for <em>changing</em> column titles.</p>

<p><strong>EXPECTED OUTPUT:</strong> Unnamed 2 &amp; 3 changed to Sales_Commented and Sales_No_Comment, respectively. </p>

<p>Similarly Unnamed 5 &amp; 6 change to Validation_Commented and Validation_No_Comment.</p>

<p>Again, repeated for all 100 columns of file. </p>

<p>EDIT: Due to the large number of cols in the file, creating a manual list to store column names is not a viable solution. I have already seen this elsewhere on SO. Also, the amount of columns and departments (Sales, Validation) changes in different excel files with the mapping. So a dynamic solution is required. </p>

<pre><code>  Sales Sales_Commented Sales_No_Comment Validation Validation_Commented Validation_No_Comment
0             Commented       No comment                       Commented            No comment
1     x                                                                x                      
2                                      x                                                      
3                     x                           x                                          x
</code></pre>

<p>As a python novice, I considered a possible approach for the solution using the limited knowledge I have, but not sure what this would look like as a workable code.</p>

<p>I would appreciate all help and guidance.</p>
","11900800","","11900800","","2019-08-26 14:43:22","2019-09-13 16:19:56","Renaming columns in dataframe w.r.t another specific column","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"49327905","1","49328902","","2018-03-16 19:06:16","","5","417","<p>Given the following data:</p>

<pre><code>df1

               a       b    c
 1/1/2017   -162    1525     -41
 1/2/2017    192    1530      86
 1/3/2017     33    1520    -124
 1/4/2017    173    1502    -108
 1/5/2017    194    1495     -31
 1/6/2017    -15    1520     -46
 1/7/2017     52    1525     181
 1/8/2017     -2    1530    -135
 1/9/2017     37    1540      65
1/10/2017    197    1530      73

df2

              a
1/3/2017     33
1/6/2017    -15
1/7/2017     52
1/8/2017     -2
1/9/2017     37
</code></pre>

<p>How can I produce at chart using matplotlib, that plots <code>'b'</code> column of <code>df1</code> and on top of that, puts markers on same plot line, but using the index points from <code>df2</code>.</p>

<p>The desired chart would look something like this:</p>

<p><a href=""https://i.stack.imgur.com/5gMAS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5gMAS.png"" alt=""enter image description here""></a></p>

<p>I looked at <a href=""https://stackoverflow.com/questions/8409095/matplotlib-set-markers-for-individual-points-on-a-line"">this answer</a> but can't quite adapt it.  The issue is that in the example they use the values, but in my case the part that is common between the two data-sets is the index</p>

<p>This is code from the referenced question that I tried:</p>

<pre><code>xs = df1['b']
ys = df2['a'] # ---&gt; this doesn't make sense here....
markers_on = df2.index
plt.plot(xs, ys, '-gD', markevery=markers_on)
plt.show()
</code></pre>

<p>But the chart comes back empty:</p>

<pre><code>TypeError: &lt;class 'NoneType'&gt; type object None
</code></pre>

<p>I also tried</p>

<pre><code>xs = df1['b']
markers_on = list(df2.index)
plt.plot(xs, '-gD', markevery=markers_on)
plt.show()
</code></pre>

<p>But I get</p>

<pre><code>ValueError: `markevery` is iterable but not a valid form of numpy fancy indexing
</code></pre>
","3396911","","3396911","","2018-03-19 22:06:17","2018-03-19 22:06:17","How to plot overlapping series using line and markers?","<python><python-3.x><pandas><matplotlib><time-series>","1","5","0","","","CC BY-SA 3.0","1"
"56930145","1","56930296","","2019-07-08 07:25:05","","1","415","<p>Need to run multiple single-factor (univariate) regression models in python between a column in a dataframe and several other columns in the same dataframe</p>

<p>-</p>

<h2><a href=""https://i.stack.imgur.com/YEqyE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YEqyE.png"" alt=""enter image description here""></a></h2>

<p>so based on the image, i want to run regression models between x1 &amp; dep, x2 &amp; dep and so on and so forth</p>

<p>Want to output - beta, intercept, R-sq, p-value, SSE, AIC, BIC, Normality test of residuals etc</p>
","11753064","","5745500","","2019-07-08 09:31:51","2019-07-08 09:31:51","univariate regression in python","<python-3.x><pandas><jupyter-lab>","1","2","","","","CC BY-SA 4.0","1"
"56766994","1","56768047","","2019-06-26 07:01:51","","-1","407","<p>I have more than 1000 csv files , i want to combine  where csv filename first five digits are same in to one csv file.</p>

<pre><code>    input:
    100044566.csv
    100040457.csv
    100041458.csv
    100034566.csv
    100030457.csv
    100031458.csv
    100031459.csv


import pandas as pd
import os
import glob
path_1 =''
all_files_final = glob.glob(os.path.join(path_1, ""*.csv""))
names_1 = [os.path.basename(x1) for x1 in all_files_final]
final = pd.DataFrame()

for file_1, name_1 in zip(all_files_final, names_1):
    file_df_final = pd.read_csv(file_1,index_col=False)
    #file_df['file_name'] = name
    final = final.append(file_df_final)
final.to_csv('',index=False)


</code></pre>

<p>i used the above code but its merging all files in to one csv file , i dont know have to make selection based on the name </p>

<p>so from above input
output 1: combine first three csv files in one csv file because filename first five digits are same.  </p>

<p>output 2: combine next 4 files in one csv files because filename first five digits are same.</p>
","11327242","","11327242","","2019-06-26 07:13:21","2019-06-26 08:04:05","How to combine multiple csv files based on file name","<python-3.x><pandas><numpy><pandas-groupby>","1","4","","","","CC BY-SA 4.0","1"
"57261495","1","","","2019-07-29 21:14:47","","1","404","<p>I am having trouble piecing together the last part of a puzzle. The entire code is shown below, which includes a non-essential username and password to a site where I am scraping data. </p>

<p>After looping through part numbers from an Excel file using </p>

<pre class=""lang-py prettyprint-override""><code>pd.read_excel()
</code></pre>

<p>Selenium is used to scrape various items of the website in question; the code then writes these values to the output window successfully.</p>

<p>As opposed to writing the data to an output window, I aim to write to the same Excel file I am pulling data from, writing it to the appropriate columns.</p>

<p>In the final <code>for</code> loop of the code, I initially tried to write the variables (which were printing to the screen) to Excel by appending </p>

<pre class=""lang-py prettyprint-override""><code>.to_excel('filePathHere') 
</code></pre>

<p>to the variable in question. As an example, I attempted </p>

<pre class=""lang-py prettyprint-override""><code>description.to_excel('pathToFile/output.xlsx')
</code></pre>

<p>Which yield an error of <code>EOL while scanning string literal (&lt;string&gt;, line 1)</code></p>

<p>I then thought, maybe this variable needs to be converted to a DataFrame, so I then tried</p>

<pre class=""lang-py prettyprint-override""><code>description_DataFrame = pd.DataFrame(description)
description_DataFrame.to_excel('pathToFile/output.xlsx')
</code></pre>

<p>which resulted in the same error message. </p>

<p>I am not even sure if this is the correct logic to write each item to the existing (or new) file. If it is, I found an explanation on how to deal with long strings here: <a href=""https://stackoverflow.com/questions/3561691/python-syntaxerror-eol-while-scanning-string-literal"">StackOverFlow EOL Error</a> but none of my data constitutes as long strings, so I can't see how that applies.</p>

<p>I then start to think I might need to create a dictionary, and then append to it.
So I then removed any attempts from above and tried:</p>

<pre class=""lang-py prettyprint-override""><code>description = []
description.append(mfg_part)
mfg_part.to_excel('pathToFile/output.xlsx')
</code></pre>

<p>Which still give me the same EOL error.</p>

<p>I am not to sure what is wrong, and why I can't write the variables <code>mfg_part, mfg_OEM, description</code> to their respective columns in the loaded Excel file.</p>

<p>Any hints / tips would be greatly appreciated.   </p>

<p>complete working code, printing to the screen is as follows:</p>

<pre class=""lang-py prettyprint-override""><code>import time
#Need Selenium for interacting with web elements
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.keys import Keys

#Need numpy/pandas to interact with large datasets
import numpy as np
import pandas as pd
import itertools


# load in manufacture part number from a collection of components, via an Excel file
mfg_id_list = pd.read_excel(""C:/Users/James/Documents/Python Scripts/jupyterNoteBooks/ScrapingData/MasterQuoteTemplate.xls"")['Model']

# Create a dictionary to store product and price
# While the below works just fine, we want to create en empty pandas dataframe, so we can output to Excel later
productInfo = {}

chrome_path = r""C:\Users\James\Documents\Python Scripts\jupyterNoteBooks\ScrapingData\chromedriver_win32\chromedriver.exe""
driver = webdriver.Chrome(chrome_path)
driver.maximize_window()
driver.get(""https://www.tessco.com/login"")

userName = ""FirstName.SurName321123@gmail.com""
password = ""PasswordForThis123""

#Set a wait, for elements to load into the DOM
wait10 = WebDriverWait(driver, 10)
wait20 = WebDriverWait(driver, 20)
wait30 = WebDriverWait(driver, 30)

elem = wait10.until(EC.element_to_be_clickable((By.ID, ""userID""))) 
elem.send_keys(userName)

elem = wait10.until(EC.element_to_be_clickable((By.ID, ""password""))) 
elem.send_keys(password)

#Press the login button
driver.find_element_by_xpath(""/html/body/account-login/div/div[1]/form/div[6]/div/button"").click()

for i in mfg_id_list:

    #Expand the search bar
    searchBar = wait10.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ""#searchBar input"")))

    #Enter information into the search bar
    #If cell is not blank

    if len(str(i)) != 0:
        searchBar.send_keys(Keys.CONTROL, 'a')
        searchBar.send_keys(i)
        driver.find_element_by_css_selector('a.inputButton').click()
        time.sleep(5)

        try:
            # wait for the products information to be loaded
            products = wait10.until(EC.presence_of_all_elements_located((By.XPATH,""//div[@class='CoveoResult']"")))
            #isProductsThere = driver.find_element_by_xpath(""//div[@class='CoveoResult']"")

            if products:
                # iterate through all products in the search result and add details to dictionary
                for product in products:

                    # get product info such as OEM, Description and Part Number
                    productDescr = product.find_element_by_xpath("".//a[@class='productName CoveoResultLink hidden-xs']"").text
                    mfgPart = product.find_element_by_xpath("".//ul[@class='unlisted info']"").text.split('\n')[3]
                    mfgName = product.find_element_by_tag_name(""img"").get_attribute(""alt"")

                    # There are multiple classes, some are ""class sale"" or else.
                    #We will locate by CSS
                    price = product.find_element_by_css_selector(""div.price"").text.split('\n')[1]


                    # add details to dictionary
                    productInfo[mfgPart, mfgName, productDescr] = price

                # prints the searched products information   
                for (mfg_part, mfg_OEM, description), price in productInfo.items():
                    mfg_id = mfg_part.split(': ')[1]
                    if mfg_id == i:
                        #Here is where I would write to an Excel file
                        #And where I made attempts as described above
                        print('________________________________________________')
                        print('Part #:', mfg_id)
                        print('Company:', mfg_OEM)
                        print('Description:', description)
                        print('Price:', price)
                        print('________________________________________________')


                #time.sleep(5)
                #driver.close()

            else:
                mfg_id = ""Not on Tessco""
                mfg_OEM = ""Not on Tessco""
                description = ""Not on Tessco""
                price = ""Not on Tessco""
                #driver.close()
                print(""Item was not found on Tessco.com"")


        except Exception as e:
            print('________________________________________________')            
            print(e)
            mfg_id = ""Not on Tessco""
            mfg_OEM = ""Not on Tessco""
            description = ""Not on Tessco""
            price = ""Not on Tessco""
            #driver.close()
            print(""Item was not found on Tessco.com"")
            print('________________________________________________')

driver.close()
</code></pre>
","455748","","","","","2019-07-29 21:14:47","Python, Selenium, Pandas DataFrame and Excel","<excel><python-3.x><pandas><selenium><dataframe>","0","0","","","","CC BY-SA 4.0","1"
"56928421","1","56928980","","2019-07-08 04:28:14","","1","398","<p>I have data set that contains weekly data but I need to calculate the average of it basis the weightage of the row if that week crosses the month. For example:</p>

<pre><code>  Current_Week             Sales
0 29/Dec/2013-04/Jan/2014  3685.236419
1 05/Jan/2014-11/Jan/2014  3784.023564
2 12/Jan/2014-18/Jan/2014  3726.933727
3 19/Jan/2014-25/Jan/2014  3690.440944
4 26/Jan/2014-01/Feb/2014  3731.523630
5 02/Feb/2014-08/Feb/2014  3753.882783
6 09/Feb/2014-15/Feb/2014  3643.997381
7 16/Feb/2014-22/Feb/2014  3696.243919
8 23/Feb/2014-01/Mar/2014  3718.254426
</code></pre>

<p>Ultimately the desired output is:</p>

<pre><code>Month       Sales
1-Jan-2014  3727.09
1-Feb-2014  3703.57
</code></pre>

<p>The thing to note is that for input dataframe at row 0, I need to calculate the <code>weightage</code> of <code>the number of days in that week for that month</code> so that it can be used to calculate the average of the sales later. For eg for month January</p>

<p><a href=""https://i.stack.imgur.com/Z2kd0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Z2kd0.png"" alt=""enter image description here""></a></p>

<p>As you can see that, the monthly Sales for Jan is calculated from summing all the averaged sales and then divided by the number of weighted days: <code>16505.69 / 4.42 = 3727.09</code></p>

<p>I know that I have to first the split the time series into two rows in the event the data crosses the month, and then <code>sum</code> them and <code>aggregate</code> them. Am I missing something?</p>
","2829204","","","","","2019-07-08 05:51:11","Pandas split weekly time series and groupby month","<python><python-3.x><pandas><dataframe>","2","2","","","","CC BY-SA 4.0","1"
"57652192","1","","","2019-08-26 05:26:11","","0","397","<p>I was trying to plot a graph from a dataframe that was created using pivoting a raw csv file. I tried to put the index ""Month"" as x-axis at the time of plotting using xticks</p>

<pre><code>(ax = df.plot(xticks=df.index)
ylab = ax.set_ylabel('Device sold')
</code></pre>

<p>But everytime one error comes after executing the code:</p>

<pre><code>f'units: {x!r}') from e matplotlib.units.ConversionError: Failed to convert value(s) to axis units: Index(['April', 'August', 'February', 'January', 'July', 'June', 'March', 'May'], dtype='object', name='Month')
</code></pre>

<p>The data where i tried to plot the above code is as below :</p>

<p><a href=""https://i.stack.imgur.com/IorzT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IorzT.png"" alt=""[Pivoted data][1][1]: https://i.stack.imgur.com/wqq51.png""></a></p>
","11974157","","11974157","","2019-08-26 08:51:05","2019-08-26 08:51:05","Plotting Pandas dataframe from pivot table, where index is the x-axis","<python><python-3.x><pandas><matplotlib><pivot>","0","6","","","","CC BY-SA 4.0","1"
"48759049","1","","","2018-02-13 03:41:23","","1","397","<p>Python 3.6, Pandas 0.22.0:
I have imported a .csv file called Data.csv. It contains weather data with columns ""NAME"" ""DATE"" ""SNOW"" that references the location name, the date in MM/DD/YYYY format, and the amount of snowfall on that day. I want to group all rows by ""NAME"", then calculate monthly averages of ""SNOW"" so the output displays one row for ""NAME"", ""DATE"" (as a single, monthly entry), and ""SNOW"" with the average for the month. My ""DATE"" column is in datetime format. These entries are over years 2016 and 2017. After everything is grouped by monthly snowfall average, I need to make two new .csv files for each year - 2016, 2017. Here is what I have so far:</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.read_csv('Data.csv', sep = ',')
df1 = df.loc[:, [""NAME"",""DATE"",""SNOW""]]
df1[""DATE""] = pd.to_datetime(df1.DATE)
date['DATE'] = df1.groupby(pd.Grouper(key='DATE', freq=""M"")).mean()
df1.to_csv('meansnow2017.csv', sep = ',', header=True)
</code></pre>

<p>The ""DATE"" column was originally an object. I cannot figure out how to layer these groupby statements in a way that makes sense. This gives me the desired column headers, but does not group my ""DATE"" by month or calculate my averages. Any help is appreciated!</p>
","9297791","","","","","2018-02-13 03:41:23","Use Pandas to take Monthly Averages of data in a .csv file and save by Year in new .csv file","<python-3.x><pandas><pandas-groupby>","0","1","1","","","CC BY-SA 3.0","1"
"48505874","1","48506242","","2018-01-29 16:31:17","","0","395","<p>I'd like to take a dataframe and visualize how useful each column is in a k-neighbors analysis so I was wondering if there was a way to loop through dropping columns and analyzing the dataframe in order to produce an accuracy for every single combination of columns. I'm really not sure if there are some functions in pandas that I'm not aware of that could make this easier or how to loop through the dataframe to produce every combination of the original dataframe. If I have not explained it well I will try and create a diagram.</p>

<p>a | b | c |                | labels |</p>

<p>1 | 2 | 3 |                |   0    |</p>

<p>5 | 6 | 7 |                |   1    |</p>

<p>The dataframe above would produce something like this after being run through the splitting and k-neighbors function:</p>

<p>a &amp; b = 43%</p>

<p>a &amp; c = 56%</p>

<p>b &amp; c = 78%</p>

<p>a &amp; b &amp; c = 95%</p>
","5089092","","5089092","","2018-01-29 16:53:40","2018-01-30 15:31:05","Producing every combination of columns from one pandas dataframe in python","<python><python-3.x><pandas><numpy>","1","1","","","","CC BY-SA 3.0","1"
"51971096","1","51971440","","2018-08-22 16:15:53","","0","395","<p>I found the thread located here:</p>

<p><a href=""https://stackoverflow.com/questions/22917108/appending-row-to-pandas-dataframe-adds-0-column"">Appending row to pandas df adds 0 column</a></p>

<p>but I still don't understand what I am doing wrong.</p>

<pre><code>df4 = pd.DataFrame({'Q':['chair', 'desk', 'monitor', 'chair'], 'R':['red', 'blue', 'yellow', 'purple'], 'S': ['english', 'german', 'spanish', 'english']})

df4

         Q       R        S
0    chair     red  english
1     desk    blue   german
2  monitor  yellow  spanish
3    chair  purple  english


&gt;&gt; df5 = df4

&gt;&gt;&gt; df5 = df5.append(['Q'] * 2, ignore_index=True)

&gt;&gt;&gt; df5

         Q       R        S    0
0    chair     red  english  NaN
1     desk    blue   german  NaN
2  monitor  yellow  spanish  NaN
3    chair  purple  english  NaN
4      NaN     NaN      NaN    Q
5      NaN     NaN      NaN    Q
&gt;&gt;&gt; 
</code></pre>

<p>In my particular case, why did it add the 0 column? My initial DF is not empty.</p>
","5560898","","9209546","","2018-08-22 16:25:51","2018-08-23 14:28:04","Appending row to pandas DF adds 0 column - clarification","<python><python-3.x><pandas><dataframe>","2","1","","","","CC BY-SA 4.0","1"
"49653802","1","49654159","","2018-04-04 14:34:38","","3","393","<p>I have a dataframe with with string variables with duplicate identifiers and a lot of empty cells. </p>

<p>I want to group by the identifier and have all the values in one row. In case that a column has multiple entries vor one identifier, I need a new row with a suffix, so I can identify it later. </p>

<p>Here is my data</p>

<pre><code>ID   name1  name2   name3   name4   name5   name6   name7   name8
Tom  sarah          mike                
Tom                                 john    john        
Gen  paul                           
Gen         sandra                      
Gen                 lara    lara    lara    lara        
Gen                                                 mike    mike
Lara bill   bill    bill                    
Lara                josh    josh            
Lara                kevin   kevin   kevin   mike    
Lara                                        devon   devon   devon
</code></pre>

<p>This is the code I have been usign so far from <a href=""https://stackoverflow.com/questions/19530568/can-pandas-groupby-aggregate-into-a-list-rather-than-sum-mean-etc"">here</a>:</p>

<pre><code>grouped = df1.groupby('ID')
df1 = grouped.aggregate(lambda x: tuple(x))
</code></pre>

<p>Which gives me the following results:</p>

<pre><code>                      name1                    name2                  name3  \
ID                                                                            
Gen   (paul, nan, nan, nan)  (nan, sandra, nan, nan)  (nan, nan, lara, nan)   
Lara  (bill, nan, nan, nan)    (bill, nan, nan, nan)  (bill, nan, nan, nan)   
Tom            (sarah, nan)               (nan, nan)             (nan, nan)   

                        name4                    name5  \
ID                                                       
Gen     (nan, nan, lara, nan)    (nan, nan, lara, nan)   
Lara  (nan, josh, kevin, nan)  (nan, josh, kevin, nan)   
Tom               (mike, nan)              (nan, john)   

                       name6                    name7                   name8  
ID                                                                             
Gen    (nan, nan, lara, nan)    (nan, nan, nan, mike)   (nan, nan, nan, mike)  
Lara  (nan, nan, kevin, nan)  (nan, nan, mike, devon)  (nan, nan, nan, devon)  
Tom              (nan, john)               (nan, nan)              (nan, nan) 
</code></pre>

<p>But this is, what I actually want and I just can not figure out how to do this: </p>

<pre><code>ID   name1  name2   name3   name3_suffixA   name3_suffixB   name4   name4_suffixA   name5   name6   name6_suffixA   name7   name8
Tom  sarah          mike                                                            john    john            
Gen  paul   sandra  lara                                    lara                    lara    lara                    mike    mike
Lara bill   bill    bill    josh            kevin           josh    kevin           kevin   mike    devon           devon   devon
</code></pre>

<p>The actuall name of the suffix doesn't matter, nor does it matter, if the additional entries are presented at the end or in between. </p>

<p>There are some similiar questions, I know that. But I couldn't work any of the cases/solutions and I would really appreciate some help.  </p>
","8880321","","","","","2018-04-04 16:06:27","Pandas groupby, aggregate on string variable and move up empty cells","<python-3.x><pandas><aggregate><pandas-groupby>","2","0","1","","","CC BY-SA 3.0","1"
"56815836","1","56815930","","2019-06-29 07:27:30","","6","390","<p>I have an input dataframe which can be generated from the code given below</p>

<pre><code>  df = pd.DataFrame({'subjectID' :[1,1,2,2],'keys': 
  ['H1Date','H1','H2Date','H2'],'Values': 
  ['10/30/2006',4,'8/21/2006',6.4]})
</code></pre>

<p>The input dataframe looks like as shown below</p>

<p><a href=""https://i.stack.imgur.com/jMv0Q.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/jMv0Q.png"" alt=""enter image description here""></a></p>

<p>This is what I did</p>

<pre><code>s1 = df.set_index('subjectID').stack().reset_index()

s1.rename(columns={0:'values'}, 
             inplace=True)
d1 = s1[s1['level_1'].str.contains('Date')]
d2 = s1[~s1['level_1'].str.contains('Date')]

d1['g'] = d1.groupby('subjectID').cumcount()
d2['g'] = d2.groupby('subjectID').cumcount()

d3 = pd.merge(d1,d2,on=[""subjectID"", 'g'],how='left').drop(['g','level_1_x','level_1_y'], axis=1)
</code></pre>

<p>Though it works, I am afraid that this may not be the best approach. As we might have more than 200 columns and 50k RECORDS. Any help to improve my code further is very helpful. </p>

<p>I expect my output dataframe to look like as shown below</p>

<p><a href=""https://i.stack.imgur.com/CuU9e.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/CuU9e.png"" alt=""enter image description here""></a></p>
","10829044","","","","","2019-06-29 07:47:27","Create a new column based on previous row value and delete the current row","<python><python-3.x><pandas><dataframe><for-loop>","1","0","1","","","CC BY-SA 4.0","1"
"58533403","1","58533834","","2019-10-24 02:42:38","","0","389","<p>I have a pandas dataframe like below:</p>

<pre><code>data=[['A',1,30],
      ['A',1,2],
      ['A',0,4],
      ['A',1,4],
      ['B',0,5],
      ['B',1,1],
      ['B',0,5],
      ['B',1,8]]

df = pd.DataFrame(data,columns=['group','var_1','var_2'])
</code></pre>

<p>I want to create a series of values with index based on below condition:</p>

<p>Step 1) Increment should always happen from 1st row of 'var_2'of each group. For example: for group A, the increment should start from 30 and for group B, increment should start from 5 </p>

<p>Step 2) Incremented value where 'var_1"" = 1</p>

<p>My desired output:</p>

<pre><code>0    30
1    31
3    32
5    6
7    7
</code></pre>
","8340881","","","","","2019-10-24 03:43:40","increment column value by 1 based on the previous row","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"56974974","1","56975221","","2019-07-10 16:26:45","","0","382","<p>I am trying to bin a multi-year time series of dates and float values. I'm trying to aggregate each day in to 15 minute bins.  So I group the data set by day and then resample in 15 minute increments on each day.</p>

<p>The results seemed odd so I took a closer look at the behaviour of the resampling.  The code below summarizes the kind of results I observed (I run it in repl.it)</p>

<pre><code>aindex = pd.to_datetime([
""2013-04-05 04:15:31"",
""2013-04-05 05:15:18"",
""2013-04-05 05:15:19"",
""2013-04-05 05:15:19"",
""2013-04-05 05:17:15"",
""2013-04-05 07:06:31"",
""2013-04-09 04:15:31"",
""2013-04-09 05:15:18"",
""2013-04-09 05:15:19"",
""2013-04-09 05:15:19"",
""2013-04-09 05:17:15"",
""2013-04-09 07:06:31"",
""2013-04-09 07:21:28"",
""2013-04-09 09:18:19"",
""2013-04-09 09:19:19"",
""2013-04-09 09:21:31""])
a = pd.Series([-4.50e+08,
-4.80e+08,
-6.10e+08,
-5.80e+08,
-5.70e+08,
-5.710e+08,
-4.598432e+08,
-4.814140e+08,
-6.109284e+08,
-5.870819e+08,
-5.759888e+08,
-5.713363e+08,
-5.275122e+07,
-2.853787e+08,
-2.523782e+08,
-4.273267e+08],aindex)
print(a)
print(a.groupby(a.index).apply(lambda x: x))
print(a.resample(""15T"", base=0).apply(lambda x: x))
print(a.groupby(a.index).resample(""15T"").apply(lambda x: x))
</code></pre>

<p>'groupby' behaves as expected but note that each value of 'x' is type pd.Series. 'resample' also returns type pd.Series but appears to miss values when I display it in repl.it or Jupyter but if you change .apply(lambda x: x) to  .apply(lambda x: list(x)) you can see there are actually multiple values.  'groupby'+'resample' almost does what I expected ie. each day has 15 minute bins except the indexing is wrong anywhere a 'resample' returned more than one value.</p>

<p>I'm trying to understand what I'm seeing so I can apply the process with confidence.  Is this correct behaviour and if so why?</p>

<p>Note: To clarify a bit more my expectations.  If I look at the result of a resample for one day then resample includes empty bins:</p>

<pre><code>2013-04-05 04:15:00   -450000000.0
2013-04-05 04:30:00            NaN
2013-04-05 04:45:00            NaN
2013-04-05 05:00:00            NaN
2013-04-05 05:15:00   -570000000.0
2013-04-05 05:30:00            NaN
2013-04-05 05:45:00            NaN
2013-04-05 06:00:00            NaN
2013-04-05 06:15:00            NaN
2013-04-05 06:30:00            NaN
2013-04-05 06:45:00            NaN
2013-04-05 07:00:00   -571000000.0
2013-04-05 07:15:00            NaN
2013-04-05 07:30:00            NaN
2013-04-05 07:45:00            NaN
2013-04-05 08:00:00            NaN
2013-04-05 08:15:00            NaN
2013-04-05 08:30:00            NaN
2013-04-05 08:45:00            NaN
2013-04-05 09:00:00            NaN
2013-04-05 09:15:00            NaN
2013-04-05 09:30:00            NaN
2013-04-05 09:45:00            NaN
2013-04-05 10:00:00            NaN
</code></pre>

<p>But if a groupby is done first I don't get empty bins.  Why not?:</p>

<pre><code>...
2013-04-05 04:15:31  2013-04-05 04:15:00   -450000000.0
2013-04-05 05:15:18  2013-04-05 05:15:00   -480000000.0
2013-04-05 05:15:19  2013-04-05 05:15:00   -580000000.0
2013-04-05 05:17:15  2013-04-05 05:15:00   -570000000.0
2013-04-05 07:06:31  2013-04-05 07:00:00   -571000000.0
...
</code></pre>
","11318930","","11318930","","2019-07-10 17:38:55","2019-07-11 09:01:14","Pandas resample() Series giving incorrect indexes","<python-3.x><pandas><time-series>","1","1","","","","CC BY-SA 4.0","1"
"56977170","1","56978349","","2019-07-10 19:07:28","","0","380","<p><a href=""https://i.stack.imgur.com/9EdvE.jpg"" rel=""nofollow noreferrer"">Picture Of Table</a></p>

<p>I have been trying to extract a table from a website, only if the table contains a specific substring. </p>

<p>I use requests to open a URL and the pandas.html to extract the tables. However, by doing this, I either extract all tables or a specific table by index and I want to find a way to only extract the table that has my keyword.</p>

<pre><code>import requests
import pandas as pd

#url is the website, html opens the site and df_list is extracting all tables

url = 'https://www.sec.gov/Archives/edgar/data/880432/000114420415073214/v427721_def14a.htm'

html = requests.get(url).content

df_list = pd.read_html(html)
</code></pre>

<p>From here I can print <code>df_list[index]</code> but I want the table with my keyword. I have tried the following: (none return anything)</p>

<pre><code>for i in range(len(df_list)):
    if 'Fees Earned' in df_list:
        print (df_list[i])

for i in range(len(df_list)):
    if any(""Fees Earned"" in s for s in df_list):
        print(df_list[i])
</code></pre>

<p>If I try:</p>

<pre><code>print(any(sub in mystring for mystring in mylist))
</code></pre>

<p>I only receive the output ""False""</p>
","11766640","","11766640","","2019-07-10 20:38:52","2019-07-11 07:05:45","Finding Specific Table from Website by String using Pandas","<python><python-3.x><pandas><python-requests>","2","10","1","","","CC BY-SA 4.0","1"
"57861364","1","57861436","","2019-09-09 21:22:46","","6","379","<p>I have a pandas dataframe like this</p>

<pre class=""lang-py prettyprint-override""><code>  Windows Linux Mac
0 True    False False
1 False   True  False
2 False   False True
</code></pre>

<p>I want to combine these three columns in a single column like this</p>

<pre class=""lang-py prettyprint-override""><code>  OS
0 Windows
1 Linux
2 Mac
</code></pre>

<p>I know that I can write a simple function like this</p>

<pre class=""lang-py prettyprint-override""><code>def aggregate_os(row):
   if row['Windows'] == True:
      return 'Windows'
   if row['Linux'] == True:
      return 'Linux'
   if row['Mac'] == True:
      return 'Mac'
</code></pre>

<p>which I can call like this</p>

<pre class=""lang-py prettyprint-override""><code>df['OS'] = df.apply(aggregate_os, axis=1)
</code></pre>

<p>The problem is that my dataset is huge and this solution is too slow. Is there a more efficient way of doing this aggregation?</p>
","12043687","","","","","2019-09-09 21:36:19","Melt multiple boolean columns in a single column in pandas","<python-3.x><pandas><performance><dataframe>","2","0","3","","","CC BY-SA 4.0","1"
"57160670","1","57161259","","2019-07-23 09:13:17","","0","379","<p>I have a dataframe that looks somewhat like this:</p>
<pre><code>data = [{&quot;x&quot; : &quot;john&quot;, &quot;y&quot; : 0.0997, &quot;hue&quot; : 'cat. 1'},
            {&quot;x&quot; : &quot;john&quot;, &quot;y&quot; : 0.2337, &quot;hue&quot; : 'cat. 1'}, 
            {&quot;x&quot; : &quot;lisa&quot;, &quot;y&quot; : 0.1997, &quot;hue&quot; : 'cat. 2'},
            {&quot;x&quot; : &quot;lisa&quot;, &quot;y&quot; : 0.9957, &quot;hue&quot; : 'cat. 3'},
            {&quot;x&quot; : &quot;john&quot;, &quot;y&quot; : 0.8197, &quot;hue&quot; : 'cat. 2'}]

data = pd.DataFrame(data)
</code></pre>
<p>I try to plot a <code>seaborn</code> category plot:</p>
<pre><code>sns.catplot(x=data.x, y=data.y, hue=data.hue, size=8, data=data)
</code></pre>
<p>A plot is actually returned - but with weird axes and along with an error message:</p>
<p><a href=""https://i.stack.imgur.com/7hgvO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7hgvO.png"" alt=""enter image description here"" /></a></p>
<p>The error I get is:</p>
<p><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</code></p>
<p>I played arround with the margins. But the only improvement I got is the error:</p>
<p><code>ValueError: left cannot be &gt;= right</code></p>
<p>What am I doing wrong? How can I solve the issue/issues?</p>
","5779017","","-1","","2020-06-20 09:12:55","2019-11-19 16:59:56","Problems with Seaborn Category Plot","<python-3.x><pandas><plot><error-handling><seaborn>","1","0","","","","CC BY-SA 4.0","1"
"50417282","1","","","2018-05-18 18:28:46","","11","377","<p>I ran into a weird bug this morning that I'm hoping to get some background info on. I have a work around, but I'd like to know if there is some deeper issue. I could not find anything on SE or elsewhere online.</p>

<p>The bug:
If I import pandas before matplotlib.pyplot I get the error message:</p>

<pre><code>Traceback (most recent call last):
  File ""test.py"", line 5, in &lt;module&gt;
    import matplotlib.pyplot as plt
  File ""/usr/lib64/python3.6/site-packages/matplotlib/pyplot.py"", line 115, in &lt;module&gt;
    _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup()
  File ""/usr/lib64/python3.6/site-packages/matplotlib/backends/__init__.py"", line 32, in pylab_setup
    globals(),locals(),[backend_name],0)
ModuleNotFoundError: No module named 'matplotlib.backends.backend_qt4agg'
</code></pre>

<p>Code that works:</p>

<pre><code>import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas

plt.plot([1,2,3],[3,2,1])
plt.show()
</code></pre>

<p>Code that doesn't work any more:</p>

<pre><code>import pandas
import matplotlib as mpl
import matplotlib.pyplot as plt

plt.plot([1,2,3],[3,2,1])
plt.show()
</code></pre>

<p>Note: running the code from a file fails (python3 test.py), but running it from the command line interpreter works.</p>

<p>I attempted to update my python packages this morning, though I didn't think it did anything. I wanted to get Pandas 23 to use a function I saw in their docs. I updated pip3 ('pip install --user pip') thinking it would have the most recent version. I had some trouble with pip 10.0.1 (see <a href=""https://github.com/pypa/pip/issues/5221"" rel=""noreferrer"">https://github.com/pypa/pip/issues/5221</a> ), but that seems to have been magically resolved despite not really doing any of the proposed solutions on that thread. In any case, it turned out I was using the most recent version of Pandas available on pip (0.20.1), so I kept that. I also did a 'dnf update' this morning too trying to solve the pip issue.</p>

<p>I'm using Python 3.6.5, Pandas 0.20.1, and Matplotlib 2.0.0. Running Fedora 26. My code was working fine (with pandas importing before pyplot) until the attempted updates.</p>

<p>The point is, I did several things that could have broken it and I'd like to find out why. It seems wrong that the import order should matter. Any ideas on what is causing this? If it is actually a bug (and not just a pebkac issue), where do I report it?</p>

<p>Cheers</p>
","9812984","","9812984","","2018-07-19 19:58:09","2019-12-23 05:33:37","ModuleNotFoundError if I import pandas before pyplot","<python-3.x><pandas><matplotlib><pip><importerror>","1","8","","","","CC BY-SA 4.0","1"
"57700077","1","","","2019-08-28 21:10:47","","0","370","<p>I'm working with some data where I'm trying to convert an entire column to a different format (ie from object to datetime or from object to numeric) using methods not resetting values. Each line of code below returns the 'SettingwithCopyWarning' error:</p>

<pre><code>#converting euro values column 'value' to numeric values:

df['value'] = pd.to_numeric(df.value, errors='coerce')

#converting object to datetime in order to extract year:

df['date'] = pd.to_datetime(df['date'])

df['date'] = df['date'].dt.year
</code></pre>

<p>If I leave any of the above lines in, it causes an error. If I take all of them out, the code doesn't raise any warnings.</p>

<p>After some research, I learned the 'SettingwithCopyWarning' crops up when chained assignments are used, and the view is a copy of the dataframe as opposed to the dataframe itself, (ref: <a href=""https://www.dataquest.io/blog/settingwithcopywarning/"" rel=""nofollow noreferrer"">https://www.dataquest.io/blog/settingwithcopywarning/</a>).</p>

<p>I also learned that the general form to avoid chained assignments is <code>df.loc[&lt;mask or index label values&gt;, &lt;optional column&gt;] = &lt; new scalar value or array like&gt;</code> (ref:<a href=""https://stackoverflow.com/questions/32357311/python-pandas-how-to-avoid-chained-assignment"">python pandas: how to avoid chained assignment</a>).</p>

<p>I tried to wrangle something together like this just to test out the form:</p>

<pre><code>df.loc[df['value']] = pd.to_numeric(df.value, errors='coerce')
</code></pre>

<p>but it returns an error like:</p>

<pre><code>KeyError: ""['$3.40m' '$3.90m' '$12.60m' '$13.80m' '$123.80m' '$171.20m'\n '$205.2m' '$214.40m' '$221.03m'] not in index""
</code></pre>

<p>which is making me think the general form I tried to stuff it in is confusing it for a dictionary and raising a KeyError. </p>

<p>After looking around, I'm not sure how to apply this to entire columns (like my code) that are using methods (dot functions) without using chained assignments.</p>

<p>Is there a way around this?</p>

<p>Edit: </p>

<p>Lines above the given code:</p>

<pre><code>parent_df = pd.DataFrame.from_records(data, columns = ['date', value'])

df = parent_df[parent_df.date.str.contains('.*201[4-9]')]
</code></pre>
","4459644","","4459644","","2019-08-28 21:40:31","2019-08-28 21:40:31","Avoiding 'SettingwithCopyWarning' while converting column data with methods","<python-3.x><pandas><dataframe><warnings><chained-assignment>","0","4","1","","","CC BY-SA 4.0","1"
"49368371","1","49369353","","2018-03-19 16:54:05","","1","368","<p>I have a column in a data frame with mixed date formats. How do I segregate it according to the different date formats.</p>

<p>For e.g I want something like this</p>

<p>df1 = dataframe[dataframe['Cl_date'] is '%d%b%y']
df2 = dataframe[dataframe['Cl_date'] is '%b%y]</p>

<p>Please help</p>
","9463244","","6567270","","2018-03-19 23:19:14","2018-03-19 23:19:14","Filter panda column according to the type of date format","<python-3.x><pandas><date><datetime>","2","3","","","","CC BY-SA 3.0","1"
"49013318","1","49013470","","2018-02-27 16:21:16","","4","367","<p>I have a pandas datafame where the rows in a particular column are sets of id's. I would like to aggregate across a 15min period and find all such unique id's.</p>

<pre><code>timestamp  |         ids           |  some_int
00:03:00     {id1, id2, id3}           5
00:10:00     {id2, id4, id7, id10}     9
00:25:00     {id7, id22, id24}         10
00:45:00     {id23, id30}              24


df.resample('15min').agg({'ids': ??, 'some_int': sum)
</code></pre>

<p>I've tried sum and a few other transformations on the ids column but I don't quite have it yet. </p>
","3429954","","9209546","","2018-03-02 04:01:05","2020-08-26 11:05:04","How to aggregate columns of sets?","<python><python-3.x><pandas><dataframe>","3","1","1","","","CC BY-SA 3.0","1"
"57073481","1","","","2019-07-17 10:11:08","","2","365","<p>I want to merge the rows with similar column ids, there are multiple columns in the dataset approx 50. Now in one row with id=""ABC"", there are values in 25 columns and in another row with id=""ABC"" there are values in the rest 25 columns. I want to have only a single unique id with values in all 50 columns.
Basically, want to merge rows with duplicate ids.
But If there is no value in id column but there is a value in id2 or id3 then the row should not be deleted.</p>

<p>GIVEN DATAFRAME :</p>

<pre><code>    id         value1  value2  value3   value4  id2    id3
    ABC        100                       ABD    AND    
    ABC                 101     UBC                    DND
    XYZ        200              ANF      BAC    ALC    BLC
    XYZ                 202
               200      300     QWE      RTY    FGH    IJK
                        501     UIO      JKL    QWR     
</code></pre>

<p>EXPECTED OUTPUT:</p>

<pre><code>    id       value1    value2 value3  value4  id2   id3
    ABC        100       101    UBC     ABD   AND   DND
    XYZ        200       202    ANF     BAC   ALC   BLC
               200       300    QWE     RTY   FGH   IJK
                         501    UIO     JKL   QWR

</code></pre>

<p>There are around 50 different columns in the actual dataset.</p>
","11573965","","11573965","","2019-07-18 06:51:10","2019-07-18 06:51:10","How to merge 2 rows of a Python Data frame with same column id into one row?","<python><python-3.x><pandas><dataframe><dataset>","1","0","","","","CC BY-SA 4.0","1"
"57659277","1","57660165","","2019-08-26 13:53:45","","1","365","<p>For example when executing the following logistic regression model on my data in Python . . .</p>

<pre><code>### Logistic regression with ridge penalty (L2) ###
from sklearn.linear_model import LogisticRegression
log_reg_l2_sag = LogisticRegression(penalty='l2', solver='sag', n_jobs=-1)
log_reg_l2_sag.fit(xtrain, ytrain)
</code></pre>

<p>I have not specified a range of ridge penalty values. Is the optimum ridge penalty explicitly calculated with a formula (as is done with the ordinary least squares ridge regression), or is the optimum penalty chosen from a default range of penalty values? The documentation isn't clear on this.</p>
","1574783","","1574783","","2019-08-26 14:07:24","2019-08-26 14:44:49","How is L2 (ridge) penalty calculated in sklearn LogisticRegression function?","<python-3.x><logistic-regression><sklearn-pandas>","1","0","","","","CC BY-SA 4.0","1"
"56855015","1","","","2019-07-02 14:46:29","","0","363","<p>I had a dataframe like below:</p>

<pre><code>    startdate   terminationdate
0   1997-07-13  2004-09-29
1   1999-07-26  2016-03-23
2   2003-04-01  NaT
3   2007-06-01  NaT
4   2009-06-01  NaT

</code></pre>

<p>I would like to get the output to calculate the tenure in months. For null value in terminationdate, I would like to use current date to calculate.</p>

<p>I tried the code below:</p>

<pre><code>
def tenure(df):

    if df['terminationdate'] != np.nan:
        tenure = (df['terminationdate'] - df['startdate'])/np.timedelta64(1, 'M')

    else:
        tenure = (datetime.datetime.now() - df['startdate'])/np.timedelta64(1, 'M')
    return tenure
</code></pre>

<p>The tenure of NaT value could not be calculated with above code.</p>
","11729738","","11729738","","2019-07-02 15:11:07","2019-07-02 15:13:03","Calculate date time difference of two columns with null value in dataframe","<python><python-3.x><pandas>","3","4","1","","","CC BY-SA 4.0","1"
"49247108","1","","","2018-03-13 01:52:40","","1","363","<p>I am new to Python 3, coming over from R.</p>

<p>I have a very large time series file (10gb) which spans 6 months. It is a csv file where each row contains 6 fields:  Date, Time, Data1, Data2, Data3, Data4.  ""Data"" fields are numeric. I would like to iterate through the file and create &amp; write individual files which contain only one day of data.  The individual dates are known only by the fact that the date field suddenly changes.  Ie, they don't include weekends, certain holidays, as well as random closures due to unforseen events so the vector of unique dates is not deterministic.  Also, the number of lines per day is also variable and unknown.</p>

<p>I envision reading each line into a buffer and comparing the date to the previous date. </p>

<p>If the next date = previous date, I append that line to the buffer. I repeat this until next date != previous date, at which point I write the buffer to a new csv file which contains only that day's data (00:00:00 to 23:59:59). </p>

<p>I had trouble appending the new lines with pandas dataframes, and using readline into a list just got too mangled for me. Looking for Pythonic advice.</p>
","7696283","","7696283","","2018-03-13 02:42:50","2018-03-13 22:17:06","Process Large (10gb) Time Series CSV file into daily files","<python><python-3.x><pandas>","2","4","","","","CC BY-SA 3.0","1"
"57191944","1","57192017","","2019-07-24 22:16:08","","1","357","<p>I am trying to create a grid of Subplots for a predetermined x &amp; y data. The functions should iterate through a pandas DataFrame, identify Categorical variables and plot the x &amp; y data with a line for each level of a given categorial variable. The number of plots is equal to the number of Categorical variables, and the number of lines on each plot should be reflective of the number of categories for that variable.   </p>

<p>I initially tried to group the Dataframe in a For loop on a given categorical  variable, but I have had some mixed results. I think My issue is in how I am assigning what axis the lines are getting drawn on.</p>

<pre class=""lang-py prettyprint-override""><code>
def grouping_for_graphs(df,x_col, y_col,category,func):
    '''
    funtion to group dataframe given a variable and 
    aggregation function

    '''
    X = df[x_col].name
    y = df[y_col].name
    category = df[category].name

    df_grouped = df.groupby([X, category])[y].apply(func)
    return df_grouped.reset_index()


# create a list of categorical variables to plot
cat_list = []
col_list = list(df.select_dtypes(include = ['object']).columns)

for col in col_list:
    if len(df[col].unique()) &lt; 7:
        cat_list.append(col)


# create plots and axes
fig, axs = plt.subplots(2, 2, figsize=(30,24))
axs = axs.flatten()
# pick plot function
plot_func = plt.plot

# plot this
for ax, category in zip(axs, cat_list):
    df_grouped = grouping_for_graphs(df,x_col, y_col,category,agg_func)
    x_col = df_grouped.columns[0]
    y_col = df_grouped.columns[-1]
    category = str(list(df_grouped.columns.drop([x_lab, y_lab]))[0])
    for feature in list(df_grouped[category].unique()):
        X = df_grouped[df_grouped[category] == feature][x_col]
        y = df_grouped[df_grouped[category] == feature][y_col]
        ax.plot = plot_func(X,y)
        ax.set_xlabel(x_col)
        ax.set_ylabel(y_col)
        ax.set_title(feature)

</code></pre>

<p>Other than getting an error that ax.plot is a 'list' object and is not callable, all the lines drawn are put on the final plot of the subplots.</p>
","11633672","","","","","2019-07-24 22:24:17","Iterate through DataFrame categorical columns to create subplots","<python><python-3.x><pandas><matplotlib>","1","0","","","","CC BY-SA 4.0","1"
"48778415","1","","","2018-02-14 01:40:43","","1","356","<p>I have a pandas dataframe with a datetime index and column of scores.</p>

<pre><code>Pandas Dataframe
Index              Score
2016-09-01          5
2016-09-15          6
.
2017-01-05          3
.
2017-12-24          2
</code></pre>

<p>How can I with seaborn:</p>

<p>(1) box plot the scores grouped by month such that the x-axis is continuous from the earliest date (Sep 2016) to the latest date (Dec 2017).</p>

<p>(2) line plot the mean score each month such that:</p>

<p>(a) the x-axis reflects the months and there are 2 line plots representing the years 2016 and 2017.</p>

<p>(b) the x-axis is continuous from the earliest date (Sep 2016) to the latest date (Dec 2017) with a single line plot.</p>

<p>I have tried searching for the answers, going down the path of groupby pandas objects but couldn't quite find it. Thanks for any advice.</p>

<p>EDIT:</p>

<p>I tried seaborn.pointplot(x=df.index.month, y=df) and seaborn.boxplot(x=df.index.month, y=df) and it gave me the below. </p>

<p>This is wrong as the x-tick '12' data actually belongs to the year 2016 and should show to the left of the rest of the x-ticks which are from 2017. Obviously this is because I used .index.month which doesn't discriminate the year but I couldn't find other ways to do this. Also, do the point values represent the mean cos I evidently did not pass in a mean function.</p>

<p><a href=""https://i.stack.imgur.com/IyY2c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IyY2c.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/WIcm8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WIcm8.png"" alt=""enter image description here""></a></p>
","5018660","","5018660","","2018-02-14 14:34:32","2018-02-14 14:34:32","Plot Pandas Dataframe with Seaborn","<python-3.x><pandas><seaborn>","0","1","","","","CC BY-SA 3.0","1"
"41962819","1","41963403","","2017-01-31 16:41:33","","1","354","<p><strong>Problem</strong></p>

<p>I have some floating point arrays where I need to identify the indexes where the values freeze, where the array has the same value for a period. </p>

<p><strong>Example</strong></p>

<pre><code>x = np.linspace( 0, 30, 1000 )
y1 = np.sin( x )
y2 = np.sin( x )
# Introduce some errors!
y2[ (x&gt;5)&amp;(x&lt;=8) ] = -0.2
y2[ (x&gt;15)&amp;(x&lt;=16) ] = -1
y2[ (x&gt;16)&amp;(x&lt;17) ] = 1
</code></pre>

<p><code>y1</code> and <code>y2</code> look like this:</p>

<p><a href=""https://i.stack.imgur.com/CQkVG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CQkVG.png"" alt=""enter image description here""></a></p>

<p>And I want to know where those horizontal lines are.</p>

<hr>

<p><strong>I have tried</strong></p>

<ul>
<li><p>Loop:</p>

<pre><code>error = np.zeros_like( y2 )
for i in range(1,y2.shape[-1]-1):
    if y2[i-1] == y2[i] == y2[i+1]:
        error[i-1] = 1
        error[i] = 1
        error[i+1] = 1
</code></pre></li>
</ul>

<p>Of course I can define a window length before the <code>for</code>... But it feels clumsy as soon as I have several nested loops, for different columns and periods.</p>

<ul>
<li><p><code>np.diff</code> → The problem here is that sometimes the derivative is 0 by itself. Also, it is common that the signal changes very slowly (temperature), so I end up checking for adjacent (zero-) values in a loop again.</p></li>
<li><p>I started looking for some indexing like: <code>y2[ where(y2==previous(y2) &amp; y2 == next(y2) ]</code>, but haven't succeeded so far. This is what I am trying now, with <code>pandas.DataFrame</code>.</p></li>
</ul>

<p>I would like to have either the indexes where the horizontal lines appear, or maybe replace them with <code>np.nan</code>. I also need to be able to play a bit with the window width (will be most probably around 10 values in the end).</p>

<p>Any ideas?</p>

<hr>

<p><strong>Context</strong></p>

<p>What I actually have is a <code>pandas.DataFrame</code> with temperature measurements, where I get floating point values along with timestamps of the recording time. Sometimes the error seems to ""freeze"" and gives a straight line instead of the measurements. Sometimes it is the last measurement repeated, sometimes it's just a random value. I need to know when each sensor failed. Of course, the duration of the failures is different every time. Each signal is about 30,000 elements long.</p>

<hr>

<p>Using:    python 3.5.2    numpy 1.11.1    pandas 0.18.1</p>
","776515","","776515","","2017-01-31 18:09:18","2017-02-05 03:47:59","How to find invalid values in a continuous signal?","<python><python-3.x><pandas><numpy><floating-point>","1","3","","","","CC BY-SA 3.0","1"
"49514737","1","49515232","","2018-03-27 13:49:46","","2","348","<p>I am trying to plot a subplot of 9 (in this example but the number would be variable for other use cases) line graphs showing the count of data points by county/area.</p>

<p>So far I have: </p>

<pre><code>Surrey1 = df[df.county == 'Surrey']
Surrey2 = Surrey1.county.groupby(df.date_stamp).value_counts()


East_Sussex1 = df[df.county == 'East Sussex']
East_Sussex2 = East_Sussex1.county.groupby(df.date_stamp).value_counts()

West_Sussex1 = df[df.county == 'West Sussex']
West_Sussex2 = West_Sussex1.county.groupby(df.date_stamp).value_counts()


Buck1 = df[df.county == 'Buckinghamshire']
Buck2 = Buck1.county.groupby(df.date_stamp).value_counts()


Norfolk1 = df[df.county == 'Norfolk']
Norfolk2 = Norfolk1.county.groupby(df.date_stamp).value_counts()


Suffolk1 = df[df.county == 'Suffolk']
Suffolk2 = Suffolk1.county.groupby(df.date_stamp).value_counts()

Essex1 = df[df.county == 'Essex']
Essex2 = Essex1.county.groupby(df.date_stamp).value_counts()

Kent1 = df[df.county == 'Kent']
Kent2 = Kent1.county.groupby(df.date_stamp).value_counts()

# Create the fig
fig, axes = plt.subplots(nrows=8, ncols=1, figsize=(12,6))

# Now plot
pd1_N.plot(ax = axes[0], subplots=True, legend=False) 
pd2_S.plot(ax = axes[1], subplots=True, legend=False)
pd3_ES.plot(ax = axes[2], subplots=True, legend=False)
pd4_WS.plot(ax = axes[3], subplots=True, legend=False)
pd5_B.plot(ax = axes[4], subplots=True, legend=False)
pd6_S.plot(ax = axes[5], subplots=True, legend=False)
pd7_E.plot(ax = axes[6], subplots=True, legend=False)
pd8_K.plot(ax = axes[7], subplots=True, legend=False)
</code></pre>

<p>Which produces: </p>

<p><a href=""https://i.stack.imgur.com/6gQm4.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6gQm4.jpg"" alt=""enter image description here""></a></p>

<p>Is there a quicker/more efficient way to do this? Tips on how to make the graph a little more presentable would be appreciated as well! 
Update: </p>

<p>I'm now using the a very simple function to do this quicker for a variable metric: </p>

<pre><code>def plot_freq(metric, graph_width, graph_height):
    plot = str(metric)
    df.groupby(plot)['date_stamp'].value_counts().unstack(0).plot(subplots=True, figsize=(graph_width, graph_height))
    print(""This plot shows the number of data points by"", metric)
</code></pre>

<p><a href=""https://i.stack.imgur.com/RbeZL.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RbeZL.jpg"" alt=""enter image description here""></a></p>
","7081275","","7081275","","2018-03-27 14:48:15","2018-03-27 14:48:15","Python, most efficient way to subplot pandas data frame","<python-3.x><pandas><matplotlib>","1","1","0","","","CC BY-SA 3.0","1"
"57741176","1","57741318","","2019-08-31 19:46:37","","2","343","<p>I have the following text:</p>

<p><strong>""Hi there, my name is sam! I love spicy hand pulled noodles. I also like to game alot.""</strong></p>

<p>My goal is to convert this paragraph into a dataframe of tokenized words per sentences. (Where the number of rows is equal to the number of sentences, and the number of columns is equal to the number of words in the longest sentence).</p>

<p>I start off creating a dataframe of tokenized sentences:</p>

<pre><code>from nltk.tokenize import sent_tokenize, word_tokenize

df = pd.DataFrame({""sentences"": sent_tokenize(paragraph)})
</code></pre>

<p>The result is:</p>

<pre><code>    sentences
0   Hi there, my name is sam!
1   I love spicy hand pulled noodles.
2   I also like to game alot.
</code></pre>

<p>Then I converted each sentence (row) into a list of tokenized words: </p>

<pre><code>df[""tokens""] = df.sentences.apply(word_tokenize)
</code></pre>

<p>The result is (if i print that column alone):</p>

<pre><code>0    [Hi, there, ,, my, name, is, sam, !]
1    [I, love, spicy, hand, pulled, noodles, .]
2    [I, also, like, to, game, alot, .]
</code></pre>

<p>What I'd like to happen next is something like this <strong>(need help here)</strong>:</p>

<pre><code>      w1   w2     w3      w4     w5       w6       w7     w8
0     Hi   there  ,       my     name     is       sam    !
1     I    love   spicy   hand   pulled   noodles  .      NaN
2     I    also   like    to     game     alot     .      NaN
</code></pre>

<p>Where the number of columns is equal to the length of the longest word_tokenized sentence. For the sentences shorter than the longest one, I'd like the empty columns to contain NaN values (or even 0.0). Is there a way to achieve this with pandas commands?</p>
","9431573","","","","","2019-08-31 20:34:31","How to create dataframe of tokenized words (columns) per sentence (rows)?","<python><python-3.x><pandas>","2","2","","","","CC BY-SA 4.0","1"
"40988438","1","","","2016-12-06 05:29:54","","0","336","<p>I am trying to build a recommendation engine to recommend products to the customers according to their buying history.</p>

<p>The code I am using currently is as follows:</p>

<pre><code>#drop the user column for now
df1 = df.drop('ID', 1)  

data_ibs = pd.DataFrame(index=df1.columns,columns=df1.columns)

#Let's fill in those empty spaces with cosine similarities

#Loop through the columns
for i in range(0,len(data_ibs.columns)) :

#Loop through the columns for each column
  for j in range(0,len(data_ibs.columns)):
    data_ibs.ix[i,j] = 1-cosine(df1.ix[:,i],df1.ix[:,j]) #Fill in placeholder with cosine similarities

# Create a placeholder items for closes neighbours to an item
data_neighbours = pd.DataFrame(index=data_ibs.columns,columns=range(1,8)) 

# Loop through our similarity dataframe and fill in neighbouring item names
for i in range(0,len(data_ibs.columns)):
  data_neighbours.ix[i,:7] = data_ibs.ix[0:,i].order(ascending=False)[:7].index

# --- End Item Based Recommendations --- # 

# --- Start User Based Recommendations --- #

# Helper function to get similarity scores
def getScore(history, similarities):
  return sum(history*similarities)/sum(similarities) 

# Create a place holder matrix for similarities, and fill in the user name column
data_sims = pd.DataFrame(index=df.index,columns=df.columns)
data_sims.ix[:,0] = df.ix[:,0] 
</code></pre>

<p>Up to now it run smoothly without any problems, but when I run the next step, it takes a long time to produce the result and I do not know how long will it take until it finishes. </p>

<pre><code>#Loop through all rows, skip the user column, and fill with similarity scores
for i in range(0,len(data_sims.index)):
  for j in range(1,len(data_sims.columns)):
    user = data_sims.index[i]
    product = data_sims.columns[j]
    if df.iloc[i,j] &gt;= 1:
      data_sims.iloc[i,j] = 0
    else:
      product_top_names = data_neighbours.ix[product][1:7]
      product_top_sims = data_ibs.ix[product].order(ascending=False)[1:7]
      user_purchases = df.ix[user,product_top_names]
      data_sims.ix[i, j] = getScore(user_purchases,product_top_sims) 
</code></pre>

<p>I tried to use a small subset of the data and I confirmed it is working properly. However, when I try with the whole dataset which is some 4 million rows, it stucked for a long time and may take a day or so to finish.</p>

<p>Can you find the piece/pieces of my code which slow the process or if possible I would appreciate if you could suggest a better way to improve the runtime compared to the current method. </p>
","6511941","","6511941","","2016-12-06 06:34:30","2016-12-06 06:34:30","How to improve iterations runtime in a pandas dataframe","<python><python-2.7><python-3.x><pandas><recommendation-engine>","0","4","1","","","CC BY-SA 3.0","1"
"41077997","1","41083129","","2016-12-10 16:59:20","","0","333","<p>my code is like this:</p>

<pre><code>raw_data={'Crest_height':[crest_day.Crest],
          'Date':[crest_day.Date], 
          'Flood_Response':[flood_response]}

Flood_data=pd.DataFrame(raw_data)
Flood_data.to_csv(r'crest_day.csv', index=False)
</code></pre>

<p>I have this output in a crest_day.csv file: </p>

<pre><code>   Crest_height Date    Flood_Response
   ""4    10.82
   Name: Crest, dtype: float64"" ""4   2016-08-18
   Name: Date, dtype: datetime64[ns]""   Major Flood
</code></pre>

<p>I want to have a clean output in csv file, not the dtypes, index etc, 
     crest_day has two columns (Crest and Date) in float while       flood_response is a string </p>

<p>both have been returned from a function</p>
","7112155","","3707607","","2016-12-10 22:27:50","2016-12-11 04:56:11","How to remove the dtype, index and name while writing to a csv file","<python-2.7><python-3.x><csv><pandas>","1","1","","","","CC BY-SA 3.0","1"
"50052606","1","","","2018-04-26 22:16:27","","-1","333","<p>I have set of files in my folder and wants to capture the date in the file name and add it to a column while reading through Pandas Data Frame.</p>

<p>I have filenames like- 
 X_04_24_2018.txt
 Y_04_25_2018.txt
 Z_04_26_2018.txt</p>

<p>Lets say the file has 2 columns with the sample content in it, I would need a data frame with the below format. Appreciate you help on this.</p>

<pre><code>Col1    Col2    Date        File
XXX     ABC     4/24/2018   X
YYY     BCA     4/25/2018   Y
ZZZ     CBA     4/26/2018   Z
</code></pre>
","9339184","","9209546","","2018-04-26 22:46:43","2018-04-26 22:46:43","How to extract Date from the filename and add column while reading thorough Pandas DF in Python?","<python><python-3.x><python-2.7><pandas>","1","0","","","","CC BY-SA 3.0","1"
"56666030","1","56666321","","2019-06-19 10:55:18","","2","330","<p>I have many lists which consists of 1d data. like below:</p>

<pre><code>list1 = [1,2,3,4...]
list2 = ['a','b','c'...] 
</code></pre>

<p>Now, I have to create dataframe like below:</p>

<pre><code>df = [[1,'a'],[2,'b'],[3,'c']]
</code></pre>

<p>I need this dataframe so that I can profile each column using pandas_profiling.
Please suggest.</p>

<p>I have tried</p>

<pre><code>list1+list2
</code></pre>

<p>but its giving data like below:</p>

<pre><code>list3=[1,2,3,4...'a','b'...]
</code></pre>

<p>used numpy hpstack too, but not working</p>

<pre><code>import pandas as pd
import pandas_profiling
import numpy as np

list3 = np.hstack([[list1],[list2]])

array([[1,2,3,4,'a','b','c'..]],dtype='&lt;U5')
</code></pre>
","11549604","","470433","","2019-07-04 19:30:18","2019-07-04 19:30:18","How to create a dataframe with multiple lists/arrays in python","<python><python-3.x><pandas><numpy><pandas-profiling>","2","3","","","","CC BY-SA 4.0","1"
"49646310","1","49647316","","2018-04-04 08:33:16","","1","326","<p>Good Morning,</p>

<p>I am using NLTK to get synonyms out of a frame of words.</p>

<pre><code>print(df)

col_1   col_2
Book      5
Pen       5 
Pencil    6

def get_synonyms(df, column_name):

df_1 = df[""col_1""]

for i in df_1:
    syn = wn.synsets(i)

    for synset in list(wn.all_synsets('n'))[:2]:
        print(i, ""--&gt;"", synset)
        print(""-----------"")

        for lemma in synset.lemmas():
            print(lemma.name())
            ci = lemma.name()

return(syn)
</code></pre>

<p>And it does work, but I would like to get the following dataframe, with the first ""n"" synonyms, of each word in ""col_1"":</p>

<pre><code>print(df_final)

    col_1     synonym
    Book      booklet
    Book      album
    Pen       cage   
    ...
</code></pre>

<p>I tried initializing an empty list, before both synset and lemma loop, and appending, but in both cases it didn't work; for example:</p>

<pre><code>synonyms = []
            for lemma in synset.lemmas():
                print(lemma.name())
                ci = lemma.name()
                synonyms.append(ci)
</code></pre>
","8618380","","","","","2018-04-04 09:23:40","Create a dataframe with NLTK synonyms","<python-3.x><pandas><nltk><synonym>","1","0","","","","CC BY-SA 3.0","1"
"48906155","1","48906228","","2018-02-21 12:38:14","","1","324","<p>I wrote a function that reads in a csv file, performs some calculations and writes output to the same file. To append the calculated values to a new column, I'm using <code>df.at[index, column_name] = value</code>.</p>

<p>This is my code</p>

<pre><code>def total_calc(n):
     input = pd.read_csv(file_name)
     input['calc'] = 0.0
     for index, row in input.iterrows():

       # perform calculations

     input.at[index, 'calc'] = calc_value
     input.to_csv(file_name, index=False)
</code></pre>

<p>When I use the function for multiple values of n, it is writing the values in the same column overwriting the values of previous n values in the dataframe. 
I tried using i in the function and giving <code>index+i</code>, something like this:</p>

<pre><code>def total_calc(i,n):
     input = pd.read_csv(file_name)
     input['calc'] = 0.0
     for index, row in input.iterrows():

       # perform calculations

     input.at[index+i, 'calc'] = calc_value
     input.to_csv(file_name, index=False)

total_calc(1,2)
total_calc(2,8)
</code></pre>

<p>However, the column values are still overwritten. Is there any way to write the columns for multiple values in the function to the same file without overwriting?</p>

<p>so these are my current dataset columns</p>

<pre><code>names values wickets score
</code></pre>

<p>I need this after running all required functions</p>

<pre><code>names values wickets score calc calc1 calc2
</code></pre>
","8879248","","8879248","","2018-02-21 12:59:05","2018-03-01 09:06:05","overwriting column values when using df.at","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"49650618","1","49650798","","2018-04-04 12:07:43","","3","323","<pre><code>import pandas as pd 
sample = pd.DataFrame({'k1':[1.1455,2.444,3.5,4.9],
                      'k2':['b','c','d','e']})
</code></pre>

<h2>it can rename columns successfully</h2>

<pre><code>sample.rename(columns = {
      'k1' : '3',
      'k2' : '5'},inplace = True)
</code></pre>

<h2>case 1: don't know the problem in the function -rename columns</h2>

<pre><code>def rename1(df):
    print(df)
    test1 = df.rename(columns = {
              'k1' : 'num',
              'k2' : 'name'},inplace = True)  

    print(test1)

    return test1
rename1(sample)
</code></pre>

<h2>Q1: Why the output will be none?</h2>

<h2>case 2: 1. roundup the number 2. rename all columns</h2>

<pre><code>def rename2(df):
    print(df)

    test2 = []
    test2 = df.rename(columns = {
      'k1' : df['num'].apply(lambda num : int(round(num))),
      'k2' : df['name']},inplace = True)   
    print(test2)
    return test2
rename2(sample)
</code></pre>

<h2>roundup data</h2>

<pre><code>print(sample['k1'].apply(lambda num : int(round(num))))
</code></pre>

<h2>Q2: How to roundup the value based on the specific column properly?</h2>

<h2>expected the result</h2>

<pre><code>     num  name
0       1  b
1       2  c
2       4  d
3       5  e
</code></pre>

<p>This is my sample data. I'm new in python. I'm trying to rename multiple columns for my data frame but I don't know the problem.</p>
","8540946","","8540946","","2018-04-04 12:15:51","2018-04-04 12:30:37","python rename multiple columns and roundup dataframe","<python><python-3.x><pandas><numpy>","1","2","","","","CC BY-SA 3.0","1"
"49241883","1","","","2018-03-12 18:12:45","","2","320","<p>I have large dataset which i am reading from text file and I want to perform an operation on it. I use </p>

<pre><code>T[fields[0:-1]]=T[fields[0:-1]].astype(float) 
</code></pre>

<p>to be sure that all the values are float. I get the Error setting an array element with a sequence on one the columns. I changed <code>T.replace('NaN', np.nan)</code> the NaN to nan but still the same issue.  I used  </p>

<pre><code>dtypeCount =[T.iloc[:,i].apply(type).value_counts() for i in range(T.shape[1])]
</code></pre>

<p>to determine the type of the data on that column and this is the results </p>

<pre><code> Name: PD_PRESSURE, dtype: int64, &lt;class 'NoneType'&gt;    3676479
 &lt;class 'float'&gt;        192217
</code></pre>

<p>Due to size of the dataset I can't figure out where this coming from. Any thought on how I can solve this or how how I can find what is causing this? </p>

<p>Thanks in advance. </p>

<hr>

<p>Update: Full Error message</p>

<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-33-fa0a78194654&gt; in &lt;module&gt;()
    162     if Aggregate_Flag==1:
    163         # This line make sure that all the data are defined as float
--&gt; 164         T[fields[0:-1]]=T[fields[0:-1]].astype(float)
    165         # defining the function inside the loop is not the best practice. However, since the number of iterations
    166         #( number of file are small), I put it insider the loop to improve the readibility of the code.

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\util\_decorators.py in wrapper(*args, **kwargs)
    116                 else:
    117                     kwargs[new_arg_name] = new_arg_value
--&gt; 118             return func(*args, **kwargs)
    119         return wrapper
    120     return _deprecate_kwarg

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\generic.py in astype(self, dtype, copy, errors, **kwargs)
   4002         # else, only a single dtype is given
   4003         new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors,
-&gt; 4004                                      **kwargs)
   4005         return self._constructor(new_data).__finalize__(self)
   4006 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals.py in astype(self, dtype, **kwargs)
   3460 
   3461     def astype(self, dtype, **kwargs):
-&gt; 3462         return self.apply('astype', dtype=dtype, **kwargs)
   3463 
   3464     def convert(self, **kwargs):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals.py in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)
   3327 
   3328             kwargs['mgr'] = self
-&gt; 3329             applied = getattr(b, f)(**kwargs)
   3330             result_blocks = _extend_blocks(applied, result_blocks)
   3331 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals.py in astype(self, dtype, copy, errors, values, **kwargs)
    542     def astype(self, dtype, copy=False, errors='raise', values=None, **kwargs):
    543         return self._astype(dtype, copy=copy, errors=errors, values=values,
--&gt; 544                             **kwargs)
    545 
    546     def _astype(self, dtype, copy=False, errors='raise', values=None,

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals.py in _astype(self, dtype, copy, errors, values, klass, mgr, **kwargs)
    623 
    624                 # _astype_nansafe works fine with 1-d only
--&gt; 625                 values = astype_nansafe(values.ravel(), dtype, copy=True)
    626                 values = values.reshape(self.shape)
    627 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\dtypes\cast.py in astype_nansafe(arr, dtype, copy)
    701 
    702     if copy:
--&gt; 703         return arr.astype(dtype)
    704     return arr.view(dtype)
    705 

ValueError: setting an array element with a sequence.
</code></pre>

<hr>

<p>If I exclude column <code>PD_PRESSURE</code>, I don't receive any error.</p>

<p>I also tried <code>T['PD_PRESSURE'].dtype(float)</code> and I get the error but for other columns it works fine.</p>

<p>If I run <code>T[fields[0:-1]]=T[fields[0:-1]]</code> it works fine by itself, based on these I thought probably the error is coming from <code>PD_Pressure</code> column.</p>

<hr>

<p><code>T.info()</code> returns</p>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt; 
DatetimeIndex: 3868696 entries, 2000-01-01 to 2017-04-11 
columns (total 6 columns): 
A object 
B object 
C object 
D object 
PD_PRESSURE object 
F object 
dtypes: object(6) 
memory usage: 1.0+ GB
</code></pre>
","9463464","","190597","","2018-03-12 19:45:17","2018-03-12 19:45:17","Error setting an array element with a sequence, when using Pandas, astype (float)","<python><python-3.x><pandas>","0","9","","","","CC BY-SA 3.0","1"
"57653577","1","57653897","","2019-08-26 07:34:59","","0","320","<p>I want to delete the character between ""()"" and ""[]"" of the column named 'RegionName'. The image of the DataFrame is given below.<a href=""https://i.stack.imgur.com/WdUvN.png"" rel=""nofollow noreferrer"">Screenshort of dataframe</a> 
Please, tell me how can I do that!</p>
","10795243","","","","","2019-08-26 07:57:52","Delete sub string between two characters of Pandas Data frame with python","<python-3.x><pandas><data-science><data-cleaning>","2","7","","","","CC BY-SA 4.0","1"
"41354394","1","41354726","","2016-12-28 01:58:55","","1","319","<p>I have a directory full of <code>txt</code> documents:</p>

<pre><code>.
├── file.txt
├── file.txt
├── file.txt
...
├── file.txt
└── file.txt
</code></pre>

<p>How can I read into a pandas dataframe all the documents?, in other words my objective is to store in a pandas dataframe object some documents like this (*):</p>

<pre><code>    id  text_blob
0   file_name.txt   Lore lipsum dolor done
1   file_name.txt   Lore lipsum ...
2   file_name.txt   dolor ...
3   file_name.txt   lore lipsum lore ...
4   file_name.txt   dolor...
</code></pre>

<p>So far I tried the below code. However, it is not pythonic and I have some formating mistakes (e.g. space issues, <code>'""</code>, formating.):</p>

<pre><code>import glob, os, csv, argparse, sys

def retrive(directory_path):
    for filename in sorted(glob.glob(os.path.join(directory_path, '*.txt'))):
        with open(filename, 'r') as f:
            important_stuff = f.read().splitlines()
            oneline = [' '.join(important_stuff)]
            yield filename.split('/')[-1] + ', ' +str(oneline).strip('[]""')

def trans(directory,directory2):
            test = tuple(retrive(directory))
            with codecs.open(directory2,'w', encoding='utf8') as out:
                csv_out=csv.writer(out, delimiter='|')
                csv_out.writerow(['name','text_blob'])
                for row in test:
                    csv_out.writerow(row.split(', ', 1))


input_d = '../in'
out_d = '../out'



trans(input_d,out_d)
</code></pre>
","7265880","","7265880","","2016-12-28 02:39:17","2016-12-28 02:51:07","How to collect into a pandas dataframe a set of documents?","<python><python-3.x><pandas><dataframe>","1","2","","","","CC BY-SA 3.0","1"
"57560922","1","57561330","","2019-08-19 16:48:26","","0","319","<p>I've seen this error plenty of other times when I've messed up calling my functions that return iterables, but this time I'm pretty stuck. To my knowledge I've followed the sytax here correctly. Can anyone spot what I'm missing?</p>

<p>What I've done so far:</p>

<ul>
<li><p>Verified that the column in question is formatted correctly and free of NaNs</p></li>
<li><p>Validated the regex in an external tool</p></li>
<li><p>Rewritten the statement</p></li>
</ul>

<p>Here's the code that I am using to generate the regex and the filter my dataframe:</p>

<pre><code>import pandas as pd
from datetime import datetime, timedelta

data = ['2019-01-01']
only_onboarding = pd.DataFrame(data, columns = ['ClosedDate'])

cycle_times = pd.DataFrame;
today = datetime.today();
for i in range(today.month - 1):  # Regex Model: 2019-08-\d\d$
    regx = """";

    if (i + 1 &lt; 10):
        regx = str(today.year) + '-0' + str(i + 1) + '-\d\d$';
    else:
        regx = str(today.year) + '-' + str(i + 1) + '-\d\d$';

of_month = only_onboarding['ClosedDate'].filter(lambda x: re.match(regx, x));
</code></pre>
","2658898","","2658898","","2019-08-19 17:11:21","2019-08-19 17:41:58","TypeError: 'function' object is not iterable when I am not calling any functions to iterate on","<python><python-3.x><pandas>","2","6","","2019-08-19 18:12:10","","CC BY-SA 4.0","1"
"49167557","1","49167587","","2018-03-08 07:24:37","","0","317","<p>The date column that I have in the dataframe is of <code>float</code> type.</p>

<p>I was able to convert it in to YYYY-MM-DD format using the below:</p>

<pre><code>pd.to_datetime(df['Date'], format = '%Y%m%d')
</code></pre>

<p>How can I convert it to YYYY-MM? tried the below and it didn't help.</p>

<pre><code>pd.to_datetime(df['Date'], format = '%Y%m')
</code></pre>
","848510","","","","","2018-03-08 07:26:15","Convert float to YYYYMM Pandas","<python-3.x><pandas>","1","1","","","","CC BY-SA 3.0","1"
"49616387","1","","","2018-04-02 18:07:58","","1","316","<p>just wondering how I would go about running multiple t-tests between two data sets without having to run the t-test code for each column individually. I know that's vague, so I'll try my best to elaborate: </p>

<p>I started with a single data set of 50 rows and 30 columns. The rows represent individual patients while each column represents expression level of a certain gene for each patient (row). I have separated the data set evenly to create 2 groups: long-term survivors and short-term survivors. I want to run 2-sample t-tests between the two groups for each gene, without having to plug in all the numbers for each group and each gene. What I currently am doing looks like this: </p>

<pre><code>long_term_survivor_Gene1 = [0.38,0.78,0.94,0.32,-0.8,-1.03]
short_term_survivor_Gene1 = [0.35,0.05,-0.88,-0.32,-2.13,-1.97]
## I used more samples than that, but I shortened it for brevity
stats.ttest_ind(long_term_survivor_Gene1,short_term_survivor_Gene1)
</code></pre>

<p>Now, I spoke with my professor and he suggested using a for-loop to be able to do this for every gene (not just Gene1 individually) using one simple block of code. He didn't go into much detail, so is this possible for my data set? Thank you for any help and please let me know if there is any further info you may need in order to help, I'm new to coding in general. </p>
","9211267","","","","","2018-04-02 18:07:58","Creating for-loop to run multiple t-tests between multiple columns of 2 data sets in Python 3?","<python-3.x><pandas><for-loop><dataframe><analysis>","0","4","","","","CC BY-SA 3.0","1"
"57195340","1","57195432","","2019-07-25 06:07:25","","1","314","<p>I have a pandas dataframe and a list of nodes as follows.</p>

<pre><code>     node     title     description
0  ""node1""  ""nn nn.""  ""nnnn nnnn""
1  ""node2""  ""mm mm.""  ""mmmm mmmm""
2  ""node3""  ""ll ll.""  ""llll llll""
3  ""node4""  ""jj jj.""  ""jjjj jjjj""

nodes = [[""node4"", 0.9], [""node2"", 1.0], [""node3"", 0.8]]
</code></pre>

<p>I want to add another column to the dataframe as <code>level</code> where </p>

<ul>
<li><code>high</code> means the value of the node is above <code>0.8</code></li>
<li><code>med</code> means the value of the node is in between <code>0.8-0.6</code></li>
<li><code>low</code> means the value of the node is below <code>0.6</code></li>
<li><code>N/A</code> means the node is not in <code>nodes</code> list.</li>
</ul>

<p>So my output should look as follows.</p>

<pre><code>     node     title     description  level
0  ""node1""  ""nn nn.""  ""nnnn nnnn""     N/A
1  ""node2""  ""mm mm.""  ""mmmm mmmm""     high
2  ""node3""  ""ll ll.""  ""llll llll""     med
3  ""node4""  ""jj jj.""  ""jjjj jjjj""     high
</code></pre>

<p>I am currently using the below code to check if the node is in the list or not (which returns a boolean value <code>0</code> and <code>1</code>).</p>

<pre><code>df['node'].isin(nodes).astype(int)
</code></pre>

<p>However, I am not sure how I can if conditions to divide the data into categories.</p>

<p>I am happy to provide ore details if needed.</p>
","10704050","","11607986","","2019-07-25 06:19:30","2019-07-25 06:20:58","How to add multiple labels to a pandas dataframe in python","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"48838207","1","","","2018-02-17 05:32:15","","0","309","<p>I have a dataframe like below:</p>

<pre><code>House   hexNum              Name   ID1  ID2  DPD    Time1                          Time2                           P1    Pr1 Pr2  Addr    hex2        bool1 bool2 bool3  path
Momma   0x0000000002a71c89  Matt   123  10   0x12   2016-05-18 17:20:57 UTC+0000   2016-05-18 17:20:57 UTC+0000    1555  19  NDN  555.18  0xa1900000  True  True  False  /mnt/c/dir1/thing2
Momma   0x0000000002a71c89  Matt   123  10   0x12   2016-05-18 17:20:57 UTC+0000   2016-05-18 17:20:57 UTC+0000    1555  19  NDN  555.18  0xa2000000  True  False False  /mnt/c/dir1/thing3
Momma   0x0000000002a71c89  Matt   123  10   0x12   2016-05-18 17:20:57 UTC+0000   2016-05-18 17:20:57 UTC+0000    1555  19  NDN  555.18  0xa2100000  False True  False  /mnt/c/dir1/thing4
Dadda   0x0000000002a71c90  Adam   124  10   0x13   2016-05-18 17:20:57 UTC+0000   NaN                             1533  19  NDA  123.23  0xb2800000  True  False False  /mnt/c/dir1/thing2
Dadda   0x0000000002a71c90  Adam   124  10   0x13   2016-05-18 17:20:57 UTC+0000   NaN                             1533  19  NDA  123.23  0xb2900000  True  False True   /mnt/c/dir2/thing3
Dadda   0x0000000002a71c90  Adam   124  10   0x13   2016-05-18 17:20:57 UTC+0000   NaN                             1533  19  NDA  123.23  0xb3000000  True  False False  /mnt/c/dir2/thing4
</code></pre>

<p>How can i convert to a nested json output like below? I have tried my hand at the method here ""<a href=""https://stackoverflow.com/questions/44750514/pandas-dataframe-to-nested-json"">Pandas Dataframe to Nested JSON</a>"". This seems like it would do what im looking for but i am having some difficulty with the second nest and additional data. Any help would be appreciated. </p>

<pre><code>[
{""House"":""Momma"",
    {""Name"": ""Matt"",
        ""Name Properties"": [{
            ""hexNum"":""0x0000000002a71c89"",
            ""ID1"":123,
            ""ID2"":10,
            ""DPD"":0x12,
            ""Time1"":""2010-05-18 17:20:57 UTC+0000"",
            ""Time2"":""2016-05-18 17:20:57 UTC+0000"",
            ""Addr Properties"":{
                ""Addr"":555.18,
                ""P1"":1555,
                ""Pr1"":19,
                ""Pr2"":""NDN""},
            ""Path"": [
                {""/mnt/c/dir1/thing2"":{""hex2"":""0xa1900000"",""bool1"":True,""bool2"":True,""bool3"":False}},
                {""/mnt/c/dir1/thing2"":{""hex2"":""0xa1900000"",""bool1"":True,""bool2"":True,""bool3"":False}},
                {""/mnt/c/dir1/thing2"":{""hex2"":""0xa1900000"",""bool1"":True,""bool2"":True,""bool3"":False}}
                ]
            }]
    },
},
{""House"":""Dadda"",   
    {""Name"": ""Adam"",
        ""Name Properties"": [{
            ""hexNum"":""0x0000000002a71c89"",
            ""ID1"":123,
            ""ID2"":10,
            ""DPD"":0x12,
            ""Time1"":""2010-05-18 17:20:57 UTC+0000"",
            ""Time2"":""2016-05-18 17:20:57 UTC+0000"",
            ""Addr Properties"":{
                ""Addr"":555.18,
                ""P1"":1555,
                ""Pr1"":19,
                ""Pr2"":""NDN""},
            ""Path"": [
                {""/mnt/c/dir1/thing2"":{""hex2"":""0xa1900000"",""bool1"":True,""bool2"":True,""bool3"":False}},
                {""/mnt/c/dir1/thing2"":{""hex2"":""0xa1900000"",""bool1"":True,""bool2"":True,""bool3"":False}},
                {""/mnt/c/dir1/thing2"":{""hex2"":""0xa1900000"",""bool1"":True,""bool2"":True,""bool3"":False}}
                ]
            }]
    }
}
]
</code></pre>
","6916973","","","","","2018-02-17 05:32:15","pandas dataframe to nested json multiple levels of nesting","<python><json><python-3.x><pandas>","0","2","2","","","CC BY-SA 3.0","1"
"48706714","1","","","2018-02-09 13:13:51","","0","303","<p>I have an hdf5 file which has 28 datasets inside. Each dataset is of different dimensions. for example the first dataset is [60,8] and the last one is [60,1]. </p>

<p>I want to loop through the HDF5 file, read all the data in each of the dataset and write it to a pandas dataframe. In the end I should have a dataframe of size [60, 218]. So far, i've tried the following code. But my code hangs. </p>

<p>Could someone spot the error in my code and tell me a better way to do this?</p>

<pre><code>q=h5py.File('AM_B0_D3.7_2016-04-13T215000.flac.h5', 'r') #reading the hdf5 file
dataset_names_list=[]
q.visit(dataset_names_list.append)#creating a list of datasets in the hdf5 file
ten_min_df= pd.DataFrame()
for i in dataset_names_list:
     x=q[i][:]
     if x.shape[1]&gt;1:
         col1=[i + str(num) for num in range(0, x.shape[1])]
         temp=pd.DataFrame(data=x, columns=col1)
         ten_min_df=ten_min_df.append(temp)
     else:
         col2=[i]
         temp=pd.DataFrame(data=x, columns=col2)
         ten_min_df=ten_min_df.append(temp)
</code></pre>
","9240584","","","","","2018-02-10 09:14:18","stuck with a simple pandas dataframe looping","<python><python-3.x><pandas><for-loop><dataframe>","1","2","","","","CC BY-SA 3.0","1"
"41481482","1","41481650","","2017-01-05 09:35:38","","0","298","<p>I have a list of labels. </p>

<p>I also have a pandas series that has multiple rows/items, some of which have their index labels in that list and some others have index labels not in that list. </p>

<p>I want to delete the rows/items of that series that have labels not in that list. </p>

<p>The only way I could come up with is to iterate on the series, check if the index label is in that list or not and delete it if not. </p>

<p>Is there any other easier way? And if not, could someone provide a code to do this way, as I also couldn't get it to work (not sure how to iterate on a series and how to compare its index labels to labels in a list). </p>

<p>Your help would be very appreciated.</p>
","3008221","","2543372","","2017-01-05 09:38:40","2017-01-05 09:49:12","How can I delete rows/items of pandas series with indices' labels not in a list?","<python><python-2.7><python-3.x><pandas><series>","1","3","","","","CC BY-SA 3.0","1"
"48957986","1","48958039","","2018-02-24 00:15:54","","2","298","<p>Using: Python 3.6, Pandas 0.22</p>

<p>I have a .csv file that I need to get an average from based on the month, and location. This is one line from the data, there are many more with multiple locations and dates: </p>

<p><code>NAME   DATE    SNOW
GRAND RAPIDS GERALD R FORD INTERNATIONAL AIRPORT, MI US 1/1/2016, 0.7</code></p>

<p>So far I have successfully sorted the info based on the month, and average per month:</p>

<pre><code>df2 = pd.read_csv(""Data.csv"")

gname = df2.groupby('NAME')

for NAME,  NAME_df2 in gname:
    df2['DATE'] = pd.to_datetime(df2['DATE'])
    df2.groupby(df2['DATE'].dt.strftime('%B'))['SNOW'].mean().sort_values()
</code></pre>

<p>When executed:</p>

<pre><code>  DATE
August       0.000000
July         0.000000
June         0.000000
September    0.000000
May          0.000562
October      0.000966
November     0.019712
April        0.155592
March        0.248475
February     0.319048
January      0.622969
December     1.129986
Name: SNOW, dtype: float64
</code></pre>

<p>My issue is that this code is just giving the total average of every month at all the locations in an endless loop. I cannot figure out how to get my output to sort the data based on average snowfall at each location, per month.</p>
","7637930","","","","","2019-02-15 15:23:58","Python Pandas Average by Location and Date/Month","<python><python-3.x><pandas>","2","0","0","","","CC BY-SA 3.0","1"
"33792988","1","33793070","","2015-11-19 00:10:00","","1","295","<p>I have two dataframes like this:</p>

<p>DF1</p>

<pre><code>    ID
    10C
    25Y
    66B
    100W
</code></pre>

<p>DF2</p>

<pre><code>    ID
    10C
    5
    25Y
    66B
</code></pre>

<p>I want to check to see if any of the values in DF1 appear in DF2 and if so, add either a 1 (if it exists) or 0 (if it doesn't) to a new column such as</p>

<pre><code>  ID    Appears
  10C      1
  25Y      1
  66B      1
  100W     0
</code></pre>

<p>I know this is a really simple problem but it is giving me fits.</p>

<p>Been trying something like</p>

<pre><code>df3 = df1.merge(df2, on='ID', how='left')
df3.fillna(0)
df3['Appear'][df3.ID_x &gt; 0] = 1
df3['Appear'][df3.ID_x = 0] = 0
</code></pre>
","3682157","","625914","","2015-11-19 00:31:06","2015-11-19 00:31:06","Check if ID in one DF exists in another DF (Python 3, Pandas)","<python-3.x><numpy><pandas>","1","0","","","","CC BY-SA 3.0","1"
"56656842","1","56657287","","2019-06-18 20:49:23","","0","294","<p>I have the following input file in csv:</p>

<p><strong>INPUT</strong></p>

<pre><code>ID,GroupID,Person,Parent
ID_001,A001,John Doe,Yes
ID_002,A001,Mary Jane,No
ID_003,A001,James Smith;John Doe,Yes
ID_004,B003,Nathan Drake,Yes
ID_005,B003,Troy Baker,No
</code></pre>

<p>The desired output is the following:</p>

<p>** DESIRED OUTPUT**</p>

<pre><code>ID,GroupID,Person
ID_001,A001,John Doe;Mary Jane;James Smith
ID_003,A001,John Doe;Mary Jane;James Smith
ID_004,B003,Nathan Drake;Troy Baker
</code></pre>

<p>Basically, I want to group by the same GroupID and then concatenate all the values present in Person column that belong to that group. Then, in my output, for each group I want to return the ID(s) of those rows where the Parent column is ""Yes"", the GroupID, and the concatenated person values for each group.</p>

<p>I am able to concatenate all person values for a particular group and remove any duplicate values from the person column in my output. Here is what I have so far:</p>

<pre><code>import pandas as pd

inputcsv = path to the input csv file
outputcsv = path to the output csv file

colnames = ['ID', 'GroupID', 'Person', 'Parent']
df1 = pd.read_csv(inputcsv, names = colnames, header = None, skiprows = 1)

#First I do a groupby on GroupID, concatenate the values in the Person column, and finally remove the duplicate person values from the output before saving the df to a csv.

df2 = df1.groupby('GroupID')['Person'].apply(';'.join).str.split(';').apply(set).apply(';'.join).reset_index()

df2.to_csv(outputcsv, sep=',', index=False)
</code></pre>

<p>This yields the following output:</p>

<pre><code>GroupID,Person
A001,John Doe;Mary Jane;James Smith
B003,Nathan Drake;Troy Baker
</code></pre>

<p>I can't figure out how to include the ID column and include all rows in a group  where the Parent is ""Yes"" (as shown in the desired output above).</p>
","11228769","","","","","2019-06-18 21:37:49","How do I extract values from different columns after a groupby in pandas?","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"49908710","1","49908757","","2018-04-18 20:42:54","","4","293","<p>I have the following pandas DataFrame in Python3.x:</p>

<pre><code>import pandas as pd

dict1 = {
    'ID':['first', 'second', 'third', 'fourth', 'fifth'], 
    'pattern':['AAABCDEE', 'ABBBBD', 'CCCDE', 'AA', 'ABCDE']
}

df = pd.DataFrame(dict1)

&gt;&gt;&gt; df
       ID   pattern
0   first  AAABCDEE
1  second    ABBBBD
2   third     CCCDE
3  fourth        AA
4   fifth     ABCDE
</code></pre>

<p>There are two columns, <code>ID</code> and <code>pattern</code>. The string in <code>pattern</code> with the longest length is in the first row, <code>len('AAABCDEE')</code>, which is length 8. </p>

<p>My goal is to standardize the strings such that these are the same length, with the trailing spaces as <code>?</code>. </p>

<p>Here is what the output should look like:</p>

<pre><code>&gt;&gt;&gt; df
       ID   pattern
0   first  AAABCDEE
1  second  ABBBBD?? 
2   third  CCCDE???
3  fourth  AA??????
4   fifth  ABCDE???
</code></pre>

<p>If I was able to make the trailing spaces <code>NaN</code>, then I could try something like:</p>

<pre><code>df = df.applymap(lambda x: int(x) if pd.notnull(x) else str(""?""))
</code></pre>

<p>But I'm not sure how one would efficiently (1) find the longest string in <code>pattern</code> and (2) then add <code>NaN</code> add the end of the strings up to this length? This may be a convoluted approach...</p>
","4596596","","","","","2018-04-18 22:07:30","How to standardize strings between rows in a pandas DataFrame?","<python><python-3.x><pandas><dataframe>","3","0","","","","CC BY-SA 3.0","1"
"48859167","1","48859234","","2018-02-19 03:28:37","","3","290","<p>This problem contains 3 separate dataframes. 
df1 represents the 'Total' of products 1,2,3, containing 'value1', 'value2'
df2 represents the 'Customer1' of products 1,2,3, containing 'value1', 'value2'
df3 represents the 'Customer2' of products 1,2,3, containing 'value1', 'value2'</p>

<p>df2 &amp; df3 are essentially subsets of df1.  </p>

<p>I would like to create another dataframe that subtracts df2&amp;df3 from df1 and label this df4. I want df4 to be 'remaining customers' within the 'Market' Column.</p>

<p>This is what I have done so far</p>

<pre><code>import pandas as pd


d1 = {'Market': ['Total', 'Total','Total'], 'Product Code': [1, 2, 3], 
'Value1':[10, 20, 30], 'Value2':[5, 15, 25]}
df1 = pd.DataFrame(data=d1)
df1



d2 = {'Market': ['Customer1', 'Customer1','Customer1'], 'Product Code': [1, 
2, 3], 'Value1':[3, 14, 10], 'Value2':[2, 4, 6]}
df2 = pd.DataFrame(data=d2)
df2


d3 = {'Market': ['Customer2', 'Customer2','Customer2'], 'Product Code': [1, 
2, 3], 'Value1':[3, 3, 4], 'Value2':[2, 6, 10]}
df3 = pd.DataFrame(data=d3)
df3
</code></pre>

<p>This produces the following result..</p>

<pre><code>Market  Product Code  Value1  Value2
0  Total             1      10       5
1  Total             2      20      15
2  Total             3      30      25
  Market  Product Code  Value1  Value2
0  Customer1             1       3       2
1  Customer1             2      14       4
2  Customer1             3      10       6
  Market  Product Code  Value1  Value2
0  Customer2             1       3       2
1  Customer2             2       3       6
2  Customer2             3       4      10
</code></pre>

<p>To create df4, I try the following code and get an error 'TypeError: unsupported operand type(s) for -: 'str' and 'str''  Can anyone help?</p>

<pre><code>df4 = df1-(df2+df3)

print(df4)
</code></pre>
","7638546","","9209546","","2018-03-02 03:45:45","2018-03-02 03:45:45","Subtracting values of attributes within one Pandas Dataframe from another dataframe","<python><python-3.x><pandas>","4","0","","","","CC BY-SA 3.0","1"
"57800645","1","57800716","","2019-09-05 07:41:39","","1","287","<p>I have two data frames </p>

<p>The first frame is my IDs, some 'old code' matches to one 'Master ID'. Some OLD code are not matched to a Master ID.</p>

<p><strong>ID Dataframe</strong></p>

<pre><code>MASTER ID  OLD CODE 

  MASTER1    1A
  MASTER1    1B
  MASTER2    2
  MASTER3    3
             4
</code></pre>

<p><strong>Sales</strong></p>

<pre><code>OLD CODE  Salesvalues  
1A         10           
1B         15           
2           6           
3           8   
4           5
</code></pre>

<p>If I am doing a right join or an outer join, it returns more rows then my original sales table. How I can make a join <strong>on the first matching 'MASTER ID'</strong> match and keeping the same number of rows(no multiple duplicate rows). I would like if there is no match for the'old code' on 'master ID', that will returns NA. </p>

<p><strong>Expected Merge dataframe</strong> </p>

<pre><code>  OLD CODE  Salesvalues  MASTER ID (Join column) 
    1A         10           MASTER1
    1B         15           MASTER1
    2           6           MASTER2
    3           8           MASTER3
    4           5             NA
</code></pre>
","9861647","","9861647","","2019-09-05 08:23:06","2019-09-05 08:42:49","Python Pandas merge right join on first match","<python-3.x><pandas><merge><jointable>","1","1","","2019-10-23 07:47:31","","CC BY-SA 4.0","1"
"57297749","1","","","2019-07-31 19:31:58","","0","286","<p>I have a column with values like this:</p>

<pre><code>columnA
[12,4352,545]
[123123,5436,665]
[234,646,5747]
</code></pre>

<p>And when I write the DataFrame containing this column to a CSV, I want to remove the brackets around each array in the column. I've tried <code>str.replace</code> and <code>str.strip</code>, but the braces are never removed. I've also tried converting them all to tuples and then removing the parentheses instead, to no avail. </p>
","8825740","","6361531","","2019-07-31 20:18:44","2019-07-31 20:18:44","Removing brackets from a DataFrame column when exporting to CSV","<python-3.x><pandas><jupyter-notebook>","2","3","","","","CC BY-SA 4.0","1"
"48573148","1","","","2018-02-01 23:00:00","","1","284","<p>I'm looking for a faster way to do odds ratio tests on a large dataset. I have about 1200 variables (see var_col) I want to test against each other for mutual exclusion/ co-occurrence. An odds ratio test is defined as (a * d) / (b * c)), where a, b c,d are number of samples with (a) altered in neither site x &amp; y  (b) altered in site x, not in y (c) altered in y, not in x (d) altered in both. I'd also like to calculate the fisher exact test to determine statistical significance. The scipy function fisher_exact can calculate both of these (see below).</p>

<pre><code>#here's a sample of my original dataframe
sample_id_no  var_col
       0    258.0
       1    -24.0
       2   -150.0
       3    149.0
       4    108.0
       5   -126.0
       6    -83.0
       7      2.0
       8   -177.0
       9   -171.0
      10     -7.0
      11   -377.0
      12   -272.0
      13     66.0
      14    -13.0
      15     -7.0
      16      0.0
      17    189.0
      18      7.0
      13    -21.0
      19     80.0
      20    -14.0
      21    -76.0
       3     83.0
      22   -182.0

import pandas as pd
import numpy as np
from scipy.stats import fisher_exact
import itertools

#create a dataframe with each possible pair of variable
var_pairs = pd.DataFrame(list(itertools.combinations(df.var_col.unique(),2) )).rename(columns = {0:'alpha_site', 1: 'beta_site'})

#create a cross-tab with samples and vars
sample_table = pd.crosstab(df.sample_id_no, df.var_col)

odds_ratio_results = var_pairs.apply(getOddsRatio, axis=1, args = (sample_table,))

#where the function getOddsRatio is defined as:
def getOddsRatio(pairs, sample_table):   

    alpha_site, beta_site = pairs
    oddsratio, pvalue = fisher_exact(pd.crosstab(sample_table[alpha_site] &gt; 0, sample_table[beta_site] &gt; 0))
    return ([oddsratio, pvalue])
</code></pre>

<p>This code runs very slow, especially when used on large datasets. In my actual dataset, I have around 700k variable pairs. Since the getOddsRatio() function is applied to each pair individually, it is definitely the main source of the slowness. Is there a more efficient solution?</p>
","8316379","","8316379","","2018-02-02 02:13:35","2018-02-02 02:13:35","Pandas vectorize statistical odds-ratio test","<python><python-3.x><pandas><numpy><statistics>","0","3","1","","","CC BY-SA 3.0","1"
"41831956","1","","","2017-01-24 15:26:49","","0","283","<p>I started working with pandas in python 3.4 for couple of days. I chose to work on <a href=""http://www2.informatik.uni-freiburg.de/~cziegler/BX/"" rel=""nofollow noreferrer"">Book-Crossing data set</a>.<br/>
The book information table is like this:<br/><br/>
<a href=""https://i.stack.imgur.com/Pm9PI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pm9PI.png"" alt=""Books information""></a><br/><br/>
The Book rating table is like this:<br/><br/>
<a href=""https://i.stack.imgur.com/dO2rx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dO2rx.png"" alt=""enter image description here""></a><br/><br/>
I want to grab the ""ISBN"",""Book-title"" from the book information table and merge it with the book-rating table in which both match the ""ISBN"" and after that write the results in another csv file.
I used the code below:</p>

<pre><code>udata = pd.read_csv('1', names = ('User_ID', 'ISBN', 'Book-Rating'), encoding=""ISO-8859-1"", sep=';', usecols=[0,1,2])
uitem = pd.read_csv('2', names = ('ISBN', 'Book-Title'), encoding=""ISO-8859-1"", sep=';', usecols=[0,1])
ratings = pd.merge(udata, uitem, on='ISBN')
ratings.to_csv('ratings.csv', index=False)
</code></pre>

<p>Unfortunately it doesn't work and it gives an error:<br/></p>

<pre><code>Traceback (most recent call last):
File ""C:\Users\masoud\Desktop\Dataset\data2\a.py"", line 2, in &lt;module&gt;
udata = pd.read_csv('2.csv', names = ('User_ID', 'ISBN', 'Book-Rating'),encoding=""ISO-8859-1"", sep=';', usecols=[0,1,2])
File ""C:\WinPython-64bit-3.4.3.6\python-3.4.3.amd64\lib\site-packages\pandas\io\parsers.py"", line 491, in parser_f
return _read(filepath_or_buffer, kwds)
File ""C:\WinPython-64bit-3.4.3.6\python-3.4.3.amd64\lib\site-packages\pandas\io\parsers.py"", line 278, in _read
return parser.read()
File ""C:\WinPython-64bit-3.4.3.6\python-3.4.3.amd64\lib\site-packages\pandas\io\parsers.py"", line 740, in read
ret = self._engine.read(nrows)
File ""C:\WinPython-64bit-3.4.3.6\python-3.4.3.amd64\lib\site-packages\pandas\io\parsers.py"", line 1187, in read
data = self._reader.read(nrows)
File ""pandas\parser.pyx"", line 758, in pandas.parser.TextReader.read (pandas\parser.c:7919)
File ""pandas\parser.pyx"", line 780, in pandas.parser.TextReader._read_low_memory (pandas\parser.c:8175)
File ""pandas\parser.pyx"", line 833, in pandas.parser.TextReader._read_rows (pandas\parser.c:8868)
File ""pandas\parser.pyx"", line 820, in pandas.parser.TextReader._tokenize_rows (pandas\parser.c:8736)
File ""pandas\parser.pyx"", line 1732, in pandas.parser.raise_parser_error (pandas\parser.c:22105)
pandas.parser.CParserError: Error tokenizing data. C error: Expected 8   fields in line 6452, saw 9
</code></pre>

<p>I was wondering if anybody could fix the error?</p>
","6118987","","6118987","","2017-01-24 15:40:11","2017-01-24 15:40:11","How can I read the csv file in pandas which is separated with "";""?","<python-3.x><csv><pandas><dataset>","1","2","","","","CC BY-SA 3.0","1"
"56891715","1","","","2019-07-04 16:31:51","","0","283","<p>I have a piece of sample Python that makes a waterfall visual.
It uses the bokeh lib</p>

<p>It looks great and works well in Jupyter but when I come to use it in PowerBI I get an error saying that no image was created</p>

<p>the code uses show(p) which seems to open an internet explorer page when I run it in PowerBI</p>

<p>I tried a matplotlib example and it uses :
my_plot.get_figure().savefig(""waterfall.png"",dpi=200,bbox_inches='tight')
is there something similar for bokeh lib ?</p>

<pre class=""lang-py prettyprint-override""><code>from bokeh.plotting import figure, show
from bokeh.io import output_notebook
from bokeh.models import ColumnDataSource, LabelSet
from bokeh.models.formatters import NumeralTickFormatter
import pandas as pd

#output_notebook()

# Create the initial dataframe
index = ['sales','returns','credit fees','rebates','late charges','shipping']
data = {'amount': [350000,-30000,-7500,-25000,95000,-7000]}
df = pd.DataFrame(data=data,index=index)

# Determine the total net value by adding the start and all additional transactions
net = df['amount'].sum()

df['running_total'] = df['amount'].cumsum()
df['y_start'] = df['running_total'] - df['amount']

# Where do we want to place the label?
df['label_pos'] = df['running_total']

df_net = pd.DataFrame.from_records([(net, net, 0, net)],
                                   columns=['amount', 'running_total', 'y_start', 'label_pos'],
                                   index=[""net""])
df = df.append(df_net)

df['color'] = 'grey'
df.loc[df.amount &lt; 0, 'color'] = 'red'
df.loc[df.amount &gt; 0, 'color'] = 'green'
df.loc[df.amount &gt; 300000, 'color'] = 'blue'
df.loc[df.amount &lt; 0, 'label_pos'] = df.label_pos - 10000
df[""bar_label""] = df[""amount""].map('{:,.0f}'.format)

TOOLS = ""box_zoom,reset,save""
source = ColumnDataSource(df)
p = figure(tools=TOOLS, x_range=list(df.index), y_range=(0, net+40000),
           plot_width=800, title = ""Sales Waterfall"")

p.segment(x0='index', y0='y_start', x1=""index"", y1='running_total',
          source=source, color=""color"", line_width=55)

p.grid.grid_line_alpha=0.3
p.yaxis[0].formatter = NumeralTickFormatter(format=""($ 0 a)"")
p.xaxis.axis_label = ""Transactions""

labels = LabelSet(x='index', y='label_pos', text='bar_label',
                  text_font_size=""8pt"", level='glyph',
                  x_offset=-20, y_offset=0, source=source)
p.add_layout(labels)
show(p)
</code></pre>
","666262","","3406693","","2019-07-04 18:52:20","2019-07-04 18:52:20","Exporting Bokeh Plots as images","<python><python-3.x><powerbi><bokeh><pandas-bokeh>","1","0","","","","CC BY-SA 4.0","1"
"48904456","1","48904546","","2018-02-21 11:15:00","","2","280","<p>Here is a minimal example:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'x': [0, 0, np.NaN, 1], 'y': [1, 0, 0, np.NaN], 'z': [np.NaN, 1, 1, 0]}, dtype = int, index = ['a', 'a', 'b', 'b'])

       x      y      z
a      0      1    NaN
a      0      0      1
b    NaN      0      1
b      1    NaN      0
</code></pre>

<p>Values can only be 0, 1, or NaN. I want to add rows that have the same index, ignoring NaN values. The result would be here:</p>

<pre><code>       x      y      z
a      0      1      1 
b      1      0      1
</code></pre>

<p>The way I am doing it:</p>

<pre><code>df.max(level = 0)
</code></pre>

<p>Is there a faster way?</p>
","8970640","","","","","2018-02-21 11:38:44","Adding rows of one DataFrame with same index","<python-3.x><performance><pandas><dataframe>","1","0","","","","CC BY-SA 3.0","1"
"33749438","1","33755423","","2015-11-17 04:54:33","","0","280","<p>This should be easy enough, but it is taking me forever to compute this, and I can't think of any other ways to process my file. I'd appreciate any ideas.</p>

<blockquote>
  <p><strong>TL;DR:</strong> I am looking for a way to shortcut <code>for key in store.keys()</code> and run the same analysis on all data contained within
  each node in a HDF file with 261k nodes (aka keys, groups...) such that each node is processed independent of the others.</p>
</blockquote>

<p>I have a H5 file on disk, which weighs in a couple of hundred gigabytes. The file contains hundreds of thousands of nodes (261k, to be exact), and I want to process them all using the same method, but separately. Each node (or group) contains a single table, with a date-time index and three float columns of data. I want to compute quantiles for each column in each table. Basically, here is how the H5 file would look like (partial output of <code>$ ptdump -av</code>):</p>

<pre><code>/ (RootGroup) ''
    /._v_attrs (AttributeSet), 4 attributes:
     [CLASS := 'GROUP',
      PYTABLES_FORMAT_VERSION := '2.1',
      TITLE := '',
      VERSION := '1.0']
/101P09999 (Group) '101P09999'
   /101P09999._v_attrs (AttributeSet), 15 attributes:
    [CLASS := 'GROUP',
     TITLE := '101P09999',
     VERSION := '1.0',
     data_columns := [],
     encoding := 'UTF-8',
     index_cols := [(0, 'index')],
     info := {1: {'type': 'Index', 'names': [None]}, 'index': {'index_name': 'DATETIME'}},
     levels := 1,
     metadata := {'STATE': 'Georgia', 'LENGTH': 4.86258, 'COUNTRY': 'USA', 'ROAD_NUMBER': 'US-27/GA-1', 'LATITUDE': 34.88279, 'COUNTY': 'Walker', 'LONGITUDE': -85.27023, 'ROAD_NAME': 'Lafayette Rd/Martha Berry Hwy', 'DIRECTION': 'Northbound'},
     nan_rep := 'nan',
     non_index_axes := [(1, ['TTAV', 'TTPC', 'TTFT'])],
     pandas_type := 'frame_table',
     pandas_version := '0.15.2',
     table_type := 'appendable_frame',
     values_cols := ['values_block_0']]
/101P09999/table (Table(2345,), shuffle, blosc(5)) ''
  description := {
  ""index"": Int64Col(shape=(), dflt=0, pos=0),
  ""values_block_0"": Float32Col(shape=(3,), dflt=0.0, pos=1)}
  byteorder := 'little'
  chunkshape := (3276,)
  autoindex := True
  colindexes := {
    ""index"": Index(6, medium, shuffle, zlib(1)).is_csi=False}
  /101P09999/table._v_attrs (AttributeSet), 12 attributes:
   [CLASS := 'TABLE',
    FIELD_0_FILL := 0,
    FIELD_0_NAME := 'index',
    FIELD_1_FILL := 0.0,
    FIELD_1_NAME := 'values_block_0',
    NROWS := 2345,
    TITLE := '',
    VERSION := '2.7',
    index_kind := 'datetime64',
    values_block_0_dtype := 'float32',
    values_block_0_kind := ['TTAV', 'TTPC', 'TTFT'],
    values_block_0_meta := None]
</code></pre>

<blockquote>
  <p><strong>NOTE:</strong> Don't pay attention to the <code>NROWS</code> value in the above output. This output is just for February 2014, I have the data from
  all 12 months in the master table.</p>
</blockquote>

<p>Right, what I want to do is take the data in columns <code>['TTAV', 'TTPC', 'TTFT']</code>  and divide it by the group attribute <code>metadata['LENGTH']</code> (4.86258 in this case). Next I want to divide it into 6 groups based on the time stamp of the data. Within each of these 6 groups I want to then compute quantiles.</p>

<p>What I have now is a novice approach:</p>

<pre><code>with pd.HDFStore(store_path, 'r') as store:
    for key in store.keys()
        sens_data = store[key]
        # split_data = split the data into the required groups based on time stamp...
        for data in split_data:
            data /= store.get_storer(key).attrs.metadata['LENGTH']
            perc = split_data.quantile(q=np.arange(0.05, 1, 0.05)).transpose()
            # Create a column to contain sensor name:
            perc[0] = key[1:]
            perc.set_index(0, append=True, inplace=True)
            perc.index.rename(['DATA COL', 'SENS NAME'], inplace=True)
            # Merge perc into a dictionary of dataframes with keys the groups
            # the data was split into, and value a dataframe of appended percs
</code></pre>

<p>So, ultimately, the dictionary of dataframes will look like this:</p>

<pre><code>In [1]: percentiles['night']
Out[1]:                      0.05   0.10  ...  0.90  0.95
        DATA COL  SENS NAME
                  101P09999  115    118   ...  133   135
        TTAV      101P10000  95     100   ...  120   125
                  101P10001  108    109   ...  111   113
                     ...
                  101P09999  110    112   ...  133   135
        TTPC      101P10000  115    118   ...  133   135
                  101P10001  115    118   ...  133   135
                     ...
                  101P09999  115    118   ...  133   135
        TTFT      101P10000  115    118   ...  133   135
                  101P10001  115    118   ...  133   135
                     ...                  ...
</code></pre>

<p>(Forgive me for not entering random data for the rest of the rows. Also, though I have entered <code>int</code> values in my example DataFrame, the values will actually be <code>float32</code>, as seen in the <code>ptdump</code> output.) But this is how it would look like for all groups (night, morning, afternoon...), at the end of the above code. So basically, each of <code>['TTAV', 'TTPC', 'TTFT']</code>, there will be 261k rows, each corresponding to a sensor.</p>

<p>Now clearly, <code>for key in store.keys()</code> is not the way to go. (It takes about 15 minutes to just retrieve all the keys!) I find infinite examples where data for only one group and table is retrieved and processed, but nothing to help process all groups. Any thoughts? Parallel read is OK, though I have not got it to work yet (it is throwing an UnImplimented Error, when I use key to access data within nodes in separate processes. Maybe I need to pass the store to the function as well). However, the biggest problem is with <code>for key in store.keys()</code> taking 15 minutes to return a list of all keys. (Note that I don't need a list of all keys while I am processing, I am just as happy to pick up data as I parse through the file.)</p>
","3765319","","3765319","","2015-11-17 05:26:18","2015-11-17 11:10:17","Pytables efficiently read and process thousands of groups","<python-3.x><pandas><hdf5><pytables>","1","1","","","","CC BY-SA 3.0","1"
"49133681","1","","","2018-03-06 14:53:01","","0","279","<p>I have proved that the storing and retrieving of a serialized object from the cell of a pandas dataframe is failing after it is stored and loaded again from csv:</p>

<pre><code>a = df['cookie'].iloc[0]
print (type(a))
&gt;&gt; &lt;class 'requests.cookies.RequestsCookieJar'&gt;
</code></pre>

<p>then</p>

<pre><code>df.to_csv('file2.csv')
df2 = pd.read_csv('file2.csv')
b = df2['cookie'].iloc[0]
print(type(b))
&gt;&gt; &lt;class 'str'&gt;
</code></pre>

<p>in its cell, it only looks like it differs by a square bracket but</p>

<pre><code>c = '[' + b + ']'
</code></pre>

<p>..also does not fix it.</p>

<p>By the way: </p>

<pre><code>print(pd.__version__)
&gt;&gt; '0.19.2'
</code></pre>

<p>and if you need one of those objects for testing you can make one like this:</p>

<pre><code>import requests
url = 'http://www.facebook.com/'
r = requests.get(url)
c = r.cookies
</code></pre>

<p>From <a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html"" rel=""nofollow noreferrer"">pandas.DataFrame.to_csv</a> have tried adding <code>mode='wb'</code> but that only generated an error message.</p>

<p><a href=""https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html"" rel=""nofollow noreferrer"">pandas.read_csv</a> does not even contain a <code>mode</code> option so if it did work not sure how one would get it back.</p>

<p>Any ideas?</p>
","4288043","","4288043","","2018-03-06 14:58:37","2018-03-06 15:26:53","to_csv() and read_csv() for dataframe containing serialized objects","<python><python-3.x><pandas><file-io>","2","0","","","","CC BY-SA 3.0","1"
"48744607","1","48745026","","2018-02-12 10:47:11","","4","278","<p>I'm experiencing an error that is possibly a bug in pandas (v. 0.22 on Windows, Python version 3.6.3), or rather in its interaction with NumPy (v. 1.14), but I wonder if I'm missing something more profound.</p>

<p>Here's the issue: if I have two <code>Datetimeindex</code> objects of the same length and I use <code>np.maximum</code> between them, the output is as expected:</p>

<pre><code>import pandas as pd
import numpy as np
v1 = pd.DatetimeIndex(['2016-01-01', '2018-01-02', '2018-01-03'])
v2 = pd.DatetimeIndex(['2017-01-01', '2017-01-02', '2019-01-03'])
np.maximum(v1, v2)
</code></pre>

<p>returns the elementwise maximum:</p>

<blockquote>
  <p>DatetimeIndex(['2017-01-01', '2018-01-02', '2019-01-03'], dtype='datetime64[ns]', freq=None)</p>
</blockquote>

<p>However, if I try to only use one element of the two, I get an error:</p>

<pre><code>np.maximum(v1, v2[0])
</code></pre>

<blockquote>
  <p>pandas_libs\tslib.pyx in pandas._libs.tslib._Timestamp.<strong>richcmp</strong>()</p>
  
  <p>TypeError: Cannot compare type 'Timestamp' with type 'int'</p>
</blockquote>

<p>Two workarounds that work, but both are rather nasty to write, are either to use slicing or to explicitly convert to pydatetime:</p>

<pre><code>np.maximum(v1, v2[:1])
</code></pre>

<blockquote>
  <p>DatetimeIndex(['2017-01-01', '2018-01-02', '2018-01-03'], dtype='datetime64[ns]', freq=None)</p>
</blockquote>

<p>or:</p>

<pre><code>v1.to_pydatetime() - v2[0].to_pydatetime()
</code></pre>

<blockquote>
  <p>array([datetime.datetime(2017, 1, 1, 0, 0),
         datetime.datetime(2018, 1, 2, 0, 0),
         datetime.datetime(2018, 1, 3, 0, 0)], dtype=object)</p>
</blockquote>

<p>The first workaround is actually quite weird, because doing <code>v2 - v1[0]</code> works correctly, while <code>v2 - v1[:1]</code> gives an error (rather as expected this time, since the two resulting time series have unaligned indices).</p>
","4305277","","","","","2018-02-12 11:22:28","Pandas Datetimeindex with numpy.maximum gives error","<python><python-3.x><pandas><numpy>","3","0","1","","","CC BY-SA 3.0","1"
"57193412","1","58831230","","2019-07-25 02:19:36","","0","267","<p>I am using same variable names in multiple python scripts and want to avoid hard coding values for future changes. How do I keep all variables in one file like 'config' and use them in my python script?
Please help me.
I googled it too, but of no help.</p>

<p>Varibales: </p>

<pre><code>todaysdate = time.strftime(""%Y%m%d"")
folder_name = 'SourceFiles'
file_type = '.csv'
file_titles = ['Column1']
df['ID'] = df['Col1'].str[1:6]
df['EmpName'] = df['Col1'].str[7:37]
df['Salary'] = df['Col1'].str[38:58]
</code></pre>
","10808871","","309358","","2019-07-25 05:31:25","2019-11-13 07:40:24","Hard coded value variables in config/text file in python scripts","<python-3.x><variables><configuration><pandas-groupby>","1","1","","","","CC BY-SA 4.0","1"
"57657710","1","","","2019-08-26 12:11:18","","1","266","<p>I am having datetime in csv file in UTC tmezone, if i convert it from UTC to <strong><em>pytz.timezone('America/Los_Angeles')</em></strong> or any other <strong><em>America</em></strong> timezone it will omit the 2nd hour(2:MM:SS) from DST(Daylight savings time). <strong><em>Ex: 2019-03-10 00:00:00</em></strong></p>

<p>directly goes to 1st hour(1:MM:SS) to 3rd hour(3:MM:SS). If i plot line chart using converted datetime in <strong><em>C3(javascript chart library)</em></strong>, there is an one hour gap between 1st and 3rd hour.</p>

<p><strong>Note:</strong>
2nd hour data was missed while converting <strong><em>UTC</em></strong> to <strong><em>'America/Los_Angeles'</em></strong> because of <strong><em>DST</em></strong>.</p>

<p>is there any way to parse datetime without missing 2nd hour data?</p>
","9558697","","","","","2019-08-26 14:07:13","Get daylight savings(DST) applied python datetime","<python-3.x><pandas><timezone><dst><python-datetime>","1","1","1","","","CC BY-SA 4.0","1"
"56601081","1","56601899","","2019-06-14 15:19:08","","0","264","<p>I'm trying to sum columns for each row on a multi level pandas dataframe, and add the computed values on a new column. </p>

<p>The dataset I'm using is '<strong>flights</strong>' dataset from the seaborn library</p>

<pre class=""lang-py prettyprint-override""><code>
import pandas as pd
import seaborn

# Load dataset from seaborn library
flights = seaborn.load_dataset('flights')

# !!!EDIT - I added this line because it was missing!!!
# Set index for the loaded dataframe
flights_indexed = flights.set_index(['year','month'])

# Unstack the dataframe and create columns for each months
flights_unstacked = flights_indexed.unstack()

# Compute sum of each row
sum_row = flights_unstacked.sum(axis=1)
sum_row_reshape = sum_row.values.reshape(12,1)


### Put the sum of each row in a new column ###
flights_unstacked['passengers','total'] = sum_row

# alternatively,
flights_unstacked['passengers','total'] = sum_row_reshape

</code></pre>

<p>The above two methods return:</p>

<p><em>TypeError: cannot insert an item into a CategoricalIndex that is not already an existing category</em></p>

<p>Can anyone help?</p>
","7547586","","7547586","","2019-06-14 19:48:01","2019-06-19 11:17:03","Sum columns for each row of dataframe, and add new column in multi level index pandas dataframe","<python><python-3.x><pandas>","2","2","1","","","CC BY-SA 4.0","1"
"57024882","1","","","2019-07-14 05:38:32","","0","264","<p>I am following this tutorial online called <a href=""https://www.learnpython.org/en/Pandas_Basics"" rel=""nofollow noreferrer"">leanpython.org</a>, and was following the tutorial about the pandas dataframe. I ran the code below and it worked fine in the code editor on their website, but when i run it on my text editor (Brackets), it doesn't work. Is the code wrong? or is the website just out dated or something?</p>

<pre><code>import pandas as pd

dict = { ""word"": [""hi"", ""bye"", ""apple"", ""orange"", ""tree"", ""google""], ""numOfLetters"": [""2"", ""3"", ""5"", ""6"", ""4"", ""6""]}

brics = pd.DataFrame(dict)

print(brics)
</code></pre>
","11202824","","","","","2019-07-14 05:38:32","Python import pandas dataframe not working","<python><python-3.x><pandas>","0","8","","","","CC BY-SA 4.0","1"
"48505470","1","48505568","","2018-01-29 16:09:18","","1","263","<p>I am relatively new to Python, and even newer to pandas.  I am trying to develope a simple web scraper to search Indeed for job postings.  This is mostly about learning the language, but if I find a new job from it, all the better.</p>

<p>The nature of the data means there are going to be a lot of duplicates, and that is what I have seen so far. As a result, I wanted to remove the duplicates before sending the dataframe to a .csv file.  I tried implementing the DataFrame.drop_duplicates() in the code i was working on, but it didnt work.  So i created a seperate script to only test the drop.duplicates() method without having to go through all the other code first to make sure i got the syntax right and it functions as expected.  This is what I have:</p>

<pre><code>import pandas as pd
df=pd.DataFrame({'A':['1', '2', '3'], 'B':['1', '2', '4']})
print(df)
df1=df.drop_duplicates()
print(df1)
</code></pre>

<p>My expectation was that drop_duplicates() would remove the first two rows from df and assign the result to df1.  Except, they both were the same.</p>

<p>So then I tried the following figuring the default index column applied by the DataFrame was interfering:</p>

<pre><code>import pandas as pd
df=pd.DataFrame({'A':['1', '2', '3'], 'B':['1', '2', '4']})
print(df)
df1=df.drop_duplicates(subset=[""A"", ""B""])
print(df1)
</code></pre>

<p>That didnt work either.  There were a couple other iterations of the same code I tried involving 'keep' and 'inplace' but the result is always a dataframe that is the same as the original.  What am I missing?  I am expecting it to remove the first two rows since they are the same.  Are they not?  Or am I just expecting the wrong thing...</p>
","6780117","","","","","2018-01-29 16:14:38","Pandas DataFrame.drop_duplicates() missing something?","<python><python-3.x><pandas>","1","5","","2018-01-29 16:17:29","","CC BY-SA 3.0","1"
"57164629","1","","","2019-07-23 12:53:46","","0","260","<p>I have a working website made using django. I have a private GitHub repository, within it I have excel files which I want to read using <code>pandas read_excel</code> and use on the website. The reason I have made the repository private is because the data is company specific.</p>

<p>1) How do I read an excel file using pandas from a private GitHub repository? Do I need to set up personal access token?</p>

<p>2) After a user logs in to my website, is there then a way to require a further password when they navigate to try and view their company specific dataframe? For example, ""User A"" will only have access to ""Dataframe A"", and ""User B"" will only have access to ""Data frame B"".</p>

<p>On my local system, the following code works to be able to read the dataframe:</p>

<pre><code>file_path = 'C:/Users/james/Desktop/projects/path/to/excel/file
df = pd.read_excel(file_path)
</code></pre>

<p>For my live website, my code which produces the problem is:</p>

<pre><code>URL_path = 'https://github.com/path/to/excel/file/in/private/repository
df = pd.read_excel(URL_path)
</code></pre>

<p>I am able to read the excel files on my local computer, but when I try to read in from my private github, I get the following error, even though I know I am using the correct url:</p>

<p><code>urllib.error.HTTPError: HTTP Error 404: Not Found</code></p>

<p>I verified this by signing out of my github account, and trying to access the github url with my excel in it, it takes me to a <code>404 not found</code> page since I am not logged in. When I login to my github account, the same URL takes me to the correct page.</p>
","11824730","","1000551","","2019-07-23 16:52:58","2020-06-17 15:11:48","How to read an excel dataframe from a private GitHub repository using pandas?","<python><python-3.x><pandas><github>","1","0","","","","CC BY-SA 4.0","1"
"48607132","1","48608296","","2018-02-04 10:31:30","","1","258","<p>I'm new here and need help in understanding how i can work with timestamps to datetime objects that are used in pandas. I saved some data using websockets in a csv file and loaded that csv file into a pandas dataframe. In my timestamp column i'm getting contents like <code>[2018-02-04T07:49:36.867Z, 2018-02-04T07:49:56.931Z and so on]</code>. </p>

<p>I have to manipulate the other data columns using the time data, like re-sampling (using pandas) over certain durations say 1 min, 3 min etc. 
But I can't apply re-sampling as the date and time is not in correct format, like this <code>[20180204 07:49:56.931, 20180204 07:49:56:931 and so on]</code>.</p>

<p>How to achieve this transformation in pandas/python. Is it just just simple first string manipulation that i just remove these unwanted characters and then apply the datetime transformation. Any help on how to proceed would be helpful.</p>

<p>I don't even know where to start as I have never come across this type of format.  </p>
","5247643","","7811188","","2018-02-04 11:19:17","2018-02-04 12:58:12","Timestamp fetched from websockets formatting","<python><string><python-3.x><pandas><datetime>","1","3","","","","CC BY-SA 3.0","1"
"56739472","1","56739847","","2019-06-24 15:10:09","","1","256","<p>I am using <code>pd.to_datetime</code> to convert strings into <code>datetime</code>;</p>

<pre><code>df = pd.DataFrame(data={'id':['DD-83']})
pd.to_datetime(df['id'].str.replace(r'\D+', ''), errors='coerce', format='%d%m')
</code></pre>

<p><code>%d%m</code> defines zero-padded day and month, but the code still converts the above string into</p>

<pre><code>0   1900-03-08
Name: id, dtype: datetime64[ns]
</code></pre>

<p>I am wondering how to avoid it being converted into datetime (e.g. convert to <code>NaT</code> in this case), if the month and day in a string are not 0-padded. So </p>

<pre><code> DD0306 
 DD0706
 DD-83
</code></pre>

<p>will convert to</p>

<pre><code> 1900-06-03
 1900-06-07
 NaT 
</code></pre>
","766708","","766708","","2019-06-24 15:51:50","2019-06-24 17:04:39","pandas to_datetime converts non-zero padded month and day into datetime","<python><python-3.x><pandas><datetime>","1","2","","","","CC BY-SA 4.0","1"
"57742004","1","57742053","","2019-08-31 22:31:28","","0","255","<p>I have the following dataframe</p>

<pre><code>   0     1       2      3     4       5        6
0  i  love  eating  spicy  hand  pulled  noodles
1  i  also    like     to  game    alot         
</code></pre>

<p>I'd like to apply a function to create a new dataframe, but instead of the above words, the df will be populated with each words's part of speech tag.</p>

<p>I'm using <code>nltk.pos_tag</code>, and I did this <code>df.apply(nltk.pos_tag)</code>.</p>

<p>My expected output should look like this:</p>

<pre><code>   0    1    2    3    4    5    6
0  NN   NN   VB   JJ   NN   VB   NN
1  NN   DT   NN   NN   VB   DT   
</code></pre>

<p>However, I get <code>IndexError: ('string index out of range', 'occurred at index 6')</code></p>

<p>Also, I understand that nltk.pos_tag will return a tuple output in the format of: <code>('word', 'pos_tag')</code>. So some further manipulation may be required to only get the tag. Any suggestions on how to go about doing this efficiently? </p>

<hr>

<p><strong>Traceback:</strong></p>

<pre><code>Traceback (most recent call last):
  File ""PartsOfSpeech.py"", line 71, in &lt;module&gt;
    FilteredTrees = pos.run_pos(data.lower())
  File ""PartsOfSpeech.py"", line 59, in run_pos
    df = df.apply(pos_tag)
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/pandas/core/frame.py"", line 6487, in apply
    return op.get_result()
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/pandas/core/apply.py"", line 151, in get_result
    return self.apply_standard()
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/pandas/core/apply.py"", line 257, in apply_standard
    self.apply_series_generator()
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/pandas/core/apply.py"", line 286, in apply_series_generator
    results[i] = self.f(v)
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/nltk/tag/__init__.py"", line 162, in pos_tag
    return _pos_tag(tokens, tagset, tagger, lang)
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/nltk/tag/__init__.py"", line 119, in _pos_tag
    tagged_tokens = tagger.tag(tokens)
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/nltk/tag/perceptron.py"", line 157, in tag
    context = self.START + [self.normalize(w) for w in tokens] + self.END
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/nltk/tag/perceptron.py"", line 157, in &lt;listcomp&gt;
    context = self.START + [self.normalize(w) for w in tokens] + self.END
  File ""/anaconda3/envs/customer_sentiment/lib/python3.6/site-packages/nltk/tag/perceptron.py"", line 242, in normalize
    elif word[0].isdigit():
</code></pre>
","9431573","","","","","2019-08-31 22:55:07","Apply nltk.pos_tag to entire dataframe","<python-3.x><pandas><nltk><pos-tagger>","1","0","","","","CC BY-SA 4.0","1"
"41109480","1","41480844","","2016-12-12 21:01:18","","1","254","<h2>tl;dr</h2>

<p>My issue here is that I'm stuck at calculating how many rows to anticipate on each part of a full outer merge when using Pandas DataFrames as part of a combinatorics graph.</p>

<h2>Questions (repeated below).</h2>

<ol>
<li>The ideal solution would be to not require the merge and to query <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Panel.html"" rel=""nofollow noreferrer""><code>panel</code></a> objects. Given that there isn't a query method on the <code>panel</code> is there a cleaner solution which would solve this problem without hitting the memory ceiling?</li>
<li>If the answer to 2 is no, how can I calculate the size of the required merge table for each combination of sets <em>without carrying out the merge</em>? This might be a sub-optimal approach but in this instance it would be acceptable for the purpose of the application.</li>
<li>Is Python the right language for this or should I be looking at a more statistical language such as <code>R</code> or write it at a lower level (<code>c</code>, <code>cython</code>) - <em>Databases are out of the question.</em></li>
</ol>

<h2>The problem</h2>

<p>Recently I re-wrote the <a href=""https://github.com/mproffitt/py-upset/tree/feature/ISSUE-7-Severe-Performance-Degradation/src/pyupset"" rel=""nofollow noreferrer"">py-upset graphing library</a> to make it more efficient in terms of time when calculating combinations across DataFrames. I'm not looking for a review of this code, it works perfectly well in most instances and I'm happy with the approach. What I am looking for now is the answer to a very specific problem; uncovered when working with large data-sets.</p>

<p>The approach I took with the re-write was to formulate an in-memory merge of all provided dataframes on a full outer join as seen on lines <a href=""https://github.com/mproffitt/py-upset/blob/feature/ISSUE-7-Severe-Performance-Degradation/src/pyupset/resources.py#L480"" rel=""nofollow noreferrer"">480 - 502 of <code>pyupset.resources</code></a></p>

<pre><code>        for index, key in enumerate(keys):
            frame = self._frames[key]
            frame.columns = [
                '{0}_{1}'.format(column, key)
                if column not in self._unique_keys
                else
                column
                for column in self._frames[key].columns
            ]
            if index == 0:
                self._merge = frame
            else:
                suffixes = (
                    '_{0}'.format(keys[index-1]),
                    '_{0}'.format(keys[index]),
                )
                self._merge = self._merge.merge(
                    frame,
                    on=self._unique_keys,
                    how='outer',
                    copy=False,
                    suffixes=suffixes
                )
</code></pre>

<p>For small to medium dataframes using joins works incredibly well. In fact recent performance tests have shown that it'll handle 5 or 6 Data-Sets containing 10,000's of lines each in a less than a minute which is more than ample for the application structure I require.</p>

<p>The problem now moves from time based to memory based.</p>

<p>Given datasets of potentially 100s of thousands of records, the library very quickly runs out of memory even on a large server.</p>

<p>To put this in perspective, my test machine for this application is an 8-core VMWare box with 128GiB RAM running Centos7.</p>

<p>Given the following dataset sizes, when adding the 5th dataframe, memory usage spirals exponentially. This was pretty much anticipated but underlines the heart of the problem I am facing.</p>

<pre><code>  Rows | Dataframe
------------------------
 13963 | dataframe_one
 48346 | dataframe_two
 52356 | dataframe_three
337292 | dataframe_four
 49936 | dataframe_five
 24542 | dataframe_six
258093 | dataframe_seven
 16337 | dataframe_eight
</code></pre>

<p>These are not ""<em>small</em>"" dataframes in terms of the number of rows although the column count for each is limited to one unique key + 4 non-unique columns. The size of each column in <code>pandas</code> is</p>

<pre><code>column | type     | unique
--------------------------
X      | object   | Y
id     | int64    | N
A      | float64  | N
B      | float64  | N
C      | float64  | N
</code></pre>

<p>This merge can cause problems as memory is eaten up. Occasionally it aborts with a MemoryError (great, I can catch and handle those), other times the kernel takes over and simply kills the application before the system becomes unstable, and occasionally, the system just hangs and becomes unresponsive / unstable until finally the kernel kills the application and frees the memory.</p>

<p><em>Sample output (memory sizes approximate):</em></p>

<pre><code>[INFO] Creating merge table
[INFO] Merging table dataframe_one
[INFO] Data index length = 13963     # approx memory &lt;500MiB
[INFO] Merging table dataframe_two
[INFO] Data index length = 98165     # approx memory &lt;1.8GiB
[INFO] Merging table dataframe_three
[INFO] Data index length = 1296665   # approx memory &lt;3.0GiB
[INFO] Merging table dataframe_four
[INFO] Data index length = 244776542 # approx memory ~13GiB
[INFO] Merging table dataframe_five
Killed # &gt; 128GiB
</code></pre>

<p>When the merge table has been produced, it is queried in set combinations to produce graphs similar to <a href=""https://github.com/mproffitt/py-upset/blob/feature/ISSUE-7-Severe-Performance-Degradation/tests/generated/extra_additional_pickle.png"" rel=""nofollow noreferrer"">https://github.com/mproffitt/py-upset/blob/feature/ISSUE-7-Severe-Performance-Degradation/tests/generated/extra_additional_pickle.png</a></p>

<p>The approach I am trying to build for solving the memory issue is to look at the sets being offered for merge, pre-determine how much memory the merge will require, then if that combination requires too much, split it into smaller combinations, calculate each of those separately, then put the final dataframe back together (divide and conquer).</p>

<p>My issue here is that I'm stuck at calculating how many rows to anticipate on each part of the merge.</p>

<h2>Questions (repeated from above)</h2>

<ol>
<li>The ideal solution would be to not require the merge and to query <a href=""http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Panel.html"" rel=""nofollow noreferrer""><code>panel</code></a> objects. Given that there isn't a query method on the <code>panel</code> is there a cleaner solution which would solve this problem without hitting the memory ceiling?</li>
<li>If the answer to 2 is no, how can I calculate the size of the required merge table for each combination of sets <em>without carrying out the merge</em>? This might be a sub-optimal approach but in this instance it would be acceptable for the purpose of the application.</li>
<li>Is Python the right language for this or should I be looking at a more statistical language such as <code>R</code> or write it at a lower level (<code>c</code>, <code>cython</code>).</li>
</ol>

<p>Apologies for the lengthy question. I'm happy to provide more information if required or possible.</p>

<p>Can anybody shed some light on what might be the reason for this?</p>

<p>Thank you.</p>
","2452553","","2452553","","2017-01-05 09:03:39","2017-01-05 09:03:39","Calculating the size of a full outer join in pandas","<python-3.x><pandas><merge><combinatorics>","1","0","","","","CC BY-SA 3.0","1"
"57698215","1","","","2019-08-28 18:28:54","","2","247","<p><strong>Below is the code that parses the following nested jsons to corresponding pandas dataframe :</strong></p>

<pre><code>import pandas as pd

def flatten_json(nested_json):
    """"""
        Flatten json object with nested keys into a single level.
        Args:
            nested_json: A nested json object.
        Returns:
            The flattened json object if successful, None otherwise.
    """"""
    out = {}

    def flatten(x, name=''):
        if type(x) is dict:
            for a in x:
                flatten(x[a], name + a + '_')
        elif type(x) is list:
            i = 0
            for a in x:
                flatten(a, name + str(i) + '_')
                i += 1
        else:
            out[name[:-1]] = x

    flatten(nested_json)
    return out

simplejson = False
if(isinstance(sample_object2, list)):
    dict_flattened = [flatten_json(d) for d in sample_object2]
elif isinstance(sample_object2, dict):
    while isinstance(sample_object2, dict) &amp; simplejson == False:
        for key in sample_object2.keys():
            nodekey = key
            if isinstance(sample_object2[nodekey], dict) | isinstance(sample_object2[nodekey], list):
                dict_flattened = [flatten_json(d) for d in sample_object2[nodekey]]
                sample_object2 = sample_object2[nodekey]
                break
            else:
                dict_flattened = flatten_json(sample_object2)
                simplejson = True
                break
        break
else:
    print(""Invalid json"")

if simplejson:
    pdf = pd.DataFrame(dict_flattened, index=[0])
else:
    pdf = pd.DataFrame(dict_flattened)
</code></pre>

<p><strong>Input 1 : </strong></p>

<pre><code>sample_object2 = {
        ""node"":[
            {
                ""item_1"":""value_11"",
                ""item_2"":""value_12"",
                ""item_3"":""value_13"",
                ""item_4"":[""sub_value_14"", ""sub_value_15""],
                ""item_5"":{
                    ""sub_item_1"":""sub_item_value_11"",
                    ""sub_item_2"":[""sub_item_value_12"", ""sub_item_value_13""]
                }
            },
            {
                ""item_1"":""value_21"",
                ""item_2"":""value_22"",
                ""item_4"":[""sub_value_24"", ""sub_value_25""],
                ""item_5"":{
                    ""sub_item_1"":""sub_item_value_21"",
                    ""sub_item_2"":[""sub_item_value_22"", ""sub_item_value_23""]
                }
            }
        ]
    }
</code></pre>

<p><strong>Output 1:</strong></p>

<pre><code>+--------+--------+--------+------------+------------+-----------------+-------------------+-------------------+
|item_1  |item_2  |item_3  |item_4_0    |item_4_1    |item_5_sub_item_1|item_5_sub_item_2_0|item_5_sub_item_2_1|
+--------+--------+--------+------------+------------+-----------------+-------------------+-------------------+
|value_11|value_12|value_13|sub_value_14|sub_value_15|sub_item_value_11|sub_item_value_12  |sub_item_value_13  |
|value_21|value_22|nan     |sub_value_24|sub_value_25|sub_item_value_21|sub_item_value_22  |sub_item_value_23  |
+--------+--------+--------+------------+------------+-----------------+-------------------+-------------------+
</code></pre>

<p><strong>Input 2 : </strong></p>

<pre><code>sample_object2 = {
                ""item_1"":""value_11"",
                ""item_2"":""value_12"",
                ""item_5"":{
                    ""sub_item_1"":""sub_item_value_11"",
                    ""sub_item_2"":[""sub_item_value_12"", ""sub_item_value_13""]
                }
}
</code></pre>

<p><strong>Output 2:</strong></p>

<pre><code>+--------+--------+-----------------+-------------------+-------------------+
|item_1  |item_2  |item_5_sub_item_1|item_5_sub_item_2_0|item_5_sub_item_2_1|
+--------+--------+-----------------+-------------------+-------------------+
|value_11|value_12|sub_item_value_11|sub_item_value_12  |sub_item_value_13  |
+--------+--------+-----------------+-------------------+-------------------+
</code></pre>

<p><strong>Input 3 : </strong></p>

<pre><code>sample_object2 = {
    ""id"": ""0001"",
    ""type"": ""donut"",
    ""name"": ""Cake"",
    ""image"":
        {
            ""url"": ""images/0001.jpg"",
            ""width"": 200,
            ""height"": 200
        },
    ""thumbnail"":
        {
            ""url"": ""images/thumbnails/0001.jpg"",
            ""width"": 32,
            ""height"": 32
        }
}
</code></pre>

<p><strong>Output 3:</strong></p>

<pre><code>+----+-----+----+---------------+-----------+------------+--------------------------+---------------+----------------+
|id  |type |name|image_url      |image_width|image_height|thumbnail_url             |thumbnail_width|thumbnail_height|
+----+-----+----+---------------+-----------+------------+--------------------------+---------------+----------------+
|0001|donut|Cake|images/0001.jpg|200        |200         |images/thumbnails/0001.jpg|32             |32              |
+----+-----+----+---------------+-----------+------------+--------------------------+---------------+----------------+
</code></pre>

<p><strong>It works as expected for the above nested jsons. But for deeply nested jsons, the code doesnt work.</strong></p>

<p><strong>Input : </strong></p>

<pre><code>sample_object2 = {
    ""id"": ""0001"",
    ""type"": ""donut"",
    ""name"": ""Cake"",
    ""ppu"": 0.55,
    ""batters"":
        {
            ""batter"":
                [
                    { ""id"": ""1001"", ""type"": ""Regular"" },
                    { ""id"": ""1002"", ""type"": ""Chocolate"" },
                    { ""id"": ""1003"", ""type"": ""Blueberry"" },
                    { ""id"": ""1004"", ""type"": ""Devil's Food"" }
                ]
        },
    ""topping"":
        [
            { ""id"": ""5001"", ""type"": ""None"" },
            { ""id"": ""5002"", ""type"": ""Glazed"" },
            { ""id"": ""5005"", ""type"": ""Sugar"" },
            { ""id"": ""5007"", ""type"": ""Powdered Sugar"" },
            { ""id"": ""5006"", ""type"": ""Chocolate with Sprinkles"" },
            { ""id"": ""5003"", ""type"": ""Chocolate"" },
            { ""id"": ""5004"", ""type"": ""Maple"" }
        ]
}
</code></pre>

<p><strong> Expected Output:</strong></p>

<pre><code>+----+-----+----+----+-----------------+-------------------+----------+------------------------+
|id  |type |name|ppu |batters_batter_id|batters_batter_type|topping_id|topping_type            |
+----+-----+----+----+-----------------+-------------------+----------+------------------------+
|0001|donut|cake|0.55|1001             |Regular            |5001      |None                    |
|nan |nan  |nan |nan |1002             |Chocolate          |5002      |Glazed                  |
|nan |nan  |nan |nan |1003             |Blueberry          |5005      |Sugar                   |
|nan |nan  |nan |nan |1004             |Devil's Food       |5007      |Powdered Sugar          |
|nan |nan  |nan |nan |nan              |nan                |5006      |Chocolate with Sprinkles|
|nan |nan  |nan |nan |nan              |nan                |5003      |Chocolate               |
|nan |nan  |nan |nan |nan              |nan                |5004      |Maple                   |
+----+-----+----+----+-----------------+-------------------+----------+------------------------+
</code></pre>

<p><strong>But Output was:</strong></p>

<pre><code>+----+-----+----+----+-------------------+---------------------+-------------------+---------------------+-------------------+---------------------+-------------------+---------------------+------------+--------------+------------+--------------+------------+--------------+------------+--------------+------------+------------------------+------------+--------------+------------+--------------+
|id  |type |name|ppu |batters_batter_0_id|batters_batter_0_type|batters_batter_1_id|batters_batter_1_type|batters_batter_2_id|batters_batter_2_type|batters_batter_3_id|batters_batter_3_type|topping_0_id|topping_0_type|topping_1_id|topping_1_type|topping_2_id|topping_2_type|topping_3_id|topping_3_type|topping_4_id|topping_4_type          |topping_5_id|topping_5_type|topping_6_id|topping_6_type|
+----+-----+----+----+-------------------+---------------------+-------------------+---------------------+-------------------+---------------------+-------------------+---------------------+------------+--------------+------------+--------------+------------+--------------+------------+--------------+------------+------------------------+------------+--------------+------------+--------------+
|0001|donut|Cake|0.55|1001               |Regular              |1002               |Chocolate            |1003               |Blueberry            |1004               |Devil's Food         |5001        |None          |5002        |Glazed        |5005        |Sugar         |5007        |Powdered Sugar|5006        |Chocolate with Sprinkles|5003        |Chocolate     |5004        |Maple         |
+----+-----+----+----+-------------------+---------------------+-------------------+---------------------+-------------------+---------------------+-------------------+---------------------+------------+--------------+------------+--------------+------------+--------------+------------+--------------+------------+------------------------+------------+--------------+------------+--------------+
</code></pre>

<p>How to write a generic code that works for all kinds/levels of nested json? I tried tweaking the above code but couldn't do it. Any solution to this would be highly appreciated.</p>
","10647332","","10647332","","2019-08-28 18:54:51","2019-08-28 18:54:51","How to parse deeply nested JSON to pandas dataframe?","<python><python-3.x><pandas>","0","3","2","","","CC BY-SA 4.0","1"
"49137639","1","49137735","","2018-03-06 18:27:26","","4","245","<p>I want to be able to select a string between two specific substrings (as the following), but with a loop that will iterate through every row in the dataframe.</p>

<p>CODE:</p>

<pre><code>import pandas as pd

df = pd.DataFrame(['first: hello1 \nSecond this1 is1 a1 third: test1\n', 'first: hello2 \nSecond this2 is2 a2 third: test2\n', 'first: hello3 \nSecond this3 is3 a3 third: test3\n'])
df = df.rename(columns={0: ""text""})

def find_between(df, start, end):
  return (df.split(start))[1].split(end)[0]

df2 = df['text'][0]
print(find_between(df3, 'first:', '\nSecond'))
</code></pre>

<p>[OUTPUT NEEDED] Dataframe with following information:</p>

<pre><code>   output
0  hello1
1  hello2
2  hello3
</code></pre>

<p>the find_between() function is created based on <a href=""https://stackoverflow.com/questions/3368969/find-string-between-two-substrings"">Find string between two substrings</a>, but here you're only able to do that for one specific variable (df2) that is already saved as a string, as the shown example. I need to be able to do that for every row (strings) in the 'df' dataframe.</p>

<p>I would really much appreciate if anyone can help me with this! Thank you!</p>
","7897642","","7505395","","2018-03-06 18:35:35","2018-03-06 18:35:35","Select string between two substrings to every row in dataframe in Python","<python><string><python-3.x><pandas><substring>","1","0","","","","CC BY-SA 3.0","1"
"57700913","1","","","2019-08-28 22:48:09","","1","245","<p>Here is what my two dataframes look like:</p>

<p><strong>DF1</strong></p>

<pre><code>NAME   EMAIL                ID
Mark   mark@gmail.com      8974
Sam    sam@gmail.com       9823
June   june@gmail.com      0972
David  david@gmail.com     2143
</code></pre>

<p><strong>DF2</strong></p>

<pre><code>ID     ROLE-ID
2143      22
0972      34
8974      98
9823      54
</code></pre>

<p>What I need to help doing:</p>

<p>I need to COMPARE the ID column for both dataframes and if the ID from DF1 MATCHES with the ID of DF2, I need to replace the ID column in DF1 with the respective ROLE-ID from DF2. </p>

<p>The output would look like this:</p>

<p><strong>Updated DF1</strong></p>

<pre><code>NAME   EMAIL               ROLE-ID
Mark   mark@gmail.com        98
Sam    sam@gmail.com         54
June   june@gmail.com        34
David  david@gmail.com       22
</code></pre>

<p>I am using the Pandas library and tried the merge function with conditions but it didnt work</p>

<pre><code>    print(pd.merge(df1, df2, on=(df1['Id'] == df2[])))
</code></pre>
","10141423","","10141423","","2019-08-28 23:18:39","2019-08-29 00:43:42","Comparing columns in two different dataframes for matching values Pandas","<python><python-3.x><pandas><numpy><dataframe>","3","3","","2019-08-29 01:56:27","","CC BY-SA 4.0","1"
"40580013","1","40580087","","2016-11-13 23:34:37","","3","239","<p>Given the following <code>DataFrame</code>:</p>

<pre><code>   t
0  3
1  5
</code></pre>

<p>I would like to create a new column where wach entry is a list which is a function of the row it is in. In particular it should have a list with all positive integers which not greater than the entry in column <code>t</code>. So the output should be:</p>

<pre><code>   t  newCol
0  3  [1,2,3]
1  5  [1,2,3,4,5]
</code></pre>

<p>In other words, I want to apply <code>list(range(1,t+1))</code> to each row. I know how to do it in a loop, but have a long <code>DataFrame</code>, so I am looking for speed. Thank you.</p>
","6204900","","2336654","","2016-11-13 23:44:54","2016-11-14 02:14:24","Creating a new column consisting of lists in a DataFrame using pandas","<python><python-3.x><pandas><dataframe><apply>","2","1","","","","CC BY-SA 3.0","1"
"49036439","1","","","2018-02-28 18:42:02","","0","234","<p>I would like to ask if there is a way to carry last known value over few columns when doing:</p>

<pre><code>pvt_a = pd.pivot_table(data[data['Category']=='A'], values='Key',index='Store', columns='Support Status by Quarter', aggfunc='count')
pvt_a = pvt_a.cumsum(axis=1)
</code></pre>

<p>This is what I have:</p>

<p><a href=""https://i.stack.imgur.com/dknkL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dknkL.png"" alt=""enter image description here""></a></p>

<p>but I need last known value from previous column in the same row so i.e. the highlighted would carry 1 over.</p>

<p>thanks</p>
","7273886","","","","","2018-02-28 18:42:02","Pandas Pivot table cumsum() how to carry a value to the next column without np.NaN","<python-3.x><pandas>","0","4","1","2018-02-28 18:51:51","","CC BY-SA 3.0","1"
"41481319","1","41484485","","2017-01-05 09:27:39","","0","233","<p>Pandas dataframe methods include a groupby,</p>

<pre><code>import pandas as pd
df=pd.read_csv('battle.csv')
df[['region','location']].groupby('region').count()
</code></pre>

<p>This methods generates a dataframe that looks like</p>

<pre><code>region    count     
 A           5
 B           2
 C           6
</code></pre>

<p>I am trying to implement the same operation using <code>df.apply()</code> or a list comprehension to familiarize myself with coding in python. Please suggest your opinions.</p>
","7371993","","2543372","","2017-01-05 09:40:33","2017-01-05 12:28:08","I would like to know is it possible to achieve the pandas.groupby operation using list comprehension or apply method of dataframe","<python-3.x><pandas>","1","5","","","","CC BY-SA 3.0","1"
"49288557","1","","","2018-03-14 22:22:30","","0","231","<p>I have a dataset with many missing value the time interval is 5min, however, there are many missing timestamp as well.
Dataframe look like this:</p>
<pre><code>Time                   A
2000-01-01 00:00:00   NaN
2010-01-01 00:00:00   NaN
2015-01-01 00:00:00   NaN
2015-12-01 00:00:00   NaN
2015-12-01 12:40:00   NaN
2015-12-01 12:45:00   NaN


df.dropna().head(6)

Time                    A
2015-12-04 11:50:00    1.0
2016-04-11 16:15:00    1.0
2016-04-11 16:25:00    1.0
2016-04-29 22:05:00    1.0
2016-07-01 14:25:00    1.0
2016-07-23 21:20:00    1.0
</code></pre>
<p>I want to back fill the missing values for 10 days without changing the index. I used this command but there is no change in the results.</p>
<pre><code>#fill the missing data
df_filled=df.groupby(df.index).fillna(method='bfill', limit=12*240)

df_filled.dropna().head(6)

Time                    A
2015-12-04 11:50:00    1.0
2016-04-11 16:15:00    1.0
2016-04-11 16:25:00    1.0
2016-04-29 22:05:00    1.0
2016-07-01 14:25:00    1.0
2016-07-23 21:20:00    1.0
</code></pre>
<p>I appreciate if any one can guide me.</p>
<p>Thanks in advance.</p>
<h1>Update:</h1>
<p>a snapshot of one the values from df:</p>
<pre><code>12/4/2015 11:15 NaN
12/4/2015 11:20 NaN
12/4/2015 11:25 NaN
12/4/2015 11:30 NaN
12/4/2015 11:35 NaN
12/4/2015 11:40 NaN
12/4/2015 11:45 NaN
12/4/2015 11:50 1
</code></pre>
<p>What I want to backfill data up to 10 days so for the same data point it should be:</p>
<pre><code>12/4/2015 11:15 1
12/4/2015 11:20 1
12/4/2015 11:25 1
12/4/2015 11:30 1
12/4/2015 11:35 1
12/4/2015 11:40 1
12/4/2015 11:45 1
12/4/2015 11:50 1
</code></pre>
","9463464","","-1","","2020-06-20 09:12:55","2018-03-14 23:01:08","Pandas: backfilling missig Data and keeping the index","<python><python-3.x><pandas><missing-data><pandas-groupby>","1","5","","","","CC BY-SA 3.0","1"
"57803933","1","57804301","","2019-09-05 10:55:57","","0","230","<p>I have a list of dataframes (df_cleaned) created from multiple csv files chosen by the user.</p>

<p>My objective is to save each dataframe within the df_cleaned list as a separate csv file locally.</p>

<p>I have the following code done which saves the file with its original title. But I see that it overwrites and manages to save a copy of <em>only</em> the last dataframe. </p>

<p>How can I fix it? According to my very basic knowledge perhaps I could use a break-continue statement in the loop? But I do not know how to implement it correctly.</p>

<pre><code>for i in range(len(df_cleaned)):
    outputFile = df_cleaned[i].to_csv(r'C:\...\Data Docs\TrainData\{}.csv'.format(name))
print('Saving of files as csv is complete.')
</code></pre>
","11900800","","11900800","","2019-09-05 11:03:40","2019-09-05 11:16:51","Avoid overwriting of files with ""for"" loop","<python-3.x><pandas><csv><dataframe>","2","2","","","","CC BY-SA 4.0","1"
"40508974","1","40509120","","2016-11-09 14:10:13","","1","227","<p>I get this data frame: </p>

<pre><code>               Item ................. 
0              Banana (From Spain)... 
1              Chocolate ............ 
2              Apple (From USA) ..... 
               ............
</code></pre>

<p>And I want change all Item's names by removing the parenthesis, getting finally </p>

<pre><code>               Item ................. 
0              Banana ............... 
1              Chocolate ............ 
2              Apple ................ 
               ............
</code></pre>

<p>I thought, I should use replace but there are too much data so I'm thinking in use something like </p>

<pre><code>import re

    for i in dataframe.index:
       if bool(re.search('.*\(.*\).*', dataframe.iloc[i][""Item""])):
          dataframe.ix[i,""Item""] = dataframe.iloc[i][""Item""].split("" ("")[0]
</code></pre>

<p>But I'm not sure if is the most efficient way.</p>
","7136476","","3134251","","2016-11-09 14:16:14","2016-11-09 14:18:08","Replace values in data frame with Pandas","<python><python-3.x><pandas><dataframe><data-science>","2","1","","","","CC BY-SA 3.0","1"
"57163530","1","57166071","","2019-07-23 11:46:31","","0","226","<p>I have created a heatmap of my whole dataset above, however now i want to create a heatmap for the 10 most correlated points to my target variable. I have printed the shape of the data and of the mask, they are both 10 x 10, however a i get this error:</p>

<pre><code>ValueError: Mask must have the same shape as the data.
</code></pre>

<p>I have used a dummy dataset from kaggle as an example. </p>

<pre><code>    corr = training.corr()

    mask = np.zeros_like(corr, dtype=np.bool)
    mask[np.triu_indices_from(mask)] = True

    f, ax = plt.subplots(figsize=(12, 9))

    sns.heatmap(corr, mask=mask,vmin=0.0, vmax=1.0, center=0,
        square=True, linewidths=.5, cbar_kws={""shrink"": .5})
</code></pre>

<p>The first part of my code (above) computed a heatmap of my whole dataset with a mask on the top half of the data. </p>

<pre><code>    k = 10

    cols = corr.nlargest(k, 'SalePrice')['SalePrice'].index

    cm = np.corrcoef(training[cols].values.T)

    sns.set(font_scale=1.25)

    f, ax = plt.subplots(figsize=(12, 9))

    mask2 = np.zeros_like(cm, dtype=np.bool)
    mask2[np.triu_indices_from(mask2)] = True

    hm = sns.heatmap(cm, vmin=0.0, vmax=1.0, mask=mask, cbar=True, 
    annot=True, square=True, fmt='.2f', annot_kws={'size': 10},
    yticklabels=cols.values, xticklabels=cols.values)

    plt.show()
</code></pre>

<p>This should output a heatmap with the top half (repeated values) masked.</p>
","10511518","","4316405","","2019-07-23 14:03:17","2019-07-23 14:10:10","How do i put a mask on this heatmap?","<python-3.x><pandas><numpy><visualization><seaborn>","1","1","","","","CC BY-SA 4.0","1"
"49762649","1","49762990","","2018-04-10 20:49:39","","0","221","<p>New coder here, trying to run some t-tests in Python 3.6. Right now, to run my t-tests between my 2 data sets, I have been doing the following: </p>

<pre><code>import plotly.plotly as py
import plotly.graph_objs as go
from plotly.tools import FigureFactory as FF
import numpy as np
import pandas as pd
import scipy
from scipy import stats

long_term_survivor_GENE1 = [-0.38,-0.99,-1.04,0.1, etc..]
short_term_survivor_GENE1 = [0.32, 0.33,0.96, etc...]
stats.ttest_ind(long_term_survivor_GENE1,short_term_survivor_GENE1)
</code></pre>

<p>Which requires me to manually enter the values for each column of both data sets for each specific gene (GENE1 in this case). Is there any way to be able to call for the values from the data set so that Python can just read the values without me typing them out myself? For example, some way that I can just say: </p>

<pre><code>long_term_survivor_GENE1 = ##call values from GENE1 column from dataset 1##
short_term_survivor_GENE1 = ## call values from GENE1 column from dataset 2## 
</code></pre>

<p>Thanks for any help, and sorry that I'm not very well-versed in this stuff. Appreciate any feedback/tips. If you have any other questions, please let me know! </p>
","9211267","","","","","2018-04-10 21:18:05","How to import values from a column of csv dataset into python for t-test?","<python-3.x><pandas><csv><statistics><t-test>","1","0","","","","CC BY-SA 3.0","1"
"57456596","1","57456775","","2019-08-12 06:35:08","","3","219","<p>I have a data frame in which I am making updations like writing and adding the background colour to a specific cell. It looks fine in <code>dataframe</code>. But when I convert <code>dataframe</code> to excel, found the below error.</p>

<p><code>df.to_excel(""test_sheet.xls"")</code> is causing the error.</p>

<pre class=""lang-html prettyprint-override""><code>Traceback (most recent call last):
  File ""scoring_model.py"", line 441, in &lt;module&gt;
    pf = ScoringModel()
  File ""scoring_model.py"", line 12, in __init__
    self.start_scoring()
  File ""scoring_model.py"", line 185, in start_scoring
    df.to_excel(""test_sheet.xls"")
  File ""D:\Projects\Scoring Model\venv\lib\site-packages\pandas\io\formats\style.py"", line 187, in to_excel
    engine=engine)
  File ""D:\Projects\Scoring Model\venv\lib\site-packages\pandas\io\formats\excel.py"", line 662, in write
    freeze_panes=freeze_panes)
  File ""D:\Projects\Scoring Model\venv\lib\site-packages\pandas\io\excel.py"", line 1708, in write_cells
    style = self._convert_to_style(cell.style, fmt)
  File ""D:\Projects\Scoring Model\venv\lib\site-packages\pandas\io\excel.py"", line 1771, in _convert_to_style
    style = xlwt.easyxf(xlwt_stylestr, field_sep=',', line_sep=';')
  File ""D:\Projects\Scoring Model\venv\lib\site-packages\xlwt\Style.py"", line 733, in easyxf
    field_sep=field_sep, line_sep=line_sep, intro_sep=intro_sep, esc_char=esc_char, debug=debug)
  File ""D:\Projects\Scoring Model\venv\lib\site-packages\xlwt\Style.py"", line 638, in _parse_strg_to_obj
    raise EasyXFCallerError('section %r is unknown' % section)
xlwt.Style.EasyXFCallerError: section 'fill' is unknown
</code></pre>



<p>using <code>python3.5</code> and <code>xlwt 1.3</code> latest.
Does anyone found the same error?</p>

<p>Thanks</p>
","4949165","","","","","2020-08-26 09:16:47","xlwt.Style.EasyXFCallerError: section 'fill' is unknown","<python><excel><python-3.x><pandas><xlwt>","2","0","","","","CC BY-SA 4.0","1"
"40819474","1","","","2016-11-26 14:40:33","","0","217","<p>I have a pandas data frame <code>df</code> on which i use: <code>df.groupby('something').groups</code> to get a dict with layout { group : list of indices per group }.</p>

<pre><code>maxProteinPeptidesDict = defaultdict(list, df.loc[peptideIndices].groupby(""Master Protein Accessions"").groups)
(...)
for multipleProteinsString, nonUniqueIndices in multipleProteinPeptidesDict.items():
    multipleProteins = multipleProteinsString.split('; ')
    for protein in multipleProteins: # extend the possibly (probably) already existing entry in the dict.
        maxProteinPeptidesDict[protein].extend(nonUniqueIndices)
</code></pre>

<p>However, the lists in the dict values (<code>maxProteinPeptidesDict[protein]</code>) are of type Int64Index. I then want to extend them, but Int64Index has no extend method.
How can i circumvent this? Is casting every single Int64Index to a list the only option?</p>

<p>BTW1: I'm using a defaultdict because for technical reasons I cannot know whether the entry <code>maxProteinPeptidesDict[protein]</code> already exists or not before I start adding (more) values to it.
BTW2: a groupby object also has an an attribute 'indices' but this seems to return something different than the actual indices in the dataframe...</p>
","3160007","","3160007","","2016-11-27 09:37:59","2016-11-27 09:37:59","python extend pandas.DataFrame.groupby().groups value list of type int64index","<python><python-3.x><pandas>","0","3","","","","CC BY-SA 3.0","1"
"57262533","1","57270284","","2019-07-29 23:40:08","","2","216","<p>I'm trying to create a new column in a pandas dataframe with its values based on values from two other columns.</p>

<p>I tried using nested if statements to check the values in each column, but it ended up only checking the first row and filling the rest of the rows based on the first entry.</p>

<pre><code>if np.where(df['Person A']=='Yes'):
    if np.where(df['Person B']=='Yes'):
        df['Consensus'] = 'Person A said yes, then Person B said yes'
    elif np.where(df['Person B']=='No'):
        df['Consensus'] = 'Person B said yes, then person B said no'
</code></pre>

<p>Let me know if I can clarify anything - I'm a first timer!</p>
","11854686","","","","","2019-07-30 11:10:04","Fill column C based on values in columns A & B using pandas dataframe","<python-3.x><pandas>","2","1","1","","","CC BY-SA 4.0","1"
"57905053","1","57905078","","2019-09-12 10:45:16","","1","216","<p>I would like to create a dataframe from 2 python lists that I have.</p>

<p>Say I have the following 2 lists;</p>

<pre class=""lang-py prettyprint-override""><code>x = [1,2,3]
z = ['a','v','d']
</code></pre>

<p>I have initialized a dataframe </p>

<pre class=""lang-py prettyprint-override""><code>data = pd.DataFrame(columns = ['Type', 'Data']

</code></pre>

<p>and this is one of the things that I have tried doing.</p>

<pre class=""lang-py prettyprint-override""><code>df = data.append({'Type': x, 'Data' : z}, ignore_index = True)
</code></pre>

<p>but it results in the following dataframe</p>

<pre class=""lang-py prettyprint-override""><code>   Type       Data
0 [1, 2, 3]  [a, v, d]
</code></pre>

<p>However, this is what I actually want it to look like</p>

<pre class=""lang-py prettyprint-override""><code>   Type    Data
0  1      a
1  2      v
2  3      d

</code></pre>

<p>How would I do this? 
Thank you in advance.</p>
","6302043","","","","","2019-09-12 11:28:39","Creating a dataframe with lists in python","<python><python-3.x><pandas><list><dataframe>","3","2","","","","CC BY-SA 4.0","1"
"56627249","1","56628993","","2019-06-17 08:11:12","","0","215","<p>I have a datetime column[TRANSFER_DATE] in an excel sheet shows dates formated as 
1/4/2019 0:45 when this date is selected, in it appears as 
01/04/2019 00:45:08 am using a python scrip to read this column[TRANSFER_DATE] which shows the datetime as 01/04/2019 00:45:08</p>

<p>However when i try to compare the column[TRANSFER_DATE] whith another date, I get this error
Can only use .dt accessor with datetimelike ""
ValueError: : ""Can only use .dt accessor with datetimelike values"" while evaluating</p>

<p>implying those values are not actually recognized as datetime values</p>

<p>mask_part_date = data.loc[data['TRANSFER_DATE'].dt.date.astype(str) == '2019-04-12']</p>
","4861903","","","","","2019-06-17 10:02:46","Date Manipulation and Comparisons Python,Pandas and Excel","<excel><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57905093","1","","","2019-09-12 10:47:41","","0","214","<p>After reading CSV file and after fillna('') my TransactionBatch column value converted into scientific format. Please advice how to fix this issue.</p>

<p>Screenshot for your reference <a href=""https://i.stack.imgur.com/hdeAO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hdeAO.png"" alt=""float_to_object""></a></p>

<p><strong>CSV file content.</strong>
""UID"",""Date"",""Description"",""DisbursementAccount"",""TransactionBatch"",""ReceiptNumber"",""Amount""
""ID1"",2013-02-01 12:00:00,""FEE CHANGED"",""R001"",""2013084"",""00"",6100.0000
""ID2"",,,,,,
""ID3"",2008-11-03 12:00:00,""RECEIPT"",""R001"",""2009008"",""1089621"",39.0000</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.read_csv(r'G:\20190904_071321_FH.csv', sep=',', encoding='utf-8')

print (df['TransactionBatch'].head())
df.fillna('', inplace=True)

print (df['TransactionBatch'].head())
</code></pre>
","5337897","","","","","2019-09-12 11:06:21","python pandas float64 to object after using fillna","<python-3.x><pandas>","1","2","","","","CC BY-SA 4.0","1"
"41175434","1","41175518","","2016-12-16 00:15:00","","2","214","<p>I have a dataframe. I wanto to change the values in the column ""label"".
The values must go from 1 to 7, but # 4 must not be used. Also there must be 2 of each one.</p>

<p>I have managed to do it. But my method is only useful for small dataframes.
So how can I make it automatic for bigger dataframes?</p>

<pre><code>#Original dataframe
df = pd.DataFrame(np.random.rand(12, 5))
label=np.array([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3])
df['label'] = label
df

#My attempt :)
df['label'] = [1, 1, 2, 2, 3, 3, 5, 5, 6, 6, 7, 7]
df
</code></pre>

<p>ORIGINAL DATAFRAME</p>

<p><a href=""https://i.stack.imgur.com/BHjL7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BHjL7.png"" alt=""enter image description here""></a></p>

<p>EXPECTED DATAFRAME (# 4 is missing!!!)</p>

<p><a href=""https://i.stack.imgur.com/YQh6u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/YQh6u.png"" alt=""enter image description here""></a></p>
","6106842","","2336654","","2016-12-16 00:26:04","2016-12-16 00:26:04","How to change a column values with non-continuous numbers in python?","<python-3.x><pandas><numpy>","1","2","","","","CC BY-SA 3.0","1"
"57556518","1","","","2019-08-19 12:14:04","","1","213","<p>I have plotted a static heatmap using one column of a dataframe, which I have used to create a pivot table.</p>

<p>I now want to create an animated heatmap which reads each column of the original dataframe, pivots it and then updates the heatmap.</p>

<p>My original data frame is a bigger version of the below:</p>

<pre><code>                       value                    
percentage_time         0.00      0.15      0.16
region                                          
Anterior Distal     0.111212  0.119385  0.116270
Anterior Proximal   0.150269  0.153613  0.168188
Lateral Distal      0.130440  0.137157  0.136494
Lateral Proximal    0.171977  0.182251  0.181090
Medial Distal       0.077468  0.082064  0.082553
Medial Proximal     0.194924  0.198803  0.199339
Posterior Distal    0.164124  0.171221  0.166328
Posterior Proximal  0.131310  0.145706  0.136094
</code></pre>

<p>An example of the pivot table I am using:</p>

<pre><code>antpost   Anterior   Lateral  Posterior    Medial
distprox                                         
Proximal  0.150269  0.171977   0.131310  0.194924
Distal    0.111212  0.130440   0.164124  0.077468
</code></pre>

<p>This is the code I'm using, whilst I'm not getting errors, the pressure map but it does not update - it is the colour of all zero values.</p>

<pre><code>column_total_number = len(region_pressure_data.columns)
row_total_number = len(region_pressure_data.index)
fig = plt.figure()

def init():
      sns.heatmap(np.zeros((2, 4)), vmin = 0, vmax = 1, square=True)

def animate():
    column_number = 0
    while column_number &lt; column_total_number:
        region_pressure_data_one_time = region_pressure_data.iloc[:,column_number:column_number + 1]

        index_parts = np.array(region_pressure_data_one_time.index.str.split().values.tolist())

        region_pressure_data_one_time[""antpost""] = index_parts[:,0]
        region_pressure_data_one_time[""distprox""] = index_parts[:,1]

        pivot = region_pressure_data_one_time.pivot(index=""distprox"", columns = ""antpost"")
        rowTitles = [""Proximal"", ""Distal""]
        pivot = pivot.reindex([rowTitles])
        pivot.columns = pivot.columns.droplevel()
        pivot.columns = pivot.columns.droplevel()
        columnTitles = [""Anterior"", ""Lateral"", ""Posterior"", ""Medial""]
        pivot = pivot[columnTitles]

        plt.clf()
        sns.heatmap(pivot, vmin = 0, vmax = 1, annot=True)
        column_number += 1

anim = animation.FuncAnimation(fig, animate, init_func=init, frames=20, repeat = False)
</code></pre>

<p>Is there something that I am doing very wrong or is there a more straightforward way of achieving what I want? Thanks in advance for the help! :)</p>
","11196704","","11196704","","2019-08-19 12:41:34","2019-08-19 12:41:34","Animate seaborn heatmap using pandas and matplotlib","<python><python-3.x><pandas><matplotlib><seaborn>","0","5","","","","CC BY-SA 4.0","1"
"57653822","1","57653842","","2019-08-26 07:53:20","","1","213","<pre><code>import pandas as pd

df = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9],[10,11,12]],columns=['A','B','C'])
df[df['B']%2 ==0]['C'] = 5
</code></pre>

<p>I am expecting this code to change the value of columns C to 5, wherever B is even. But it is not working. </p>

<p>It returns the table as follow</p>

<pre><code>    A   B   C
0   1   2   3
1   4   5   6
2   7   8   9
3   10  11  12
</code></pre>

<p>I am expecting it to return</p>

<pre><code>    A   B   C
0   1   2   5
1   4   5   6
2   7   8   5
3   10  11  12
</code></pre>
","9283026","","9283026","","2019-08-26 07:55:07","2019-08-26 08:15:39","How to set value to a cell filtered by rows in python DataFrame?","<python><python-3.x><pandas>","3","0","","","","CC BY-SA 4.0","1"
"49014242","1","49014778","","2018-02-27 17:09:20","","0","212","<p>I have a python script I want to run via a cronjob, it would create several plots based on some data and save them to a directory on my server. I'm using <code>pandas.plot</code>. The script runs fine on my local machine, and also using a python notebook on the server. However, when running in the terminal with <code>$ python myscript.py</code> I get:</p>

<pre><code>raise RuntimeError('Invalid DISPLAY variable')
RuntimeError: Invalid DISPLAY variable
</code></pre>

<p>A similar matplotlib question (not helpful as it's specific to MPL): <a href=""https://stackoverflow.com/questions/35737116/runtimeerror-invalid-display-variable"">RuntimeError: Invalid DISPLAY variable</a>. Here the solution seems to route the console output somewhere else.</p>

<pre><code>plt.switch_backend('agg')
</code></pre>

<p>or</p>

<pre><code>matplotlib.use('agg')
</code></pre>

<p>I'm assuming that the error is related to there being no ""display"" for the plot to show on. In my script I have the plot being assigned to a variable, yet I can't seem to prevent it printing to console.</p>

<pre><code>for query_metadata, dataframe in query_results.items():

    df = dataframe

    df.reset_index(inplace = True)

    fig = df.plot.area(xticks=df.index, rot = 90, figsize = (20,8))
    fig.set_xticklabels(df['date1'])
    fig.set_title(key)
    fig_ext = fig.get_figure()

    fig_ext.savefig(""{0}/{1}.png"".format(save_dir, key))
</code></pre>
","4916699","","","","","2018-02-27 17:39:42","Pandas Plot: Suppress Plot output to allow running Script in Bash","<python><python-3.x><pandas><cron>","1","0","","","","CC BY-SA 3.0","1"
"49286456","1","49286501","","2018-03-14 19:42:38","","3","211","<p>I have Dataframe like:</p>

<pre><code>         age    gender  occupation     zip_code
user_id             
1         24    M       technician      85711
2         53    F       other           94043
3         23    M       writer          32067
4         24    M       technician      43537
5         33    F       other           15213
6         42    M       executive       98101
7         57    M       administrator   91344
8         36    M       administrator   05201
9         29    M       student         01002
10        53    M       lawyer          90703
</code></pre>

<p>I have to get Male ratio per occupation and sort it from the most to the least.</p>

<p>I tried this and after this not able to proceed:</p>

<pre><code>users.groupby(['occupation','gender']).gender.count()
</code></pre>
","5894276","","5916727","","2018-03-14 19:47:24","2018-03-14 19:53:28","Python Groupby with sorting","<python><python-3.x><group-by><pandas-groupby>","2","0","0","","","CC BY-SA 3.0","1"
"56835074","1","","","2019-07-01 11:42:02","","1","208","<p>I got the following <code>GeoDataFrame</code> taken from a CSV file and after some slincing and <code>CRS</code> and <code>geometry</code> asignment</p>

<pre><code>    ctf_nom         geometry                                    id      
0   Prunus mahaleb  POINT (429125.795043319 4579664.7564311)    2616    
1   Betula pendula  POINT (425079.292045901 4585098.09043407)   940     
2   Betula pendula  POINT (425088.115045896 4585093.66943407)   940     
3   Abelia triflora POINT (429116.661043325 4579685.93743111)   2002    
4   Abies alba      POINT (428219.962044021 4587346.66843531)   797  
</code></pre>

<p>I've converted the <code>geometry</code> from a <code>str</code> through:</p>

<pre><code>from shapely import wkt

df['geometry'] = df['geometry'].apply(wkt.loads)
df_geo = gpd.GeoDataFrame(df, geometry = 'geometry')
</code></pre>

<p>and asigned a crs by :</p>

<pre><code>df_geo.crs = {'init' :'epsg:25831'}
df_geo.crs
</code></pre>

<p>when I'm trying to save again the reduced geodataframe by  <code>gdf.to_file()</code> function it returns the following attribute error:</p>

<p><code>AttributeError: 'Series' object has no attribute 'has_z'</code></p>

<p>How can I solve this?</p>
","11215581","","11215581","","2020-01-06 21:31:17","2020-01-06 21:31:17","AttributeError: 'Series' object has no attribute 'has_z'","<python-3.x><attributeerror><geopandas><writetofile>","0","0","","","","CC BY-SA 4.0","1"
"57372742","1","57372883","","2019-08-06 09:15:39","","1","203","<p>I have a dataframe <code>obs</code> of size 1.5 million records. I would like to fill in NA's with default values as shown below.</p>

<pre><code>obs = pd.DataFrame({'person_id' :[1,2,3],'obs_date':['12/31/2007','11/25/2009',np.nan],
       'hero_id':[2,4,np.nan],'date2':['12/31/2017',np.nan,'10/06/2015'],
       'heroine_id':[1,np.nan,5],'date3':['12/31/2027','11/25/2029',np.nan],
       'bud_source_value':[1250000,250000,np.nan],
       'prod__source_value':[10000,20000,np.nan]})
</code></pre>

<p>The logic is to fill 3 default values based on column name.</p>

<p>1)  cols ending with <code>id</code> - fillna with <code>0</code></p>

<p>2)  cols ending with <code>value</code> - fillna with ' ' (blank/empty)</p>

<p>3)  cols containing <code>date</code> - fillna with <code>12/31/2000</code> </p>

<p>Though my code below works fine (based on SO suggestion), Is there anyway to fasten it?</p>

<pre><code>%%timeit
c = obs.columns.str
c1 = c.endswith('id')
c2 = c.endswith('value')
c3 = c.contains('date')

obs_final = np.select([c1,c2,c3], [obs.fillna(0), obs.fillna(''), 
obs.fillna(""12/31/2000"")])
obs_final = pd.DataFrame(obs_final, columns=obs.columns)
</code></pre>

<p>It takes <code>19.5 s ± 303 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code> - Is this normal?</p>

<p>Is there anyway to improve it?</p>
","10829044","","10829044","","2019-08-06 09:40:06","2019-08-06 09:52:44","Efficient way to fill default values for multiple columns on Big size dataframe","<python><python-3.x><pandas><dataframe><default-value>","2","0","","","","CC BY-SA 4.0","1"
"57103260","1","","","2019-07-18 22:14:26","","0","203","<p>I am reading an excel file and dumping the content into a pandas dataframe. I am using sqlalchemy to upload the entire dataframe into a MySQL database.</p>

<p>Functionally everything works, however I ran into an issue when I tried to upload an excel document where the columns included the % symbol.</p>

<p>One of my functions renames the dataframe's headers with row[0], deletes row[0], and then re-indexes.</p>

<p>When I try to read this dataframe it throws an error that there is an illegal hex character (%)- which is true.</p>

<p>I don't want to modify this column name because it is how it comes up in the excel file I'm uploading, and the database expects it to remain the same.</p>

<p>How can I get the pandas dataframe to accept the special (%) character in the column name? e.g. ""Sales % of Total""</p>

<pre><code>example data:
         [0]           [1]
    [0] Sales | Sales % of Total
    [1] $100  |        50%

dataframe.rename(columns=dataframe.iloc[0]).drop(dataframe.index[0]).reset_index(drop=True)

expected output:
       [Sales]  [Sales % of Total]
    [0] $100  |        50%
</code></pre>

<p>Throws error:</p>

<p>URLDecover: Illegal hex  characters in escape (%) pattern - Error at index 0</p>
","3249399","","3249399","","2019-07-19 15:31:15","2019-07-19 15:48:02","How to rename pandas dataframe with a name that includes special characters?","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"56853946","1","","","2019-07-02 13:46:05","","0","202","<p>""Code was developed in pandas=0.24.2, and I need to make the code work in pandas=0.20.1. What is the alternative for pd.notna as it is not working in pandas version 0.20.1. </p>

<pre><code>df.loc[pd.notna(df[""column_name""])].query(....).drop(....)
</code></pre>

<p>I need an alternative to pd.notna to fit in this line of code to work in pandas=0.20.1</p>
","11628512","","","","","2019-11-15 21:45:31","Is there any alternative for pd.notna ( pandas 0.24.2). It is not working in pandas 0.20.1?","<python-3.x><pandas>","1","2","1","","","CC BY-SA 4.0","1"
"56769678","1","","","2019-06-26 09:39:58","","2","201","<p>Is it somehow possible to get the rows of a resample operation to apply a custom function:</p>

<p>Let's assume we have a DataFrame <code>df</code> that contains for instance birthdays of children and their names and the number of friends they have:</p>

<pre><code>birthday    name   friends
datetime_1  Alice  10
datetime_2  Bob    5
   ...      ...    ...
datetime_n  Tom    12
</code></pre>

<p>If we now resample by some time frequency and try to apply a custom function:</p>

<pre><code>df.resample(""w"").apply(my_func)
</code></pre>

<p>It will only pass the input as separate series and not as rows. There is no <code>axis</code> argument in the case of <code>Resampler.apply</code>. So would there be any way to achieve something like I want. Or is there another built-in way that could be used to build custom behaviour into the reduction part? For example if I would want to return the most frequent name weighted by the number of friends that name is associated with in a given time interval.</p>
","6313007","","6313007","","2019-06-26 16:35:37","2019-06-26 19:16:19","Pandas resample apply function on whole rows?","<python-3.x><pandas><time>","1","0","","","","CC BY-SA 4.0","1"
"57196057","1","","","2019-07-25 07:00:14","","1","200","<p>I have 2 excel file and I want to read a specific column from file1 and write that whole column to file2's specific column without changing other columns and rows.</p>

<p>As of now I am able to read specific column from file1 using pandas but not able to add that column to specific column of file2.</p>

<pre><code>import pandas as pd
df1 = pd.read_excel(""file1.xlsx"")
name_df=df1[""name""]

df2=pd.read_excel(""file2.xlsx"")
name2_df=df2[""name""]
df3=name2_df.append(name_df, ignore_index=True)
writer=pd.ExcelWriter('output.xlsx',engine=""xlsxwriter"")
df3.to_excel(writer,""Sheet1"",index=False)
writer.save()
</code></pre>

<p>Expected result should be names of file1 added to names of file2 and everything remains same but this code giving me only names.</p>

<p>File 1
<a href=""https://i.stack.imgur.com/jF7Ty.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jF7Ty.jpg"" alt=""File 1 looks like this""></a></p>

<p>File 2
<a href=""https://i.stack.imgur.com/8K1Um.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8K1Um.jpg"" alt=""File 2 looks like this""></a></p>

<p>Desired Output
<a href=""https://i.stack.imgur.com/WlZaC.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WlZaC.jpg"" alt=""My Desired Output""></a></p>
","8906215","","8906215","","2019-07-25 10:46:12","2019-07-25 11:48:05","How to read specific column of one excel and write it to specific column of another excel file in python","<excel><python-3.x><pandas><xlsxwriter>","1","0","","","","CC BY-SA 4.0","1"
"56678664","1","56678917","","2019-06-20 03:43:33","","-2","200","<p>I want to very simply call a column in a df based on a different columns value.</p>

<p>Below is what I would use, but how can I add another method on top of this that says give me all the values in a column called minutes or <strong><em>df['minutes']</em></strong> if another called <strong><em>column_name</em></strong> is a specific value?</p>

<pre><code>df.loc[df['column_name'] == some_value]
</code></pre>

<p>Sample Data:</p>

<pre><code>column_name | Minutes
   1-5           19
   6-10          22
   11-15          8
   1-5           11
   6-10          33
</code></pre>

<p>I want to filter for column_name = any value in that column and return all values under the minutes column. </p>

<p>so if column_name is 1-5 return all values in Minutes column</p>
","8797830","","8797830","","2019-06-20 04:03:42","2019-06-20 04:28:30","Calling a column name based on a different columns values?","<python><python-3.x><pandas><data-science><data-analysis>","2","4","","2019-06-20 04:39:21","","CC BY-SA 4.0","1"
"41764360","1","41764454","","2017-01-20 13:02:05","","4","199","<p>I am looking for a customized version of the <code>Series.str.zfill(width)</code> method. This method add zeros in a string such that the string has <code>width</code> characters. I am looking for something that does this, but with any characters (or sequence of characters), not only with 0. For example adding '-' as many times as needed from the left so that the string has <code>width</code> characters.</p>
","6204900","","","","","2017-01-20 13:09:27","special string formatting within a Series in Pandas","<python><python-3.x><pandas><string-formatting><series>","2","0","","","","CC BY-SA 3.0","1"
"57047762","1","","","2019-07-15 22:10:08","","1","197","<p>I want to normalize my ambient temperature column (Ta).<br>
Here is my code:   </p>

<pre><code>df['Ta'] = df['Ta'].apply(lambda v: (v - df['Ta'].min())) / (df['Ta'].max() - df['Ta'].min())
</code></pre>

<p>It works well. But, it is very slow. The file size is 20 MB with the shape of (300000, 8).   </p>

<p>Is there any other faster solution to this?</p>
","9595563","","7964527","","2019-07-16 00:24:56","2019-07-16 00:24:56","Python: faster normalization","<python><python-3.x><pandas><normalization><normalize>","2","3","","","","CC BY-SA 4.0","1"
"57370431","1","57371155","","2019-08-06 06:51:04","","3","196","<p>I have two dataframes as given below</p>

<pre><code>t1 = pd.DataFrame({'person_id':[1,2,3],'observation_date':[np.nan,np.nan,np.nan],'observation_datetime':[np.nan,np.nan,np.nan]})

t2 = pd.DataFrame({'person_id':[1,2,3],'value_as_string':['5/28/2007','5/30/2007','6/4/2007']}).set_index('person_id')['value_as_string']
</code></pre>

<p>They look like as shown below</p>

<p><a href=""https://i.stack.imgur.com/17hCn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/17hCn.png"" alt=""enter image description here""></a></p>

<p>This is what I tried to get the output</p>

<pre><code>t1['observation_date'] = t1['person_id'].map(t2)
t1['observation_date'] = pd.to_datetime(t1['observation_date'])
t1['observation_datetime'] = pd.to_datetime(t1['observation_date']).dt.strftime('%m/%d/%Y %H:%M:%S')
</code></pre>

<p>Though this works fine, it takes lot of time in real data</p>

<p>Please note that I am trying to do this on <code>t1</code> dataframe of size <strong>1 million</strong> records and <code>t2</code> dataframe of size of <strong>15k</strong> records. So any efficient approach would be helpful</p>

<p>I expect my output dataframe to look like as shown below</p>

<p><a href=""https://i.stack.imgur.com/nL8UE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nL8UE.png"" alt=""enter image description here""></a></p>
","10829044","","10829044","","2019-08-06 07:53:54","2019-08-06 08:53:49","Elegant and Efficient way to map dates from one dataframe to another - Big data","<python><python-3.x><pandas><dataframe><datetime>","3","1","","","","CC BY-SA 4.0","1"
"57411602","1","57412160","","2019-08-08 11:33:32","","0","195","<p>I have a column with 16 days, 256 days, 450 days as values, which was obtained by subtracting 2 date columns (eg. 2010-11-10 - 2010-11-1). I want to bin the dates into 4 categories (0-30 days as 1, 30-90 days as 2, 90-180 days as 3 and greater than 180 days as 4).</p>

<p>I tried converting the column into categorical and then tried to split the (16 days to '16' and 'days') but got an error.</p>

<ol>
<li>df_merged['Case_Duration'] = df_merged['DateOfResolution'] -df_merged['DateOfRegistration'] </li>
</ol>

<h3>DateOfRegistration and DateOfResolution are date fields (eg. 2010-11-1)</h3>

<ol start=""2"">
<li>df_merged['Case_Duration'] = df_merged['Case_Duration'].astype('category')</li>
</ol>

<h3>to convert 'Case_Duration' column to category</h3>

<ol start=""3"">
<li>df_Days = df_merged[""Case_Duration""].str.split("" "", n = 1, expand = True)</li>
</ol>

<h3>to split the 'Case_Duration' column values. (eg. 16 days -> '16' and 'days')</h3>

<p>But this step gives an error -> can only use .str accessor with string values, which use np.object_ dtype in pandas</p>

<p>Desired output: </p>

<p><a href=""https://i.stack.imgur.com/eCjbc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eCjbc.png"" alt=""enter image description here""></a></p>
","5897846","","10140310","","2019-08-08 13:03:51","2019-08-08 13:03:51","How to bucket/bin the dates in python?","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57375899","1","57376113","","2019-08-06 12:18:24","","0","193","<p>I have the following code that calculates the difference between <code>start_date</code> &amp; <code>end_date</code>:</p>

<pre><code>from tkinter import *
import math
import pandas as pd
from datetime import datetime

root = Tk()

frame = Frame(root)
#frame.pack()

label1 = Label(root, text=""Peak"")
label1.grid(row=0, sticky=E)
label2 = Label(root, text=""Base"")
label2.grid(row=1, sticky=E)
label3 = Label(root, text=""Total Consumption"")
label3.grid(row=2, sticky=E)
label4 = Label(root, text=""Start day in %Y-%m-%d %H:%M:%S"")
label4.grid(row=3, sticky=E)
label5 = Label(root, text=""End day in %Y-%m-%d %H:%M:%S"")
label5.grid(row=4, sticky=E)
label6 = Label(root, text=""Diff in days (n)"")
label6.grid(row=5, sticky=E)

entry1 = Entry(root)
entry1.grid(row=0, column=1)
entry2 = Entry(root)
entry2.grid(row=1, column=1)
entry3 = Entry(root)
entry3.grid(row=2, column=1)
entry4 = Entry(root)
entry4.grid(row=3, column=1)
entry5 = Entry(root)
entry5.grid(row=4, column=1)
entry6 = Entry(root)
entry6.grid(row=5, column=1)

def date_checker():
    try:
        start_date = datetime.strptime(entry4.get(), '%Y-%m-%d %H:%M:%S')
        end_date = datetime.strptime(entry5.get(), '%Y-%m-%d %H:%M:%S')
        dates = pd.date_range(start_date, end_date, freq = 'S').tolist()
        dates = pd.Series(dates)
        dates_diff = abs((end_date - start_date).days)
        dates = dates.iloc[:-1]
        label = Label(root, text = 'Dates accepted!')
        label.grid(row=6, column=1)
        entry6.delete(0, END)
        entry6.insert(0, dates_diff)
    except ValueError:
        label2 = Label(root, text = 'Incorrect date format, should be YYYY-MM-DD HH:MM:SS')
        label2.grid(row=7, columnspan=2)
        raise ValueError(""Incorrect date format, should be YYYY-MM-DD HH:MM:SS"")

button1 = Button(root, text='Dates Checker', command = date_checker)
button1.grid(row=6, column=4)
button2 = Button(root, text='Generate Graph')
button2.grid(row=7, column=4)
button3 = Button(root, text='Quit', command = root.quit)
button3.grid(row=8, column=4)

root.mainloop()
</code></pre>

<p>What this code does is that it takes <code>start_date</code> &amp; <code>end_date</code> from the user and calculates the difference between the two dates. This code works absolutely fine.</p>

<p>But when I input <code>start_date</code> as <code>2018-01-01 00:00:00</code> &amp; <code>end_date</code> as <code>2018-12-31 00:00:00</code>, it takes forever to calculate and give the output.</p>

<p>Is there a way the above code, or the <code>date_checker</code> function in particular, be optimized so that it takes less time?</p>
","11853632","","","","","2019-08-06 13:42:01","Calculating the difference between 2 days in tkinter gui takes a lot of time","<python><python-3.x><pandas><tkinter>","1","3","","","","CC BY-SA 4.0","1"
"57165219","1","57165286","","2019-07-23 13:25:47","","1","190","<p>I've a column of strings in a DataFrame which contains comma-separated numbers. I need to extract the maximum value along each row from the strings. The maximum value returned should be the max till the 13th index from the beginning.</p>

<p>I've tried splitting the sting using ',' as a separator to convert it into a list with expand option enabled. Then I'm using the assign method of Pandas to find the max value along the vertical axis.</p>

<pre class=""lang-py prettyprint-override""><code>sample_dt1 = sample_dt['pyt_hist'].str.split(',', expand=True).astype(float)
sample_dt = sample_dt.assign(max_value=sample_dt1.max(axis=1))
</code></pre>

<p>Sample Data:</p>

<pre class=""lang-py prettyprint-override""><code>index    pyt_hist
0        0,0,0,0,0,0,0,0,0,0,0
1        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
2        0,0,0,360,420,392,361,330,300,269,239,208,177
3        0,0,0,0,0,0,0,0,0,0,0,0,0,0,23,0,23,0,0,56,0
</code></pre>

<p>Expected Result:</p>

<pre class=""lang-py prettyprint-override""><code>index    pyt_hist                                           max_value
0        0,0,0,0,0,0,0,0,0,0,0                              0
1        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0      0
2        0,0,0,360,420,392,361,330,300,269,239,208,177      420
3        0,0,0,0,0,0,0,0,0,0,0,0,0,0,23,0,23,0,0,56,0       0
</code></pre>

<p>Results obtained using my code:</p>

<pre class=""lang-py prettyprint-override""><code>index    pyt_hist                                           max_value
0        0,0,0,0,0,0,0,0,0,0,0                              0.0
1        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0      0.0
2        0,0,0,360,420,392,361,330,300,269,239,208,177      420.0
3        0,0,0,0,0,0,0,0,0,0,0,0,0,0,23,0,23,0,0,56,0       56.0
</code></pre>
","6325113","","","","","2019-07-31 07:00:31","Extracting Max value along each row from strings in column","<python><python-3.x><pandas><numpy>","2","0","","","","CC BY-SA 4.0","1"
"48860897","1","48861067","","2018-02-19 06:53:50","","2","190","<p>I have a dataset as below:</p>

<pre><code>df=pd.DataFrame([[""Sam is 5"", 2000],[""John is 3 years and 6 months"",1200],[""Jack is 4.5 years"",7000],[""Shane is 25 years old"",2000]], columns = ['texts','amount'])

print(df)

    texts                          amount
0   Sam is 5                        2000
1   John is 3 years and 6 months    1200
2   Jack is 4.5 years               7000
3   Shane is 25 years old           2000
</code></pre>

<p>I want to extract Age values from <code>df['texts']</code> and use it to calculate new column <code>df['value']</code>.</p>

<pre><code>df['value'] = df['amount'] / val 
</code></pre>

<p>where val is numeric values from <code>df['texts']</code></p>

<p>Here's my code</p>

<pre><code>val = df['texts'].str.extract('(\d+\.?\d*)', expand=False).astype(float)
df['value'] = df['amount']/val
print(df)
</code></pre>

<p>output:</p>

<pre><code>    texts                          amount     value
0   Sam is 5                       2000     400.000000
1   John is 3 years and 6 months   1200     400.000000
2   Jack is 4.5 years              7000     1555.555556
3   Shane is 25 years old          2000     80.000000
</code></pre>

<p>Expected Output:</p>

<pre><code>    texts                          amount     value
0   Sam is 5                       2000     400.000000
1   John is 3 years and 6 months   1200     342.85
2   Jack is 4.5 years              7000     1555.555556
3   Shane is 25 years old          2000     80.000000
</code></pre>

<p>The issue in above code is I am not able to figure out how can I convert 3 years 6 months into 3.5 years.</p>

<p><strong>Additional Info</strong>: Text column contains only Age values that too in order Years and months. </p>

<p>Any suggestions are welcome. Thanks</p>
","7932273","","7932273","","2018-02-19 07:26:04","2018-02-19 07:26:04","Extracting Age values from text to create new column in pandas","<python><regex><python-3.x><pandas>","1","4","","","","CC BY-SA 3.0","1"
"48572166","1","","","2018-02-01 21:40:19","","0","188","<p>Does anyone have any clue why I encounter the weird error whenever I try to load a file with pd.read_excel in pycharm, when it works fine in Jupyter notebook?</p>

<p>Here is the error:</p>

<pre><code>-------------------------------------------------------------------------------
ERROR in app [/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/flask/app.py:1560]:
Exception on /catalogue/etl/ [GET]
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/flask/app.py"", line 1612, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/flask/app.py"", line 1598, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/flask_restplus/api.py"", line 313, in wrapper
    resp = resource(*args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/flask/views.py"", line 84, in view
    return self.dispatch_request(*args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/flask_restplus/resource.py"", line 44, in dispatch_request
    resp = meth(*args, **kwargs)
  File ""/Users/anant/project/app/resources/catalogueController.py"", line 18, in get
    catalogabc.transform()
  File ""/Users/anant/project/app/models/catalog_abc_ETL.py"", line 55, in transform
    self.extractPageAndConvertToCsv(each_file)
  File ""/Users/anant/project/app/models/catalog_abc_ETL.py"", line 98, in extractPageAndConvertToCsv
    sheet = pd.read_excel(io=filename, sheetname=pageNumber)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/util/_decorators.py"", line 118, in wrapper
    return func(*args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/excel.py"", line 230, in read_excel
    io = ExcelFile(io, engine=engine)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/excel.py"", line 285, in __init__
    io, _, _ = get_filepath_or_buffer(self._io)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/io/common.py"", line 211, in get_filepath_or_buffer
    raise ValueError(msg.format(_type=type(filepath_or_buffer)))
ValueError: Invalid file path or buffer object type: &lt;class 'app.models.catalog_abc_ETL.Catalogue_abc'&gt;
</code></pre>

<p>Here is the code:</p>

<pre><code>    def load(self):
        import os
        list_of_files = os.listdir(os.getcwd())
        for each_file in list_of_files:
            if each_file.startswith('temp') and each_file.endswith(
                    '.csv'):  # since its all type str you can simply use startswith
                print(each_file)
                self.uploadTransformedCSV(each_file)

 def extractPageAndConvertToCsv(filename, delimiter=';', pageNumber=1):
        #df = pd.ExcelFile(filename)
        #sheet = df.parse(pageNumber)
        sheet = pd.read_excel(io=filename, sheetname=pageNumber)
        sheet = sheet[sheet.columns[0:2]]
        sheet.insert(loc=0, column='feed_time_stamp', value=datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'))
        sheet.to_csv(filename + "".csv"", sep=delimiter, index=False)
</code></pre>

<p>P.S. this works fine in the Jupiter notebook but not pycharm</p>
","6300508","","","","","2018-02-01 21:40:19","Pandas unable to read from file path in pycharm, but works fine in Jupiter","<python><python-3.x><pandas><pycharm><jupyter-notebook>","0","2","1","","","CC BY-SA 3.0","1"
"57551081","1","57551231","","2019-08-19 05:49:02","","3","187","<p>I have a dataframe as shown below</p>

<pre><code>df1_new = pd.DataFrame({'person_id': [1, 1, 3, 3, 5, 5],'obs_date': ['7/23/2377  12:00:00 AM', 'NA-NA-NA NA:NA:NA', 'NA-NA-NA NA:NA:NA', '7/27/2277  12:00:00 AM', '7/13/2077  12:00:00 AM', 'NA-NA-NA NA:NA:NA']})
</code></pre>

<p><a href=""https://i.stack.imgur.com/ih5ow.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ih5ow.png"" alt=""enter image description here""></a></p>

<p>I would not like to use <code>pd.to_datetime</code> approach because of year constraint (upper limit) it imposes. <a href=""https://stackoverflow.com/questions/57550238/elegant-and-efficient-way-retain-date-values-as-is-without-oob-error?noredirect=1#comment101563798_57550238"">OOB error here</a></p>

<p>The below is what I tried but it isn't efficient as you can see below</p>

<pre><code>yr = df1_new['obs_date'][0][5:9]
m = df1_new['obs_date'][0][2:4]
d = df1_new['obs_date'][0][0]
t = df1_new['obs_date'][0][11:19]
output = yr + ""-"" + m + ""-"" + d + "" "" + t
</code></pre>

<p>Is there any other efficient and elegant way to achieve the below expected output without using <code>pd.datetime</code> functions</p>

<p><a href=""https://i.stack.imgur.com/oirQQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oirQQ.png"" alt=""enter image description here""></a></p>

<p><strong>updated screenshot</strong></p>

<p><a href=""https://i.stack.imgur.com/q9LEg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q9LEg.png"" alt=""enter image description here""></a></p>

<p><strong>try/except screenshot</strong></p>

<p><a href=""https://i.stack.imgur.com/c57eF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/c57eF.png"" alt=""enter image description here""></a></p>
","10829044","","10829044","","2019-08-19 07:18:47","2019-08-19 07:18:47","How to format datetime values in columns without to_datetime functions in pandas?","<python><python-3.x><pandas><dataframe><datetime>","3","5","","","","CC BY-SA 4.0","1"
"56716585","1","","","2019-06-22 15:08:05","","0","186","<p>I have a csv file and tryng to open it using numpy.loadtxt.
if I open it using pandas, the file will look like this small example:</p>

<p>small example:</p>

<pre><code>Name       Accession  Class Species  Annotation  CF330
NaN             NaN         NaN          NaN         NaN         NaN
A2M     NM_000014.4  Endogenous           Hs         NaN     11495.0
ACVR1C     NM_145259.2  Endogenous           Hs         NaN        28.0
ADAM12     NM_003474.5  Endogenous           Hs         NaN      1020.0
ADGRE1  NM_001256252.1  Endogenous           Hs         NaN        42.0
</code></pre>

<p>I am trying to open the file using numpy.loadtxt and using the following code:</p>

<pre class=""lang-py prettyprint-override""><code>with open('datafile1.csv') as f:
    for line in f:
        FH = np.loadtxt(line, delimiter=',', skiprows=1)

print(FH)
</code></pre>

<p>but it returns this error:</p>

<pre><code>ValueError: could not convert string to float: 
</code></pre>

<p>do you know how to fix the problem?</p>

<p>here is the original data set:</p>

<pre><code>Name,Accession,Class,Species,Annotation,CF330
,,,,,
A2M,NM_000014.4,Endogenous,Hs,,11495
ACVR1C,NM_145259.2,Endogenous,Hs,,28
ADAM12,NM_003474.5,Endogenous,Hs,,1020
ADGRE1,NM_001256252.1,Endogenous,Hs,,42
</code></pre>
","10559978","","10559978","","2019-06-22 15:16:26","2019-06-22 16:13:41","opening csv file in a numpy.txt in python3","<python-3.x><pandas><csv><numpy>","2","2","","","","CC BY-SA 4.0","1"
"48743647","1","48744003","","2018-02-12 09:56:34","","1","186","<p>I have a pandas multiindex with mostly numerical values, but some None, NaN, or ""-"" in the data as well. Something like this:</p>

<pre><code>                0         1         2         3
bar one -0.096648 -0.080298  0.859359 -0.030288
    two       NaN -0.431791  1.923893 -1.544845
    thr -0.358526  1.416211  1.589617  0.284130
baz one  0.639951 -0.008833         -  0.042315
    two  0.705281      None -1.108522  0.471676
</code></pre>

<p>Now I need to identify for each level 0 index which row has the smallest numerical value in column 0 and extract value for column 3 for that row. (ignoring NaN, None and -)</p>

<p>For example, for 'bar' I compare -0.096648, NaN, -0.358526 and the smallest of these is -0.358526 so I want the value 0.284130 (from the column 3)</p>

<p>I'm sure this is simple, but I'm not very familiar with these multi-index tables and just got lost and frustrated.</p>
","6592934","","","","","2018-02-12 10:21:47","Python pandas multiindex select values","<python-3.x><pandas><multi-index>","1","0","","","","CC BY-SA 3.0","1"
"57656079","1","57659400","","2019-08-26 10:24:36","","1","184","<h1>Need to drop a sub-column of multi-index data frame created from pivot table</h1>
<p>Need to drop a sub-column only at specific columns(month) dynamically</p>
<p>I have a dataframe created from pivot table and need to drop a sub-column at specific columns dynamically...
<br><strong>if todays date is less than 15</strong> i need to drop the sub-column <strong>Bill1</strong> for all the months except <strong>Sep-19</strong>(<strong>current month</strong>)
<br><strong>if todays date is greater than 15</strong>, it should drop the sub-column <strong>Bill1</strong> for all the months except <strong>Oct-19</strong>(<strong>next month</strong>)</p>
<pre><code>data_frame1 = pd.pivot_table(data_frame, index=['PC', 'Geo', 'Comp'], values=['Bill1', 'Bill2'], columns=['Month'], fill_value=0)
data_frame1 = data_frame1.swaplevel(0,1, axis=1).sort_index(axis=1)
tuples = [(a.strftime('%b-%y'), b) if a!= 'All' else (a,b) for a,b in data_frame1.columns]
data_frame1.columns = pd.MultiIndex.from_tuples(tuples)
</code></pre>
<p><b>output:</b></p>
<pre><code>              jan-19             Feb-19        Mar-19
             Bill1 Bill2      Bill1 Bill2     Bill1 Bill2     
PC Geo Comp
A  Ind   OS   1     1.28        1    1.28      1    1.28
</code></pre>
<p><b> desired Output:</b></p>
<pre><code>               jan-19      Feb-19       Mar-19
               Bill2       Bill2     Bill1 Bill2     
PC Geo Comp
A  Ind   OS     1.28        1.28      1    1.28
</code></pre>
","11906428","","-1","","2020-06-20 09:12:55","2019-09-04 06:29:40","drop a column from a multi-level column index","<python-3.x><pandas><pivot-table>","1","7","1","","","CC BY-SA 4.0","1"
"33716820","1","33717169","","2015-11-15 05:40:42","","0","183","<p>I have a dataframe of historical election results and want to calculate an additional column that applies a basic math formula for records for winning candidates and copies a value over for the rest of them. </p>

<p>Here is the code I tried:</p>

<pre><code>va2 = va1[['contest_id', 'year', 'district', 'office', 'party_code', 
           'pct_vote', 'winner']].drop_duplicates()
va2['vote_waste'] = va2['winner'].map(lambda x: (-.5) + va2['pct_vote'] 
                       if x == 'w' else va2['pct_vote'])
</code></pre>

<p>This gave me a new column where each row contained the calculation for every row in every row.</p>
","5435124","","2626968","","2015-11-16 16:21:38","2015-11-16 16:21:38","Calculating a new column in pandas","<python><python-3.x><pandas><ipython-notebook>","2","1","","","","CC BY-SA 3.0","1"
"48984825","1","","","2018-02-26 09:07:17","","0","182","<p>I've got an Excel file with multiple sheets with same structure. 
Number of rows varies on every sheet, but <code>pd.read_excel()</code> returns <code>df</code> with <code>nb_rows == nb_rows on the first sheet</code>.
I've checked Excel sheets with <code>CTRL+down</code> - there is no empty lines in the middle of the sheet.
How can I fix the problem?</p>

<p>The example code is follows:</p>

<pre><code>import pandas as pd
xls_sheets = ['01', '02', '03']
fname = 'C:\\data\\data.xlsx'
xls = pd.ExcelFile(fname)
for sheet in xls_sheets:
    df = pd.read_excel(io=xls, sheet_name=sheet)
    print(len(df))
</code></pre>

<p>Output:</p>

<pre><code>&gt;&gt; 4043  #Actual nb_rows = 4043
&gt;&gt; 4043  #Actual nb_rows = 11015
&gt;&gt; 4043  #Actual nb_rows = 5622
</code></pre>

<p>python 3.5, pandas 0.20.1</p>
","6385180","","","","","2018-08-04 09:22:42","Pandas read_excel for multiple sheets returns same nb_rows on every sheet","<python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"57375811","1","","","2019-08-06 12:13:00","","1","182","<p>So I'm newbie in python and I have a project making contour plot map. I have a data in xlsx contain <code>x</code>, <code>y</code>, and <code>z</code> values. <strong><code>x</code></strong> and <strong><code>y</code></strong> are the coordinates and <strong><code>z</code></strong> are the measurement values.</p>

<pre><code>1. x        ;  y       ; z
2. 110.4482 ; 7.04428  ; 0.177
3. 110.4451 ; 7.04366  ; 0.102
4. 110.4432 ; 7.04432  ; 0.482
5. 110.4407 ; 7.04434  ; 0.504
</code></pre>

<p>I want to make a contour like <a href=""https://i.stack.imgur.com/JoTUj.png"" rel=""nofollow noreferrer"">This</a>
<a href=""https://i.stack.imgur.com/JoTUj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JoTUj.png"" alt=""enter image description here""></a>
I tried to make a contour but when I run it, it appear a blank picture <a href=""https://i.stack.imgur.com/kquWM.png"" rel=""nofollow noreferrer"">This</a> 
<a href=""https://i.stack.imgur.com/kquWM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kquWM.png"" alt=""enter image description here""></a></p>

<p>and a note:</p>

<blockquote>
  <p>warnings.warn(""No contour levels were found""</p>
  
  <p>UserWarning: No contour levels were found within the data range.</p>
  
  <p>xa[xa &lt; 0] = -1 RuntimeWarning: invalid value encountered in less</p>
  
  <p>: usr/local/lib/python3.6/dist-packages/matplotlib/contour.py:1243:
  UserWarning: No contour levels were found within the data range.<br>
  warnings.warn(""No contour levels were found""</p>
</blockquote>

<p>This is the code I used:</p>

<pre><code>import numpy as np
import pandas as pd
import matplotlib as ml
import matplotlib.pyplot as plt
from scipy.interpolate import griddata

xlsx_path =('Book1.xlsx')
df = pd.read_excel(xlsx_path)

x = df.iloc[1:20,0]
y = df.iloc[1:20,1]
z = df.iloc[1:20,2]
xi = np.linspace(6, 8, 20)
yi = np.linspace(109, 111, 20)
zi = griddata((x, y), z, (xi[None,:], yi[:,None]), method='cubic')
plt.contour(xi, yi, zi)
plt.show()
</code></pre>

<p>How can I fix it?</p>
","11280235","","3666197","","2019-08-06 13:11:31","2019-08-06 13:11:31","How to make contour plot map and overcome no contour level were found?","<python-3.x><pandas><numpy><matplotlib><contour>","1","3","2","","","CC BY-SA 4.0","1"
"49135308","1","","","2018-03-06 16:09:47","","0","181","<p>Suppose I have two arrays, <code>x</code> and <code>y</code>, where <code>y</code> is a subset of <code>x</code>:</p>

<pre><code>x = [1, 2, 3, 4, 5, 6, 7, 8, 9]
y = [3, 4, 7]
</code></pre>

<p>I want to return an array like:</p>

<pre><code>ret = [False, False, True, True, False, False, True, False, False]
</code></pre>

<p>If <code>y</code> were just a single number it would be easy enough (<code>x == y</code>), but I tried the equivalent <code>x in y</code> and it didn't work. Of course, I could do it with a for loop, but I'd rather there was a neater way.</p>

<p>I've tagged this Pandas since <code>x</code> is actually a Pandas series (a column in a dataframe). <code>y</code> is a list, but can be made to be a NumPy array or Series if needed.</p>
","979286","","","","","2018-03-06 16:29:56","Test if every element of an array is in another array","<python><python-3.x><pandas><numpy>","3","2","","2018-03-06 16:14:08","","CC BY-SA 3.0","1"
"40560614","1","40561149","","2016-11-12 07:28:50","","0","181","<p>I have some data that looks like this:</p>

<pre><code>----system---- ---load-avg--- ----total-cpu-usage---- ------memory-usage----- -dsk/total- --io/total- ---paging-- -net/total-
  date/time   | 1m   5m  15m |usr sys idl wai hiq siq| used  buff  cach  free| read  writ| read  writ|  in   out | recv  send
10-11 00:00:01|0.67 0.42 0.31|  2   0  98   0   0   0|25.0G 16.9M 6331M  189M|2101k  901k|30.4  28.3 |  63B   75B|   0     0
10-11 00:00:03|0.67 0.42 0.31|  4   0  95   0   0   0|25.0G 16.9M 6332M  190M|  50k 1142k|4.00  18.0 |   0     0 | 310k 6765B
10-11 00:00:05|0.62 0.41 0.31|  4   0  95   0   0   0|25.0G 16.9M 6333M  189M| 116k 2534k|3.50   113 |   0     0 | 484k   27k
10-11 00:00:07|0.62 0.41 0.31|  7   1  92   0   0   0|25.0G 16.9M 6335M  187M| 154k 2372k|4.00   128 |   0     0 |1159k   24k
10-11 00:00:09|0.62 0.41 0.31|  5   0  95   0   0   0|25.0G 16.9M 6336M  185M|   0  1556k|   0  38.5 |   0     0 | 396k 4172B
10-11 00:00:11|0.73 0.44 0.32|  4   1  95   0   0   0|25.0G 16.9M 6336M  184M| 136k 2732k|3.50   139 |   0     0 | 270k   28k
</code></pre>

<p>You can generate test data with <a href=""https://linux.die.net/man/1/dstat"" rel=""nofollow noreferrer""><code>dstat</code></a>.</p>

<p>I want to import it into data frame like this (Python 3.5.2,pandas 0.18.1):</p>

<pre><code>  date/time     1m   5m  15m  usr sys idl wai hiq siq  used  buff  cach  free  read  writ  read  writ   in   out   recv  send
10-11 00:00:01 0.67 0.42 0.31   2   0  98   0   0   0 25.0G 16.9M 6331M  189M 2101k  901k 30.4  28.3    63B   75B    0     0
10-11 00:00:03 0.67 0.42 0.31   4   0  95   0   0   0 25.0G 16.9M 6332M  190M   50k 1142k 4.00  18.0     0     0   310k 6765B
10-11 00:00:05 0.62 0.41 0.31   4   0  95   0   0   0 25.0G 16.9M 6333M  189M  116k 2534k 3.50   113     0     0   484k   27k
10-11 00:00:07 0.62 0.41 0.31   7   1  92   0   0   0 25.0G 16.9M 6335M  187M  154k 2372k 4.00   128     0     0  1159k   24k
10-11 00:00:09 0.62 0.41 0.31   5   0  95   0   0   0 25.0G 16.9M 6336M  185M    0  1556k    0  38.5     0     0   396k 4172B
10-11 00:00:11 0.73 0.44 0.32   4   1  95   0   0   0 25.0G 16.9M 6336M  184M  136k 2732k 3.50   139     0     0   270k   28k
</code></pre>

<p>This is my expressions but doesn't work:</p>

<pre><code>path='/opt/dstat.2016-11-10'    
dstat=pd.read_table(path,skiprows=1,header=0,sep=r""\|{\s}*|\s+"")
</code></pre>

<p>I don't want edit text file.</p>
","5452748","","6394138","","2016-11-12 09:20:15","2016-11-12 12:44:43","How to import the output of dstat into pandas?","<python><python-3.x><pandas>","1","0","1","","","CC BY-SA 3.0","1"
"57023563","1","57025071","","2019-07-13 23:39:20","","1","181","<p>after getting data of the Bitcoin (BTC-USD) from Yahoo, I try to create a new column to show if the Close price is higher than the Open price of of each day.</p>

<p>What I want to do is to create a column that shows 1 when the Close is higher than the Open. And a 0 when that condition is not true. </p>

<p>When I try to compare the Close and the Open values I receive the next message : ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</p>

<p>I have tried several things to fix it (like importing the data as a CSV instead directly from Yahoo) but unfortunately I couldn't find the solution. </p>

<p>Below you can see the code that I am using:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import pandas_datareader as dr
df = dr.data.get_data_yahoo('btc-usd',start = '01-01-2015', end= '31-12-2018')
df.head(2)

df['X'] = [1 if (df.loc[ei,'Close'] &gt; df.loc[ei,'Open'])  else 0 for ei in df.index] #---&gt; The error is produced in this line of code
df.tail()
</code></pre>

<p>Below you can see the error message: </p>

<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-45-eb64775bf24f&gt; in &lt;module&gt;
----&gt; 1 df['X'] = [1 if (df.loc[ei,'Close'] &gt; df.loc[ei,'Open'])  else 0 for ei in df.index]
      2 df.tail()

&lt;ipython-input-45-eb64775bf24f&gt; in &lt;listcomp&gt;(.0)
----&gt; 1 df['X'] = [1 if (df.loc[ei,'Close'] &gt; df.loc[ei,'Open'])  else 0 for ei in df.index]
      2 df.tail()

C:\ProgramData\Anaconda3\lib\site-packages\pandas\core\generic.py in __nonzero__(self)
   1476         raise ValueError(""The truth value of a {0} is ambiguous. ""
   1477                          ""Use a.empty, a.bool(), a.item(), a.any() or a.all().""
-&gt; 1478                          .format(self.__class__.__name__))
   1479 
   1480     __bool__ = __nonzero__

ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
</code></pre>

<p>I would like to get the result of this formula. Having 1 when the Close is > Open and 0 when the conditional is false.</p>
","11781149","","6622587","","2019-07-13 23:43:29","2019-07-14 06:32:02","How to fix 'The truth value of a Series is ambiguous' in Python produced after comparing simple data","<python><python-3.x><pandas>","2","5","1","","","CC BY-SA 4.0","1"
"48797775","1","48797911","","2018-02-14 23:13:05","","0","181","<p><strong>I'm retrieving survey results from Lime Survey via its API (Remote Control):</strong></p>

<p><a href=""https://i.stack.imgur.com/w08hw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w08hw.png"" alt=""enter image description here""></a></p>

<p><strong>And I manage to get it into a DataFrame. But it's just 1 column per row:</strong></p>

<p><a href=""https://i.stack.imgur.com/6dEgN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6dEgN.png"" alt=""enter image description here""></a></p>

<p><strong>The data looks like this.</strong></p>

<p><a href=""https://i.stack.imgur.com/NEzo5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NEzo5.png"" alt=""enter image description here""></a></p>

<p>What I want to be able to do is get averages of the data by question and category. From the example below, q10[wor1], q10[wor2], . . ., q10[wor7] give the 7 questions that are part of category q10.</p>

<p>How to first select all the data for wor1, wor2, ..., wor7, separately, so that I can do stats on each of those individual questions.</p>

<p>Then how do I select all data for q10* so that I can do stats for the entire group?</p>

<p>Even without trying to separate the category from the question, I haven't been able to select just all the 'q10[wor1]' data.</p>
","122630","","","","","2018-02-14 23:29:09","How to select columns of data from a DataFrame","<python><python-3.x><pandas><limesurvey>","1","2","","","","CC BY-SA 3.0","1"
"56771904","1","56772020","","2019-06-26 11:35:58","","1","180","<p>I am getting below error while converting Lists of list to dataframe:</p>

<p>raise ValueError('DataFrame constructor not properly called!')
ValueError: DataFrame constructor not properly called!</p>

<p>I have used numpy to split the list and now need to convert those lists of list to a dataframe:</p>

<pre><code>
    import numpy as np
    import pandas as pd

    def SplitList():
        l = np.array([6,2,5,1,3,6,9,7,6])
        n = 3

        list = l.reshape((len(l)//n), n).T
        print(list)

    df = pd.DataFrame(list)
</code></pre>
","11403894","","11607986","","2019-06-26 12:28:14","2019-07-05 07:01:58","Getting ValueError: DataFrame constructor not properly called while creating a dataframe from lists of list","<python-3.x><pandas><dataframe><type-conversion><nested-lists>","1","0","","","","CC BY-SA 4.0","1"
"56997982","1","","","2019-07-11 22:17:51","","1","178","<p>I have a simple dataframe with two columns and would like to transform it into a matrix with same number of columns and rows.</p>

<p>I have several combinations as rows and would like to copy those into columns in order to have a view of the strength of the relationships on a visual level (possibly a chord diagram).</p>

<p>This is a sample the dataframe I'm using:</p>

<pre><code>variables         count
a                  13
b                   9
c                   8
d                   6
e                  10
f                  12
g                   9
h                   7
a_b                 7
a_b_h               8
</code></pre>

<p>And this is the output I would like to have, taking into account that I have several combinations, not just pair wise.</p>

<pre><code>         a  b  c  d  e  f  g  h  a_b a_b_h
a       13  7  0  0  0  0  0  0  0    0
b        7  9  0  0  0  0  0  0  0    0
c        0  0  8  0  0  0  0  0  0    0
d        0  0  0  6  0  0  0  0  0    0
e        0  0  0  0 10  0  0  0  0    0
f        0  0  0  0  0 12  0  0  0    0
g        0  0  0  0  0  0  9  0  0    0
h        0  0  0  0  0  0  0  7  0    0
a_b      0  0  0  0  0  0  0  0  7    0
a_b_h    0  0  0  0  0  0  0  0  0    8
</code></pre>

<p>And so on, with all the combinations.</p>

<p>The aim is to then build a chord diagram like the one below - which shows the strength of the relationships between variables</p>
","8542692","","","","","2019-07-11 22:47:46","Create matrix from simple pandas dataframe","<python><python-3.x><pandas><numpy>","1","1","","","","CC BY-SA 4.0","1"
"57101797","1","57102116","","2019-07-18 19:54:52","","1","178","<p>I have data from an excel sheet I have summarized in a pandas crosstab. I want to categorize the data further by summing related rows. </p>

<p>Here is my crosstab:</p>

<blockquote>
<pre><code>class_of_orbit         Elliptical  GEO  LEO  MEO  All
users
Civil                           0    0   36    0   36
Civil/Government                0    0    2    0    2
Commercial                      3   99  412    0  514
Government                      9   14   38    0   61
Government/Civil                0    0   10    0   10
Government/Commercial           0    2   81    0   83
Government/Military             0    0    1    0    1
Military                        9   67   66    0  142
Military/Civil                  0    0    2    0    2
Military/Commercial             0    0    0   32   32
All                            21  182  648   32  883
</code></pre>
</blockquote>

<p>I only want 4 groups: civil, govt,commercial, and military. If ""Government"" is in the name, I want to sum all the rows that contain it. If ""Military"" is in the name I want to sum the rows into a military row....</p>

<p>What is the best way to do this?</p>
","10047675","","10047675","","2019-07-18 20:52:37","2019-07-18 22:10:43","How do I sum rows inside of pandas crosstab and make a new crosstab?","<python><python-3.x><pandas><crosstab>","3","0","","","","CC BY-SA 4.0","1"
"57799510","1","","","2019-09-05 06:24:26","","0","177","<p>I've been working in this project long time ago and I have some trouble analyzing the clusters. Basically, i'm reading some data from an csv file with 4000 records using panda read_csv() (a excel file exported as csv), then I clean the extracted data by removing the punctuation, tokenizing and stemming, in the following step, I create the Tdidf Matrix and, using k-means, I make the clusters.</p>

<p>I've used the following libs:</p>

<p>word_tokenize, SnowballStemmer, TfidfVectorizer, cosine_similarity, KMeans, MDS. With python 3.</p>

<pre class=""lang-py prettyprint-override""><code>from __future__ import print_function
import os
import nltk
import pandas as pd
from nltk.tokenize import word_tokenize
from nltk.stem.snowball import SnowballStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.manifold import MDS
from scipy.spatial.distance import cdist
import matplotlib.pyplot as plt
from string import punctuation
import numpy as np


# Creación de un corpus de documentos a partir de una ruta (PATH) donde se encuentran varios documentos.
# Se genera una lista de textos de cada documentos junto con su nombre de archivo de origen

def CrearCorpus(path):

    df = pd.read_csv('./llamadas.csv', usecols=['motivo', 'respuesta'], delimiter=';')

    corpus = []

    for i in range(1, 4050):
        problema = str(df['motivo'][i])
        solucion = str(df['respuesta'][i])
        problema_final = problema + ' ' + solucion
        corpus.append([problema_final, 'document ' + str(i + 1)])
    return (corpus)

# Eliminar ""stopwords"" de un texto

def _RemoveStopwords(sentence):
    word_tokens = word_tokenize(sentence)
    stop_words = set(stopwords.words('spanish'))
    filtered_sentence = [w for w in word_tokens if not w in stop_words]
    filtered_sentence = """"
    for w in word_tokens:
       if w not in stop_words:
           filtered_sentence = filtered_sentence + "" ""+w
    return(filtered_sentence)

# Lee  cada uno de los documentos desde el corpus y genera los textos y sus identificadores (titles)

def read_documents(path):
    corpus = CrearCorpus(path)
    documents = []
    titles = []
    for c in range(len(corpus)):
        (doc, fn) = corpus[c]
        titles.append(fn)
        documents.append(doc)
    return ((documents, titles))


# Elimina puntuación de los documentos

def removePuntuaction(documents):
    translator = str.maketrans('', '', punctuation)
    for i in range(len(documents)):
        documents[i] = documents[i].translate(translator)
    return (documents)


# Realizar lematización de un texto en Español

def Stemmer(text):
    stemmer = SnowballStemmer('spanish')
    words_stem = stemmer.stem(text)
    return (words_stem)


# Realiza tokenización y lematización de un texto

def tokenize_and_stem(textdata):
    text = word_tokenize(textdata)
    lista = []
    for elem in text:
        word = elem.lower()
        nuevo = Stemmer(word)
        lista.append(nuevo)
    return (lista)


# Realiza tokenización de un un texto

def tokenize_only(text):
    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
    filtered_tokens = tokens
    return filtered_tokens


# Crear vocabulario de términos a partir del corpus de documentos
# Se crea una tabla (FRAME) que representa un ""vocabulario"" de las palabras de cada documento

def crear_vocabulario(documents):
    # crear dos listas, una lematizada y otra con tokens
    totalvocab_stemmed = []
    totalvocab_tokenized = []
    for i in documents:
        allwords_stemmed = tokenize_and_stem(i)
        totalvocab_stemmed.extend(allwords_stemmed)
        allwords_tokenized = tokenize_only(i)
        totalvocab_tokenized.extend(allwords_tokenized)
    vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index=totalvocab_stemmed)
    return (vocab_frame)


# Crea una matrix tf x idf a partir de los textos ""tokenizados"" y lematizados

def crear_matriz_tfidf():
    tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,
                                       min_df=0.2,
                                       use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1, 3))
    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)
    terms = tfidf_vectorizer.get_feature_names()
    return ((tfidf_matrix, terms))


# Realiza clustering K-means de la matrix tf x idf (entrega los clusters)

def clustering(tfidf_matrix, num_clusters):
    km = KMeans(n_clusters=num_clusters)  # Crea objeto de datos KMeans
    km.fit(tfidf_matrix)  # Realiza K-means propiamente tal
    clusters = km.labels_.tolist()

    return ((clusters, km))


# Muestra estadísticas de los clusters y objetos (documentos y palabras) generados

def cluster_stats(clusters, titles, km, tfidf_matrix):

    # abrimos el archivo csv
    df = pd.read_csv('./llamadas.csv', usecols=['Plataforma'], delimiter=';')

    films = {'title': titles, 'documents': documents, 'cluster': clusters}
    frame = pd.DataFrame(films, index=[clusters], columns=['title', 'cluster'])
    frame['cluster'].value_counts()  # number of docs per cluster
    print(""Top terminos por cluster:"")
    print()
    # Ordenar centros de clusters segun proximidad al centroid
    order_centroids = km.cluster_centers_.argsort()[:, ::-1]

    # analizamos cluster por cluster
    for i in range(num_clusters):

        print(""Palabras de Cluster %d:"" % i, end='')
        for ind in order_centroids[i, :5]:  # replace 6 with n words per cluster
            print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'),
                  end=',')
        print()
        print()

        # variable para ver el largo de la cantidad de documentos involucrados en el cluster
        list_row = []

        print(""IDs de cluster %d:"" % i, end='')
        for title in frame.loc[i]['title'].values.tolist():
            print(' %s,' % title, end='')

        # extraemos la linea de cada documento para analizar de que plataforma es
        count_snd = 0
        count_mateonet = 0
        print()
        numbers_rows = frame.loc[i]['title'].values.tolist()
        for index in range(0, len(numbers_rows)):
            row = numbers_rows[index].strip('document ')
            list_row.append(int(row))

        # contamos cuantos elementos son de que plataforma
        for cols in range(0, len(list_row)):
            value_platform = df['Plataforma'][list_row[cols]]
            if value_platform == 'SND ':
                count_snd = count_snd + 1
            if value_platform == 'Mateonet ':
                count_mateonet = count_mateonet + 1

        # indices por plataforma
        print()
        print('Plataforma SND: ' + str(count_snd))
        print()
        print('Plataforma Mateonet: ' + str(count_mateonet))
        print()
        print('Cantidad de preguntas: ' + str(len(frame.loc[i]['title'].values.tolist())))
        print()
        print()

#MAIN

# Ajustar esta variable con la ruta a un directorio que contenga varios documentos
# Give the location of the file
PATH = ""./llamadas.csv""

num_clusters = 100 # Maximo numero de clusters es 5

(documents, titles) = read_documents(PATH)
documents = removePuntuaction(documents)
vocab_frame = crear_vocabulario(documents)

print('Existen ' + str(vocab_frame.shape[0]) + ' itemes en vocab_frame')

(tfidf_matrix, terms) = crear_matriz_tfidf()
(clusters, km) = clustering(tfidf_matrix, num_clusters)
cluster_stats(clusters, titles, km, tfidf_matrix)
</code></pre>

<p><a href=""https://github.com/felipefuller/faq/blob/master/data_analysis.py"" rel=""nofollow noreferrer"">https://github.com/felipefuller/faq/blob/master/data_analysis.py</a></p>

<p>I can create up to 167 clusters, but when I update it to create 168 or more clusters it raises the following errors: </p>

<pre class=""lang-py prettyprint-override""><code>Traceback (most recent call last):
  File ""/Users/felipefuller/.virtualenvs/faq/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 2657, in get_loc
    return self._engine.get_loc(key)
  File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/index.pyx"", line 132, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 987, in pandas._libs.hashtable.Int64HashTable.get_item
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 993, in pandas._libs.hashtable.Int64HashTable.get_item
KeyError: 167

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/felipefuller/dev/faq/cluster.py"", line 324, in &lt;module&gt;
    cluster_stats(clusters, titles, km, tfidf_matrix)
  File ""/Users/felipefuller/dev/faq/cluster.py"", line 187, in cluster_stats
    if (len(frame.loc[i]['title'].values.tolist()) &gt;= 250):
  File ""/Users/felipefuller/.virtualenvs/faq/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1500, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File ""/Users/felipefuller/.virtualenvs/faq/lib/python3.7/site-packages/pandas/core/indexing.py"", line 1913, in _getitem_axis
    return self._get_label(key, axis=axis)
  File ""/Users/felipefuller/.virtualenvs/faq/lib/python3.7/site-packages/pandas/core/indexing.py"", line 141, in _get_label
    return self.obj._xs(label, axis=axis)
  File ""/Users/felipefuller/.virtualenvs/faq/lib/python3.7/site-packages/pandas/core/generic.py"", line 3583, in xs
    drop_level=drop_level)
  File ""/Users/felipefuller/.virtualenvs/faq/lib/python3.7/site-packages/pandas/core/indexes/multi.py"", line 2571, in get_loc_level
    indexer = self._get_level_indexer(key, level=level)
  File ""/Users/felipefuller/.virtualenvs/faq/lib/python3.7/site-packages/pandas/core/indexes/multi.py"", line 2652, in _get_level_indexer
    code = level_index.get_loc(key)
  File ""/Users/felipefuller/.virtualenvs/faq/lib/python3.7/site-packages/pandas/core/indexes/base.py"", line 2659, in get_loc
    return self._engine.get_loc(self._maybe_cast_indexer(key))
  File ""pandas/_libs/index.pyx"", line 108, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/index.pyx"", line 132, in pandas._libs.index.IndexEngine.get_loc
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 987, in pandas._libs.hashtable.Int64HashTable.get_item
  File ""pandas/_libs/hashtable_class_helper.pxi"", line 993, in pandas._libs.hashtable.Int64HashTable.get_item
KeyError: 167
</code></pre>

<p>So it should be able to manage easily more than 167 clusters, because i'm using more than 4000 records.</p>

<p>Thank you!</p>
","4494900","","4494900","","2019-09-05 17:04:20","2019-09-05 17:04:20","Error pandas._libs.hashtable.Int64HashTable.get_item when creating cluster","<python><python-3.x><pandas><data-mining><sklearn-pandas>","1","2","","","","CC BY-SA 4.0","1"
"56802883","1","56803007","","2019-06-28 07:45:23","","2","176","<p>Source Data:</p>

<pre><code>20  7369    CLERK
30  7499    SALESMAN
30  7521    SALESMAN
20  7566    MANAGER
30  7654    SALESMAN
30  7698    MANAGER
10  7782    MANAGER
20  7788    ANALYST
10  7839    PRESIDENT
30  7844    SALESMAN
20  7876    CLERK
30  7900    CLERK
20  7902    ANALYST
</code></pre>

<p>Requirement:
<code>012345678901234567890123456789</code></p>

<p>Hi All,<br></p>

<p>I am reading this .dat file data into python pandas successfully.
Left to right length of the data in a row is 30 (012345678901234567890123456789)
My requirement is,
I need to derive 3 columns</p>

<pre><code>From left to right: 1 to 4 (length 4) spaces as DEPTNO 
From left to right: 5 to 13 (length 9) spaces as EMPNO 
From left to right: 14 to 30 (length 9) spaces as EMPNO 
</code></pre>

<p>I tried this code:</p>

<pre><code>import pandas as pd    
with open('Emp.dat','r') as f:
    next(f) # skip first row
    df = pd.DataFrame(l.rstrip().split() for l in f)
</code></pre>

<p>Required Output:</p>

<pre><code>DEPTNO  EMPNO   JOB
20      7369    CLERK
30      7499    SALESMAN
30      7521    SALESMAN
20      7566    MANAGER
30      7654    SALESMAN
30      7698    MANAGER
10      7782    MANAGER
20      7788    ANALYST
10      7839    PRESIDENT
30      7844    SALESMAN
20      7876    CLERK
30      7900    CLERK
20      7902    ANALYST
</code></pre>
","10808871","","11607986","","2019-06-28 07:53:11","2019-06-28 08:03:11","Deriving multiple columns from single column data in python 3 pandas","<python><python-3.x><pandas>","2","0","0","2019-06-30 22:17:20","","CC BY-SA 4.0","1"
"57339053","1","","","2019-08-03 14:05:36","","0","175","<p>I'm trying to predict sales with multiple linear regression with variables X1 = customers and X2 = KiloWattHour(kWh). But when I try in Excel and try in Python, the results are different.</p>

<p>Data in Excel:</p>

<pre><code>Sales (Y) KWH (X1) Customer(X2)
2,72       3,13      174
2,59       3,03      175
2,81       3,28      175
2,66       3,14      117
2,80       3,29      87
2,71       3,13      74
2,93       3,33      68
2,71       3,10      104
</code></pre>

<p>Data in CSV imported to Python:</p>

<pre><code>Sales (Y) KWH (X1) Customer(X2)
2.72       3.13      174
2.59       3.03      175
2.81       3.28      175
2.66       3.14      117
2.80       3.29      87
2.71       3.13      74
2.93       3.33      68
2.71       3.10      104
</code></pre>

<p>Code for reading the CSV file:</p>

<pre><code>import pandas as pd
import numpy as np

from sklearn import linear_model
import statsmodels.api as sm

data = pd.read_csv('/code/master_data.csv')

print(data)

</code></pre>

<p>This is code for prediction using linear regression:</p>

<pre><code>x = data[['kwhpenjualan','totalpelanggan']]
y = data['totalpendapatan']

x_1 = sm.add_constant(x)

model = sm.OLS(y, x_1)
result = model.fit()
result.params
</code></pre>

<p>This is the result in Excel:</p>

<pre><code>Intercept     -2,345215066
KWH (X1)      1,618236605
Customer (X2) 0,002576039
</code></pre>

<p>This is the result in Python:</p>

<pre><code>Intercept         127.619065
KWH               -45.949302
Customer.         50.262137
dtype: float64
</code></pre>

<p>Can you help me solve this problem?</p>
","2994473","","6395052","","2019-08-03 15:16:06","2019-08-03 15:16:06","Why does Analyse Data in Excel give different result from OLS Stats Model in Python?","<python><python-3.x><pandas><numpy><statsmodels>","0","3","0","","","CC BY-SA 4.0","1"
"41172448","1","41172531","","2016-12-15 20:02:05","","2","171","<p>Using Pandas, python 3. Working in jupyter.</p>

<p>Ive made this graph below using the following code:</p>

<pre><code>temp3 = pd.crosstab(df['Credit_History'], df['Loan_Status']) 
temp3.plot(kind = 'bar', stacked = True, color = ['red', 'blue'], grid = False)
print(temp3)
</code></pre>

<p><a href=""https://i.stack.imgur.com/Op0Bz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Op0Bz.png"" alt=""Stacked Bars""></a></p>

<p>And then tried to do the same, but with divisions for Gender. I wanted to make this:
<a href=""https://i.stack.imgur.com/WwEIG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WwEIG.png"" alt=""Four Bars""></a></p>

<p>So I wrote this code:
<a href=""https://i.stack.imgur.com/eA3Yw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eA3Yw.png"" alt=""enter image description here""></a></p>

<p>And made this monstrosity. I'm unfamiliar with pivot tables in pandas, and after reading documentation, am still confused. I'm assuming that <code>aggfunc</code> affects the values given, but not the indices. How can I separate the loan status so that it reads as different colors for 'Y' and 'N'?</p>

<p>Trying a method similar to the methods used for <code>temp3</code> simply yields a key error:</p>

<pre><code>temp3x = pd.crosstab(df['Credit_History'], df['Loan_Status', 'Gender']) 
temp3x.plot(kind = 'bar', stacked = True, color = ['red', 'blue'], grid = False)
print(temp3)
</code></pre>

<p>How can I make the 'Y' and 'N' appear separately as they are in the first graph, but for all 4 bars instead of using just 2 bars?</p>
","7143036","","","","","2017-03-06 11:17:55","Why won't barchart in Pandas stack different values?","<python><python-3.x><pandas><data-munging>","3","0","","","","CC BY-SA 3.0","1"
"57264509","1","","","2019-07-30 04:54:06","","0","170","<p>I used this code to convert my float numbers into an integer, however, it does not work. Here are all step I gone through so far:</p>

<pre><code>Step 1: I converted timestamp1 and timestamp2 to datetime in order subtract and get days:

a=pd.to_datetime(df['timestamp1'], format='%Y-%m-%dT%H:%M:%SZ')
b=pd.to_datetime(df['timestamp2'], format='%Y-%m-%dT%H:%M:%SZ')
df['delta'] = (b-a).dt.days

Step 2: Converted the strings into integers as the day:
df['delta'] = pd.to_datetime(df['delta'], format='%Y-%m-%d', errors='coerce')
df['delta'] = df['delta'].dt.day

Step 3: I am trying to convert floats into integers.

categorical_feature_mask = df.dtypes==object
categorical_cols = df.columns[categorical_feature_mask].tolist()

        from sklearn.preprocessing import LabelEncoder
        le = LabelEncoder()
        df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col)) 
df[categorical_cols].head(10)


However, it throws an error TypeError: ('argument must be a string or number', 'occurred at index col1')
</code></pre>
","11846355","","11846355","","2019-07-30 06:34:36","2019-07-30 06:34:36","Float is not converting to integer pandas","<python-3.x><pandas><floating-point><integer>","1","2","","","","CC BY-SA 4.0","1"
"57657233","1","","","2019-08-26 11:39:13","","1","168","<p>I have a pandas dataframe with column values in string format and a datetime index. I want to create a new column which will have a list of values of a column for last two days. Is it possible to achieve this using pandas?</p>

<p>original datafarme:</p>

<pre><code>        date col1 col2
0 2018-07-08    a    b
1 2018-07-09    c    d
2 2018-07-10    e    f
3 2018-07-11    g    h
4 2018-07-12    i    j
5 2018-07-13    k    l
6 2018-07-14    m    n
</code></pre>

<p>Final dataframe:</p>

<pre><code>        date col1 col2  col3
0 2018-07-08    a    b   NaN
1 2018-07-09    c    d   NaN
2 2018-07-10    e    f  b, d
3 2018-07-11    g    h  d, f
4 2018-07-12    i    j  f, h
5 2018-07-13    k    l  h, j
6 2018-07-14    m    n  j, l
</code></pre>
","11372663","","9081267","","2019-08-26 12:36:50","2019-08-26 14:13:13","Using Pandas rolling function on text columns","<python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"57047013","1","","","2019-07-15 20:49:32","","0","167","<p>My dataset <code>df</code> looks like this. It is a <code>minute</code> based dataset.</p>

<pre><code>time, Open, High
2017-01-01 00:00:00, 1.2432, 1.1234
2017-01-01 00:01:00, 1.2432, 1.1234
2017-01-01 00:02:00, 1.2332, 1.1234
2017-01-01 00:03:00, 1.2132, 1.1234
...., ...., ....
2017-12-31 23:59:00, 1.2132, 1.1234
</code></pre>

<p>I want to find the hourly <code>rolling mean</code> for <code>Open</code> column but should be flexible so that I can also find hourly <code>rolling mean</code> for other columns.</p>

<p>What did I do?</p>

<p>I am able to find the <code>daily rolling average</code> like given below, but how do I find for the hour basis so that I do not find <code>mean</code> for the entire day</p>

<pre><code># Pandas code to find the rolling mean for a single day

df
.assign(1davg=df.rolling(window=1*24*60)['Open'].mean()) 
.groupby(df['time'].dt.date) 
.last() 
</code></pre>

<p>Please note that changing this line of code does not work because I already tried it: <code>window=1*24*60</code> to <code>window=60</code></p>
","9161607","","9161607","","2019-07-15 23:47:23","2019-07-16 17:25:17","Pandas calculate hourly rolling mean","<python-3.x><pandas><dataframe>","1","3","1","","","CC BY-SA 4.0","1"
"56677381","1","56677461","","2019-06-20 00:04:57","","0","167","<p>I am trying to extract multiple domain names from the following data frame:</p>

<pre><code>    email
0   test1@gmail1.com; test1@gmail2.com
1   test3@gmail3.com; test4@gmail4.com
2   test5@gmail5.com
</code></pre>

<p>I can split and extract the first email address using the following code:</p>

<pre><code>orig = []
mylist = []
for i in df['email']:
    orig.append(i)
    i = i[ i.find(""@"") : ]
    i = i.split("";"")
    i = ';'.join(i)
    mylist.append(i)
</code></pre>

<p>After appending the lists to a data frame I get the following result:</p>

<pre><code>    origemail                           newemail
0   test1@gmail1.com; test1@gmail2.com  @gmail1.com; test1@gmail2.com
1   test3@gmail3.com; test4@gmail4.com  @gmail3.com; test4@gmail4.com
2   test5@gmail5.com  @gmail5.com
</code></pre>

<p>The result I am after:
(these email addresses may not be limited to two, it could be more.)</p>

<pre><code>    origemail                           newemail
0   test1@gmail1.com; test1@gmail2.com  @gmail1.com; @gmail2.com
1   test3@gmail3.com; test4@gmail4.com  @gmail3.com; @gmail4.com
2   test5@gmail5.com                    @gmail5.com
</code></pre>

<p>Can someone please point me in the right direction to achieve the desired output? Thanks in advance.</p>
","10644025","","","","","2019-06-20 00:18:47","Extract domain names from multiple email addresses in Data Frame","<python-3.x><pandas><split>","2","0","","","","CC BY-SA 4.0","1"
"57803945","1","","","2019-09-05 10:56:38","","0","165","<p>What is <code>M8[D]</code> type in the following? Is there some references for full type we can set?</p>

<pre><code>import numpy as np
import pandas as pd
d = pd.DataFrame({'col1': ['2015-07-01']})
print(d.astype('M8[D]'))
print(np.datetime64('1979-03-22').dtype)
</code></pre>
","11962775","","","","","2019-09-05 20:21:23","M8[D] for datetime","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"40580842","1","","","2016-11-14 01:50:47","","0","165","<p>I am looking for a way to efficiently perform a particular operation on each column of a <code>DataFrame</code>. In each column, if an entry is followed by some sequence consisting of the same number which is lower than the entry, followed by the entry again, and this sequence is not longer than <em>k</em>, then fill in the sequence by the ""bracketing"" entry.</p>

<p>For example, suppose <em>k=2</em>. Then, the following <code>DataFrame</code></p>

<pre><code>   0  1  2  3  4
0  6  6  6  6  6
1  7  7  5  6  6
2  7  7  5  5  5
3  6  7  5  5  5
4  5  6  6  6  9
5  9  9  9  9  6
</code></pre>

<p>should be transformed into:</p>

<pre><code>   0  1  2  3  4
0  6  6  6  6  6
1  7  7  5  6  6
2  7  7  5  6  5
3  6  7  5  6  5
4  5  6  6  6  9
5  9  9  9  9  6
</code></pre>

<p>Notice that only the sequence of 5's in column 3 is filled. In columns 0 and 1 the sequence of 7's is left as is because <em>7>6</em>. In column 2, <em>5&lt;6</em> but the sequence is of length 3, which is above <em>k=2</em>. Column 4 is left unchanged because the sequence of 5's is followed by an entry (9) that is different from the entry preceeding it (6).</p>

<p><strong>Summary of conditions for filling:</strong>:</p>

<ol>
<li>The sequence consists of the same number, and this number is lower than the entry before the sequence starts.</li>
<li>The sequence is of length less than or equal to k.</li>
<li>The entry right after the sequence is the same as the entry preceeding the sequence.</li>
</ol>

<p>I can do it in a loop but it takes awfuly long as my <code>DataFrame</code> is large. Can someone suggest an efficent (speed) way of acheiving this?</p>
","6204900","","6204900","","2016-11-14 02:34:30","2016-11-14 02:34:30","Fast conditional forward fill in a pandas DataFrame","<python><python-3.x><pandas><dataframe>","0","2","","","","CC BY-SA 3.0","1"
"57418397","1","57418545","","2019-08-08 18:03:15","","2","164","<p>I have the following dataframe</p>

<pre><code>data = {'Name':['Tom', 'nick', 'krish', 'jack'], 'Age':[20, 21, 19, 18], 'Height':[23, 43, 123, 12], 'Hair_Width':[21, 11, 23, 14]} 
df = pd.DataFrame(data) 
df

    Name    Age Height  Hair_Width
0   Tom     20  23      21
1   nick    21  43      11
2   krish   19  123     23
3   jack    18  12      14
</code></pre>

<p>I performed a melt operation on this dataframe as follows:</p>

<pre><code>pd.melt(df, id_vars=['Name'], value_vars=['Age', 'Height'])
df
    Name    variable    value  
0   Tom     Age         20     
1   nick    Age         21     
2   krish   Age         19     
3   jack    Age         18     
4   Tom     Height      23     
5   nick    Height      43     
6   krish   Height      123    
7   jack    Height      12     
</code></pre>

<p>However, I would like to combine the new melted dataframe with a variable from the original (wide) dataframe, to get the following desired output:</p>

<pre><code>    Name    variable    value  Hair_Width
0   Tom     Age         20     21
1   nick    Age         21     11
2   krish   Age         19     23
3   jack    Age         18     14
4   Tom     Height      23     21
5   nick    Height      43     11
6   krish   Height      123    23
7   jack    Height      12     14
</code></pre>

<p>I would love to hear any suggestions on how this can be accomplished.</p>

<p>Edit: A lot of people correctly pointed out that the original dataset is in tidy format. That is correct- it is just used as a simple example. The actual data frame is not tidy to start.</p>
","10029062","","10029062","","2019-08-08 18:42:43","2019-08-08 18:42:43","How to combine a wide and a long dataframe in pandas?","<python><python-3.x><pandas><dataframe><melt>","4","1","","","","CC BY-SA 4.0","1"
"49417311","1","49418111","","2018-03-21 22:06:50","","2","163","<pre><code>helpful
'[2, 4]'
'[0, 0]'
'[0, 1]'
'[7, 13]'
'[4, 6]'
</code></pre>

<p>Column name helpful has a list inside the string. I want to split 2 and 4 into separate columns.</p>

<pre><code>[int(each) for each in df['helpful'][0].strip('[]').split(',')]
</code></pre>

<p>This works the first row but if I do </p>

<pre><code>[int(each) for each in df['helpful'].strip('[]').split(',')]
</code></pre>

<p>gives me attribute error</p>

<pre><code>AttributeError: 'Series' object has no attribute 'strip'
</code></pre>

<p>How can I print out like this in my dataframe??</p>

<pre><code>helpful not_helpful
2       4
0       0
0       1
7       13
4       6
</code></pre>
","8828442","","9209546","","2018-03-22 01:03:11","2018-03-22 14:01:36","Convert literal string to list inside python","<python><python-3.x><pandas><dataframe>","3","3","","","","CC BY-SA 3.0","1"
"57300003","1","","","2019-07-31 23:24:47","","1","163","<p>I have constructed a pandas Dataframe with 2 columns which are 'data' and 'label'. 'data' column in a dense matrix and 'label' is integer type.</p>

<pre class=""lang-py prettyprint-override""><code>data     object
label     int64
</code></pre>

<p>my sample data from dataframe</p>

<pre class=""lang-py prettyprint-override""><code>
                                                data   label
0  [[[[[0 0 5 4 5]]]], [[[[0 0 5 1 4]]]], [[[[5 5...      0
1  [[[[[0 0 4 4 1]]]], [[[[0 0 3 3 1]]]], [[[[4 3...      0
2  [[[[[0 0 4 1 2]]]], [[[[0 0 2 3 4]]]], [[[[4 2...      0
3  [[[[[0 0 1 2 5]]]], [[[[0 0 3 2 3]]]], [[[[1 3...      0
4  [[[[[0 0 2 5 5]]]], [[[[0 0 3 4 1]]]], [[[[2 3...      0

</code></pre>

<p>When I try to convert the Dataframe as Tensorflow dataset it gives me an error. I extracted the labels and data as follows</p>

<pre class=""lang-py prettyprint-override""><code>target = df.pop('label')
dataset = tf.data.Dataset.from_tensor_slices((df.values, target.values))
</code></pre>

<p>This gives me the following error.</p>

<pre class=""lang-py prettyprint-override""><code>TypeError: Expected binary or unicode string, got matrix([[0, 5, 1, 4, 4],
        [5, 0, 5, 2, 4],
        [1, 5, 0, 0, 0],
        [4, 2, 0, 0, 0],
        [4, 4, 0, 0, 0]], dtype=int32)
</code></pre>

<p>How can I convert my 'data' column to TensorFlow dataset?</p>
","4972453","","4972453","","2019-07-31 23:43:15","2020-06-10 16:33:44","Convert Pandas Dataframe as Tensorflow dataset with 2D array as data","<python-3.x><pandas><tensorflow>","1","5","2","","","CC BY-SA 4.0","1"
"57663316","1","57668373","","2019-08-26 18:47:25","","5","162","<p>I'm new to Python and initializing parameters for a X number of model runs. I need to create every possible combination from N dictionaries, each dictionary having nested data.</p>

<p>I know that I need to use itertools.product somehow, but I'm stuck on how to navigate the dictionaries. Maybe I shouldn't even be using dictionaries but json or something. I also know that this will create A LOT of parameters/runs.</p>

<p>EDIT: added clarification from comment.
I want to create a function that takes n dictionaries ---eg. def func(dict*) ---- as input and creates every possible combination of all of those individual key/ value pairs across all of the dictionaries, returning one big DF with all combinations.</p>

<p>My data looks like this:</p>

DICTIONARY 1

<pre class=""lang-py prettyprint-override""><code>{
    ""chisel"": [
        {""type"": ""chisel""},
        {""depth"": [152, 178, 203]},
        {""residue incorporation"": [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},
        {""timing"": [""10-nov"", ""10-apr""]},
    ],
    ""disc"": [
        {""type"": ""disc""},
        {""depth"": [127, 152, 178, 203]},
        {""residue incorporation"": [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},
        {""timing"": [""10-nov"", ""10-apr""]},
    ],
    ""no_till"": [
        {""type"": ""user_defined""},
        {""depth"": [0]},
        {""residue incorporation"": [0.0]},
        {""timing"": [""10-apr""]},
    ],
}
</code></pre>

DICTIONARY 2

<pre class=""lang-py prettyprint-override""><code>{
    ""nh4_n"":
        {
            ""kg/ha"":[110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225],
            ""fertilize_on"":""10-apr""
        },
    ""urea_n"":
        {
            ""kg/ha"":[110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225],
            ""fertilize_on"":""10-apr""
        }
}
</code></pre>

DICTIONARY 3

<pre class=""lang-py prettyprint-override""><code>{
    ""maize"": {
        ""sow_crop"": 'maize',
        ""cultivar"": ['B_105', 'B_110'],
        ""planting_dates"": [
            '20-apr', '27-apr', '4-may', '11-may', '18-may', '25-may', '1-jun', '8-jun', '15-jun'],
        ""sowing_density"": [8],
        ""sowing_depth"": [51],
        ""harvest"": ['maize'],
    }
}
</code></pre>

<p>For example, with the three dictionaries above, I would take the dict 'chisel' and itertools.product it somehow with each nested dictionary in dict 2(eg. 'nh4_n') and each nested dict in dict 3 (in this case there is only one, so with each different cultivar, planting date, etc.). I also want to use the keys in each key-value pair as a DF column heading.</p>

<p><img src=""https://i.stack.imgur.com/kACOS.png"" alt=""enter image description here""></p>
","11958749","","8928024","","2019-08-26 22:20:05","2019-08-27 06:06:56","Every product/combination of nested dictionaries saved to DataFrame","<python><python-3.x><pandas>","1","2","2","","","CC BY-SA 4.0","1"
"57705230","1","57789562","","2019-08-29 07:33:10","","0","160","<p><strong>BACKGROUND:</strong> I have a large excel file converted to .csv. Need to be able to detect the end of the file/dataframe and delete all rows and columns after that. The data has the following format (approx 100 cols and 200 rows):</p>

<pre><code>&gt;&gt;&gt;&gt;&gt;Spec. No     Text     .....     LastColumnName     UnnamedN1     UnnammedN2     UnnamedN3     UnnamedN4     .....
0    (some text)  (some text)             0
1    (some text)  (some text)                           2
2    (some text)  (some text)                                          
3
4
5
</code></pre>

<p><strong>(1)</strong> The # of columns and the column titles change with every file. However it is certain that the <em>last column with a name + following 2 columns</em> are the bounds on the column axis even if they contain no values (in this case UnnamedN4 and N5 would be deleted). </p>

<p><strong>(2)</strong> Delete all empty rows after the last one with <code>(some text)</code> in <em>Spec No + Text</em> (in this case rows 3, 4, 5 would be deleted). This will always have some text in the last necessary/relevant row.  </p>

<p><strong>ALREADY TRIED:</strong> Looked at several posts on SO but couldn't find a complete solution for the problem.</p>

<p><strong>EXPECTED:</strong> New dataframe that looks like -</p>

<pre><code>&gt;&gt;&gt;&gt;&gt;Spec. No     Text     .....     LastColumnName     UnnamedN1     UnnammedN2
0    (some text)  (some text)             0
1    (some text)  (some text)                           2
2    (some text)  (some text)                                          
</code></pre>

<p>Python novice, seeking some help and guidance.</p>
","11900800","","11900800","","2019-08-29 07:59:19","2019-09-04 13:33:23","Detect end of file (cols & rows) in dataframe and delete all extra","<python-3.x><pandas><dataframe>","2","0","","","","CC BY-SA 4.0","1"
"57026428","1","57026464","","2019-07-14 10:11:31","","1","159","<p>I am trying to match a list object with pandas DataFrame, i've a condition here where the list object which contains the column names so, sometimes the DataFrame may contains the all names which are in <code>matchObj</code> but times when the DataFrame will only have few column names only in that situation it fails to do the Job.</p>

<p>below is my example lists <code>matchObj</code> and <code>matchObj1</code> for example:</p>

<pre><code>&gt;&gt;&gt; matchObj = ['equity01',  'equity02',  'equity1'  'equity2']
&gt;&gt;&gt; matchObj1 = ['equity01',  'equity02']
</code></pre>

<p>Below is the DataFrame:</p>

<pre><code>&gt;&gt;&gt; df
   equity01  equity02  equity03  equity04  equity05
0         1         4         7         2         5
1         2         5         8         3         6
2         3         6         9         4         7
</code></pre>

<p>While i'm using the list <code>matchobj1</code> against df it works as it founds the column names.</p>

<pre><code>&gt;&gt;&gt; print(df[matchObj1])
   equity01  equity02
0         1         4
1         2         5
2         3         6
</code></pre>

<p>However, it fails to work with <code>matchobj</code> because df doest not contain the <code>equity1 equity2</code> thus throws the <code>KeyError: ""['equity1equity2'] not in index""</code></p>

<pre><code>&gt;&gt;&gt; print(df[matchObj])
Traceback (most recent call last):
  File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
  File ""/grid/common/pkgs/python/v3.6.1/lib/python3.6/site-packages/pandas/core/frame.py"", line 2133, in __getitem__
    return self._getitem_array(key)
  File ""/grid/common/pkgs/python/v3.6.1/lib/python3.6/site-packages/pandas/core/frame.py"", line 2177, in _getitem_array
    indexer = self.loc._convert_to_indexer(key, axis=1)
  File ""/grid/common/pkgs/python/v3.6.1/lib/python3.6/site-packages/pandas/core/indexing.py"", line 1269, in _convert_to_indexer
    .format(mask=objarr[mask]))
KeyError: ""['equity1equity2'] not in index""
</code></pre>
","10295972","","","","","2019-07-14 10:17:18","pandas to match columns names based on a list object based check","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"49662333","1","49682329","","2018-04-05 00:22:17","","-1","159","<p>I would like to apply a function to each column of my <code>DataFrame</code> but do so by group (MWE below). Is there a more elegant way of doing this than what I have?</p>

<pre><code>import numpy as np
import pandas as pd
df1 = pd.DataFrame(data = {""a"": [1,2,3,4,5], ""b"": [6,7,8,9,10]}, index = pd.Index([0,0,0,1,1], name=""someindex""))
df1.groupby(""someindex"").apply(lambda g: g.apply(lambda x: np.average(x)))
</code></pre>

<p>This is just a toy example but the level of generality I had was one where the function can return the same shape as the group but uses all the elements in the computation (so standardizing the columns, for example).</p>
","714319","","714319","","2018-04-05 00:42:46","2018-04-05 22:12:22","apply a function to each column of a DataFrame by group in pandas","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"57101579","1","","","2019-07-18 19:38:23","","-1","159","<p>this is my first post and my programming knowledge is rather limited. I would appreciate if answers are given using python since that is the only language I am familiar with. I have also tried using pandas and xlrd, so continuing with those would be nice.</p>

<p>The problem is this: I have a list of companies that is about 30,000 entries long. The list is in the form of a Google Sheet, but this can easily be exported to Excel. Each row of the sheet represents a company, and each column represents a piece of info pertaining to that company. </p>

<p>My boss wants a team of us to go through this sheet and sort each company into categories based upon their industry, this would obviously take a very long time. It involves searching up each company name, finding its industry, then copying the row on the unsorted sheet and pasting it into a row on the categorized sheet.</p>

<p>My thought was that using if statements, we could sort through a good chunk of the work pretty fast. </p>

<p>For example, let's say a category of companies was called ""Construction"". If the company name contains the words ""construction"", ""steel"" or something of the sort, we could automatically move that company (and all its info) to this new category called ""Construction"". The new destination would preferably be a sheet on a new excel file. Any items that don't have any of the industry keywords in them go into the unsorted sheet.</p>

<p>What I struggle with is finding a way to sort through the data, keep all of the info pertaining to each company together, and moving/copying a row from one place to another. What functions might I use? How might I tackle this?</p>

<p>The goal is to have MOST, not all, of the 30,000 entries sorted automatically based on their names, so I may only need to sort through a few thousand unsorted items that didn't match any keywords manually. </p>

<p>Any help with this issue is greatly appreciated, and I'm more than happy to provide any additional details anyone may need to help this working.</p>
","11804907","","","","","2019-07-18 19:58:26","Sorting an Excel Spreadsheet with Python","<python><excel><python-3.x><pandas><xlrd>","1","3","","","","CC BY-SA 4.0","1"
"49244476","1","49245120","","2018-03-12 21:12:27","","1","157","<p>I have a data structure that looks like this:</p>

<pre><code>[[     date     person  
  0    1/1/2018 john   
  1    1/3/2018 jane   
  2    1/9/2018 john ]
 [     date     person  
  0    2/1/2018 john   
  1    2/3/2018 jane   
  2    2/9/2018 john ]]
</code></pre>

<p>It is a list of lists, but there are no commas between each element in the list. </p>

<p>Therefore, when attempting to convert the list to a <code>pandas</code> <code>DataFrame</code>, assertion errors occur because the nested list is technically one column, whereas the target <code>df</code> is two columns.</p>

<p>How would I add commas to the nested list or otherwise convert it into a <code>df</code> that looks like the one below?</p>

<pre><code>date      person
1/1/2018  john
1/3/2018  jane
1/9/2018  john
2/1/2018  john
2/3/2018  jane
2/9/2018  john
</code></pre>
","7668467","","868546","","2018-03-12 21:22:31","2018-03-12 22:02:55","Add Commas Between Elements","<python><python-3.x><list><pandas>","1","9","","","","CC BY-SA 3.0","1"
"56929119","1","56929419","","2019-07-08 05:59:24","","3","157","<p>I am working on a multilabel classification problem. Every value in X is a categorical value. Original data is below</p>

<pre><code>ID  X1  X2  X3  Y
111 AA  LL  KK  MMM
111 AA  LL  KK  MMM
111 BB  LL  jj  NNN
121 HH  DD  uu  III
121 HH  DD  yy  OOO
121 HH  LL  aa  PPP
</code></pre>

<p>I am trying to convert this to a dataframe where every unique value present in columns (X1, X2, X3, Y) will become a new column and every ID will have a single record. The expected output I am trying to get is </p>

<pre><code>ID  X1_AA   X1_BB   X1_HH   X2_LL   X2_DD   X3_KK   X3_jj   X3_uu   X3_yy   x3_aa   Y_MMM   Y_NNN   Y_III   Y_OOO   Y_PPP
111 1   1   0   1   0   1   1   0   0   0   1   1   0   0   0
121 0   0   1   1   1   0   0   1   1   1   0   0   1   1   1
</code></pre>

<p>I tried using pandas get_dummies, it is creating dummy column, but id's are duplicated. Here Y is my target column. Multiple values of Y for an ID means ID has accessed multiple channels.</p>

<p><strong>Also please suggest if I can directly use original data by creating dummy columns for X and Y in classification</strong></p>
","4464558","","4464558","","2019-07-08 06:29:19","2019-07-08 07:26:31","Input data creation for Multilabel classification","<python-3.x><pandas><multilabel-classification>","2","5","0","","","CC BY-SA 4.0","1"
"57799958","1","57801599","","2019-09-05 06:55:25","","2","156","<p>I have a DataFrame that has a ID column and Value column that only consist (0,1,2). I want to capture only those rows, if there is a transition from (0-1) or (1-2) in value column. This process has to be done for each ID separately.</p>

<p>I tried to do the groupby for ID and using a difference aggregation function. So that i can take those rows for which difference of values is 1. But it is failing in certain condition.</p>

<pre><code>df=df.loc[df['values'].isin([0,1,2])]
df = df.sort_values(by=['Id'])
df.value.diff()
</code></pre>

<p>Given DataFrame:</p>

<p>Index &nbsp; UniqID &nbsp; Value</p>

<p>1 &emsp;&emsp;&emsp;a &emsp;&emsp;&emsp;1</p>

<p>2 &emsp;&emsp;&emsp;a &emsp;&emsp;&emsp;0</p>

<p>3 &emsp;&emsp;&emsp;a &emsp;&emsp;&emsp;1</p>

<p>4 &emsp;&emsp;&emsp;a &emsp;&emsp;&emsp;0</p>

<p>5 &emsp;&emsp;&emsp;a &emsp;&emsp;&emsp;1</p>

<p>6 &emsp;&emsp;&emsp;a &emsp;&emsp;&emsp;2</p>

<p>7 &emsp;&emsp;&emsp;b &emsp;&emsp;&emsp;0</p>

<p>8 &emsp;&emsp;&emsp;b &emsp;&emsp;&emsp;2</p>

<p>9 &emsp;&emsp;&emsp;b &emsp;&emsp;&emsp;1</p>

<p>10 &emsp;&emsp;&emsp;b &emsp;&emsp;&emsp;2</p>

<p>11 &emsp;&emsp;&emsp;b &emsp;&emsp;&emsp;0</p>

<p>12 &emsp;&emsp;&emsp;b &emsp;&emsp;&emsp;1</p>

<p>13 &emsp;&emsp;&emsp;c &emsp;&emsp;&emsp;0</p>

<p>14 &emsp;&emsp;&emsp;c &emsp;&emsp;&emsp;1</p>

<p>15 &emsp;&emsp;&emsp;c &emsp;&emsp;&emsp;2</p>

<p>16 &emsp;&emsp;&emsp;c &emsp;&emsp;&emsp;2</p>

<p>Expected Output:</p>

<p>2 &emsp;&emsp;&emsp;a &emsp;&emsp;&emsp;0</p>

<p>3 &emsp;&emsp;&emsp;a &emsp;&emsp;&emsp;1</p>

<p>4 &emsp;&emsp;&emsp;a &emsp;&emsp;&emsp;0</p>

<p>5 &emsp;&emsp;&emsp;a &emsp;&emsp;&emsp;1</p>

<p>6 &emsp;&emsp;&emsp;a &emsp;&emsp;&emsp;2</p>

<p>9 &emsp;&emsp;&emsp;b &emsp;&emsp;&emsp;1</p>

<p>10 &emsp;&emsp;&emsp;b &emsp;&emsp;&emsp;2</p>

<p>11 &emsp;&emsp;&emsp;b &emsp;&emsp;&emsp;0</p>

<p>12 &emsp;&emsp;&emsp;b &emsp;&emsp;&emsp;1</p>

<p>13 &emsp;&emsp;&emsp;c &emsp;&emsp;&emsp;0</p>

<p>14 &emsp;&emsp;&emsp;c &emsp;&emsp;&emsp;1</p>

<p>15 &emsp;&emsp;&emsp;c &emsp;&emsp;&emsp;2</p>

<p>Only expecting those rows when there is a transition from either 0-1 or 1-2.</p>

<p>Thank you in advance.</p>
","10468411","","10468411","","2019-09-06 09:29:04","2019-09-11 08:24:34","How to select rows in a DataFrame based on every transition for particular values in a particular column?","<python-3.x><pandas><dataframe><data-science><data-manipulation>","3","11","","","","CC BY-SA 4.0","1"
"50068661","1","50069100","","2018-04-27 18:53:58","","0","153","<p>I have a column named 'comment1abc'</p>

<p>I am writing a piece of code where I want to see that if a column contains certain string 'abc'</p>

<pre><code>df['col1'].str.contains('abc') == True
</code></pre>

<p>Now, instead of hard coding 'abc', I want to use a substring like operation on column 'comment1abc' (to be precise, <strong>column name</strong>, not the column values)so that I can get the 'abc' part out of it. For example below code does a similar job</p>

<pre><code>x = 'comment1abc'
x[8:11]
</code></pre>

<p>But how do I implement that for a column name ? I tried below code but its not working.</p>

<pre><code>for col in ['comment1abc']:
    df['col123'].str.contains('col.names[8:11]')
</code></pre>

<p>Any suggestion will be helpful.</p>

<p>Sample dataframe:</p>

<pre><code>f = {'name': ['john', 'tom', None, 'rock', 'dick'], 'DoB': [None, '01/02/2012', '11/22/2014', '11/22/2014', '09/25/2016'], 'location': ['NY', 'NJ', 'PA', 'NY', None], 'code': ['abc1xtr', '778abc4', 'a2bcx98', None, 'ab786c3'], 'comment1abc': ['99', '99', '99', '99', '99'], 'comment2abc': ['99', '99', '99', '99', '99']}
df1 = pd.DataFrame(data = f)
</code></pre>

<p>and sample code:</p>

<pre><code>for col in ['comment1abc', 'comment2abc']:
    df1[col][df1['code'].str.contains('col.names[8:11]') == True] = '1'
</code></pre>
","9516819","","","","","2018-04-27 19:27:58","How to substring the column name in python","<python-3.x><pandas><dataframe>","1","1","","","","CC BY-SA 3.0","1"
"56626420","1","56626649","","2019-06-17 07:16:54","","0","153","<p>I am doing kind of research and need to delete the raws containing some values which are not in a specific range using Python.</p>

<p><strong>My Dataset in Excel:</strong></p>

<p><a href=""https://i.stack.imgur.com/tvE7U.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tvE7U.png"" alt=""enter image description here""></a></p>

<ol>
<li>I want to replace the big values of column A (not within range 1-20) with NaN. Replace Big values of column B (not within range 21-40) and so on.</li>
<li>Now I want to drop/ delete the raws contains the NaN values</li>
</ol>

<p><strong>Expected output should be like:</strong></p>

<p><a href=""https://i.stack.imgur.com/uJDLa.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/uJDLa.png"" alt=""enter image description here""></a></p>
","7258535","","7652544","","2019-06-18 08:44:14","2019-06-18 08:44:14","Data Cleaning Python: Replacing the values of a column not within a range with NaN and then dropping the raws which contain NaN","<excel><python-3.x><pandas><numpy><data-cleaning>","2","0","1","","","CC BY-SA 4.0","1"
"48563186","1","48565063","","2018-02-01 12:50:01","","0","151","<p>I have a pandas dataframe which look like the following: </p>

<pre><code>  a          b       c     
 NaN         2       165     
 NaN         9       NaN     
 NaN         NaN     NaN    
  15        15       NaN      
   5         NaN     11 
</code></pre>

<p>I would like to add a column that gives me something like a summary of Null values. So I need a command which gives me for every row which columns are NULL. Something like this:</p>

<pre><code>  a          b       c     Summary
 NaN         2       165      a
 NaN         9       NaN     a + c
 NaN         NaN     NaN    a + b + c
  15        15       NaN      c      
   5         NaN     11       b
</code></pre>

<p>I could not find anything which satisfies my need on the internet.</p>
","9294498","","9209546","","2018-03-08 01:33:04","2018-03-08 01:33:04","Python Dataframe get the NaN columns for each row","<python><python-3.x><pandas><numpy><dataframe>","1","3","","","","CC BY-SA 3.0","1"
"57374409","1","57374786","","2019-08-06 10:53:17","","1","151","<p>I'm using decision tree and getting a 78% score but how can i print the predicted values  </p>

<p>I've tried </p>

<pre><code>for X,Y in zip(X_test, y_test):
    print(""Model:"", dt.predict([X][0]), ""actual:"", y)
</code></pre>

<p>but it is showing an error which says</p>

<blockquote>
  <p>Reshape your data either using array.reshape(-1, 1) if your data has a
  single feature or array.reshape(1, -1) if it contains a single sample.</p>
</blockquote>

<pre><code>import pandas as pd
import sklearn
from sklearn.tree import DecisionTreeClassifier


df = pd.read_csv(""final interview.csv"")
df = sklearn.utils.shuffle(df)
df = df.drop([""position"", ""department""], axis=1)

X = df.drop(""decision"", axis=1).values
y = df[""decision""].values

test_size = 20
X_train = X[:-test_size]
y_train = y[:-test_size]

X_test = X[-test_size:]
y_test = y[-test_size:]

dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
print(dt.score(X_test, y_test))


for X,Y in zip(X_test, y_test):
    print(""Model:"", dt.predict([X][0]), ""actual:"", y)
</code></pre>

<p>I expect the <code>predicted values  :  actual values</code></p>
","11511865","","6347629","","2019-08-06 14:18:02","2019-08-06 14:18:02","How to print the predicted values using zip","<python-3.x><pandas><scikit-learn>","3","0","","","","CC BY-SA 4.0","1"
"56804623","1","56804869","","2019-06-28 09:47:21","","1","151","<p>I have a dataframe which can be generated from the code below</p>

<pre><code>df = pd.DataFrame({'person_id' :[1,2,3],'date1':['12/31/2007','11/25/2009',np.nan],
           'hero_id':[2,4,np.nan],'date2':['12/31/2017',np.nan,'10/06/2015'],
           'heroine_id':[1,np.nan,5],'date3':['12/31/2027','11/25/2029',np.nan],
           'bud_source_value':[1250000,250000,np.nan],
           'prod__source_value':[10000,20000,np.nan]})
</code></pre>

<p>The dataframe looks like as shown below with Nan's </p>

<p><a href=""https://i.stack.imgur.com/ILgao.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ILgao.png"" alt=""enter image description here""></a></p>

<p>What I would like to do is </p>

<p>1) Fill na's with 0 (zeroes) for columns that ends with ""id""
2) Fill na's with ""unknown"" for columns that ends with ""value""
3) Fill na's with ""12/31/9999"" for columns that starts with ""date""</p>

<p>I tried the below approach but it's lengthy and feel it isn't elegant</p>

<pre><code>df2 = df.filter(regex='id$')
df2.fillna(0)

df2 = df.filter(regex='^date')
df2.fillna('12/31/9999')

df2 = df.filter(regex='value$')
df2.fillna('unknown')
</code></pre>

<p>Is there anyway to achieve this in one go? As you can see I am kind of repeating the same steps</p>
","10829044","","","","","2019-06-28 10:03:33","Elegant way to Identify columns based on regex and fill different default values","<python><regex><python-3.x><pandas><dataframe>","2","1","","","","CC BY-SA 4.0","1"
"48631131","1","","","2018-02-05 20:42:37","","0","150","<p>I have a dataframe that several columns of lemmatized text (multiple paragraphs worth of text per row - not categorical), plus some other int, datetime, and float columns.
I'd like to use the text for Affinity Propagation, to find clusters within the data. sklearn.cluster.affinitypropagation doesn't work with text data though and returns: </p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-110-41dbca660b33&gt; in &lt;module&gt;()
----&gt; 1 f = clusterer.fit(TIP_with_rats_nlp)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\cluster\affinity_propagation_.py in fit(self, X, y)
    289             similarities / affinities.
    290         """"""
--&gt; 291         X = check_array(X, accept_sparse='csr')
    292         if self.affinity == ""precomputed"":
    293             self.affinity_matrix_ = X

~\AppData\Local\Continuum\anaconda3\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)
    415         # make sure we actually converted to numeric:
    416         if dtype_numeric and array.dtype.kind == ""O"":
--&gt; 417             array = array.astype(np.float64)
    418         if not allow_nd and array.ndim &gt;= 3:
    419             raise ValueError(""Found array with dim %d. %s expected &lt;= 2.""

ValueError: could not convert string to float: ""main area improvement plan specifically have ahead time want work break unit daily learning target goal week ahead detailed weekly overview activity lesson align learning target make sure lesson opportunity student intellectually engage historical material.1 deadline planning 's week learn target write document 7 pm fill remain weekly overview template 2 plan google document share co teacher order feedback 3 weekly meeting mentor work create week learn target 4 observe structure classroom co teaching style observe rigor bring classroom step 1 -3 start 9/26/14 continue year step 4 start 9/29/14 work observe teacher week work close work imporvement especially help break planning create idea daily lesson look google document planning improve class observation""
</code></pre>

<p>What's the best way to convert this text data to something cluster.affinitypropagation can work with? 
If Affinity Propogation even the best options for this? I've looked into kNN, but I specifically want something where I don't have to pre-select the number of groups and this seemed like the best option.</p>

<p>I've looked into sklearn.preprocessing.labelencoder but that seems to only work with single strings/categorical data. I've found similar questions with solutions like:</p>

<pre><code>def encode(col):
    unq = {}
    count=0
    for item in col:
        if item not in unq:
            unq[item] = count
            count += 1
    enc = [unq[item] for item in col]
    return enc,unq
</code></pre>

<p>But that doesn't take into account each individual word in the list and merely assigns an ever increasing numeric identifier to each row.</p>

<p>Sorry if this is a dumb question.</p>
","5703997","","","","","2018-02-05 21:42:11","Encoding lemmas for use in Affinity Propagation/Finding natural clusters in text data","<python><python-3.x><pandas><scikit-learn>","1","2","","","","CC BY-SA 3.0","1"
"57370083","1","57370131","","2019-08-06 06:28:12","","2","147","<p>I have a data like this,</p>

<pre><code>         ID     datetime           
0         2  2015-01-09 19:05:39   
1         1  2015-01-10 20:33:38   
2         1  2015-01-10 21:10:00 
</code></pre>

<p>I've converted this datetime into unix time stamp</p>

<pre><code>         ID   timestamp           
0         2  1420830339
1         1  1420922018   
2         1  1420924200 
</code></pre>

<p>I want to first convert unix time stamp to EST standard and then  bin each row into a 10 minutes interval. I need a column to indicate which bin this row belongs to.</p>

<p>My min datetime is 2015-01-01 00:00:00 and I only have data for jan 2015 from 1 to 31.</p>

<p>How can I achieve this using python or pandas.</p>
","11816060","","11816060","","2019-08-06 06:33:58","2019-08-06 06:45:27","how to bin unix timestamp time into 10 minutes interval?","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57230047","1","57233525","","2019-07-27 07:21:55","","0","146","<p>I have a dataset which has data for over a month. I need to count the players who have played on the previous day. So if I am looking at date of 5th June users I need to find the number of users who were there in dates preceding 5th June even once.
Dataset is something like this:-</p>

<pre><code>Day pid1    pid2
1   1a  1b
1   1c  2e
1   1d  2w
1   1e  2q
2   1f  4r
2   1g  5t
2   2e  7u
2   2w  8i
2   2q  9o
3   4r  0yu
3   5t  5t
3   6t  1w
4   1a  2e
4   1f  9o
4   7u  6h
5   8i  4f
5   9o  3d
5   0yu 5g
5   5t  6h
</code></pre>

<p>I have tried iterating over days then pid1 and pid2 but to no avail and it is computationally expensive as I have over 5 million data points.</p>

<p>I really do not know how to approach this and the only thing I have tried is this:-</p>

<pre><code>for x in range(1, 31):
    for i in ids.iterrows():
        if i['Ids'] == zip(df4['pid1'], df['pid2']):
            print(x, i.count())
</code></pre>

<p>But it still doesn't let me iterate over only previous days and not next days.</p>

<p>I need answer that looks something like this (results are not accurate) but I need unique count of users of previous days on a given day:-</p>

<pre><code>Day Previous day users
1   0
2   2
3   2
4   5
5   5
</code></pre>
","10800593","","10800593","","2019-07-28 07:30:01","2019-07-29 08:14:10","Pandas : How to iterate over only back dates over two ID columns to give unique IDs and count?","<python-3.x><pandas><loops><time-series><iteration>","2","1","1","","","CC BY-SA 4.0","1"
"48761138","1","","","2018-02-13 07:00:31","","0","146","<p>I am using the following code:</p>

<pre><code>import csv
import pandas as pd

f = pd.read_csv(r""C:\Users\Tanmay\Desktop\moneycontrol.csv"",encoding=""ISO-8859-1"")
companies = pd.read_csv(r""C:\Users\Tanmay\Desktop\file2.csv"",encoding=""ISO-8859-1"")
count = 0
ld = open(r""C:\Users\Tanmay\Desktop\companyName.csv"",'a',encoding=""ISO-8859-1"")

for companyName,symbol in zip(companies.NAMEOFCOMPANY,companies.SYMBOL):
    count+=1
    if(count%2==0):
        companyNamestr = str(companyName)
        symbolstr = str(symbol)
        print(companyNamestr  + ""  "" + symbolstr)
        listOfNames = []
        listOfNames.append(companyNamestr)
        listOfNames.append(companyNamestr.lower())
        listOfNames.append(companyNamestr.upper())

        for searchRow in listOfNames:
            print (searchRow + ""  "")
            v = f[f.Subtitle.str.contains(searchRow)]
            try:
                v.companyName = companyNamestr
                v.to_csv(ld,header = False)
            except:
                pass   
ld.close()
</code></pre>

<p>I want to create a file which has all the articles sorted according to the company names, that is if the company name is contained in the subtitle of the news, it should be displayed in the new CSV file. 
But the problem here is if a subtitle contains more than 1 company name, it is not appearing multiple times with every company name. Once it finds the company name, it maps it with that and does not check again. What I am trying to say is that one entry for the news article is appearing only one time even if it has more than 1 company names contained in it.</p>

<p>The moneycontrol.csv file contains all the news articles. This file contains the arrtibute 'Subtitle'.
The file2.csv file contains the list of all companies with the attributes 'NAMEOFCOMPANY' and 'SYMBOL'.
The output file is companyName.csv where one column name is companyName where the name of the company appears multiple times and each entry is associated with one news article.</p>
","9327089","","","","","2018-02-13 07:00:31","Iterate over a CSV file multiple times using Pandas","<python><python-3.x><pandas><csv><export-to-csv>","0","4","1","","","CC BY-SA 3.0","1"
"57655396","1","57655441","","2019-08-26 09:41:25","","2","144","<p>I have a dataframe like shown below</p>

<pre><code>df = pd.DataFrame({'time':['2166-01-09 14:00:00','2166-01-09 14:08:00','2166-01-09 16:00:00','2166-01-09 20:00:00',
                       '2166-01-09 04:00:00','2166-01-10 05:00:00','2166-01-10 06:00:00','2166-01-10 07:00:00','2166-01-10 11:00:00',
                       '2166-01-10 11:30:00','2166-01-10 12:00:00','2166-01-10 13:00:00','2166-01-10 13:30:00']})
</code></pre>

<p>I am trying to find a time difference between rows. For which I did the below</p>

<pre><code>df['time2'] = df['time'].shift(-1)
df['tdiff'] = (df['time2'] - df['time'])
</code></pre>

<p>So, my result looks like as shown below</p>

<p><a href=""https://i.stack.imgur.com/gU9kR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gU9kR.png"" alt=""enter image description here""></a></p>

<p>I found out that there exists a function like <code>dt.days</code> and I tried</p>

<pre><code>df['tdiff'].dt.days
</code></pre>

<p>but it only gives the day component but am looking for something like 'hours` component</p>

<p>However, I would like to have my output like as shown below</p>

<p><a href=""https://i.stack.imgur.com/gZ8al.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gZ8al.png"" alt=""enter image description here""></a></p>

<p>I am sorry that I am not sure how to calculate the hour equivalent of negative time in row no 3. Might be that's an data issue. </p>
","10829044","","10829044","","2019-08-26 09:44:26","2019-08-26 09:46:45","How to fetch hours from m8[ns] object in pandas?","<python><python-3.x><pandas><dataframe><datetime>","1","0","","","","CC BY-SA 4.0","1"
"57261333","1","","","2019-07-29 21:00:30","","0","144","<p>I was trying to localize the timezone for the dataset using this code:</p>

<pre class=""lang-py prettyprint-override""><code>df['Date and Time'] = df['Date and Time'].astype('datetime64')
df['Date and Time'] = df.apply(lambda r : r['Date and Time (RW)'].tz_localize(r['Zone']), 1)
</code></pre>

<p>However, I keep on getting error message which said:</p>

<blockquote>
  <p>""'float' object has no attribute 'utcoffset'"", 'occurred at index 30852""</p>
</blockquote>

<p>When I tried to see which data is causing the error:</p>

<pre class=""lang-py prettyprint-override""><code>df[(df.index==30852)] # actually it shows that there is no data. 
</code></pre>

<p>I tried to drop the data at that index by running this code:</p>

<pre class=""lang-py prettyprint-override""><code>df.drop(df.index[30852], inplace=True)
</code></pre>

<p>but it didn't work because there's no data. </p>

<p>What could be causing this error?</p>
","11374039","","10140310","","2019-07-29 22:50:52","2019-07-29 22:50:52","'float' object has no attribute 'utcoffset'"", 'occurred at index 30852'","<python-3.x><pandas><timezone-offset>","0","3","","","","CC BY-SA 4.0","1"
"48360369","1","48367402","","2018-01-20 19:43:28","","0","143","<p>I have a registry text dump like below</p>

<pre><code>[HKLM\CurrentControlSet\somerandomthing]

[HKLM\CurrentControlSet\somerandomthing\somerandomsub]
""key1""=hex(7)""1234aa\
     123451234567788\
     124123412341234
     1243""
""key2"":C:\randomlocaltioninmydrive
""key3""-somerandomstuffwithanyoutput
</code></pre>

<p>this goes on for days, multiple duplicates, multiple different key value pair types. </p>

<p>How can i put this data into a pandas dataframe similar to the below output</p>

<pre><code>Path                                                  Type    Key      Value           
HKLM\CurrentControlSet\somerandomthing\somerandomsub  hex(7)  key1    1234aa1234512345677881241234123412341243
HKLM\CurrentControlSet\somerandomthing\somerandomsub  N/A     key2    C:\randomlocaltioninmydrive
HKLM\CurrentControlSet\somerandomthing\somerandomsub  N/A     key3    somerandomstuffwithanyoutput  
</code></pre>

<p>I have attempted to use configparser.rawconfigparser to no avail. This dataset is a raw hklm.txt file from the registry dump on a windows box. </p>
","6916973","","","","","2019-11-05 12:20:17","parsing registry text dumps into pandas","<python><python-3.x><pandas><registry><configparser>","1","1","","","","CC BY-SA 3.0","1"
"57740776","1","57772193","","2019-08-31 18:43:16","","1","142","<p>I have a column ""data"" which has json object as values. I would like to split them up.</p>

<pre><code>source = {'_id':['SE-DATA-BB3A','SE-DATA-BB3E','SE-DATA-BB3F'],  'pi':['BB3A_CAP_BMLS','BB3E_CAP_BMLS','BB3F_CAP_PAS'], 'Datetime':['190725-122500', '190725-122500', '190725-122500'], 'data': [ {'bb3a_bmls':[{'name': 'WAG 01', 'id': '105F', 'state': 'available', 'nodes': 3,'volumes-': [{'state': 'available', 'id': '330172', 'name': 'q_-4144d4e'}, {'state': 'available', 'id': '275192', 'name': 'p_3089d821ae', }]}]}
, {'bb3b_bmls':[{'name': 'FEC 01', 'id': '382E', 'state': 'available', 'nodes': 4,'volumes': [{'state': 'unavailable', 'id': '830172', 'name': 'w_-4144d4e'}, {'state': 'unavailable', 'id': '223192', 'name': 'g_3089d821ae', }]}]}
, {'bb3c_bmls':[{'name': 'ASD 01', 'id': '303F', 'state': 'available', 'nodes': 6,'volumes': [{'state': 'unavailable', 'id': '930172', 'name': 'e_-4144d4e'}, {'state': 'unavailable', 'id': '245192', 'name': 'h_3089d821ae', }]}]}
] }

input_df = pd.DataFrame(source)
</code></pre>

<blockquote>
  <p>My input_df is as below:</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/tibZM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tibZM.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>I'm expecting the output_df as below:</p>
</blockquote>

<p><a href=""https://i.stack.imgur.com/Q6yAV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Q6yAV.png"" alt=""enter image description here""></a></p>

<p>I could manage to get the columns <code>volume_id</code>   <code>volume_name</code> <code>volume_state</code>
<code>name</code>  <code>id</code>    <code>state</code> <code>nodes</code> using the below method.</p>

<pre><code>input_df['data'] = input_df['data'].apply(pd.Series) 
</code></pre>

<p>which will result as below
<a href=""https://i.stack.imgur.com/co6QN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/co6QN.png"" alt=""enter image description here""></a></p>

<pre><code>Test_df=pd.concat([json_normalize(input_df['bb3a_bmls'][key], 'volumes', ['name','id','state','nodes'], record_prefix='volume_') for key in input_df.index if isinstance(input_df['bb3a_bmls'][key],list)]).reset_index(drop=True)
</code></pre>

<p>Which will result for one ""SERVER"" - bb3a_bmls</p>

<p><a href=""https://i.stack.imgur.com/C3ZkC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/C3ZkC.png"" alt=""enter image description here""></a></p>

<p>Now, I don't have an idea how to get the parent columns ""_id"", ""pi"", ""Datetime"" back.</p>
","9274726","","","","","2019-09-04 05:59:18","Python Pandas - Handling columns with nested dictionary(json) values","<python><json><python-3.x><pandas>","1","2","","","","CC BY-SA 4.0","1"
"57703423","1","","","2019-08-29 05:18:16","","1","140","<p>I have given two dataframes below for you to test</p>

<pre><code>df = pd.DataFrame({
    'subject_id':[1,1,1,1,1,1,1,1,1,1,1],
    'time_1' :['2173-04-03 12:35:00','2173-04-03 17:00:00','2173-04-03 
         20:00:00','2173-04-04 11:00:00','2173-04-04 11:30:00','2173-04-04 
       12:00:00','2173-04-05 16:00:00','2173-04-05 22:00:00','2173-04-06 
       04:00:00','2173-04-06 04:30:00','2173-04-06 06:30:00'],
  'val' :[5,5,5,10,5,10,5,8,3,8,10]
 })


df1 = pd.DataFrame({
 'subject_id':[1,1,1,1,1,1,1,1,1,1,1],
 'time_1' :['2173-04-03 12:35:00','2173-04-03 12:50:00','2173-04-03 
           12:59:00','2173-04-03 13:14:00','2173-04-03 13:37:00','2173-04-04 
           11:30:00','2173-04-05 16:00:00','2173-04-05 22:00:00','2173-04-06 
           04:00:00','2173-04-06 04:30:00','2173-04-06 08:00:00'],
 'val' :[5,5,5,5,10,5,5,8,3,4,6]
 })
</code></pre>

<p>what I would like to do is </p>

<p>1) Find all values (from <code>val</code> column) which have been <code>same for more than 1 hour</code> in <code>each day for each subject_id</code> and get the <code>minimum of it</code> </p>

<p>Please note that <strong>values can also be captured at <code>every 15 min duration</code> as well, so you might have to consider 5 records to see <code>&gt; 1 hr</code> condition</strong>). See sample screenshot below </p>

<p>2) If there are no values which were <code>same for more than 1 hour</code> in a day, then just get the <code>minimum of that day for that subject_id</code></p>

<p>The below screenshot for one subject will help you understand and the code I tried is given below</p>

<p><a href=""https://i.stack.imgur.com/oa1EU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oa1EU.png"" alt=""enter image description here""></a></p>

<p>This is what I tried</p>

<pre><code>df['time_1'] = pd.to_datetime(df['time_1'])
df['time_2'] = df['time_1'].shift(-1)
df['tdiff'] = (df['time_2'] - df['time_1']).dt.total_seconds() / 3600
df['reading_day'] = pd.DatetimeIndex(df['time_1']).day

# don't know how to apply if else condition here to check for 1 hr criteria
t1 = df.groupby(['subject_id','reading_start_day','tdiff])['val'].min() 
</code></pre>

<p>As I have to apply this to million records, any elegant and efficient solution would be helpful</p>
","10829044","","10829044","","2019-08-29 13:47:37","2019-08-30 06:59:05","How to get minimum of each group for each day based on hour criteria","<python><python-3.x><pandas><vectorization><pandas-groupby>","4","14","","","","CC BY-SA 4.0","1"
"56975273","1","56975573","","2019-07-10 16:49:02","","4","140","<p><strong>UPDATE</strong>:</p>

<p>TomNash's answer solves the question as asked. However, attempting to use it in my real problem led to issues with quoted column names, issues when there was missing data, etc. To circumvent this I'm using CJR's suggestion in the comments to simply pickle my DataFrames.</p>

<p><strong>ORIGINAL QUESTION BELOW</strong>:</p>

<p>I have a Panda's DataFrame in memory. I would like to be able to write it to file (using <code>to_csv</code>), then use <code>read_csv</code> to read the results into a new DataFrame. I would like the original DataFrame and the new ""from file DataFrame"" to have identical data types.</p>

<p>I've attempted to get this working by using the <code>quoting</code> and <code>quotechar</code> arguments for both <code>to_csv</code> and <code>read_csv</code>. However, this doesn't seem to do the trick.</p>

<p>I understand that for <code>read_csv</code> the <code>dtype</code> argument can be used to force data types, but this isn't practical for my use case (lots of auto-generated files used for regression testing).</p>

<p>Full example below.</p>

<p><code>tmp.py</code>:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from csv import QUOTE_NONNUMERIC
import sys

print('Python version information:')
print(sys.version)
print('Pandas version information:')
print(pd.__version__)

df1 = pd.DataFrame([['A', '100', 100], ['B', '200', 200]])
print('df1:')
print(df1.info())

df1.to_csv('tmp.csv', index=False, quoting=QUOTE_NONNUMERIC,
           quotechar='""')

df2 = pd.read_csv('tmp.csv', quoting=QUOTE_NONNUMERIC, quotechar='""')
print('df2:')
print(df2.info())
</code></pre>

<p>Output from running <code>tmp.py</code>:</p>

<pre class=""lang-py prettyprint-override""><code>Python version information:
3.7.3 (default, Jun 11 2019, 01:11:15) 
[GCC 6.3.0 20170516]
Pandas version information:
0.24.2
df1:
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 2 entries, 0 to 1
Data columns (total 3 columns):
0    2 non-null object
1    2 non-null object
2    2 non-null int64
dtypes: int64(1), object(2)
memory usage: 128.0+ bytes
None
df2:
&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 2 entries, 0 to 1
Data columns (total 3 columns):
0    2 non-null object
1    2 non-null float64
2    2 non-null float64
dtypes: float64(2), object(1)
memory usage: 128.0+ bytes
None
</code></pre>

<ol>
<li>Column 1: As expected, the dtype is <code>object</code> for both DataFrames.</li>
<li>Column 2: Unexpected behavior. For <code>df1</code> the dtype is <code>object</code>, while for <code>df2</code> the dtype is <code>float64</code>.</li>
<li>Column 3: Expected behavior. <code>df1</code> has dtype <code>int64</code> while <code>df2</code> has dtype <code>float64</code>. As the <a href=""https://docs.python.org/3.7/library/csv.html#module-contents"" rel=""nofollow noreferrer"">csv module</a> describes, <code>csv.QUOTE_NONNUMERIC</code> ""Instructs the reader to convert all non-quoted fields to type float.""</li>
</ol>

<p>The contents of <code>tmp.csv</code> are below. Notice that the second column <strong>is quoted</strong>, so I would expect <code>read_csv</code> to give me an object.</p>

<p><code>tmp.csv</code>:</p>

<pre><code>0,1,2
""A"",""100"",100
""B"",""200"",200
</code></pre>
","11052174","","11052174","","2019-07-10 19:29:39","2019-07-10 19:31:27","How to get consistent dtypes after to_csv and read_csv?","<python><python-3.x><pandas><csv><dataframe>","1","5","1","","","CC BY-SA 4.0","1"
"48779475","1","48779565","","2018-02-14 04:04:11","","1","139","<h3>Sample data:</h3>

<pre><code>|   | Status                  | Failed | In Progress | Passed | Untested |
|---|-------------------------|--------|-------------|--------|----------|
| 2 | P0 Dry Run - 13/02/18   | 2.0    |             | 143.0  | 5.0      |
| 3 | P1 Test Plan - 06/02/18 | 4.0    |             | 247.0  | 367.0    |
| 4 | P2 Test plan - 03/01/18 | 22.0   | 2.0         | 496.0  | 54.0     |
</code></pre>

<h3>Code:</h3>

<pre><code>msft = pd.read_csv(""C:\\Users\\gomathis\\Downloads\\week_071.csv"") 
msft = msft[['Passed', 'Failed', 'Blocked', 'In Progress', 'Not_Implemented', 'Not Applicable', 'Clarification Opened', 'Untested']]
msft.to_csv(""C:\\Users\\gomathis\\Downloads\\week_072.csv"")
</code></pre>

<h3>Error:</h3>

<pre><code>KeyError: ""['Blocked'] not in index""
</code></pre>

<h3>Expected result:</h3>

<p>I need an exception for a column which may not be available now but in future it may come. So help me accordingly to solve this.</p>
","8379653","","7311767","","2018-02-14 04:54:36","2018-02-14 04:54:36","How to code the exception for a column in excel using pandas?","<python><python-3.x><pandas><excel-automation>","1","5","","","","CC BY-SA 3.0","1"
"49414967","1","","","2018-03-21 19:23:12","","0","139","<p>I am trying to read a csv file, that has no headers and 2 columns,</p>

<pre><code>inp = pd.read_csv('csvfile')
</code></pre>

<p>alter some of its values and save it back to a csv in the same format.</p>

<pre><code>inp.to_csv('csvfile_new')
</code></pre>

<p>Unfortunately the file is saved in only one big column. Is there a way to keep 2 columns?</p>
","7826511","","","","","2018-03-21 20:03:12","Pandas Dataframe_to_csv columns","<python><python-3.x><pandas>","1","3","","","","CC BY-SA 3.0","1"
"57105346","1","57119226","","2019-07-19 04:00:14","","1","139","<p>I built the code below and am having issues of how to transpose the results. Effectively I am looking for the following result:</p>

<pre><code>#    Column headers: 'company name',  'Work/Life Balance',   'Salary/Benefits',  'Job Security/Advancement', 'Management', 'Culture'  
#    Row 1: 3M, 3.8, 3.9, 3.5, 3.6, 3.8
#    Row 2: Google, . . .
</code></pre>

<p>Currently what happens is as follows:</p>

<pre><code>#    Column headers: 'Name', 'Rating', 'Category'
#    Row 1: 3M, 3.8, Work/Life Balance
#    Row 2: 3M, 3.9, Salary/Benefits
#    and so on . . .
</code></pre>

<p>My code thus far:</p>

<pre><code>import  requests
import pandas as pd
from bs4 import BeautifulSoup


number = []
category = []
name = []
company = ['3M', 'Google']
for company_name in company:
    try:
        url = 'https://ca.indeed.com/cmp/'+company_name
        page = requests.get(url)
        soup = BeautifulSoup(page.content, 'html.parser')
        rating = soup.find(class_='cmp-ReviewAndRatingsStory-rating')
        rating = rating.find('tbody')
        rows = rating.find_all('tr')
    except:
        pass
    for row in rows:
        number.append(str(row.find_all('td')[0].text))
        category.append(str(row.find_all('td')[2].text))
        name.append(company_name)
    cols = {'Name':name,'Rating':number,'Category':category}
    df = pd.DataFrame(cols)
    print(df)
</code></pre>

<p>What the code produces:</p>

<pre><code>      Name Rating                  Category
0       3M    3.8         Work/Life Balance
1       3M    3.9           Salary/Benefits
2       3M    3.5  Job Security/Advancement
3       3M    3.6                Management
4       3M    3.8                   Culture
5   Google    4.2         Work/Life Balance
6   Google    4.0           Salary/Benefits
7   Google    3.6  Job Security/Advancement
8   Google    3.9                Management
9   Google    4.2                   Culture
10   Apple    3.8         Work/Life Balance
11   Apple    4.1           Salary/Benefits
12   Apple    3.7  Job Security/Advancement
13   Apple    3.7                Management
14   Apple    4.1                   Culture
</code></pre>

<p>replicate result by using code below:</p>

<pre><code>import pandas as pd
name = ['3M','3M','3M','3M','3M','Google','Google','Google','Google','Google','Apple','Apple','Apple','Apple','Apple']
number = ['3.8','3.9','3.5','3.6','3.8','4.2','4.0','3.6','3.9','4.2','3.8','4.1','3.7','3.7','4.1']
category = ['Work/Life Balance',' Salary/Benefits','Job Security/Advancement','Management','Culture','Work/Life Balance',' Salary/Benefits','Job Security/Advancement','Management','Culture','Work/Life Balance',' Salary/Benefits','Job Security/Advancement','Management','Culture']
cols = {'Name':name,'Rating':number,'Category':category}
df = pd.DataFrame(cols)
print(df)
</code></pre>
","9882154","","9882154","","2019-07-19 20:09:23","2019-07-19 21:04:55","Web scraping in python using BeautifulSoup - how to transpose results?","<python-3.x><pandas><web-scraping><beautifulsoup><python-requests>","1","9","0","","","CC BY-SA 4.0","1"
"49702465","1","49702624","","2018-04-07 00:00:39","","0","139","<p>I have a csv file of weather station data which has non continuous timestamps:</p>

<pre><code>logstamp               temp     rh     snow      wind          gust    wind_dir                         
2018-01-26 21:00:00   -10.120  63.93  207.1     4.018         9.806   173.900   
2018-01-26 22:00:00    -9.750  58.54  207.0     3.856        11.149   158.500   
2018-01-26 23:00:00    -9.710  60.92  206.9     6.505        13.759   159.100   
2018-01-27 00:00:00   -10.110  57.45  206.7     6.602        12.488   167.700   
2018-01-28 13:00:00    -7.574  84.90  212.4     5.594        15.736   134.100   
2018-01-28 14:00:00    -4.347  88.20  213.1     5.663        15.242   170.700   
2018-01-28 15:00:00    -1.360  89.30  213.0     4.896        19.051   175.300   
</code></pre>

<p>I would like to use the pandas reindex function to add rows where there are missing timestamps so I can interpolate data for the missing times. For example our weather station cuts out Jan 27 in the above table. </p>

<p>I tried using the reindex function with pandas. This results in new interpolated rows for the missing times, however it turns all of the original column data to NaN.</p>

<pre><code>ts1 = pd.read_csv(""mtmaya-2018-02-20.csv"", index_col='logstamp', infer_datetime_format='TRUE') 

index = pd.date_range(ts1.index.min(),ts1.index.max(), freq=""H"")
ts1 = ts1.reindex(index)

                     air temp  rh  snow  wind spd  wind spd max  wind dir  \
2018-01-25 14:00:00       NaN NaN   NaN       NaN           NaN       NaN   
2018-01-25 15:00:00       NaN NaN   NaN       NaN           NaN       NaN   
2018-01-25 16:00:00       NaN NaN   NaN       NaN           NaN       NaN   
</code></pre>

<p>I assume I am missing something.</p>
","9609898","","","","","2018-04-07 00:27:58","Pandas reindex turning all non-index columns to NaN","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"58731702","1","58734451","","2019-11-06 13:50:57","","1","138","<p>I am learning Python and Pandas and I am doing some exercises to understand how things work.
My question is the following: can I use the GroupBy.filter() method to select the DataFrame's rows that have a value (in a specific column) greater than the mean of the respective group?</p>

<p>For this exercise, I am using the ""planets"" dataset included in Seaborn: 1035 rows x 6 columns (column names: ""method"", ""number"", ""orbital_period"", ""mass"", ""distance"", ""year"").</p>

<p>In python:</p>

<pre><code>import pandas as pd
import seaborn as sns

#Load the ""planets"" dataset included in Seaborn
data = sns.load_dataset(""planets"")

#Remove rows with NaN in ""orbital_period""
data = data.dropna(how = ""all"", subset = [""orbital_period""])

#Set display of DataFrames for seeing all the columns:
pd.set_option(""display.max_columns"", 15)

#Group the DataFrame ""data"" by ""method"" ()
group1 = data.groupby(""method"")
#I obtain a DataFrameGroupBy object (group1) composed of 10 groups.
print(group1)
#Print the composition of the DataFrameGroupBy object ""group1"".
for lab, datafrm in group1:
    print(lab, ""\n"", datafrm, sep="""", end=""\n\n"")
print()
print()
print()


#Define the filter_function that will be used by the filter method.
#I want a function that returns True whenever the ""orbital_period"" value for 
#a row is greater than the mean of the corresponding group's mean.
#This could have been done also directly with ""lambda syntax"" as argument
#of filter().
def filter_funct(x):
    #print(type(x))
    #print(x)
    return x[""orbital_period""] &gt; x[""orbital_period""].mean()


dataFiltered = group1.filter(filter_funct)
print(""RESULT OF THE FILTER METHOD:"")
print()
print(dataFiltered)
print()
print()
</code></pre>

<p>Unluckily, I obtain the following error when I run the script.</p>

<pre><code>TypeError: filter function returned a Series, but expected a scalar bool
</code></pre>

<p>It looks like x[""orbital_period""] does not behave as a vector, meaning that it does not return the single values of the Series...
Weirdly enough the transform() method does not suffer from this problem. Indeed on the same dataset (prepared as above) if I run the following:</p>

<pre><code>#Define the transform_function that will be used by the transform() method.
#I want this function to subtract from each value in ""orbital_period"" the mean
#of the corresponding group.
def transf_funct(x):
    #print(type(x))
    #print(x)
    return x-x.mean()

print(""Transform method runs:"")
print()
#I directly assign the transformed values to the ""orbital_period"" column of the DataFrame.
data[""orbital_period""] = group1[""orbital_period""].transform(transf_funct)
print(""RESULT OF THE TRANSFORM METHOD:"")
print()
print(data)
print()
print()
print()
</code></pre>

<p>I obtain the expected result...</p>

<p>Do DataFrameGroupBy.filter() and DataFrameGroupBy.transform() have different behavior?
I know I can achieve what I want in many other ways but my question is:
Is there a way to achieve what I want making use of the DataFrameGroupBy.filter() method?</p>
","12326681","","","","","2019-11-06 18:54:08","Pandas: Use DataFrameGroupBy.filter() method to select DataFrame's rows with a value greater than the mean of the respective group","<python-3.x><pandas><pandas-groupby>","1","2","1","","","CC BY-SA 4.0","1"
"41173747","1","","","2016-12-15 21:38:42","","0","138","<p>I am getting the above type error:</p>

<blockquote>
  <p>TypeError: Can't convert 'int' object to str implicitly.</p>
</blockquote>

<p>line 34, in <code>&lt;lambda&gt;</code></p>

<pre><code>output[column] = output['Index_for_s'].apply(lambda x: s(x, column, file_two))
</code></pre>

<p>when trying to run the program below:</p>

<pre><code>import pandas as pd

import os


def get_index(x, data):
    return list(data[data['COL1'] == x].index)


def suvrule(ind, col, data):
    data=pd.DataFrame(data)
    return  data.ix [ind,col].sum() 

file_one_path = input('Please enter file one: ')
file_two_path = input('Please enter file two: ')

if os.path.exists(file_one_path) and os.path.exists(file_two_path):
    file_one = pd.read_csv(file_one_path)
    file_two = pd.read_csv(file_two_path)
    try:
        assert (file_one.shape[0] == file_one.shape[0])
    except AssertionError:
        print (""Check Data."")
        exit()

    output = file_one.groupby('COL1', sort='False')['COL2'].agg('count').reset_index()
    output['Index_for_s'] = output['COL1'].apply(lambda x: get_index(x, file_one))
    cols_for_s = [col for col in file_two.columns if 'Header' not in col]
    for column in cols_for_s:
        output[column] = output['Index_for_s'].apply(lambda x: s(x, column, file_two))
    output = output.drop('Index_for_s', axis= 1)
    print (""\nWriting output to output.csv in current working directory."")
    output.to_csv(""output.csv"",  index='False')
else:
    print ('Incorrect file path')
</code></pre>
","7304040","","4370109","","2016-12-17 23:56:24","2016-12-17 23:56:24","Program is returning the following TypeError: Can't convert 'int' object to str implicitly","<python><python-3.x><pandas>","1","4","","","","CC BY-SA 3.0","1"
"48660710","1","48660743","","2018-02-07 09:53:31","","0","137","<p>I would like to trim the dataframe or create a new one so it has only unique rows considering certain columns. Now I have:</p>

<pre><code>time_original time_seconds time_round time_below time_above
273.0         21.782       22.0        0.0       52.0
273.0         21.816       22.0        0.0       52.0
273.0         21.849       22.0        0.0       52.0
273.0         21.882       22.0        0.0       52.0
273.0         104.143      104.0       74.0      134.0
273.0         104.176      104.0       74.0      134.0
273.0         104.210      104.0       74.0      134.0
</code></pre>

<p>and I would like to take into consideration the last 3 columns to print only unique rows. So to have:</p>

<pre><code>time_round time_below time_above
22.0       0.0        52.0
104.0      74.0       134.0
</code></pre>
","8069542","","","","","2018-02-07 09:55:22","extracting unique rows from dataframe in python","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 3.0","1"
"40527316","1","40613119","","2016-11-10 12:11:05","","1","135","<p>I'm working with a file that has a couple Numero Signs in it.  </p>

<p>Here are the top 3 lines copied and pasted directly from the CSV file:</p>

<pre><code>0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15
    â„– Summer  01 !    02 !    03 !    Total   â„– Winter  01 !    02 !    03 !    Total   â„– Games   01 !    02 !    03 !    Combined total
AfghanistanÂ (AFG)  13  0   0   2   2   0   0   0   0   0   13  0   0   2   2
</code></pre>

<p>When I try to import the file in Anaconda using Python 3.5 using Pandas read_csv I get the following error:</p>

<pre><code>UnicodeEncodeError:  'charmap' code can't encode character '\u2116' in position 104: character maps to &lt;undefined&gt;
</code></pre>

<p>This happens when I try:</p>

<pre><code>df=pd.read_csv('myfile.csv', encoding='utf_8')
</code></pre>

<p>I also tried the standard English codecs listed here with basically the same error code:
<a href=""https://docs.python.org/3/library/codecs.html#standard-encodings"" rel=""nofollow noreferrer"">https://docs.python.org/3/library/codecs.html#standard-encodings</a></p>

<p>What should I try differently?</p>
","3225420","","472495","","2020-05-29 23:09:04","2020-05-29 23:09:04","Proper Python Pandas read_csv encoding for '\u2116', the 'Numero Sign'","<python-3.x><csv><pandas><anaconda><importerror>","1","9","","","","CC BY-SA 4.0","1"
"57078601","1","57078727","","2019-07-17 14:45:03","","1","135","<p>I have a Pandas data frame with the following columns and values</p>

<pre><code>  Temp  Time grain_size
0 335.0 25.0 14.8
1 335.0 30.0 18.7
2 335.0 35.0 22.1
3 187.6 25.0 9.8
4 227.0 25.0 14.2
5 227.0 30.0 16.2
6 118.5 25.0 8.7
</code></pre>

<p>The data frame given the variable name <code>df</code> that has three distinct value which are <code>335.0</code>, <code>187.6</code>, <code>227.0</code>, and <code>118.5</code>; however, the values <code>187.6</code> and <code>118.5</code> only occur once.  I would like to filter the data frame such that it gets rid of values that only occur once so the final data frame looks like.</p>

<pre><code>  Temp  Time grain_size
0 335.0 25.0 14.8
1 335.0 30.0 18.7
2 335.0 35.0 22.1
4 227.0 25.0 14.2
5 227.0 30.0 16.2
</code></pre>

<p>Obviously in this simple case I know the values that only occur once and I can simply user a filtering function to weed them out.  However, I would like to automate the process so that Python will determine which values only occur once and autonomously filter them.  How can I enable this functionality?</p>
","2403819","","","","","2019-07-17 15:15:16","How to filter out values from a pandas data frame for which only one occurrence exists","<python-3.x><pandas><dataframe>","3","0","","","","CC BY-SA 4.0","1"
"57613595","1","","","2019-08-22 16:27:00","","2","135","<p>I am doing data preprocessing, so I am trying to convert the date string format into an int, but I got an error, please help me how to convert it.</p>

<p>I have data like this :</p>

<pre><code>0        Apr-12
1        Apr-12
2        Mar-12
3        Apr-12
4        Apr-12
</code></pre>

<p>I tried this :</p>

<pre><code>d=df['d_date'].apply(lambda x: datetime.strptime(x, '%m%Y'))
</code></pre>

<p>I got an error.</p>

<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-134-173081812744&gt; in &lt;module&gt;()
----&gt; 1 d=test['first_payment_date'].apply(lambda x: datetime.strptime(x, '%m%Y'))

~\Anaconda3\lib\site-packages\pandas\core\series.py in apply(self, func, convert_dtype, args, **kwds)
   4036             else:
   4037                 values = self.astype(object).values
-&gt; 4038                 mapped = lib.map_infer(values, f, convert=convert_dtype)
   4039 
   4040         if len(mapped) and isinstance(mapped[0], Series):

pandas\_libs\lib.pyx in pandas._libs.lib.map_infer()

&lt;ipython-input-134-173081812744&gt; in &lt;lambda&gt;(x)
----&gt; 1 d=test['first_payment_date'].apply(lambda x: datetime.strptime(x, '%m%Y'))

~\Anaconda3\lib\_strptime.py in _strptime_datetime(cls, data_string, format)
    563     """"""Return a class cls instance based on the input string and the
    564     format string.""""""
--&gt; 565     tt, fraction = _strptime(data_string, format)
    566     tzname, gmtoff = tt[-2:]
    567     args = tt[:6] + (fraction,)

~\Anaconda3\lib\_strptime.py in _strptime(data_string, format)
    360     if not found:
    361         raise ValueError(""time data %r does not match format %r"" %
--&gt; 362                          (data_string, format))
    363     if len(data_string) != found.end():
    364         raise ValueError(""unconverted data remains: %s"" %

ValueError: time data 'Apr12' does not match format '%m%Y'
</code></pre>
","10153905","","4685471","","2019-08-22 21:31:04","2019-08-22 21:31:04","how to convert string date time to int using python","<python-3.x><pandas><datetime>","1","3","","","","CC BY-SA 4.0","1"
"56591881","1","56602375","","2019-06-14 05:28:15","","2","133","<p>I have a web link as:</p>

<pre><code>url = ""zerodha.com/margin-calculator/SPAN""
</code></pre>

<p>Here input parameters with sample values for reference mentioned below: </p>

<pre><code>Exchange - NFO
Product - Options
Symbol - DHFL 27-JUN-19
Option Type - Calls
Strike Price - 120
Net Qty appears automatically as 1500, 
</code></pre>

<p>and Use <strong><code>SELL</code></strong> Button then Click <strong><code>ADD</code></strong> Button. </p>

<p>I want to collect the Total Margin required (in above case its <strong><code>Rs 49,308</code></strong>) which appears in the right end.</p>
","11006089","","11006089","","2019-06-14 05:34:51","2019-07-16 04:45:58","Web scrap with multiple inputs and collect Total Margin required","<python><python-3.x><pandas><web-scraping><request>","2","0","2","","","CC BY-SA 4.0","1"
"57299804","1","57300039","","2019-07-31 22:55:48","","1","133","<p>I have a pandas dataframe, which I'm exporting to an Excel file using to_excel. All of the cells have pseudorandomly generated strings of length 2 in them (either alphanumeric or just numeric). I want to color the background of some of the cells gray. I always want to color the exact same cells each time I run the .py file, but since the content of the cells is basically random, I can't use some condition on the value in the cells to color them. </p>

<p>I looked at the <a href=""https://xlsxwriter.readthedocs.io/working_with_conditional_formats.html"" rel=""nofollow noreferrer"">xlsx writer documentation on conditional formatting</a> and tried to use it in conjunction with the accepted answer on <a href=""https://stackoverflow.com/questions/44150078/python-using-pandas-to-format-excel-cell"">this question</a>. As a side note, if I just copy paste the first code block of the accepted answer, that runs just fine!</p>

<p>This is my code:</p>

<pre class=""lang-py prettyprint-override""><code>df = pd.DataFrame({'Col1': ['A1', 'B2', '3C', '4D', 'E5', '6F', 'G7'],
                   'Col2': ['00', '01', '02', '03', '04', '05', '06']})
writer = pd.ExcelWriter('shaded.xlsx', engine='xlsxwriter')
df.to_excel(writer, sheet_name='Sheet1')
workbook  = writer.book
worksheet = writer.sheets['Sheet1']

shadedFormat  = workbook.add_format({'bg_color': 'gray'})

worksheet.conditional_format('B3:B5', {'type': 'text',
                                      'criteria': 'not equal to',
                                       'value': 'longString',
                                       'format': shadedFormat})

writer.save()
</code></pre>

<p>The python interpreter does not return any errors, but when I go to open the .xlsx file, a pop up window gives ""We found a problem with some content in 'shaded.xlsx'. Do you want us to try to recover as much as we can?"" Upon saying yes, it brings up another windows, which says ""Repaired Part: /xl/worksheets/sheet1.xml part with XML error.  Catastrophic failure Line 2, column 1267."" and none of the columns are shaded. </p>

<p>I would like to do away with the error and have the specified cells shaded gray.</p>

<p>Thank you for any help/suggestions!</p>
","8345905","","","","","2019-08-01 15:00:15","Is there a way to color cells containing text using XlsxWriter?","<python><python-3.x><pandas><xlsx><xlsxwriter>","1","0","","","","CC BY-SA 4.0","1"
"57077029","1","57079650","","2019-07-17 13:27:42","","0","133","<p>I am downloading data in json format in Python 3.7 and trying to display it as an Excel spreadsheet. I tried using pandas, but I think that there is a problem in getting the data into a dataframe. I also tried a csv approach but that was not successful.</p>

<p>I am new to learning python, so maybe there is something obvious I am missing.  </p>

<pre><code>import json, requests,urllib,csv
import pandas as pd
url  = urllib.request.urlopen('https://library.brown.edu/search/solr_pub/iip/?start=0&amp;rows=100&amp;indent=on&amp;wt=json&amp;q=*')
str_response = url.read().decode('utf-8')
jsondata=json.loads(str_response)
df = pd.DataFrame(jsondata)
</code></pre>

<p>I was hoping to get a number of rows for each item, e.g., zoor0353, with the columns for each of the keys associated with it (e.g., region, date_desc, etc. -there are quite a few).  Instead, it seemed only to take the first section, returning: </p>

<pre><code>responseHeader  \
QTime                                                     1   
docs                                                    NaN   
numFound                                                NaN   
params    {'q': '*', 'indent': 'on', 'start': '0', 'rows...   
start                                                   NaN   
status                                                    0   

</code></pre>

<pre><code>                                                   response  
QTime                                                   NaN  
docs      [{'inscription_id': 'zoor0353', 'metadata': ['...  
numFound                                               4356  
params                                                  NaN  
start                                                     0  
status                                                  NaN 
</code></pre>

<p>I tried this with normalization method, but did no better.  Ultimately, I would like to use a dataset made of an appended file of many calls to this api, and am also wondering if I will need to manipulate data, and how, to get it to work with pandas.</p>
","11655195","","10140310","","2019-07-17 14:17:41","2019-07-17 21:11:44","Formatting json data to convert to xslx","<python-3.x><pandas><api>","1","4","","","","CC BY-SA 4.0","1"
"41278428","1","41286829","","2016-12-22 08:05:56","","4","132","<p>I have a 3-level dictionary like this:</p>

<pre><code>data={'2016-11-28': {'area1': {'am': -0.007, 'pm': 0.008}, 'area2': {'am': 0.0, 'pm': 0.0}, 'area3': {'am': -0.01, 'pm': -0.001}},'2016-11-29':{'area1': {'am': -0.007, 'pm': 0.008}, 'area2': {'am': 0.0, 'pm': 0.0}, 'area3': {'am': -0.01, 'pm': -0.001}}}
</code></pre>

<p>I want to convert it to a dataframe, and I tried: </p>

<pre><code>tickers=data['2016-11-28'].keys()
iterables=[tickers,['am','pm']]
index=pd.MultiIndex.from_product(iterables, names=['ticker', 'time'])
frame=pd.DataFrame(data,index=index)
</code></pre>

<p>but I got</p>

<pre><code>                2016-11-28  2016-11-29
ticker time                        
area1  am           NaN         NaN
       pm           NaN         NaN
area3  am           NaN         NaN
       pm           NaN         NaN
area2  am           NaN         NaN
       pm           NaN         NaN
</code></pre>

<p>There are no values in the dataframe, only column names and index names. What's wrong with my code? Can someone help? Thanks very much!</p>
","7329278","","4952130","","2016-12-22 12:57:59","2016-12-22 15:58:35","How to convert a 3-level dictionary to a desired format?","<python><python-3.x><pandas>","1","1","1","","","CC BY-SA 3.0","1"
"56607596","1","56607631","","2019-06-15 05:21:35","","1","131","<p>I'm trying to make a recommendation system using the knn algorithm. I imported pandas(latest version 0.24) and created a sparse matrix. And now I am using reshape function but it is showing the error.</p>

<p>I am using Jupyter Notebook. And I have imported 3 CSV's. </p>

<pre><code>books = pd.read_csv('C:\\Users\\Shivam Upadhyay\\Desktop\\Project\\books\\BX-Books.csv', sep=';', error_bad_lines=False, encoding=""latin-1"")
books.columns = ['ISBN', 'bookTitle', 'bookAuthor', 'yearOfPublication', 'publisher', 'imageUrlS', 'imageUrlM', 'imageUrlL']
users = pd.read_csv('C:\\Users\\Shivam Upadhyay\\Desktop\\Project\\books\\BX-Users.csv', sep=';', error_bad_lines=False, encoding=""latin-1"")
users.columns = ['userID', 'Location', 'Age']
ratings = pd.read_csv('C:\\Users\\Shivam Upadhyay\\Desktop\\Project\\books\\BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=""latin-1"")
ratings.columns = ['userID', 'ISBN', 'bookRating']

query_index = np.random.choice(us_canada_user_rating_pivot.shape[0])
distances, indices = model_knn.kneighbors(us_canada_user_rating_pivot.iloc[query_index, :].reshape(1, -1), n_neighbors = 6)

for i in range(0, len(distances.flatten())):
    if i == 0:
        print('Recommendations for {0}:\n'.format(us_canada_user_rating_pivot.index[query_index]))
    else:
        print('{0}: {1}, with distance of {2}:'.format(i, us_canada_user_rating_pivot.index[indices.flatten()[i]], distances.flatten()[i]))

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
&lt;ipython-input-4-d2c9c594ec59&gt; in &lt;module&gt;
      1 query_index = np.random.choice(us_canada_user_rating_pivot.shape[0])
----&gt; 2 distances, indices = model_knn.kneighbors(us_canada_user_rating_pivot.iloc[query_index, :].reshape(1, -1), n_neighbors = 6)
      3 
      4 for i in range(0, len(distances.flatten())):
      5     if i == 0:

~\AppData\Roaming\Python\Python37\site-packages\pandas\core\generic.py in __getattr__(self, name)
   5065             if self._info_axis._can_hold_identifiers_and_holds_name(name):
   5066                 return self[name]
-&gt; 5067             return object.__getattribute__(self, name)
   5068 
   5069     def __setattr__(self, name, value):

AttributeError: 'Series' object has no attribute 'reshape'
</code></pre>
","11650891","","2745495","","2019-06-15 06:08:18","2019-06-15 06:08:18","How to fix the Attribute error:'Series' object has no attribute 'reshape' in this python code?","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"33709489","1","","","2015-11-14 14:31:39","","1","129","<p>I'm using python3.5</p>

<p>After importing my csv file I want these operations:
IN:</p>

<pre><code>ULIM LLIM High Low CLO $RNG U-OCT MID L-OCT
-------------------------------------------

12785 125300 127840 127500 127475 1275 127532 126575 125618
</code></pre>

<p>OUT:</p>

<pre><code>Dir ULIM LLIM High Low CLO $RNG U-OCT MID L-OCT Test
----------------------------------------------------

S   12785 125300 127840 127500 127475 1275 127532 126575 125618 T
</code></pre>

<p>add TWO New Colums: Direction and Test</p>

<p>T= IF CLO or High or Low values within either U-OCT to ULIM range or L-OCT to LLIM range values
   else no entry</p>

<p>S= L if CLO above MID, S if CLOS below MID</p>

<p>This is what I want to learn/need help for: </p>

<p>I don't know how to do the comparing part and adding the results in the new two colums. (arrays?)
I don't know how to do that for the whole csv table (looping?)  </p>
","5561942","","5561942","","2015-11-15 13:23:08","2015-11-15 14:49:37","Python3 comparing row by row values from csv","<python><csv><python-3.x><pandas>","1","4","","","","CC BY-SA 3.0","1"
"56645287","1","56646468","","2019-06-18 08:55:37","","-2","127","<p>Hi I have received a JSON file with below format, please let me know how do i parse this JSOn file and convert them to CSV</p>

<p>JSON file format</p>

<pre><code>{'Sections': [{'MC': [[{'IsMandatory': False,
      'LD': None,
      'propNameValuepair': [{'PropertyType': 0,
        'columnSize': 0,
        'isEnabled': False,
        'isStructured': False,
        'isUpdated': False,
        'propDisplayName': 'CC',
        'propName': 'u4_c_c',
        'propValue': 'Bottle',
        'selectedComponent': None,
        'tcProperty': None},
       {'PropertyType': 0,
        'columnSize': 0,
        'isEnabled': False,
        'isStructured': False,
        'isUpdated': False,
        'propDisplayName': 'Com Com',
        'propName': 'u4_com_com',
        'propValue': 'Multi-layer',
        'selectedComponent': None,
        'tcProperty': None},
</code></pre>

<p>Code</p>

<pre><code>import json
with open('a.json') as data_file:    
      data = json.load(data_file)
import pandas as pd
df = pd.concat([pd.DataFrame(x) for x in data], ignore_index=False)
print(df)
</code></pre>

<p>Error:</p>

<blockquote>
  <p>ValueError: DataFrame constructor not properly called!</p>
</blockquote>
","7905329","","5770501","","2019-06-18 13:46:50","2019-06-18 13:46:50","How to convert an embedded JSON file to CSV in pandas python","<python><json><python-3.x><pandas><csv>","1","3","","","","CC BY-SA 4.0","1"
"57865769","1","57865861","","2019-09-10 07:14:58","","4","126","<p>How do I print/return the value based on values from another column?</p>

<pre><code>analyse = input[['SR. NO', 'COUNTRY_NAME']]
print(analyse)

SR. NO    COUNTRY_NAME
     2          Norway
     2         Denmark
     2         Iceland
     2         Finland
     3         Denmark
     3         Iceland
     4         Finland
     4          Norway
</code></pre>

<p>Here, I want to <strong>check</strong> if <strong>Norway</strong> <strong><em>or</em></strong> <strong>Denmark</strong> are <strong>present</strong> for <strong><em>every SR. NO</em></strong>, <em>return those serial numbers where <strong>either</strong> one of these 2 countries <strong>aren't found!</em></strong> I tried using groupby and iterating over countries but that didn't help. I'm stuck at that point.</p>

<p>So, the <strong>expected output</strong> is:</p>

<pre><code>[3,4]
</code></pre>
","8659610","","9698684","","2019-09-10 07:25:41","2019-09-10 08:40:38","Checking if dataframe groups contain values from list","<python><python-3.x><pandas><dataframe>","2","0","","","","CC BY-SA 4.0","1"
"57661996","1","57662108","","2019-08-26 16:56:49","","7","124","<pre><code>    dataframe = pd.DataFrame({'Date':['This 1A1619 person BL171111 the A-1-24',
                                  'dont Z112 but NOT 1-22-2001',
                                  'mix: 1A25629Q88 or A13B ok'], 
                          'IDs': ['A11','B22','C33'],
                          }) 

           Date                                 IDs
0   This 1A1619 person BL171111 the A-1-24      A11
1   dont Z112 but NOT 1-22-2001                 B22
2   mix: 1A25629Q88 or A13B ok                  C33
</code></pre>

<p>I have the dataframe above. My goal is to replace all mixed word/number combo's WITHOUT hyphens <code>-</code> e.g. <code>1A1619I</code> or <code>BL171111</code> or <code>A13B</code> but NOT <code>1-22-2001</code> or <code>A-1-24</code> with the letter <code>M</code>. I have attempted to use the code below via <a href=""https://stackoverflow.com/questions/57650538/identify-letter-number-combinations-using-regex-and-storing-in-dictionary"">identify letter/number combinations using regex and storing in dictionary</a></p>

<pre><code>dataframe['MixedNum'] = dataframe['Date'].str.replace(r'(?=.*[a-zA-Z])(\S+\S+\S+)','M') 
</code></pre>

<p>But I get this output</p>

<pre><code>                          Date              IDs     MixedNum
0   This 1A1619 person BL171111 the A-1-24  A11     M M M M M M M
1   dont Z112 but NOT 1-22-2001             B22     M M M M 1-22-2001
2   mix: 1A25629Q88 or A13B ok              C33     M M or M ok
</code></pre>

<p>when I would really want this output</p>

<pre><code>                          Date              IDs     MixedNum
0   This 1A1619 person BL171111 the A-1-24  A11     This M person M the A-1-24 
1   dont Z112 but NOT 1-22-2001             B22     dont M but NOT 1-22-2001
2   mix: 1A25629Q88 or A13B ok              C33     mix: M or M ok
</code></pre>

<p>I also tried the regex suggested here but it also didnt work for me
<a href=""https://stackoverflow.com/questions/13453999/regex-replace-mixed-numberstrings"">Regex replace mixed number+strings</a></p>

<p>Can anyone help me alter my regex?  <code>r'(?=.*[a-zA-Z])(\S+\S+\S+</code></p>
","","user11962395","2535611","","2019-08-26 17:12:27","2019-08-26 17:12:27","replace words and strings pandas","<python><regex><python-3.x><string><pandas>","1","0","1","","","CC BY-SA 4.0","1"
"57024430","1","57038585","","2019-07-14 03:46:48","","1","123","<p>I had been trying to replicated an online tutorial for plotting confusion matrix but got recursion error, tried resetting the recursion limit but still the error persists. The code is a below:</p>

<pre><code>log = LogisticRegression()
log.fit(x_train,y_train)
pred_log = log.predict(x_train)
confusion_matrix(y_train,pred_log)
</code></pre>

<p>The error I got is :</p>

<pre><code>---------------------------------------------------------------------------
RecursionError                            Traceback (most recent call last)
&lt;ipython-input-57-4b8fbe47e72d&gt; in &lt;module&gt;
----&gt; 1 (confusion_matrix(y_train,pred_log))

&lt;ipython-input-48-92d5242f8580&gt; in confusion_matrix(test_data, pred_data)
      1 def confusion_matrix(test_data,pred_data):
----&gt; 2     c_mat = confusion_matrix(test_data,pred_data)
      3     return pd.DataFrame(c_mat)

... last 1 frames repeated, from the frame below ...

&lt;ipython-input-48-92d5242f8580&gt; in confusion_matrix(test_data, pred_data)
      1 def confusion_matrix(test_data,pred_data):
----&gt; 2     c_mat = confusion_matrix(test_data,pred_data)
      3     return pd.DataFrame(c_mat)

RecursionError: maximum recursion depth exceeded
</code></pre>

<p>The shape of the train and test data is as below</p>

<pre><code>x_train.shape,y_train.shape,x_test.shape,y_test.shape 
# ((712, 7), (712,), (179, 7), (179,))
</code></pre>

<p>Tried with: <code>sys.setrecursionlimit(1500)</code><br>
But still no resolution. </p>
","9297078","","6347629","","2019-07-15 11:18:17","2019-07-15 11:18:17","Confusion Matrix : RecursionError","<python-3.x><pandas><scikit-learn>","1","3","","","","CC BY-SA 4.0","1"
"57613770","1","57614055","","2019-08-22 16:39:45","","0","121","<p>I would like to convert all the values in a col that are not float (str, nan, etc) to some garbage value like -99 (I replace this later in my code). For example</p>

<pre><code>1.23

!DIV0

5.55

&lt;0 

10.2

nan 
</code></pre>

<p>I'd like the output to be</p>

<pre><code>1.23

-99

5.55

-99

10.2

-99
</code></pre>

<p>I tried using <code>df.col.apply(lambda x: x.replace ...</code></p>

<p>but it is using the str function replace and not the pd function</p>

<p>I don't think I can use <code>applymap</code> because another col in my dataframe is strings and I need to keep it that way</p>
","11956484","","9840637","","2019-08-22 16:42:00","2019-08-22 17:02:50","Replace everything that is not a float in a column with -1","<python><python-3.x><pandas>","2","2","","","","CC BY-SA 4.0","1"
"40605527","1","","","2016-11-15 08:52:17","","0","121","<p>i have a table in pandas df, consisting of two columns.</p>

<pre><code>   |product_id   |Bigram
   ---------------------------------------------------------------------
   |111          |[('111','987'),('987','741'),('12','111')]
   |987          |[('987','1232'),('1232','987')
   |654          |('654,12'),('12,324'),('24,465')]
   |321          |[('321','741')]
   |324          |[('324','654'),('654','862'),('862','324')]
   |123          |[('123','98'),('12','123')]
</code></pre>

<p>i want to create a list L, from the Bigram column such that, all the values in each row and across every rows gets appended in the list.</p>

<p>for example. my output should be.</p>

<pre><code>L = [(['987','1232'],['1232','987'],['654,12'],['12,324'],['24,465'],
['321','741'],............['123','98'],['12','123'])]
</code></pre>

<p>Is there any way to do this? using some for loop ?</p>
","6803114","","6803114","","2016-11-15 08:56:29","2016-11-15 09:26:00","Convert rows of a column in pandas df in a list","<python><python-2.7><python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"57162718","1","57163348","","2019-07-23 11:04:16","","1","120","<p><em>User-item affinity and recommendations :</em><br>
I am creating a table which suggests ""customers who bought this item also bought algorithm ""<br>
Input dataset</p>

<pre><code>productId   userId
Prod1        a
Prod1        b
Prod1        c
Prod1        d
prod2        b
prod2        c
prod2        a
prod2        b
prod3        c
prod3        a
prod3        d
prod3        c
prod4        a
prod4        b
prod4        d
prod4        a
prod5        d
prod5        a
</code></pre>

<p>Output required</p>

<pre><code>Product1    Product2    score
Prod1       prod3
Prod1       prod4
Prod1       prod5
prod2       Prod1
prod2       prod3
prod2       prod4
prod2       prod5
prod3       Prod1
prod3       prod2
</code></pre>

<pre><code>Using code : 
#Get list of unique items
itemList=list(set(main[""productId""].tolist()))

#Get count of users
userCount=len(set(main[""productId""].tolist()))

#Create an empty data frame to store item affinity scores for items.
itemAffinity= pd.DataFrame(columns=('item1', 'item2', 'score'))
rowCount=0

#For each item in the list, compare with other items.
for ind1 in range(len(itemList)):

    #Get list of users who bought this item 1.
    item1Users = main[main.productId==itemList[ind1]][""userId""].tolist()
    #print(""Item 1 "", item1Users)

    #Get item 2 - items that are not item 1 or those that are not analyzed already.
    for ind2 in range(ind1, len(itemList)):

        if ( ind1 == ind2):
            continue

        #Get list of users who bought item 2
        item2Users=main[main.productId==itemList[ind2]][""userId""].tolist()
        #print(""Item 2"",item2Users)

        #Find score. Find the common list of users and divide it by the total users.
        commonUsers= len(set(item1Users).intersection(set(item2Users)))
        score=commonUsers / userCount

        #Add a score for item 1, item 2
        itemAffinity.loc[rowCount] = [itemList[ind1],itemList[ind2],score]
        rowCount +=1
        #Add a score for item2, item 1. The same score would apply irrespective of the sequence.
        itemAffinity.loc[rowCount] = [itemList[ind2],itemList[ind1],score]
        rowCount +=1

#Check final result
itemAffinity
</code></pre>

<p><strong>the code is running perfectly fine on a sample dataset but<br>
The code is taking too long to run in dataset containing 100,000 rows. Please help me optimize the code.</strong>  </p>
","11718651","","","","","2019-08-06 13:14:06","how can I match all the key value pair in python which running too long","<python><python-3.x><pandas><jupyter-notebook><recommendation-engine>","2","2","","","","CC BY-SA 4.0","1"
"49619194","1","49619261","","2018-04-02 21:40:24","","0","120","<p>I have a <code>DataFrame</code> with multiple columns per row which need to be split into a new rows per column</p>

<p>The <code>DataFrame</code> currently (shortened down) looks like this</p>

<pre><code>|---------------------------------------------------------------------------------------------------------------------------------|
|institution_short_name |interest_paid1 |interest_paid2 |product_detail_value_min |term_01m_value |term_02y_value |term_03m_value |
|---------------------------------------------------------------------------------------------------------------------------------|
|One                    |Z              |Q              |2000                     |0.50           |0.75           |0.75           |
|One                    |Z              |Q              |5000                     |0.50           |3.65           |3.75           |
|One                    |M              |M              |20000                    |Nan            |3.65           |Nan            | 
|---------------------------------------------------------------------------------------------------------------------------------|
</code></pre>

<p>For each <code>term</code> column, e.g i would like to extract the 01m, 02y, 03m from the name of the column as well as the value for that row under that column and explode it like the following:</p>

<pre><code>|----------------------------------------------------------------------------------------------------------|
|institution_short_name |interest_paid1 |interest_paid2 |product_detail_value_min |Term            |Value  |
|----------------------------------------------------------------------------------------------------------|
|One                    |Z              |Q              |2000                     |01m             |0.5    |
|One                    |Z              |Q              |2000                     |02y             |3.75   |
|One                    |Z              |Q              |2000                     |03m             |0.75   |
|One                    |Z              |Q              |5000                     |01m             |0.5    |
|One                    |Z              |Q              |5000                     |02y             |3.65   |
|One                    |Z              |Q              |5000                     |03m             |3.75   |
|One                    |M              |M              |20000                    |02y             |3.65   |
|----------------------------------------------------------------------------------------------------------|
</code></pre>

<p>I am not asking for a complete solution, just something to get me started. Thanks!</p>
","8624402","","","","","2018-04-02 21:46:36","Dynamically explode columns in Pandas row","<python><python-3.x><python-2.7><pandas><dataframe>","1","2","","","","CC BY-SA 3.0","1"
"57554087","1","","","2019-08-19 09:38:58","","-2","120","<p>I am simply running <code>import pandas as pd</code> to import pandas.
I am getting an indentation error which I am unable to understand.</p>

<p>I have updated everything using Anaconda.
I have attempted to import pandas in Spyder and Jupyter Notebook</p>

<p>my error message:</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd

Traceback (most recent call last):

  File ""C:\Users\g\AppData\Local\Continuum\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3296, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)

  File ""&lt;ipython-input-2-3f7aa48ad27f&gt;"", line 3, in &lt;module&gt;
    import pandas as pd

  File ""C:\Users\g\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\__init__.py"", line 49, in &lt;module&gt;
    from pandas.io.api import *

  File ""C:\Users\g\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\io\api.py"", line 8, in &lt;module&gt;
    from pandas.io.excel import ExcelFile, ExcelWriter, read_excel

  File ""C:\Users\g\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\io\excel.py"", line 34, in &lt;module&gt;
    from pandas.io.parsers import TextParser

  File ""C:\Users\g\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\io\parsers.py"", line 1122
    L            self._engine = CParserWrapper(self.f, **self.options)
                                                                      ^
IndentationError: expected an indented block
</code></pre>
","8339485","","8339485","","2020-05-01 21:29:43","2020-05-01 21:29:43","Python 3.7 Indentation error when importing pandas as pd","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57231831","1","57232204","","2019-07-27 11:50:00","","0","120","<p>I wanted to seek efficient ways to find rows meet some conditions in a dataframe. 
The dataframe had n rows and 3 columns. The value were <code>-1</code> or <code>0</code> or <code>1</code>.<br>
I wanted to find the rows which meet two conditions. </p>

<ol>
<li>condition: the value of the row(row0) !=-1; </li>
<li>condition: the diagonal of the array generated by the next 3 rows (row1, row2, row3) = 1. </li>
</ol>

<p>I used loop method to walk all rows and find rows meet the conditions. But, this was not a efficient way especially when there was a big dataframe and the step only was the first step.</p>

<pre><code># Given a dataframe (n*3) 

randNum=random.choices(range(-1,2),k=333) # k=3*int
frame=pd.DataFrame(np.array(randNum).reshape(-1,3))

# its values = -1,0,1, like this:
# In [126]:frame
# Out[126]:
#     0  1  2
# 0    1  0  0
# 1    1 -1  1
# 2    1  1  1
# 3   -1 -1  1
# 4   -1  0 -1
# 5    1  1 -1
# ...
# 105 -1 -1 -1
# 106 -1 -1  0
# 107 -1 -1  0
# 108  0 -1  1
# 109 -1  0  1
# 110  1  0  1
#  I want find the row(s) that all of the values of
#  columns('0','1','2')!=-1, and while the value of 
#  the diagonal of next three rows =1, like this:
#       0  1  2
# row0  v1 v2 v3     # v1!=v2!=v3!=-1, it may be 1 or 0.
# row1   1  v  v
# row2   v  1  v          # v =-1 or 0 or 1
# row3   v  v  1
# the diagonal of rows (row1,row2,row3)=1
</code></pre>

<p>I want to find the row0 in the DataFrame. It may be resolved by loop        method, but is there exist an efficient solution? Thanks a lot !</p>
","11842118","","9898643","","2019-07-27 13:00:44","2019-07-30 17:53:01","Are there some efficient ways to find row(s) meeted conditons which referred to the values in next some rows?","<python><python-3.x><pandas><numpy><dataframe>","4","0","","","","CC BY-SA 4.0","1"
"57101192","1","","","2019-07-18 19:07:09","","1","119","<p>I have two pandas Dataframes, using python3.x:</p>

<pre><code>import pandas as pd

dict1 = {0:['chr1','chr1','chr1','chr1','chr2'], 
    1:[1, 100, 150, 900, 1], 2:[100, 200, 500, 950, 100], 
    3:['feature1', 'feature2', 'feature3', 'feature4', 'feature4'], 
    4:[0, 0, 0, 0, 0], 5:['+','+','-','+','+']}

df1 = pd.DataFrame(dict1)

print(df1)

##       0    1    2         3  4  5
## 0  chr1    1  100  feature1  0  +
## 1  chr1  100  200  feature2  0  +
## 2  chr1  150  500  feature3  0  -
## 3  chr1  900  950  feature4  0  +
## 4  chr2    1  100  feature4  0  +

dict2 = {0:['chr1','chr1'], 1:[155, 800], 2:[200, 901], 
    3:['feature5', 'feature6'], 4:[0, 0], 5:['-','+']}

df2 = pd.DataFrame(dict2)
print(df2)
##       0    1    2         3  4  5
## 0  chr1  155  200  feature5  0  -
## 1  chr1  800  901  feature6  0  +
</code></pre>

<p>The columns to focus on in these dataframes are the first three columns: location, start, and end. Each start:end value represents a distance on location (e.g. <code>chr1</code>, <code>chr2</code>, <code>chr3</code>). </p>

<p>I would like to output the intersection of <code>df1</code> against <code>df2</code>. Here is the correct output:</p>

<pre><code>chr1    155 200 feature2    0   +
chr1    155 200 feature3    0   -
chr1    900 901 feature4    0   +
</code></pre>

<p><strong>Explanation:</strong> We find the intersection of <code>df1</code> against <code>df2</code>. So, <code>feature2</code> and <code>feature3</code> intersect <code>df2</code> at 155 to 200. <code>feature4</code> overlaps <code>df2</code> at 900 to 901.</p>

<p>What is the most efficient (in terms of runtime and RAM) to find the intersections?</p>

<p>EDIT: There is a Python package which does something similar here: <a href=""https://daler.github.io/pybedtools/intersections.html"" rel=""nofollow noreferrer"">https://daler.github.io/pybedtools/intersections.html</a></p>
","5269850","","5269850","","2019-07-18 21:56:43","2019-07-18 21:56:43","Using pandas, find the intersecting regions between two DataFrames?","<python><python-3.x><pandas><dataframe><merge>","1","9","","","","CC BY-SA 4.0","1"
"56786962","1","56789781","","2019-06-27 08:28:05","","0","118","<p>I am using the naturalearth_lowres pre-set map of geopandas and I need the area in km² of each country for further calculation. I have tried several solutions I found in the internet, but none of them worked for me.</p>

<p>One of he solutions that did not work is <a href=""https://gis.stackexchange.com/questions/218450/getting-polygon-areas-using-geopandas"">this one</a>. The values I get from this calculation are not fitting the real values at all (compared them with values from wikipedia).</p>

<p>I just need some sort of a formula for the area.</p>
","11587026","","","","","2019-06-27 11:11:54","I need to calculate the area of each country","<python-3.x><geopandas>","1","6","","","","CC BY-SA 4.0","1"
"56890407","1","","","2019-07-04 14:52:58","","1","117","<p>I would like to color code individual cells of a dataframe based on certain tolerance values, and I have tried multiple ways of doing it, but for some reason, I cannot figure out why Jupyter Notebook is not color coding my dataframe. </p>

<p>Pandas styling: <a href=""https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html</a></p>

<p>I have a following table and I'm willing to iterate over rows: <strong>[row, 1:6]</strong>. </p>

<pre><code>|Parameter| Method 1 | Method 2 | Method 3 | Method 4 | Method 5 | Weighing % |

|Parameter 1| **99.6     | 100      | 100      | 99.8     | 100**      | 25         |

|Parameter 2| **0.4      | 0        | 0        | 0.2      | 0**        | 5          |

|Parameter 3| **100      | 100      | 100      | 100      | 100**      | 5          |

|Parameter 4| **1.3      | 1.2      | 1.1      | 0.9      | 1.4**      | 2.5        |
</code></pre>

<p>For every Parameter there are tolerance values, to which the value in a cell should be compared. Based on the comparison, a certain color will be chosen for the cell.
The cell value and tolerance values:</p>

<pre><code>values_for_comparison = [cell value, tolerance_1st_min, tolerance_1st_max, tolerance_2nd_min, tolerance_2nd_max, tolerance_3rd_min, tolerance_3rd_max, tolerance_4th_min, tolerance_4th_max, tolerance_5th_min]
</code></pre>

<p>In main program:</p>

<pre class=""lang-py prettyprint-override""><code>o = 0
rows_length_table3_df = table3_df.shape[0]
p = 1
columns_length_table3 = all_table1s.shape[1]
q = 1

while o &lt; rows_length_table3_df:
    while q &lt; columns_length_table3:
        value = table3_df.iloc[o, p]
        values_for_comparison = [value,
                                 tolerance_values_conclusions_tables_df.iloc[o, 1],
                                 tolerance_values_conclusions_tables_df.iloc[o, 2],
                                 tolerance_values_conclusions_tables_df.iloc[o, 3],
                                 tolerance_values_conclusions_tables_df.iloc[o, 4],
                                 tolerance_values_conclusions_tables_df.iloc[o, 5],
                                 tolerance_values_conclusions_tables_df.iloc[o, 6],
                                 tolerance_values_conclusions_tables_df.iloc[o, 7],
                                 tolerance_values_conclusions_tables_df.iloc[o, 8],
                                 tolerance_values_conclusions_tables_df.iloc[o, 9]]
        table3_df.style.applymap(comparison_conclusions.colors_tables(values_for_comparison), subset = [table3_df.iloc[o, p]])
        p += 1
        q += 1
    p = 1
    q = 1
    o += 1
    values_for_comparison = [None] * 10

exec(""table3_df.to_csv(os.path.join(conclusions_folder_path, r'{0}_{1}_Visits_Conclusions_Statistics_Table3.csv'))"".format(name_of_the_reference_point_folder, total_number_of_visits_at_the_reference_point))
print(table3_df)
</code></pre>

<p>Defining a color for an individual cell (val) based on the tolerance values (<code>values_for_comparison list[1:]</code>):</p>

<pre class=""lang-py prettyprint-override""><code>def colors_tables(values_for_comparison):
    print('colors_tables')
    def colors_tables_by_val(val, values_for_comparison):
        print('colors_tables_by_val')
        if values_for_comparison[2] &lt; values_for_comparison[1]:
            if val &lt;= values_for_comparison[1] and val &gt;= values_for_comparison[2]:
                color = 'forestgreen'
            elif val &lt; values_for_comparison[3] and val &gt;= values_for_comparison[4]:
                color = 'greenyellow'
            elif val &lt; values_for_comparison[5] and val &gt;= values_for_comparison[6]:
                color = 'yellow'
            elif val &lt; values_for_comparison[7] and val &gt;= values_for_comparison[8]:
                color = 'orange'
            elif val &lt; values_for_comparison[9]:
                color = 'red'
            else:
                color = 'white'
            print(color)

            return 'background-color: {}'.format(color)

        else:
            if val &gt;= 0 and val &lt;= values_for_comparison[2]:
                color = 'forestgreen'
            elif val &gt; values_for_comparison[3] and val &lt;= values_for_comparison[4]:
                color = 'greenyellow'
            elif val &gt; values_for_comparison[5] and val &lt;= values_for_comparison[6]:
                color = 'yellow'
            elif val &gt; values_for_comparison[7] and val &lt;= values_for_comparison[8]:
                color = 'orange'
            elif val &gt; values_for_comparison[9]:
                color = 'red'
            else:
                color = 'white'
            print(color)

            return 'background-color: {}'.format(color)
    val = values_for_comparison[0]
    colors_tables_by_val(val, values_for_comparison)
    return colors_tables_by_val
</code></pre>

<p>The colors are not applied to any cells in main program, but the colors are printed though.</p>

<p>How should I properly write the code? And at least this part of the code:</p>

<pre class=""lang-py prettyprint-override""><code>table3_df.style.applymap(comparison_conclusions.colors_tables(values_for_comparison), subset = [table3_df.iloc[o, p]])
</code></pre>

<p><strong>UPDATE 20190708</strong></p>

<p>I changed the code to following, but still not getting any colors applied to the <code>table3_df</code>. The colors are printed though:</p>

<pre class=""lang-py prettyprint-override""><code>o = 0
rows_length_table3_df = table3_df.shape[0]
p = 1
columns_length_table3 = all_table1s.shape[1]
q = 1

while o &lt; rows_length_table3_df:
    while q &lt; columns_length_table3:
        values_for_comparison = [tolerance_values_conclusions_tables_df.iloc[o, 1],
                                 tolerance_values_conclusions_tables_df.iloc[o, 2],
                                 tolerance_values_conclusions_tables_df.iloc[o, 3],
                                 tolerance_values_conclusions_tables_df.iloc[o, 4],
                                 tolerance_values_conclusions_tables_df.iloc[o, 5],
                                 tolerance_values_conclusions_tables_df.iloc[o, 6],
                                 tolerance_values_conclusions_tables_df.iloc[o, 7],
                                 tolerance_values_conclusions_tables_df.iloc[o, 8],
                                 tolerance_values_conclusions_tables_df.iloc[o, 9]]
        value = table3_df.iloc[o, p]
        def colors_tables(value):
            val = value
            print('colors_tables')
            if values_for_comparison[1] &lt; values_for_comparison[0]:
                if val &lt;= values_for_comparison[0] and val &gt;= values_for_comparison[1]:
                    color = 'forestgreen'
                elif val &lt; values_for_comparison[2] and val &gt;= values_for_comparison[3]:
                    color = 'greenyellow'
                elif val &lt; values_for_comparison[4] and val &gt;= values_for_comparison[5]:
                    color = 'yellow'
                elif val &lt; values_for_comparison[6] and val &gt;= values_for_comparison[7]:
                    color = 'orange'
                elif val &lt; values_for_comparison[8]:
                    color = 'red'
                else:
                    color = 'white'
                print(color)

                return 'background-color: %s' % color

            else:
                if val &gt;= 0 and val &lt;= values_for_comparison[1]:
                    color = 'forestgreen'
                elif val &gt; values_for_comparison[2] and val &lt;= values_for_comparison[3]:
                    color = 'greenyellow'
                elif val &gt; values_for_comparison[4] and val &lt;= values_for_comparison[5]:
                    color = 'yellow'
                elif val &gt; values_for_comparison[6] and val &lt;= values_for_comparison[7]:
                    color = 'orange'
                elif val &gt; values_for_comparison[8]:
                    color = 'red'
                else:
                    color = 'white'
                print(color)

                return 'background-color: %s' % color
        table3_df.style.applymap(colors_tables(value), subset = (o, p))
        p += 1
        q += 1
    p = 1
    q = 1
    o += 1
    values_for_comparison = [None] * 9

exec(""table3_df.to_csv(os.path.join(conclusions_folder_path, r'{0}_{1}_Visits_Conclusions_Statistics_Table3.csv'))"".format(name_of_the_reference_point_folder, total_number_of_visits_at_the_reference_point))
print(table3_df)
</code></pre>
","11533953","","11533953","","2019-07-08 08:56:00","2019-07-08 08:56:00","Pandas color coding of individual dataframe cells based on multiple tolerance values with Python in Jupyter Notebook","<python><python-3.x><pandas><jupyter-notebook><styling>","0","0","","","","CC BY-SA 4.0","1"
"57127615","1","","","2019-07-20 18:41:56","","1","117","<p>In python I have a data set with these columns:</p>

<pre><code>ID_Order    ID_Customer ID_Item DateTime_CartFinalize   Amount_Gross_Order  city_name_fa    Quantity_item
</code></pre>

<p>It is for more than 200000 transactions. 
I am going to use association rules. At the first step, I have to Change the <code>DateTime_CartFinalize</code> column to timestamp format using <code>pd.Timestamp</code> function. Before that, ensure that the date format in your <code>digiasso.csv</code> file is in this <code>2/11/18 12:29 AM</code> format.
 Hint: use the aggregate function. 
???</p>
","11113048","","4354477","","2019-07-20 18:44:32","2019-07-20 19:28:11","how change the data column to timestamp format using pd.Timestamp function","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"56666313","1","56666475","","2019-06-19 11:10:37","","1","116","<p>I am trying to upgrade pandas code before it deprecates. The goal is to check whether there are duplicate IDs and then select the rows of said IDs.
The IDs are set as the index of my_data (the pandas Dataframe I am working with).
The following code: </p>

<pre><code>dups = my_data.index.get_level_values('ID').get_duplicates()
</code></pre>

<p>returns the following warning:</p>

<pre><code>FutureWarning: 'get_duplicates' is deprecated and will be removed in a    future release. You can use idx[idx.duplicated()].unique() instead
""""""Entry point for launching an IPython kernel.
</code></pre>

<p>And then when I run: </p>

<pre><code>duplicates = my_data.loc[dups]
</code></pre>

<p>It returns a pandas Dataframe containing all the duplicates on the ID level (which is my end goal).  </p>

<p>From my understanding, there is no method or attribute to <strong>pandas.Dataframe</strong> that is called idx. 
I tried using the following: </p>

<pre><code>dups = pd.Index(my_data).duplicated()
</code></pre>

<p>it returns a numpy ndarray containing bool values which I was unable to use later to load the duplicate rows in a separate DataFrame.</p>

<p>Any ideas?</p>
","3811918","","","","","2019-06-19 11:34:12","How to swich from depricated code while using get_duplicates()","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57295261","1","","","2019-07-31 16:24:50","","0","115","<p>I have a mysql database which stores thousands of stock OHLC data for 2 years. Data is read from MySQL in the form of pandas dataframes and then submitted to celery in large batch jobs which eventually lead to ""OOM command not allowed when used memory > 'maxmemory'"".</p>

<p>I have added the following celery config options. These options have allowed my script to run longer however redis inevitably reaches 2gb memory and celery throws OOM errors.</p>

<pre><code>result_expires = 30
ignore_result = True
worker_max_tasks_per_child = 1000
</code></pre>

<p>From the redis side I have tried playing with the maxmemory policy using both allkeys-lru and volatile-lru. Neither seem to make a difference.</p>

<p>When celery hits the OOM error the redis cli shows max memory usage and no keys?</p>

<pre><code># Memory
used_memory:2144982784
used_memory_human:2.00G
used_memory_rss:1630146560
used_memory_rss_human:1.52G
used_memory_peak:2149023792
used_memory_peak_human:2.00G
used_memory_peak_perc:99.81%
used_memory_overhead:2144785284
used_memory_startup:987472
used_memory_dataset:197500
used_memory_dataset_perc:0.01%
allocator_allocated:2144944880
allocator_active:1630108672
allocator_resident:1630108672
total_system_memory:17179869184
total_system_memory_human:16.00G
used_memory_lua:37888
used_memory_lua_human:37.00K
used_memory_scripts:0
used_memory_scripts_human:0B
number_of_cached_scripts:0
maxmemory:2147483648
maxmemory_human:2.00G
maxmemory_policy:allkeys-lru
allocator_frag_ratio:0.76
allocator_frag_bytes:18446744073194715408
allocator_rss_ratio:1.00
allocator_rss_bytes:0
rss_overhead_ratio:1.00
rss_overhead_bytes:37888
mem_fragmentation_ratio:0.76
mem_fragmentation_bytes:-514798320
mem_not_counted_for_evict:0
mem_replication_backlog:0
mem_clients_slaves:0
mem_clients_normal:2143797684
mem_aof_buffer:0
mem_allocator:libc
active_defrag_running:0
lazyfree_pending_objects:0
</code></pre>

<p>And there are zero keys?</p>

<pre><code>127.0.0.1:6379[1]&gt; keys *
(empty list or set)
</code></pre>

<p>When I run this same code in subsets of 200*5 requests (then terminate) everything runs successfully. Redis memory usage caps around 100mb and when the python process terminates all the memory usage drops as expected. This leads me to believe I could probably implement a handler to do 200*5 requests at a time however I suspect that the python process (my script) terminating is what is actually freeing memory in celery/redis...</p>

<p>I would like to avoid subsetting this and process everything in MySQL in one shot. About 5000 pandas dataframes * 5 tasks total.</p>

<p>I do not understand why the memory usage in redis continues to grow when I am forgetting all results immediately following retrieving them? </p>

<p>Here is an example for how this is done in my code:</p>

<pre><code>def getTaskResults(self, caller, task):
    #Wait for the results and then call back
    #Attach this Results object in the callback along with the data
    ready = False
    while not ready:
        if task.ready():
            ready = True
            data = pd.read_json(task.get())
            data.sort_values(by=['Date'], inplace=True)
            task.forget()
            return caller.resultsCB(data, self)
</code></pre>

<p>This is probably my ignorance with redis but if there are no keys how is it consuming all that memory, or how can I validate what is actually consuming that memory in redis?</p>

<p>Since I store the taskID of every call to celery in an object I have confirmed that trying to do a task.get after adding in task.forget throws an error. </p>
","8167973","","","","","2019-07-31 16:24:50","Redis memory usage continues to climb when using task.forget()","<python-3.x><pandas><redis><celery>","0","2","","","","CC BY-SA 4.0","1"
"56740485","1","56740618","","2019-06-24 16:17:45","","0","114","<p>Note for the suggested duplicate on this question: Categorical sorting does not work for this as it only uses a subset of the strings in the column for sorting. if you set this to a categorical index it will make all the non listed 'categories' /strings nulls.</p>

<p>Original Question:
I have a working example but I feel like there must be a better / more efficient way to calculate these results.</p>

<p>I have a large data frame of machine data where the order of events is not maintained properly within each timestamp. This looks like the input event column below. You can see the selected events have been reordered according to the event_order list within each timestamp. </p>

<p>Input is event.
Desired output is sorted_output event in last column. Horizontal lines added to show sorting is only within each timestamp block.</p>

<p>timestamps have been simplified to integers.
event names have been simplified as well. These are not the alphabet but full string names in the non example data.</p>

<p>Is there a more efficient way to do this? </p>

<pre><code>                  input      sorted_output
    timestamp     event      event
0           0      wer       wer   
_________________________________
1           1       up       dog
2           1      def       def
3           1      abc       abc
4           1      dog      fast
5           1      prq       prq
6           1      cde       cde
7           1     fast        up
8           1      bnm       bnm
_________________________________
9           2      ert       ert
10          2      and       and
11          2      ert       ert
12          2      ghj       ghj
13          2  streets      down
14          2     down   streets
_________________________________
15          3     runs       dog
16          3      dog      runs
17          3      ert       ert
18          3       up        up
19          3      dfg       dfg
20          3      prq       prq
</code></pre>

<p>Working code</p>

<pre><code>import pandas as pd

df = pd.DataFrame(
    [
        {'timestamp': 0, 'event': 'wer'},
        {'timestamp': 1, 'event': 'up'},
        {'timestamp': 1, 'event': 'def'},
        {'timestamp': 1, 'event': 'abc'},
        {'timestamp': 1, 'event': 'dog'},
        {'timestamp': 1, 'event': 'prq'},
        {'timestamp': 1, 'event': 'cde'},
        {'timestamp': 1, 'event': 'fast'},
        {'timestamp': 1, 'event': 'bnm'},
        {'timestamp': 2, 'event': 'ert'},
        {'timestamp': 2, 'event': 'and'},
        {'timestamp': 2, 'event': 'ert'},
        {'timestamp': 2, 'event': 'ghj'},
        {'timestamp': 2, 'event': 'streets'},
        {'timestamp': 2, 'event': 'down'},
        {'timestamp': 3, 'event': 'runs'},
        {'timestamp': 3, 'event': 'dog'},
        {'timestamp': 3, 'event': 'ert'},
        {'timestamp': 3, 'event': 'up'},
        {'timestamp': 3, 'event': 'dfg'},
        {'timestamp': 3, 'event': 'prq'},
    ]
)
df = df[['timestamp', 'event']]

# events to sort in order (they aren't actually alphabetical this is mock data)
events_to_sort = ['dog', 'runs', 'fast', 'up', 'and', 'down', 'streets']

# this method gleaned from here https://stackoverflow.com/questions/23482668/sorting-by-a-custom-list-in-pandas
sorter_index = dict(zip(events_to_sort, range(len(events_to_sort))))

# create a temporary rank column for sorting
df['sort_col'] = df['event'].map(sorter_index)

ev_ind = df.event.isin(events_to_sort)

# loop through each timestamp block
for time in df.timestamp.unique():
    # limit to only sortable events within the timestamp
    section_index = df.timestamp.eq(time) &amp; ev_ind
    df_temp = df.loc[section_index]

    if len(df_temp) &gt; 1:
        # if there is more than 1 sortable event tag sort and set the values back to the original df
        df.loc[section_index, 'event'] = df_temp.sort_values(by='sort_col')['event'].values

# drop temp sorting col
df = df.drop('sort_col', axis=1)
</code></pre>
","5597304","","5597304","","2019-06-24 17:08:43","2019-06-24 17:31:33","Efficient sorting of select rows within same timestamps according to custom order","<python><python-3.x><pandas><numpy><sorting>","2","1","","2019-07-01 13:59:26","","CC BY-SA 4.0","1"
"56786572","1","56786616","","2019-06-27 08:03:10","","2","113","<p>I wrote a function to determine the weekday. When I try to apply it to my data frame face the following error. </p>

<pre><code>data['date'].head() 

0    2016-01-01
1    2016-01-01
2    2016-01-01
3    2016-01-01
4    2016-01-01

def weekday_determination(col):
    year, month, day = (int(x) for x in col.split('-'))
    ans = datetime.date(year, month, day)
    return ans.strftime('%A')

data['week_day'] = data['date'].apply(weekday_determination, axis=1)
</code></pre>

<p>I face the following error:</p>

<blockquote>
  <p>TypeError: weekday_determination() got an unexpected keyword argument 'axis'</p>
</blockquote>
","11568585","","5276797","","2019-06-27 08:07:40","2019-06-27 08:07:40","How to apply a function on a column","<python-3.x><pandas><apply>","1","2","","2019-06-30 22:17:06","","CC BY-SA 4.0","1"
"57369483","1","57369593","","2019-08-06 05:33:46","","1","113","<p>I have a data set like this:-</p>

<pre><code>S.No.,Year of birth,year of death
1,  1,  5
2,  3,  6
3,  2,  -
4,  5,  7
</code></pre>

<p>I need to calculate population on till that years let say:-</p>

<pre><code>year,population
1   1
2   2
3   3
4   3
5   4
6   3
7   2
8   1
</code></pre>

<p>How can i solve it in pandas?
Since i am not good in pandas.
Any help would be appreciate.</p>
","7352806","","","","","2019-08-06 05:56:05","How can i calculate population in pandas?","<python-3.x><pandas><logic>","1","1","","","","CC BY-SA 4.0","1"
"57340425","1","","","2019-08-03 17:00:12","","4","113","<p>I have two columns in a dataframe. The first one contains a string in each row. The second contains a set of strings for each row. How can i check, for each row, whether the value from the first column is in the set of the second using pandas functions and that its efficient?</p>

<p>pd.DataFrame([np.random.randint(5, size=12), np.random.randint(5, size=(12,5))]).T</p>

<p><a href=""https://i.stack.imgur.com/UlhZB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UlhZB.png"" alt=""enter image description here""></a></p>

<p>How to check if the value from column 0 in the list of column 1</p>
","309457","","309457","","2019-08-04 02:50:32","2019-08-04 02:50:32","Check whether column value is in another column which values is a list","<python-3.x><pandas>","4","1","","","","CC BY-SA 4.0","1"
"49563833","1","49563977","","2018-03-29 19:39:31","","1","113","<p>I need to set a value of a cell in a pandas DataFrame that has a MultiIndex. Is there a way similar to pd.DataFrame.at[index, column] that I can use on a MultiIndex DataFrame.</p>

<pre><code>import pandas as pd
arrays = [[""a"", ""b"", ""c""], [""a"", ""b"", ""c""]]
multi_index = pd.MultiIndex.from_product(iterables=arrays, names=[""i"", ""k""])
partial_corr = pd.DataFrame(index=multi_index, columns=arrays[0])


partial_corr
Out[5]: 
       a    b    c
i k               
a a  NaN  NaN  NaN
  b  NaN  NaN  NaN
  c  NaN  NaN  NaN
b a  NaN  NaN  NaN
  b  NaN  NaN  NaN
  c  NaN  NaN  NaN
c a  NaN  NaN  NaN
  b  NaN  NaN  NaN
  c  NaN  NaN  NaN
</code></pre>

<p>In the DataFrame partial_corr, I'd like to be able to get/set the value of </p>

<pre><code>partial_corr.at[""a"", ""b"", ""b""]
</code></pre>

<p>where the first entry in .at[] is for index ""i"", the second entry is for index ""k"", and the third entry is for the column...similar to the way a single index DataFrame can return the value of df.at[""a"", ""b""] where the first entry is the index and the second entry is the column.</p>
","7890155","","","","","2018-03-29 19:48:59","Is there an equivalent to pd.DataFrame.at[index, column] for a MultiIndex DataFrame?","<python><python-3.x><pandas><dataframe><multi-index>","1","0","","","","CC BY-SA 3.0","1"
"56803454","1","56804089","","2019-06-28 08:28:06","","1","113","<p>Below is my piece of code.</p>

<pre><code>import numpy as np

filename1=open(f)
xf = np.loadtxt(filename1, dtype=float)
</code></pre>

<p>Below is my data file.</p>

<pre><code>0.14200E+02 0.18188E+01 0.44604E-03
0.14300E+02 0.18165E+01 0.45498E-03
0.14400E+02-0.17694E+01 0.44615E+03
0.14500E+02-0.17226E+01 0.43743E+03
0.14600E+02-0.16767E+01 0.42882E+03
0.14700E+02-0.16318E+01 0.42033E+03
0.14800E+02-0.15879E+01 0.41196E+03
</code></pre>

<p>as one can see there are negative values that take up the space between 2 values this causes numpy to give</p>

<pre><code>ValueError: Wrong number of columns at line 3
</code></pre>

<p>This is just small snippet of my code. I want to read this data using numpy or pandas. Any suggestion would be great. </p>

<p>Edit 1:</p>

<p>@ZarakiKenpachi I used your suggestion of sep=' |-' but it gives me extra 4th column with NaN values.</p>

<p>Edit 2:</p>

<p>@Serge Ballesta nice suggestion but all these are some kind of pre-processing. I want some kind of inbuild function to do this in pandas or numpy.</p>

<p>Edit 3:</p>

<p><strong>Important Note</strong> it should be noted that there also negative sign in 0.4373E-03</p>

<p>Thank-you</p>
","5202279","","5202279","","2019-06-28 09:42:12","2019-06-28 10:10:53","Reading irregular colunm data into python 3.X using pandas or numpy","<python><python-3.x><pandas><numpy>","3","8","1","","","CC BY-SA 4.0","1"
"57556490","1","57556947","","2019-08-19 12:12:12","","0","111","<p>I need to insert multiple new columns in between columns in a dataframe</p>

<p><b>Input dataframe:</b></p>

<pre><code>  PC GEO BL RL JanTOTAL  BL RL FebTOTAL
   A USA  1  1        2   1  1        2
   B IND  1  1        2   1  1        2
</code></pre>

<p><br><b> Expected Output dataframe </b></p>

<pre><code>PC GEO Jan-Month        BL RL JanTOTAL     Feb-Month        BL RL FebTOTAL 
A  USA  2019-01-01       1  1        2    2019-02-01         1  1       2
B  IND  2019-01-01       1  1        2    2019-02-01         1  1       2
</code></pre>
","11906428","","11884237","","2019-08-19 12:21:24","2019-08-19 12:38:36","How to insert a new column in between columns in dataframe","<excel><python-3.x><pandas>","1","0","","2019-08-19 12:39:33","","CC BY-SA 4.0","1"
"48707952","1","","","2018-02-09 14:23:26","","2","111","<p><a href=""https://i.stack.imgur.com/aWseB.png"" rel=""nofollow noreferrer"">df1</a>
<a href=""https://i.stack.imgur.com/SHo0t.png"" rel=""nofollow noreferrer"">df2</a></p>

<p>I am new with python, pandas and Stack Overflow, so I will appreciate any help. I have two panda dataframes, the first one is in ascending order(values from 0 to 100 in steps of 0.1), the second one has 26000 values from 2.3 to 38.5, in no order, some values are also repeated in that dataframe. What I am trying to do is, for each value in the first dataframe, find how many values in the second dataframe are less than or equal to that value in an efficient way.</p>

<p>My code below does it in 45 seconds, but I'd like it to be done in around 10.</p>

<p>Thanks in advance:</p>

<p>Code:</p>

<pre><code>def get_CDF2(df1, df2): 
    x=df1 #The first dataframe is already sorted in ascending order
    y = np.sort(df2, axis=0) #Sort the columns of the second dataframe in ascending order
    df_res = []  # keep the results here
    yi = iter(y)  # Use of an iterator to move over y
    yindex = 0
    flag = 0 #Flag, when set to 1 no comparison is done
    y_val = next(yi)
    for value in x:

        if flag &gt;=1:
            df_res.append(largest_ind)#append the number of y_val smaller than value
            #yindex+1
        else:
            # Search through y to find the index of an item bigger than value
            while (y_val) &lt;= (value) and yindex &lt; len(y)-1:
                y_val= next(yi) #Point at the next value in df2
                yindex += 1 #Keep track of how many y_val are smaller than value
            '''if for any value in df1 we iterate through the entire df2 and they are all less, that means
            the rest of values in df1 will have the same effect since df1 is in ascending other, so no need to iterate again,
            just set flag to 1'''
            if ((yindex==len(y)-1)) and ((y_val &lt;= float(value))): 
                flag=1
                largest_ind=yindex+1
                df_res.append(largest_ind)#append the number of y_val smaller than value
            else:
                df_res.append(yindex) #append the number of y_val smaller than value

    return df_res
</code></pre>

<p>df1:</p>

<pre><code>     0. ,   0.1,   0.2,   0.3,   0.4,   0.5,   0.6,   0.7,   0.8,
     0.9,   1. ,   1.1,   1.2,   1.3,   1.4,   1.5,   1.6,   1.7,
     1.8,   1.9,   2. ,   2.1,   2.2,   2.3,   2.4,   2.5,   2.6,
     2.7,   2.8,   2.9,   3. ,   3.1,   3.2,   3.3,   3.4,   3.5,
     3.6,   3.7,   3.8,   3.9,   4. ,   4.1,   4.2,   4.3,   4.4,
     4.5,   4.6,   4.7,   4.8,   4.9,   5. ,   5.1,   5.2,   5.3,
     5.4,   5.5,   5.6,   5.7,   5.8,   5.9,   6. ,   6.1,   6.2,
     6.3,   6.4,   6.5,   6.6,   6.7,   6.8,   6.9,   7. ,   7.1,
     7.2,   7.3,   7.4,   7.5,   7.6,   7.7,   7.8,   7.9,   8. ,
     8.1,   8.2,   8.3,   8.4,   8.5,   8.6,   8.7,   8.8,   8.9,
     9. ,   9.1,   9.2,   9.3,   9.4,   9.5,   9.6,   9.7,   9.8,
     9.9,  10. ,  10.1,  10.2,  10.3,  10.4,  10.5,  10.6,  10.7,
    10.8,  10.9,  11. ,  11.1,  11.2,  11.3,  11.4,  11.5,  11.6,
    11.7,  11.8,  11.9,  12. ,  12.1,  12.2,  12.3,  12.4,  12.5,
    12.6,  12.7,  12.8,  12.9,  13. ,  13.1,  13.2,  13.3,  13.4,
    13.5,  13.6,  13.7,  13.8,  13.9,  14. ,  14.1,  14.2,  14.3,
    14.4,  14.5,  14.6,  14.7,  14.8,  14.9,  15. ,  15.1,  15.2,
    15.3,  15.4,  15.5,  15.6,  15.7,  15.8,  15.9,  16. ,  16.1,
    16.2,  16.3,  16.4,  16.5,  16.6,  16.7,  16.8,  16.9,  17. ,
    17.1,  17.2,  17.3,  17.4,  17.5,  17.6,  17.7,  17.8,  17.9,
    18. ,  18.1,  18.2,  18.3,  18.4,  18.5,  18.6,  18.7,  18.8,
    18.9,  19. ,  19.1,  19.2,  19.3,  19.4,  19.5,  19.6,  19.7,
    19.8,  19.9,  20. ,  20.1,  20.2,  20.3,  20.4,  20.5,  20.6,
    20.7,  20.8,  20.9,  21. ,  21.1,  21.2,  21.3,  21.4,  21.5,
    21.6,  21.7,  21.8,  21.9,  22. ,  22.1,  22.2,  22.3,  22.4,
    22.5,  22.6,  22.7,  22.8,  22.9,  23. ,  23.1,  23.2,  23.3,
    23.4,  23.5,  23.6,  23.7,  23.8,  23.9,  24. ,  24.1,  24.2,
    24.3,  24.4,  24.5,  24.6,  24.7,  24.8,  24.9,  25. ,  25.1,
    25.2,  25.3,  25.4,  25.5,  25.6,  25.7,  25.8,  25.9,  26. ,
    26.1,  26.2,  26.3,  26.4,  26.5,  26.6,  26.7,  26.8,  26.9,
    27. ,  27.1,  27.2,  27.3,  27.4,  27.5,  27.6,  27.7,  27.8,
    27.9,  28. ,  28.1,  28.2,  28.3,  28.4,  28.5,  28.6,  28.7,
    28.8,  28.9,  29. ,  29.1,  29.2,  29.3,  29.4,  29.5,  29.6
</code></pre>

<p>df2:</p>

<pre><code>0         12.993
1         12.054
2         21.957
3         10.917
4         33.890
5         10.597
6         22.911
7          7.431
8         10.437
9         19.165
10        12.169
11        14.847
12        10.093
13        10.795
14        14.419
15        27.199
16        15.045
17        12.764
18         7.766
19        18.066
20        10.254
21        16.922
22         7.011
23        10.322
24        11.619
25        25.719
26        18.142
27        14.557
28        26.367
29        13.443
30        17.318
31        10.971
32         6.073
33        20.050
34        11.863
35        25.619
36        18.326
37        30.830
38        13.130
39        11.734
40        14.457
41        22.659
42        16.479
43        17.845
44        23.712
45        16.670
46        10.322
47        16.250
48        20.920
49        17.479
50        15.526
51        15.732
52        19.836
53        10.513
54        24.818
55        10.933
56        14.785
57        25.253
58        15.732
59        14.290
60        23.979
61        24.788
62        12.420
63        21.324
64         9.658
65        24.307
66        17.601
67        12.352
68        18.089
69        23.353
70        12.718
71        18.707
72         9.147
73        17.494
74         8.743
75        22.407
76        16.227
77        15.396
78        16.807
79        26.733
80        14.084
81        19.516
82        15.106
83        21.187
84        13.008
85        13.618
86        16.266
87        19.706
88         6.591
89        14.999
90        16.449
91        18.883
92        15.243
93        15.976
94        18.242
95        16.662
96         6.691
97        16.952
98        25.940
99        23.018
100       29.365
101       14.564
102       15.625
103        9.727
104        7.652
105       12.726
106        7.263
107       19.943
108       17.540
109        7.469
110       10.360
111       17.898
112       20.393
113        7.011
114       15.999
115       12.985
116       16.624
117       18.753
118       12.520
119       13.488
120       17.959
121       16.433
122       14.518
123       12.909
124       19.752
125        9.277
126       25.566
127       19.272
128       10.360
129       22.148
130       20.294
131       18.402
132       17.631
133       17.341
134       13.672
135       19.600
136       20.653
137       15.999
138       15.480
139       30.655
140       15.426
141       16.067
142       29.838
143       13.099
144       12.184
145       15.693
146       26.031
147       16.052
148        8.087
149       16.754
150       17.029
151       16.601
152        9.956
153       20.363
154       11.215
155       15.106
156       13.809
157       23.178
158       21.484
159       13.359
160       31.860
161       14.564
162       19.737
163       19.424
164       29.556
165       15.678
166       22.148
167       28.389
168       21.309
169       22.262
170       11.314
171        8.018
172       24.551
173       14.740
174       15.716
175       24.269
176       20.042
177       15.968
178       11.337
179       27.618
180       22.522
181       19.066
182        9.323
183       20.622
184       13.092
185       15.464
186       21.171
187       11.604
188       19.050
189       15.823
190       33.859
191       15.106
192       13.549
193       17.296
194       13.740
195       12.054
196       10.955
197       21.164
198       14.427
199        9.719
200       12.176
201        9.742
202       21.278
203       20.515
204       18.265
205        9.666
206       13.870
207       15.968
208       13.313
209       16.517
210       18.417
211       15.419
212       20.523
213       15.655
214       26.977
215       13.084
216       31.349
217       29.854
218       13.008
219       11.306
220       22.384
221       20.798
222       17.433
223       12.916
224       11.284
225       20.248
226        9.803
227       10.376
228        9.315
229       14.976
230       16.327
231        9.590
232       16.830
233       23.979
234       11.558
235       13.183
236       18.776
237       20.416
238        9.163
239       10.345
240       28.252
241       22.888
242       20.538
243        6.912
244       24.040
245        8.682
246       31.929
247       14.908
248       19.195
249       17.112
250       18.379
251       15.869
252       13.794
253       14.129
254       12.458
255       10.795
256       25.291
257       26.382
258       20.881
</code></pre>
","8418091","","3877338","","2018-02-09 17:11:32","2018-02-10 09:01:32","How do I compare values in two dataframe in an efficient way","<python-3.x><pandas><jupyter-notebook>","1","6","","","","CC BY-SA 3.0","1"
"40694073","1","","","2016-11-19 14:35:04","","0","110","<p>How do I implement the syntax for filtering dataframes in Pandas? (<code>df[df.column1 &gt; someValue]</code>) </p>

<p>I am trying to make a class that have the same syntax of Pandas when filtering dataframes. </p>

<p>How do I replicate the syntax for a Dataframe <code>df = DataFrame(someData)</code>  like this one:</p>

<pre><code>df[df.column1 &gt; someValue]
</code></pre>

<p>I implemented the methods <code>__getattr__</code> and <code>__getitem__</code> for the syntaxes of </p>

<pre><code>df.column1 
df['column1']
</code></pre>

<p>But I don't know how to link both together. Also, I could not find the function to copy from Pandas code. </p>

<p>Either an implementation to this problem or the reference to the function in Pandas would be of great help. </p>

<p><strong>Edit</strong>:(Solution)</p>

<p>Following the hint on the answers I implemented the <code>__getitem__</code> function as follows:</p>

<pre><code>from tier tools import compress

def __getitem__(self, name):
    """"""Get items with [ and ]
    """"""
    #If there is no expression, return a column
    if isinstance(name, str):
      return self.data[name]

    #if there was an expression return the dataframe filtered
    elif isinstance(name, list):
      ind = list(compress(range(len(name)), name))
      temp = DataFrame([[self.data[c].values[i] 
                            for i in ind] 
                           for c in self.columns],
                           columns=self.columns)
      return temp
</code></pre>

<p>Note that I also had to implement the comparison methods for my column class (Series). 
The full code can be seen <a href=""https://github.com/mauhcs/pagode/blob/master/pagode.py"" rel=""nofollow"">here</a>.</p>
","1494511","","1494511","","2016-11-20 14:00:12","2016-11-20 14:00:12","How to replicate Pandas syntax? (To filter data frames)","<python-3.x><pandas><syntax><pythonista>","2","0","","","","CC BY-SA 3.0","1"
"49824804","1","","","2018-04-13 20:39:13","","5","110","<p>Say I create an empty dataframe:</p>

<pre><code>df = pd.DataFrame()
</code></pre>

<p>and I add a dict via <code>df.append()</code>:</p>

<pre><code>df.append({'A': 'foo', 'B': 'bar'}, ignore_index=True)
</code></pre>

<p>This gives me the intended result of </p>

<pre><code>     A    B
0  foo  bar
</code></pre>

<p>However, if there are any booleans in the dict values, i.e.,</p>

<pre><code>df.append({'A': True, 'B': False}, ignore_index=True)
</code></pre>

<p>The booleans are converted into floats.</p>

<pre><code>     A    B
0  1.0  0.0
</code></pre>

<p>Why this is happening / how can I prevent this conversion? I'd prefer not do anything to the finished dataframe if possible (i.e., prefer not to coerce from float back to boolean).</p>

<p><strong>EDIT</strong>: found my own solution, but still would like to know why the above behavior is happening. My solution is:</p>

<pre><code>df.append(pd.DataFrame.from_dict({'A': True, 'B': False}, orient='index').T, ignore_index=True)
</code></pre>

<p>Which gives the desired </p>

<pre><code>      A      B
0  True  False
</code></pre>
","7327411","","7327411","","2018-04-13 20:44:50","2018-04-13 23:02:16","df.append() with dicts converts booleans to 1s and 0s","<python><python-3.x><pandas>","1","2","1","","","CC BY-SA 3.0","1"
"41109082","1","41109092","","2016-12-12 20:34:54","","1","109","<p>I have a dataframe as below</p>

<pre><code>           0         1      2         3         4         5
    0  0.428519  0.000000  0.0  0.541096  0.250099  0.345604
    1  0.056650  0.000000  0.0  0.000000  0.000000  0.000000
    2  0.000000  0.000000  0.0  0.000000  0.000000  0.000000
    3  0.849066  0.559117  0.0  0.374447  0.424247  0.586254
    4  0.317644  0.000000  0.0  0.271171  0.586686  0.424560
</code></pre>

<p>I would like to modify it as below </p>

<pre><code>    0      0     0.428519
    0      1     0.000000
    0      2     0.0
    0      3     0.541096
    0      4     0.250099
    0      5     0.345604
    1      0     0.056650
    1      1     0.000000
    ........
</code></pre>
","3988268","","","","","2016-12-12 20:41:57","Pandas Modify Dataframe","<python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"57370844","1","","","2019-08-06 07:20:57","","0","108","<p>I currently trying to extract several dataframes from a dictionary. The problem is, that the number of dataframes will vary, sometimes I'll have two dataframes in there and sometimes 30.</p>

<p>At the beginning I create a dictionary (dict_of_exceptions) from a dataframe (exceptions_df). In this dictionary I'll have several dataframes depending on how many different 'Source Wells' I have. With the current code I can extract the first dataframe from the dictionary which is j:</p>

<pre class=""lang-py prettyprint-override""><code>dict_of_exceptions = {k: v for k, v in exceptions_df.groupby('Source Well') }

print (dict_of_exceptions)

for k in dict_of_exceptions.keys(): 
    j = dict_of_exceptions[k]
</code></pre>

<p>Could someone help me modify the last line to go trough the dictionary and extract each dataframe (and name them like the corresponding key)?</p>
","11887908","","10140310","","2019-08-06 07:33:05","2019-08-06 09:44:26","How to extract several dataframes from dictionary","<python-3.x><pandas><dataframe><dictionary><key>","1","3","","","","CC BY-SA 4.0","1"
"57552670","1","","","2019-08-19 08:04:40","","3","107","<p>I am pre-processing data for analytics. But I am kind of stuck with the pre-processing.</p>

<p><strong>My data (5 data rows, see the numbers on the left side)</strong> </p>

<pre><code>1   ""#3,185 in Beauty &amp;amp; Personal Care (See Top 100 in Beauty &amp;amp; Personal Care)
.zg_hrsr { margin: 0; padding: 0; list-style-type: none; }
.zg_hrsr_item { margin: 0 0 0 10px; }
.zg_hrsr_rank { display: inline-block; width: 80px; text-align: right; }

    #11
    inÂ Men's Foil Shavers

    #11
    inÂ Men's Foil Shavers
    ""
2   ""#874 in Beauty &amp;amp; Personal Care (See Top 100 in Beauty &amp;amp; Personal Care)

.zg_hrsr { margin: 0; padding: 0; list-style-type: none; }
.zg_hrsr_item { margin: 0 0 0 10px; }
.zg_hrsr_rank { display: inline-block; width: 80px; text-align: right; }

    #6
    inÂ Men's Foil Shavers


    #6
    inÂ Men's Foil Shavers
    ""
3   ""#266 in Beauty &amp;amp; Personal Care (See Top 100 in Beauty &amp;amp; Personal Care)

.zg_hrsr { margin: 0; padding: 0; list-style-type: none; }
.zg_hrsr_item { margin: 0 0 0 10px; }
.zg_hrsr_rank { display: inline-block; width: 80px; text-align: right; }

    #1
    inÂ Men's Foil Shavers

    #1
    inÂ Men's Foil Shavers
    ""
4   ""#652 in Beauty &amp;amp; Personal Care (See Top 100 in Beauty &amp;amp; Personal Care)

.zg_hrsr { margin: 0; padding: 0; list-style-type: none; }
.zg_hrsr_item { margin: 0 0 0 10px; }
.zg_hrsr_rank { display: inline-block; width: 80px; text-align: right; }

    #3
    inÂ Men's Foil Shavers


    #3
    inÂ Men's Foil Shavers
    ""
5   ""#124,111 Paid in Kindle Store (See Top 100 Paid in Kindle Store)

.zg_hrsr { margin: 0; padding: 0; list-style-type: none; }
.zg_hrsr_item { margin: 0 0 0 10px; }
.zg_hrsr_rank { display: inline-block; width: 80px; text-align: right; }

    #2
    inÂ Borneo Travel Guides

    ""
</code></pre>

<p>I want to have this data in a 'Tidy format' for data analysis. </p>

<ul>
<li>I need the numbers after the hashtag ('#')</li>
<li>I need the corresponding category (e.g.: in Beauty, Men's foil Shavers </li>
</ul>

<p>Optimal would be an dynamical solution, since row 5 (for example) is adding new categories (e.g.:Paid in Kindle Store, inÂ Borneo Travel Guides). </p>

<p><strong>Desired outcome:</strong>  </p>

<pre><code> Beauty &amp;amp        Men's Foil Shavers  Paid in Kindle store   Â Borneo Travel Guides
3,185              11                      nan                     nan
  874               6                      nan                     nan
  266               1                      nan                     nan
  652               3                      nan                     nan
  nan             nan                      124,111                 2
</code></pre>

<p>You will see that the difficulty comes in with row 5, where new categories are introduced.  </p>

<p><strong>What have I tried?</strong></p>

<p>I tried to split the column after the '#', and take the first X characters.  </p>

<pre><code>x = df['Column'].str.split('#', expand=True).rename(columns=lambda x: f""Column_{x+1}"")
df = pd.concat([df, x], axis=1)
</code></pre>

<p>The result is: </p>

<ol>
<li>Multiple columns, containing the numbers and categories (e.g.: 300 in Beauty)</li>
<li>Unfortunately, the columns have mixed values (e.g.: 300 in Beauty, 200 in inÂ Borneo Travel Guides). </li>
<li>This means that I cannot process the data properly, since the columns containing multiple categories. </li>
</ol>

<p><strong>Future Steps:</strong> The only thing that I can think of, is 'hardcode' the categories, e.g.: </p>

<ul>
<li>Column Beauty = If Beauty in column_X, then take the number, else NAN. (I guess that this will lead to a columns Beauty, containing only numbers if it is in the category.  </li>
</ul>

<p>Hope that someone can help me! </p>

<p><strong>Many thanks in advance</strong> </p>
","5917999","","5917999","","2019-08-20 13:40:30","2019-08-20 13:40:30","How can I (dynamically) clean my pandas column, with making every class as new column with the corresponding values?","<python><python-3.x><pandas><numpy>","0","0","2","","","CC BY-SA 4.0","1"
"57654372","1","57655409","","2019-08-26 08:31:36","","5","107","<p>EDITED AS PER COMMENTS</p>

<p><strong>Background:</strong> Here is what the current dataframe looks like. The row labels are information texts in original excel file. But I hope this small reproduction of data will be enough for a solution? Actual file has about 100 columns and 200 rows.</p>

<p>Column headers and Row #0 values are repeated with pattern shown below -- except the <code>Sales</code>  or <code>Validation</code> text changes at every occurrence of column with an existing title.</p>

<p>One more column <em>before</em> sales with text in each row. Mapping of Xs done for this test. Unfortunately, found no elegant way of displaying text as part of output below. </p>

<pre><code> Sales Unnamed: 2  Unnamed: 3  Validation Unnamed: 5 Unnamed: 6
0       Commented  No comment             Commented  No comment                                   
1     x                                             x                        
2                            x          x                                                
3                x                                             x             
</code></pre>

<p><strong>Expected Output</strong>: Replacing the X with 0s, 1s and 2s depending on which column they are in (Commented / No Comment)</p>

<pre><code> Sales Unnamed: 2  Unnamed: 3  Validation Unnamed: 5 Unnamed: 6
0       Commented  No comment             Commented  No comment                                   
1     0                                            1                        
2                            2          0                                                
3                1                                             2  
</code></pre>

<p><strong>Possible Code</strong>: I assume the loop would look something like this:</p>

<pre><code>while in row 9:
    if column value = ""commented"":

        replace all ""x"" with 1

    elif row 9 when column valkue = ""no comment"":

        replace all ""x"" with 2

    else:

        replace all ""x"" with 0
</code></pre>

<p>But being a python novice, I am not sure how to convert this to a working code. I'd appreciate all support and help.</p>
","11900800","","11900800","","2019-08-26 09:42:27","2019-08-26 11:03:09","Replace values in multiple untitled columns to 0, 1, 2 depending on column","<python-3.x><pandas><dataframe>","1","14","","","","CC BY-SA 4.0","1"
"57102609","1","","","2019-07-18 21:04:12","","0","107","<p>Im excluding rows from my df that fill certain conditions </p>

<pre><code>df[~((df['Wood_type'] == 'pine') &amp; (df['wood_size'] == 20))] 
</code></pre>

<p>I would like to also exclude, the numbers that start with 0 in the column 'Serial'</p>

<pre><code>df[~((df['Wood_type'] == 'pine') &amp; (df['wood_size'] == 20) &amp; (df['Serial'] == range(0) == 0))]
</code></pre>

<p>I tried the above, no result.</p>
","10906075","","2988730","","2019-07-18 21:45:47","2019-07-18 21:45:47","How to check if a serial number starts with 0","<python><python-3.x><pandas>","1","10","","","","CC BY-SA 4.0","1"
"56798998","1","","","2019-06-27 22:10:16","","1","107","<p>I have a simple code below to grab Financial information from Yahoo Finance.</p>

<p>In the loop shown below, at the end of the loop, I try to concat the Pandas Dataframe together.</p>

<p>When I try to print output, there are easily some outputs missing.  The 2nd item on the list is ""VOO"" and I should be getting data there.  I can't figure out what I'm doing wrong.  Please help.  Thanks.</p>

<hr>

<pre><code>import yfinance as yf
import pandas as pd

urls=[
'AAPL',
'voo',
'msft'
    ]

tickerArray2 = pd.DataFrame()

for url in urls:
    tickerTag = yf.Ticker(url)
    tickerArray = tickerTag.actions
    tickerArray['ticker'] = url
    pdArray = pd.DataFrame(tickerArray)
    tickerArray2 = pd.concat([tickerArray2,tickerArray])

print(tickerArray2)
</code></pre>

<hr>

<p>Output should look like this: 
(for some reason, I'm missing a bunch of data, esp the one with VOO)</p>

<pre><code>            Dividends  Stock Splits ticker
Date                                      
1987-05-11    0.00214           0.0   AAPL
1987-06-16    0.00000           2.0   AAPL
1987-08-10    0.00214           0.0   AAPL
...               ...           ...    ...
2012-02-14    0.20000           0.0   VOO
2012-05-15    0.20000           0.0   VOO
2012-08-14    0.20000           0.0   VOO
...               ...           ...    ...
2012-02-14    0.20000           0.0   msft
2012-05-15    0.20000           0.0   msft
2012-08-14    0.20000           0.0   msft
</code></pre>
","11660600","","9609447","","2019-06-27 22:12:24","2019-11-11 13:04:10","Missing Data in the dataframe when I use pandas concat","<python-3.x><pandas>","1","6","","","","CC BY-SA 4.0","1"
"56853092","1","56853268","","2019-07-02 13:01:09","","4","107","<p>I have 2 dataframes :</p>

<pre><code>ID             word
1              srv1
2              srv2
3              srv1
4              nan
5              srv3
6              srv1
7              srv5
8              nan
</code></pre>

<pre><code>ID             word
1              nan
2              srv12
3              srv10
4              srv8
5              srv4
6              srv7
7              nan
8              srv9
</code></pre>

<p>What I need is to merge thoses 2 dataframes on ID and combine the column word to get :</p>

<pre><code>ID             word
1              srv1 
2              srv2 , srv12
3              srv1 , srv10
4              srv8
5              srv3 , srv4
6              srv1 , srv7
7              srv5
8              srv9
</code></pre>

<p>With the following code</p>

<pre class=""lang-py prettyprint-override""><code>merge = pandas.merge(df1,df2,on=""ID"",how=""left"")
merge[""word""] = merge[word_x] + "" , "" + merge[""word_y""]
</code></pre>

<p>I am getting:</p>

<pre><code>ID             word
1              nan 
2              srv2 , srv12
3              srv1 , srv10
4              nan
5              srv3 , srv4
6              srv1 , srv7
7              nan
8              nan
</code></pre>

<p>Which it is not the correct solution.</p>
","9501913","","","","","2019-07-02 13:20:50","Merge and combine 2 columns of different dataframe","<python><python-3.x><pandas><dataframe>","3","2","0","","","CC BY-SA 4.0","1"
"57865368","1","57865557","","2019-09-10 06:45:54","","0","106","<p>I created a toy data set to try and figure out how df.groupby works. </p>

<pre><code>df = pd.DataFrame({""A"": [1,2,3,1,2,3,1,2,3,1,2,3], ""B"": ['m','f','m','m','f','m','f','f','f','m','f','m'],
                  'target': [0,0,0,1,1,0,1,0,0,1,1,0]})
</code></pre>

<p>My 'target' variable has just 2 levels, 0 and 1. I can get a count total of each for variable <code>'B'</code>, like this:</p>

<pre><code>b = df.groupby('B').target.value_counts()
</code></pre>

<p>And the output looks like this:</p>

<pre><code>    B  target
f  0         3
   1         3
m  0         4
   1         2
Name: target, dtype: int64
</code></pre>

<p>But <code>'B'</code> categorical. What I would like to do is, for each level of <code>'B'</code>, get the ratio </p>

<blockquote>
  <p>(value_counts for target=1)/(value_counts for target=0)</p>
</blockquote>

<p>For example, </p>

<ul>
<li>for B=f, I need 3/3</li>
<li>for B=m, I need 2/4.</li>
</ul>
","4744845","","8353711","","2019-09-10 06:51:52","2019-09-10 07:04:32","Ratio of value_counts for a binary target variable grouped by another column","<python><python-3.x><pandas><pandas-groupby>","2","0","1","","","CC BY-SA 4.0","1"
"57259813","1","57260107","","2019-07-29 18:48:37","","0","106","<p>My dataset <code>df</code> look like this:</p>

<pre><code>date           high
2018-01-01     -1
2018-01-02     1
2018-01-03     -2
2018-01-04     0
...., ....
2018-12-31     1
</code></pre>

<p>Where,</p>

<pre><code>-2 &gt;= high &lt;= 2
</code></pre>

<p><code>high</code> is always between <code>-2</code> and <code>2</code></p>

<p>I want to sort the value of <code>high</code> in the following pattern:</p>

<p>To start, Group all <code>0</code> and sort by date and so on for other values. </p>

<p>Sort the <code>high</code> value in the following order:</p>

<pre><code>0
1
-1
2
-2
</code></pre>

<p>It would be best if it's flexible enough that I can change the order if required.</p>

<p>I know how to sort in <code>asc</code> or <code>desc</code> by doing this:</p>

<pre><code>df.sort_values(by='high', ascending=False)
</code></pre>

<p>Could you please help me solve how do I sort using predetermined values?</p>
","9161607","","9161607","","2019-07-29 19:07:40","2019-07-29 19:12:56","How to sort with predetermined order in Pandas","<python-3.x><pandas><dataframe>","1","7","","2019-07-29 19:35:56","","CC BY-SA 4.0","1"
"56977916","1","56978052","","2019-07-10 20:07:46","","1","106","<p>I have some measurements organized in <em>*.csv</em> files as follows:</p>

<pre><code>m_number,value
0,0.154
1,0.785
…
55,0.578
NaN,NaN
0,1.214
1,0.742
… 
</code></pre>

<p>So there is always a set of <em>x</em> measurements (<em>x</em> should be constant inside a single file but it's not guaranteed and I have to check this number) separated by a <em>NaN</em> line.</p>

<p>After reading the data into a dataframe, I want to reorganize it for later usage:</p>

<pre><code>  m_number    value 1      value 2      value 3      value 4  
0        0      0.154        0.214        0.229        0.234       
1        1      0.785        0.742        0.714        0.771
...
55      55      0.578        0.647        0.597        0.623
</code></pre>

<p>Each set of measurements should be one column.</p>

<p>Here's a snippet of the code:</p>

<pre class=""lang-py prettyprint-override""><code>split_index = df.index[df_benchmark['id'].isnull()]
df_sliced = pd.DataFrame()
for i, index in enumerate(split_index):
    if i == 0:
        df_sliced = df.loc[0:index - 1].copy()
    else:
        #ToDo: Rename first column to 'value 1' if more than 1 measurement
        temp = df['value'].loc[0:index - 1].copy()
        temp.reset_index(drop=True, inplace=True)
        df_sliced['value '+str(i)] = temp
    df.drop(df.index[0:index - split_index[i - 1]], inplace=True)
</code></pre>

<p>The code works, but I do not like my current approach. So I'm asking if there's a better and more elegant solution for this problem.</p>

<p>Best,
Julz</p>
","7353600","","1478537","","2019-07-11 13:19:25","2019-08-24 14:22:02","Merge multiple measurements into a pandas dataframe","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"33068463","1","","","2015-10-11 18:17:02","","3","106","<p>I have the following db, I want to group PAS_DATE for a range of 3 days, for example form 2015-03-06 to 2015-03-09</p>

<pre><code>PAS_DATE    RED_DATE       TOT
2015-03-06  2015-03-07     2
            2015-03-17    14
            2015-12-22     1
2015-03-07  2015-03-08     3
            2015-03-19     6
            2015-10-14     2
            2015-12-07     1
2015-03-08  2015-09-16     8
2015-03-09  2015-03-09     7
            2015-03-15     6
            2015-03-18     8
            2015-04-04    15
            2015-04-12    19
            2015-05-04    44
            2015-08-17     5
            2015-09-09    13
            2015-12-06     3
            2015-12-13     3
2015-03-10  2015-03-10     7
</code></pre>

<p>The output should be :</p>

<pre><code>PAS_DATE    RED_DATE      TOT
2015-03-09  2015-03-07     2
            2015-03-17    14
            2015-12-22     1
            2015-03-08     3
            2015-03-19     6
            2015-10-14     2
            2015-12-07     1
            2015-09-16     8
            2015-03-09     7
            2015-03-15     6
            2015-03-18     8
            2015-04-04    15
            2015-04-12    19
            2015-05-04    44
            2015-08-17     5
            2015-09-09    13
            2015-12-06     3
            2015-12-13     3
2015-03-12  2015-03-10     7
</code></pre>

<p>Is there a way to perform this code without iterations in Pandas or Python?</p>
","3333155","","","","","2015-10-12 17:29:59","Grouping a range of dates in Python","<python><python-2.7><python-3.x><pandas>","1","5","","","","CC BY-SA 3.0","1"
"48622263","1","48625387","","2018-02-05 12:08:40","","1","105","<p>I have a dataframe 'df' which contains patient data. I want to create a network of patient movement from that data. The data looks like the following:</p>

<pre><code>ID      start_date          stop_date           ward               bed
11111   15/12/2015 13:42    20/01/2016 12:55    Hematology         537-1
11111   30/10/2015 19:40    14/12/2015 16:44    ICU                184-2
11111   14/12/2015 16:44    15/12/2015 13:42    Internal Medicine  537-1
11127   05/11/2015 12:49    11/11/2015 14:42    Anestesiology      304
11127   11/11/2015 14:42    11/11/2015 16:12    Anestesiology      348-2
11127   04/11/2015 12:07    05/11/2015 12:49    General surgery    325-3
11127   11/11/2015 16:12    18/11/2015 21:24    General surgery    348-2
11127   18/11/2015 21:24    02/01/2016 06:45    ICU                183-5
11132   06/11/2015 17:24    30/11/2015 18:11    Internal Medicine  528-2
11132   30/11/2015 18:11    02/12/2015 17:04    Cardiology         539-1
11132   02/12/2015 17:04    03/12/2015 20:40    Internal Medicine  557-1
11132   03/12/2015 20:40    11/01/2016 18:00    Internal Medicine  536-1
</code></pre>

<p>It has separate columns for patient ID, start date, stop date, ward, bed. As you see in the data, a patient ID is appearing multiple times and I want to look at the entries related to every patient and also start dates are not properly sorted out. I used groupby in pandas to group entries related to patient ID and also sorted out based on start date:</p>

<pre><code>grouped = df.sort_values(['ID','start_date'],ascending=True).groupby('ID')
</code></pre>

<p>This works fine but the next things is I wanted to create a directed edge list (or an adjacency matrix by wards) based on wards. For example, patient ID 11111 was first admitted to ICU, then moved to internal medicine and then to Hematology and to get an edge list like this. </p>

<pre><code>Number     From                    To                            
1          ICU                     Internal Medicine             
2          Internal Medicine       Hematology                     
3          General surgery         Anestesiology                 
4          Anestesiology           Anestesiology                 
5          Anestesiology           General surgery               
6          General surgery         ICU                           
7          Internal Medicine       Cardiology                    
8          Cardiology              Internal Medicine             
9          Internal Medicine       Internal Medicine             
</code></pre>

<p>After creating the full edge list, I wanted to see how many patients in total move from (let's say) ICU to Internal Medicine and count the entries from ICU to Internal Medicine and use that as a weight for network plot. Any suggestion, how could I do that in Pandas? I tried to loop over grouped data using ""for name, group in grouped:"" and <code>print(group['ward'])</code> shows the ward entries for every patient but I cannot write this information to another dataframe or list. Any help will be extremely appreciated. Thanks.      </p>

<p>Dataset:</p>

<pre><code>{'ID': {0: 11111,
  1: 11111,
  2: 11111,
  3: 11127,
  4: 11127,
  5: 11127,
  6: 11127,
  7: 11127,
  8: 11132,
  9: 11132,
  10: 11132,
  11: 11132},
 'bed': {0: '537-1',
  1: '184-2',
  2: '537-1',
  3: '304',
  4: '348-2',
  5: '325-3',
  6: '348-2',
  7: '183-5',
  8: '528-2',
  9: '539-1',
  10: '557-1',
  11: '536-1'},
 'start_date': {0: '15/12/2015 13:42',
  1: '30/10/2015 19:40',
  2: '14/12/2015 16:44',
  3: '05/11/2015 12:49',
  4: '11/11/2015 14:42',
  5: '04/11/2015 12:07',
  6: '11/11/2015 16:12',
  7: '18/11/2015 21:24',
  8: '06/11/2015 17:24',
  9: '30/11/2015 18:11',
  10: '02/12/2015 17:04',
  11: '03/12/2015 20:40'},
 'stop_date': {0: '20/01/2016 12:55',
  1: '14/12/2015 16:44',
  2: '15/12/2015 13:42',
  3: '11/11/2015 14:42',
  4: '11/11/2015 16:12',
  5: '05/11/2015 12:49',
  6: '18/11/2015 21:24',
  7: '02/01/2016 06:45',
  8: '30/11/2015 18:11',
  9: '02/12/2015 17:04',
  10: '03/12/2015 20:40',
  11: '11/01/2016 18:00'},
 'ward': {0: 'Hematology',
  1: 'ICU',
  2: 'Internal Medicine',
  3: 'Anestesiology',
  4: 'Anestesiology',
  5: 'General surgery',
  6: 'General surgery',
  7: 'ICU',
  8: 'Internal Medicine',
  9: 'Cardiology',
  10: 'Internal Medicine',
  11: 'Internal Medicine'}}
</code></pre>
","9316299","","4829258","","2018-02-05 15:05:51","2018-02-06 15:14:22","writing a column data of a groupby object to another dataframe","<python-3.x><pandas>","3","3","","","","CC BY-SA 3.0","1"
"33118651","1","33125259","","2015-10-14 07:01:08","","2","105","<p>I want to select a column at index 1 and then one random column between the index of 2 and 5.  This code works in selecting a random column:</p>

<pre><code>train_cols = train.columns[[random.sample(range(2, 5), 1)]]
</code></pre>

<p>But when I try to add the ""constant"" column at index 1 it doesn't work</p>

<pre><code>train_cols = train.columns[1,[random.sample(range(2, 5), 1)]]
</code></pre>

<p>Any help would be great!  Thank you</p>
","3682157","","","","","2015-10-14 13:01:07","Select a specific column in a DF and a random column (python3, pandas)","<python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"57374843","1","57375797","","2019-08-06 11:19:22","","1","105","<p>My original file for training purpose have 25Gb. My machine has 64Gb of RAM. Importing data with default options always ends up in ""Memory Error"", therefore after reading some posts, I find out that the best option is to define all data types. </p>

<p>For purpose of this question I use a CSV file of: 100.7Mb (it's a mnist data set pulled from <a href=""https://pjreddie.com/media/files/mnist_train.csv"" rel=""nofollow noreferrer"">https://pjreddie.com/media/files/mnist_train.csv</a>)</p>

<p>When I import it with default options in pandas:</p>

<pre><code>keys = ['pix{}'.format(x) for x in range(1, 785)]
data = pd.read_csv('C:/Users/UI378020/Desktop/mnist_train.csv', header=None, names = ['target'] + keys)
# you can also use directly the data from the internet
#data = pd.read_csv('https://pjreddie.com/media/files/mnist_train.csv',
#                    header=None, names = ['target'] + keys)
</code></pre>

<p>The default dtypes for pandas is:</p>

<pre><code>data.dtypes
</code></pre>

<p><a href=""https://i.stack.imgur.com/iS085.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iS085.png"" alt=""enter image description here""></a></p>

<p>How big is it in memory?</p>

<pre><code>import sys
sys.getsizeof(data)/1000000
</code></pre>

<blockquote>
  <p>376.800104</p>
</blockquote>

<p>If I changed dtypes to np.int8</p>

<pre><code>values = [np.int8 for x in range(1, 785)]

data = pd.read_csv('C:/Users/UI378020/Desktop/mnist_train.csv', header=None, names = ['target'] + keys, 
                   dtype = dict(zip(keys, values)))
</code></pre>

<p>My memory usage decreases to:</p>

<blockquote>
  <p>47.520104</p>
</blockquote>

<p>My question is, what would be even better data type for binary variables to decrease size even more?</p>
","4815601","","4815601","","2019-08-06 11:49:11","2019-08-06 12:12:12","Best data types for binary variables in Pandas CSV import to decrease memory usage","<python><python-3.x><pandas><csv>","1","3","","","","CC BY-SA 4.0","1"
"57655597","1","","","2019-08-26 09:54:15","","-1","105","<p>This is an old question, however no one wrote a solution for without loading the excel file. Assuming the excel file is too huge, this will be inefficient to load the existing file and save on it. Instead is there any method to append, like for ordinary files?</p>

<p>df is the dataframe I would like to append</p>

<pre><code>df.to_excel(folder+""file.xlsx"", header=False, index=False)
</code></pre>

<p>However file.xlsx exists and there is another df with the same headers.</p>

<p>How can I append df to existing file.xlsx, without loading file.xlsx?</p>
","","user10684324","","","","2019-08-26 11:29:26","Append existing excel sheet with new dataframe using python pandas without loading the old one","<python><python-3.x><pandas><openpyxl>","1","3","1","","","CC BY-SA 4.0","1"
"57454186","1","57454549","","2019-08-11 22:56:25","","0","104","<p>I am trying to access an excel file using python for my physics class. I have to generates data that follows a function but creates variance so it doesn’t line up perfectly to the function(simulating the error experienced in experiments). I did this by using the rand() function. We need to generate a lot of data sets so that we can average them together and eliminate the error/noise creates by the rand() function. I tried to do this by loading the excel file and recording the data I need, but then I can’t figure out how to get the rand() function to rerun and create a new data set. In excel it reruns when i change the value of any cell on the excel sheet, but I don’t know how to do this when I’m accessing the file with Python. Can someone help me figure out how to do this? Thank You.</p>
","9916031","","","","","2019-08-12 00:29:11","How to get the rand() function in excel to rerun when accessing an excel file through python","<excel><python-3.x><pandas><xlrd><xlwt>","1","4","","","","CC BY-SA 4.0","1"
"57193008","1","","","2019-07-25 01:09:25","","0","103","<p>I have a script that converts a part of an nc file to a csv file. The script itself works, but the problem is that I would need to specify the exact directory including the name of the file and output csv. I am interested in running the script for all nc files from folder test1 and converting it to csv's in folder test2 with the same name. I attempted modifying the script but it hasn't worked. Here is my script. </p>

<pre><code>import netCDF4
from netCDF4 import num2date, date2num, date2index
import pandas as pd
import numpy as np
import netCDF4
import sys
import os

path = r""C:\\Users\\chz08006\\Documents\\test1""

for filename in os.listdir(path):
    netcdf_file = r""C:\\Users\\chz08006\\Documents\\test1\\""+filename
    csv_file = r""C:\\Users\\chz08006\\Documents\\test2\\""+filename

    f = netCDF4.Dataset(netcdf_file)
    ssha = f.variables[""ssha""]
    lon = f.variables['lon']
    lat = f.variables['lat']
    #time = f.variables['time']
    timedim = ssha.dimensions[0]
    times = f.variables[timedim]
    dates = num2date(times[:], times.units)

    dates = [date.strftime('%Y-%m-%d %H:%M:%S') for date in dates]
    lon_list= list(lon)
    lat_list = list(lat)
    ssha_list = list(ssha)
    lon_list = [x-360 if x&gt;= 180 else x for x in lon_list]
    df = pd.DataFrame({'Time':dates,'Longitude':lon_list,'Latitude':lat_list,'SSHA':ssha_list})
    df.to_csv(csv_file)
</code></pre>

<p>My failed attempt at modifying the script was</p>

<pre><code>path = r""C:\\Users\\chz08006\\Documents\\test1""

for filename in os.listdir(path):
    netcdf_file = r""C:\\Users\\chz08006\\Documents\\test1\\""+filename
    csv_file = r""C:\\Users\\chz08006\\Documents\\test2\\""+filename
</code></pre>

<p>Previously, it would have been</p>

<pre><code>netcdf_file = r""C:\\Users\\chz08006\\Documents\\test1\\example1.nc""
csv_file = r""C:\\Users\\chz08006\\Documents\\test2\\exampleresult.csv""
</code></pre>

<p>where example1 was the nc file name and exampleresult would be the csv name.</p>
","10537172","","355230","","2019-07-25 01:24:21","2019-07-25 01:53:44","How to make python script run or all files in a directory?","<python><python-3.x><pandas><dataframe><ncdf4>","1","5","","","","CC BY-SA 4.0","1"
"57611406","1","","","2019-08-22 14:16:13","","0","102","<p>I am using python 3.7.3 [MSC v.1915 64 bit (AMD64)]
I am trying to create a dataframe with pandas.read_csv and I have two problems. On one side, neither the number of columns (1 instead of 55) nor the number of rows (19.181 instead of 2.272) matches the .xlsm file.</p>

<p>On the other side, the header and rows only show values of unreadable text instead of text strings and numbers (integers, floats).    </p>

<p>I have tried several enconding options: 'Latin-1', 'utf-8' and 'ISO-8859-1'.
Also, I have used several 'sep' options: '\t', ';'.</p>

<p>None of them seems to solve the problem.</p>

<pre><code>import pandas as pd
import csv

df = pd.read_csv(r'MyFile.xlsm', 'Sheet1', engine='python', encoding='latin-1')
df
</code></pre>

<p>The expected output would be a dataframe with 55 columns and 2272 rows filled with readable text and numbers.</p>
","11962340","","8893595","","2019-08-22 15:03:41","2019-08-22 15:03:41","pandas.read_csv returns unreadable text in just one column","<python-3.x><pandas><encoding>","1","3","","","","CC BY-SA 4.0","1"
"56771996","1","","","2019-06-26 11:42:23","","0","102","<p>I am having excel sheet which has micros(vba). I am trying to convert excel to PDF document. I am able to convert excel to PDF but its only working for non vba cells. The cells which has vba its populating as NaN or with the forumla of the cell.</p>

<p>I tired to use python library openpyxl,pandas as dataframe and reportlab to generate pdf document.</p>

<p>When I am trying to read excel which has vba its by default coming as NaN value or with the formula. If I am using pandas, I am getting NaN. If I am using openpyxl, I am getting formula.</p>

<p>Let me know anyone has a solution in python or some other library which help me to fetch data not the formula or nan.</p>

<p>my current code.</p>

<h1>Reading excel with only data getting NaN</h1>

<pre><code>workbook = load_workbook(OUTPUT_FOLDER+'/output.xlsm', data_only=True)
worksheet = workbook.get_sheet_by_name('Summary  Inputs')
ws_range = worksheet.iter_rows()
</code></pre>

<h1>Reading excel with micros</h1>

<pre><code>workbook = load_workbook(OUTPUT_FOLDER+'/output.xlsm', vba=True)
worksheet = workbook.get_sheet_by_name('Summary  Inputs')
ws_range = worksheet.iter_rows()
</code></pre>

<h1>populating data from excel to pdf document</h1>

<pre><code>pw = PDFWriter(OUTPUT_FOLDER+'/out.pdf')
            pw.setFont('Courier', 12)
            pw.setHeader('Sizer Report based on input')
            pw.setFooter('Automated Generated Report')


            in_line_total_chars=100
            longest_char_length=58
            for row in ws_range:
                try:    
                    string = ''

                    length_left_side=0
                    for cell in row:

                        left_adjust=0
                        if length_left_side==0:
                            length_left_side=len(cell.value)
                            left_adjust=longest_char_length-length_left_side+2
                        if cell.value is None:
                            string += ' ' * 11
                        else:
                            spaces=' '*left_adjust
                            string += str(cell.value)+spaces



                    pw.writeLine(' ')
                    pw.writeLine(string)
                except Exception as err:
                    logger.error(str(datetime.now())+' '+str(err))      

            pw.savePage()
            pw.close()
</code></pre>
","2871334","","2871334","","2019-06-26 12:47:29","2019-06-26 12:47:29","convert excel sheet which contain micros to pdf using python","<python-3.x><pandas><dataframe><openpyxl><xlrd>","0","2","","","","CC BY-SA 4.0","1"
"56767924","1","","","2019-06-26 07:56:51","","1","102","<p>I am using a script to plot the population values for each country to the pandas preset lowres world map, but since its a huge scale (values between 0 and 1.2B) the countries having a 0 or NaN value look exactly the same as those with about 100k. To avoid this I wanted to plot the absolute value for each country on it's place in the map, so you can differentiate better, but I do not a have a clue of how to do it.</p>

<p>The interesting part of my script looks like this:</p>

<pre><code>import geopandas as gpd
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable    

_, ax = plt.subplots(1, 1)
divider = make_axes_locatable(ax)
cax = divider.append_axes('right', size='5%', pad=0.1)
world.plot(column='pop_est', ax=ax, cax=cax, legend=True)
plt.show()
</code></pre>

<p>The subplot is used for the colorbar.</p>
","11587026","","1000551","","2019-06-26 08:01:21","2019-06-26 08:01:21","How do I plot the absolute values to a geopandas world map?","<python><python-3.x><geopandas>","0","4","","","","CC BY-SA 4.0","1"
"57803232","1","57828594","","2019-09-05 10:14:43","","0","101","<p><strong>Background:</strong></p>

<p>I ran into problem executing code from a machine learning case. I've already solved the issue with an ugly workaround so I am able to execute the notebook, but I still do not fully understand the cause of the issue.</p>

<p>The issues arises when I try to execute the following code which is used to create dummy variables using <strong>OneHotEncoder</strong> from sklearn. </p>

<pre><code>categorical_columns = ~np.in1d(train_X.dtypes, [int, float])
</code></pre>

<p>Although the codes executes without any error, it fails to recognize the numpy.int64 as int datatype therefore classifying all int64 datatype columns as categorical and parsing them into the OneHotEncoder.</p>

<p><code>train_X</code> is a pandas dataframe object with the following columns and datatypes, as you can see the integers are stored as numpy.int64.</p>

<p><a href=""https://i.stack.imgur.com/dmsV1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dmsV1.png"" alt=""dataframe""></a></p>

<p>The code was originally written in Jupyter Notebook on a Mac where it worked fine and it also ran fine in Colaboraty on the Google cloud. All others who tried running the code from Jupyter on their almost identical Windows machines had the same issue as I did when running the script.</p>

<p><strong>The Problem:</strong></p>

<p>It seems that on windows machines, the numpy.int64 is not linked to the native int datatype.</p>

<p><strong>Things I've tried and verified</strong></p>

<ol>
<li>Although dated and based on python 2.7.x this <a href=""https://github.com/numpy/numpy/issues/2951"" rel=""nofollow noreferrer"">post</a> made me believe it was a version issue, so I verified:

<ul>
<li>My machine is running on a 64bit version of windows 10</li>
<li>Python is installed as 64 bit</li>
<li>Anaconda is also installed as 64 bit</li>
<li>Used a clean environment with just pandas, numpy, sklearn and dependencies, all updated to their lastest version</li>
<li>When I run python I get the following:</li>
</ul></li>
</ol>

<p><a href=""https://i.stack.imgur.com/SLkAG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SLkAG.png"" alt=""terminal""></a></p>

<p>I noted the strange ""on win32"" here but it seems merely a product of the ""infinite wisdom of Microsoft"" according to <a href=""https://stackoverflow.com/questions/29745275/entry-message-msc-v-1500-64-bit-amd64-on-win32/"">post 1</a> and <a href=""https://stackoverflow.com/questions/29745275/entry-message-msc-v-1500-64-bit-amd64-on-win32/29745367#29745367"">post 2</a></p>

<ol start=""2"">
<li>I tried understanding the issue by reading <a href=""https://github.com/numpy/numpy/issues/2951"" rel=""nofollow noreferrer"">1</a>, <a href=""https://github.com/pandas-dev/pandas/issues/13258"" rel=""nofollow noreferrer"">2</a> and <a href=""https://stackoverflow.com/questions/12648624/python-converting-an-numpy-array-data-type-from-int64-to-int?noredirect=1&amp;lq=1"">3</a>. I've managed to compute several workarounds based on these but I still do not understand why the code works on one system but not on another.</li>
</ol>

<p><strong>Question:</strong></p>

<p>Why does numpy.int64 not translate into a native int datatype on Windows while everything is running 64 bit, where it does on Mac and other systems?</p>
","7863268","","7863268","","2019-09-05 11:04:13","2019-09-09 11:41:02","Interpreting numpy.int64 datatype as native int datatype in Python on windows x64","<python><python-3.x><pandas><numpy>","1","0","0","","","CC BY-SA 4.0","1"
"33700234","1","33700432","","2015-11-13 19:17:50","","1","101","<p>I have a 30GB csv file with 2 columns, 80M rows. One column has 80M unique elements (emails), the other column 5M uniques (anonymized senders) to which the 80M map many-to-one. I want to output a csv with only the 5M rows: sender, emails sent by sender</p>

<p>This code works in theory, in practice it'd take ~1 month to complete with a Xeon CPU core at 100%</p>

<pre><code>df = pd.read_csv('emails.csv')
uni = df.sender_id.unique()
grouped = ((i, ' '.join(df.text[df.sender_id == i])) for i in uni)
with open('/storage/test.csv', 'w') as csvfile:
    test_writer = csv.writer(csvfile)
    for i in grouped:
        test_writer.writerow(i)
csvfile.close()
</code></pre>

<p>any idea how to make this faster? I've tried parallelizing it with joblib, but I run out of RAM.</p>
","4540977","","2142505","","2015-11-13 19:26:28","2015-11-14 00:23:41","These 8 lines of python + pandas would take ~1 month to execute, help me speed them up?","<python><csv><python-3.x><pandas>","2","4","","","","CC BY-SA 3.0","1"
"48530078","1","48629285","","2018-01-30 20:34:39","","-2","101","<p>Hi Python Experts,</p>

<p>PI chart is not coming up nicely because of long column names &amp; smaller slices. In this example, you can see that i have 6 buckets but in PI chart, only 5 slices are visible. Even text labeled with slices (top right) and on top are overlapping with each other. Table width is also not sufficient to fit column content. Could you please suggest what is the best way to deal with such situation ? I am new comer to python. I am using anaconda python 3.6.</p>

<p>Code:</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
from pandas.tools.plotting import table

raw_data = {'FLAG' : ['AT-NBBO', 'BETTER-THAN-NBBO', 'ONE-SIDED-QUOTE', 'OUTSIDE-NBBO', 'OUTSIDE-NBBO-DUE-TO-OVERSIZED-BUT-NO-EXECUTION-WITHIN-NBBO', 'OUTSIDE-NBBO-DUE-TO-OVERSIZED-BUT-SOME-EXECUTION_WITHIN_NBBO'],
     'COUNT' : [10840, 8628, 84, 633, 153, 14]
   }
df = pd.DataFrame(raw_data, columns = ['FLAG', 'COUNT'])

plt.figure(figsize=(16,8))
ax1 = plt.subplot(121, aspect='equal')
df.plot(kind='pie', y = 'COUNT', ax=ax1, autopct='%1.1f%%', 
  startangle=90, shadow=False, labels=df['FLAG'], legend = True, 
fontsize=14)


# plot table
ax2 = plt.subplot(122)
plt.axis('off')
tbl = table(ax2, df, loc='right')
tbl.auto_set_font_size(False)
tbl.set_fontsize(20)
plt.show()
</code></pre>

<p>Output:
<a href=""https://i.stack.imgur.com/XN8SV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XN8SV.jpg"" alt=""Click Here to visualize the Chart""></a></p>
","1392952","","1392952","","2018-02-04 09:30:36","2018-02-05 18:36:49","how to create PI chart in python","<python><python-3.x><pandas><matplotlib><anaconda>","1","6","","","","CC BY-SA 3.0","1"
"57508123","1","","","2019-08-15 10:12:46","","0","99","<p>I'm trying to multiply certain columns in my pandas dataframe by 100. Specifically those columns that are in the middle of the df.</p>

<p>Here is the code I'm trying to use in my Jupyter Notebook:</p>

<pre><code>quality = quality[['Question','Excellent','Above average','Average','Below the average','Very poor','Total']]

quality['Excellent','Above average','Average','Below the average','Very poor'] = quality['Excellent','Above average','Average','Below the average','Very poor']*100

quality
</code></pre>

<p>I expect the output should be 11.82% from 0.1182 for a cell.</p>

<p>The output I get is the following: </p>

<blockquote>
  <p>KeyError: ('Excellent', 'Above average', 'Average', 'Below the average', 'Very poor')</p>
</blockquote>
","","user11644345","4240413","","2019-08-15 11:03:42","2019-08-15 11:03:42","How to multiply specific columns of a dataframe","<python-3.x><pandas><dataframe><multiple-columns>","1","0","","","","CC BY-SA 4.0","1"
"57801445","1","57801500","","2019-09-05 08:31:12","","0","98","<p>I am trying to split a column based on type.  I want to show numbers separate from text.</p>

<p>I have tried to add it without a loop, but the shape is different.  I therefore resorted to loop it through.  It is however only giving me the last number in all fields</p>

<p>Python input:</p>

<pre><code>newdf = pd.DataFrame()
newdf['name'] = ('leon','eurika','monica','wian')
newdf['surname'] = ('swart38','39swart','11swart','swart10')
a = newdf.shape[0]

newdf['age'] = """"
for i in range (0,a):
    newdf['age'] =  re.sub(r'\D', """",str(newdf.iloc[i,1]))

print (newdf)
</code></pre>

<p>I am expecting the age column to show <code>38,39,11,10</code>.  The answer is however all <code>""10""</code> being the last field.</p>

<p>Out:</p>

<pre><code>     name  surname age
0    leon  swart38  10
1  eurika  swart39  10
2  monica  11swart  10
3    wian  swart10  10
</code></pre>
","12024038","","6275103","","2019-09-05 08:33:43","2019-09-05 08:35:23","Split a panda column into text and numbers","<python-3.x><pandas>","2","1","1","2019-09-05 17:31:25","","CC BY-SA 4.0","1"
"57659377","1","","","2019-08-26 13:59:03","","4","98","<p>Suppose I have a list of strings with n items, say:</p>

<pre><code>    list1 = ['a','b',..'y','z']
</code></pre>

<p>Instead of appending the entire list to a dataframe (which creates n columns), I just want to append the first item to the first column, the last item to the last column and everything in between combined into the middle column.</p>

<p>I tried </p>

<pre><code>    df = pd.DataFrame(list1)
</code></pre>

<p>but this creates a separate column for each string item, resulting in n columns</p>

<p>I want the output to be a dataframe of 3 columns such as:</p>

<pre><code>         0       1        2
  0      a     b+c+d+..   z
</code></pre>

<p>Please help a noob out!</p>
","11925407","","","","","2019-08-26 14:17:04","How to append items of a list to specific columns in a dataframe?","<python><python-3.x><pandas><list><dataframe>","2","0","","","","CC BY-SA 4.0","1"
"57740548","1","57742254","","2019-08-31 18:11:47","","-1","96","<p>So I have a Dataset like this:</p>

<pre><code>     Customer_id   Lat        Lon
 0.     A          40         12
 1.     A          np.nan     np.nan
 2.     A          np.nan     np.nan
 3.     A          43         12
 4.     A          45         13
 5.     B          43         14
 6.     B          np.nan     np.nan
 7.     B          43         16
</code></pre>

<p>Where the coordinates (40,12),(43,12),(45,13),(43,14) and (43,16) are the cell towers of a certain network.</p>

<p>Then I apply some interpolation functions and it results in something like the following:</p>

<pre><code>     Customer_id   Lat        Lon
 0.     A          40         12
 1.     A          41         12
 2.     A          42         12
 3.     A          43         12
 4.     A          45         13
 5.     B          43         14
 6.     B          43         15
 7.     B          43         16
</code></pre>

<p>But these new coordinates are just estimates and not actual towers. I would like to then assign these estimations to the closest actual tower so that, for example, record 1 would be assigned to the tower (40,12).</p>

<p>I used this code</p>

<pre><code>def haversine_closest_changed(towers, row):
    all_points= towers
    lat2= all_points[:,0] #the actual latitudes of the towers
    lon2= all_points[:,1] #the actual longitudes of the towers
    l=len(lat2) #how many towers are there
    lat1=row['Expected_Lat']  #make a column with the actual latitude my value and all the towers, 
    #the point I'm looking at multiple times
    lon1=row['Expected_Lon']  #find the min distance and output the minimum 

    lat1, lon1, lat2, lon2 = map(np.radians, [lon1, lat1, lon2, lat2])

    dlat = lat2 - lat1
    dlon = lon2 - lon1

    a = np.sin(dlon/2.0)**2 + np.cos(lon1) * np.cos(lon2) * np.sin(dlat/2.0)**2

    c = 2 * np.arcsin(np.sqrt(a))
    km = 6367 * c
    idx=np.argmin(km)
    closest_point=towers[idx,]

    return closest_point
</code></pre>

<p>where towers is a pandas dataset with all the towers that exist in the network (one column for Latitude and another for Longitude) and the columns Expected_Lat and Expected_Lon are what I called the columns after I did the interpolation.</p>

<p>This piece of code returns me only 1 value for the latitude and 1 value for the longitude repeated throughout the whole column. How can I change this code to replace only the points that I have interpolated/the points that were previously NaNs by the closest tower?</p>
","11053294","","1000551","","2019-08-31 18:15:50","2019-08-31 23:32:31","Assign estimated location of a cell tower to the closest actual cell tower","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"48796002","1","48796208","","2018-02-14 20:54:37","","0","96","<p>I have some code in Python Pandas that I would like to find the similar code in R.</p>

<pre><code>data

Time
-0.3350   -0.023798
-0.3345   -0.019036
-0.3340   -0.010623
-0.3335   -0.001733
-0.3330    0.345787
Name: Pressure, dtype: float64
</code></pre>

<p>Using the data above, if I want all the rows up to and including -0.010623 I simply write:</p>

<pre><code>data[:-0.010623]
</code></pre>

<p>My question is in R how do I write this.  I have tried the following:</p>

<pre><code>tail(data$Pressure, -0.010623)
</code></pre>

<p>But it does not work.</p>
","4821959","","","","","2018-02-14 21:06:10","Equivalent R Language for Python Pandas Slice","<r><python-3.x><pandas>","1","1","","","","CC BY-SA 3.0","1"
"49371793","1","","","2018-03-19 20:25:01","","0","96","<p>I have a telegram bot and i am attempting to display the data from a specified sheet of a xlsx file  </p>

<p>this is the section of code </p>

<pre><code>def showme(bot,update):
    team = pd.ExcelFile(r""sheetname.xlsx"")
    ytd = pd.read_excel(r""sheetname.xlsx"", sheet_name=5)
    q3 = pd.read_excel(r""sheetname.xlsx"", sheet_name=3)
    update.message.reply_text(ytd)
</code></pre>

<p>But when i Call on the showme nothing happens. </p>

<p>I have the imports correct, and in the build <code>print(ytd)</code> displayed the information i wanted. </p>

<p>Any ideas where i may have gone wrong ? I do suspect it is to do with <code>update.message.reply_text(ytd)</code> part </p>
","9382114","","","","","2018-03-19 20:25:01","Using pandas to display xlsx file content","<excel><python-3.x><pandas><telegram-bot>","0","4","","","","CC BY-SA 3.0","1"
"41488516","1","41489463","","2017-01-05 15:18:51","","2","94","<p>I have a dataframe that looks like:</p>

<blockquote>
<pre><code>            Open   High    Low  Close     
Date                                                                            
2014-03-31  10.61  10.61  10.61  10.61        
2014-04-01  10.66  10.66  10.66  10.66         
2014-04-02  10.67  10.67  10.67  10.67        
2014-04-03  10.64  10.64  10.64  10.64        
2014-04-04  10.57  10.57  10.57  10.57     
2014-04-07  10.50  10.50  10.50  10.50      
2014-04-08  10.51  10.51  10.51  10.51 
</code></pre>
</blockquote>

<p>I want to add a column <code>df['Ave']</code> that multiplies some constant or function by <code>df['Ave'].shift(1)</code>. What is the most elegant way to do this?</p>

<p>When I try to do it I get a KeyError. I also considered setting <code>df['Ave'].head(1) = df['Close']</code> and then starting the function from the second row of the dataframe, but I am not sure how to do that.</p>

<p>Thanks in advance. </p>
","7204030","","","","","2017-01-05 16:19:58","Pandas column that references itself","<python><python-2.7><python-3.x><pandas>","2","1","","","","CC BY-SA 3.0","1"
"57166833","1","","","2019-07-23 14:50:30","","1","94","<p><img src=""https://i.stack.imgur.com/ZlOZk.png"" alt=""enter image description here"">
This is what I have as reference and I want to check my list with this list and score then based on the sum of POS and NEG</p>

<p><img src=""https://i.stack.imgur.com/0ZFxm.png"" alt=""""> 
I'm new to python so this might be easy. I'm calculating sentiment score of strings with reference to dataset which has list of words with +ve and -ve score. So i want to add +ve score and subtract -ve score if I find the word from  test set in reference dataset. For that I would have to access each word from test string and check with reference dataset.
This is my code:</p>

<pre class=""lang-py prettyprint-override""><code>
for i in range(0,29):
    my_string = processed[i]
    for word in my_string.split():
        for j in range(0,3013):
            senti_words=senti['LIST_OF_WORDS'].iloc[j,4]
            if  word in senti_words:
               pos=senti['LIST_OF_WORDS'].iloc[j,2]
               neg=senti['LIST_OF_WORDS'].iloc[j,3]
               sentiment=pos-neg
               a=a.append(sentiment)

</code></pre>
","11250674","","6347629","","2019-07-23 19:07:08","2019-07-23 19:07:08","How do I check each word of a string with each word of other string?","<python-3.x><pandas><nlp><sentiment-analysis>","1","9","","","","CC BY-SA 4.0","1"
"57128185","1","57128632","","2019-07-20 20:06:48","","4","94","<p>I have a pandas dataset with a column of words and a column of integer (0,1).  All words that appear between a zero (first integer, or after a 1) and a 1(including) should be put into a 2D array. </p>

<p>Let me explain:</p>

<p>Consider this pandas dataframe:</p>

<pre><code>import pandas as pd

df = pd.DataFrame(columns=['Text','Selection_Values'])
df[""Text""] = [""Hi"", ""this is"", ""just"", ""a"", ""single"", ""sentence."", ""This"", ""is another one.""]
df[""Selection_Values""] = [0,0,0,0,0,1,0,1]
print(df)
</code></pre>

<p>This is the example dataset:</p>

<pre><code>              Text  Selection_Values
0               Hi                 0
1          this is                 0
2             just                 0
3                a                 0
4           single                 0
5        sentence.                 1
6             This                 0
7  is another one.                 1
</code></pre>

<p>The expected result should be:</p>

<pre><code>[[""Hi this is just a single sentence.""],[""This is another one""]]
</code></pre>

<p><strong>Do you have any idea of how to go about this ?</strong> </p>

<p>This is what I have done so far:</p>

<pre><code>result = []

s = """"
for i in range(len(df[""Text""])):
    s += df[""Text""][i] + "" ""
    if df[""Selection_Values""][i] == 1:
        result.append([s])
        s = """"
</code></pre>

<p>It works:</p>

<pre><code>[['Hi this is just a single sentence. '], ['This is another one. ']]
</code></pre>

<p>...but it might not be the best method. It does not make use of the pandas framework at all. </p>
","6114310","","6114310","","2019-07-20 20:12:44","2019-07-20 21:12:03","Regroup pandas column into 2D list based on another column","<python><python-3.x><pandas><list>","3","0","1","","","CC BY-SA 4.0","1"
"56910136","1","56911426","","2019-07-05 23:33:30","","-1","93","<p>I am trying to take data of two sheets and comparing with each other if it matches i want to append column. Let me explain this by showing what i am doing and what i am trying to get in output using python.</p>

<p>This is my sheet1 from excel.xlsx:
<a href=""https://i.stack.imgur.com/Yn33Y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Yn33Y.png"" alt=""sheet1""></a></p>

<p>it contains four column name,class,age and group.</p>

<p>This is my sheet2 from excel.xlsx:
<a href=""https://i.stack.imgur.com/U21Wf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/U21Wf.png"" alt=""sheet2""></a></p>

<p>it contains default, and name column with extra names in it.</p>

<p>So, Now i am trying to match name of sheet2 with sheet1, if the name containing in sheet1 matches with sheet2 then i want to add default value corresponding to that name from sheet2.</p>

<p>This i need in output:
<a href=""https://i.stack.imgur.com/k0mIf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/k0mIf.png"" alt=""output""></a></p>

<p>As you can see only Ravi and Neha having default in sheet2 and that name matches with sheet1 name. Suhash and Aish dont have any default value so not anything coming there.</p>

<p>This code i tried:</p>

<pre><code>import pandas as pd
import xlrd

df1 = pd.read_excel('stack.xlsx', sheet_name='Sheet1') 
df2 = pd.read_excel('stack.xlsx', sheet_name='Sheet2') 



df1['DEFAULT'] = df1.NAME.map(df2.set_index('NAME')['DEFAULT'].to_dict())


df1.to_excel('play.xlsx',index=False)
</code></pre>

<p>and getting output excel like this:  </p>

<p><a href=""https://i.stack.imgur.com/zapj1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zapj1.png"" alt=""enter image description here""></a></p>

<p>Not getting default against Ravi.</p>

<p>Please help me with this to get this expected output using python.</p>
","11679496","","11679496","","2019-07-06 08:25:24","2019-07-06 08:25:24","Merging sheets of excel using python","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"48800976","1","48801008","","2018-02-15 06:15:47","","4","93","<p>I was trying to create panda dataframe from list as below:</p>

<pre><code>my_value = ['Aberarder Creek', 'Town of Plympton-Wyoming', 'Aylmer', '17-4091-
47723', '43.062, -82.109', 'Northern Pike Rock Bass Smallmouth Bass White 
Sucker']
df = pd.DataFrame(columns=['Lake Name','Municipality','MNRF 
District','Coordinates','Waterbody ID','Fish Species'],index=np.arange(0))
my_df = pd.DataFrame(my_value,columns=['Lake Name','Municipality','MNRF 
District','Coordinates','Waterbody ID','Fish Species'])
my_df.append(df,ignore_index=True)
df
</code></pre>

<p>Keep geeting error message like:</p>

<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in 
create_block_manager_from_blocks(blocks, axes)
   4293                 blocks = [make_block(values=blocks[0],
-&gt; 4294                                      placement=slice(0, 
len(axes[0])))]
   4295 

~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in 
make_block(values, placement, klass, ndim, dtype, fastpath)
   2718 
-&gt; 2719     return klass(values, ndim=ndim, fastpath=fastpath, 
placement=placement)
   2720 

~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in 
__init__(self, values, ndim, fastpath, placement, **kwargs)
   1843         super(ObjectBlock, self).__init__(values, ndim=ndim, 
fastpath=fastpath,
-&gt; 1844                                           placement=placement, **kwargs)
   1845 

~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py in __init__(self, values, placement, ndim, fastpath)
    114                              'implies %d' % (len(self.values),
--&gt; 115                                              len(self.mgr_locs)))
    116 

ValueError: Wrong number of items passed 1, placement implies 6
</code></pre>

<p>I wonder how come the elements in my_value doesn't match the number of columns.
So confused, I might have some misunderstanding of the panda basics, thank you!</p>
","9363293","","","","","2018-02-15 06:17:51","Python3* : Create a pandas dataframe from a list","<python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"56624946","1","56625009","","2019-06-17 04:59:50","","0","92","<p>I am trying to figure out a way in which I can calculate quantiles in pandas or python based on a column value? Also can I calculate multiple different quantiles in one output?</p>

<p>For example I want to calculate the 0.25, 0.50 and 0.9 quantiles for </p>

<p><strong>Column Minutes in df where it is &lt;= 5 and where it is > 5 and &lt;=10</strong></p>

<pre><code>df[df['Minutes'] &lt;=5]

df[(df['Minutes'] &gt;5) &amp; (df['Minutes']&lt;=10)]
</code></pre>

<p><strong>where column Minutes is just a column containing value of numerical minutes</strong></p>

<p>Thanks!</p>
","8797830","","6287308","","2019-06-17 05:06:34","2019-06-17 05:06:34","Calculating Quantiles based on a column value?","<python><python-3.x><pandas><data-science>","1","0","","","","CC BY-SA 4.0","1"
"41021202","1","","","2016-12-07 15:22:42","","2","92","<p>My goal is to analyze changes in tuition costs for private schools in urban settings vs private schools in rural settings.</p>

<p>I have a dataframe with tuition costs of all private schools in the US through time (<code>tuit_cost</code>). The dataframe <code>tuit_cost</code> contains columns of historical tuition costs as well as two columns titled <code>['State','City/Town Name']</code>. </p>

<p>I also have a separate dataframe of private schools that are classified as being in 'Urban' areas (<code>urban_schools</code>). This dataframe has only two columns -- <code>['State','City/Town Name']</code>. </p>

<p>I merged the dataframes in order to create a dataframe with only the urban schools' historical tuition data.</p>

<pre><code>urban_school_tuit = pd.merge(urban_schools, tuit_cost, how='left', left_on= ['State','City/Town Name'], right_on=['State','City/Town Name']).dropna()
</code></pre>

<p>Now I want to create a dataframe with only the rural schools' historical tuition data by dropping all of the rows in <code>urban_school_tuit</code> from <code>tuit_cost</code>.</p>

<p>What is the most efficient way to do so?</p>

<p>Thanks!</p>
","6828137","","","","","2016-12-07 20:16:20","I want to drop one Dataframe from another (the first df is a subset of the second)","<python><python-2.7><python-3.x><pandas><dataframe>","1","7","","","","CC BY-SA 3.0","1"
"57260823","1","57260944","","2019-07-29 20:16:13","","2","91","<p>I have a data frame in pandas, where 1 appears in different columns for every rows. The column where 1 appears for the first time in a row is different for different rows. I need to create an additional column (column index) in which as value I want to return the index number of the column where 1 appears for the first time in that row. </p>

<pre><code>Example dataframe:

IDs     q1    q2    q3    q4    q5    q6    q7    q8

1111    0     0     0      1    0      0     0     1

1122    0     0     1      0    0      1     0     0

the output should like this: 

IDs     q1    q2    q3    q4    q5    q6    q7    q8    column_index

1111    0     0     0      1    0      0     0     1        5

1122    0     0     1      0    0      1     0     0        4 
</code></pre>

<p>It would be helpful if anyone can provide the code useful in pandas. 
Thanks in advance.</p>
","9927250","","","","","2019-07-29 23:32:02","How to return column index for every row where a certain value appears for the first time","<python-3.x><pandas>","3","1","","","","CC BY-SA 4.0","1"
"41638369","1","41638681","","2017-01-13 15:46:54","","0","91","<p>Here is my dataframe</p>

<pre><code>                Word  1_gram-Probability
0             ('A',)            0.001461
1            ('45',)            0.000730
</code></pre>

<p>now i just want to select the row where <code>Word</code> is 45. i tried</p>

<pre><code>print(simple_df.loc[simple_df['Word']=='45'])
</code></pre>

<p>but i get</p>

<pre><code>Empty DataFrame
</code></pre>

<p>what am i missing? Is this the correct way of accessing the row? I also tried <code>('45',)</code> as the value but that did not work either.</p>
","2334092","","","","","2017-01-13 16:00:14","Pandas : Cannot select row from dataframe","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 3.0","1"
"56772397","1","","","2019-06-26 12:04:32","","0","91","<p>I have a fake dataset presenting a list of areas. These areas contain members and each member has a value.</p>

<p>I would like to count for each area, the number of unique members whose value satisfies a condition. I managed to deal with the issue but I would like to know if there is a cleaner way to do so in Pandas.</p>

<p>Here is my attempt so far:</p>

<pre><code># Building the fake dataset
dummy_dict = {
    ""area"": [""A"",""A"", ""A"",""A"",""B"",""B""],
    ""member"" : [""O1"",""O2"",""O2"",""O3"",""O1"",""O1""],
    ""value"" : [90, 200, 200, 150, 120, 120]
}
df = pd.DataFrame(dummy_dict)
# Counting the number of unique members that satisfy the condition by zone 
value_cutoff = 100
df[""nb_unique_members""] = df.groupby(""area"")[""member""].transform(""nunique"")
df.loc[df[""value""]&gt;=value_cutoff,""tmp""] = df.loc[df[""value""]&gt;=value_cutoff].groupby(""area"")[""member""].transform(""nunique"")
df[""nb_unique_members_above_cutoff""] = df.groupby(""area"")[""tmp""].transform(""mean"")
df.head()
</code></pre>

<p>Is there a better way to do so in Pandas ? Thanks in advance!</p>
","11702929","","","","","2019-06-26 12:04:32","Pandas - Count unique values in column A that satisfy condition in column B grouped by column C","<python-3.x><pandas><unique><pandas-groupby>","0","2","","","","CC BY-SA 4.0","1"
"57694292","1","57694432","","2019-08-28 14:04:39","","2","91","<p>i have a pandas dataframe and a list i want to update pandas column
using that list if value already exist then ignore that row <br>
<strong>(e.x)</strong><br></p>

<blockquote>
<pre><code>my old dataframe
  date_time           value
2018-11-01 00:00:02    100
2018-11-01 00:00:12    150
2018-11-01 00:00:22    56
2018-11-01 00:00:32    95
2018-11-01 00:00:42    700


my list:
   [""2018-11-01 00:00:02"", ""2018-11-01 00:00:07"", ""2018-11-01 00:00:12"", ""2018-11-01 00:00:17"", ""2018-11-01 00:00:22"", ""2018-11-01 00:00:27"", ""2018-11-01 00:00:32"", ""2018-11-01 00:00:37"", ""2018-11-01 00:00:42"", ""2018-11-01 00:00:47""]

my expected output:
   date_time           value
2018-11-01 00:00:02    100
2018-11-01 00:00:07    nan
2018-11-01 00:00:12    150
2018-11-01 00:00:17    nan
2018-11-01 00:00:22    56
2018-11-01 00:00:27    nan
2018-11-01 00:00:32    95
2018-11-01 00:00:37    nan
2018-11-01 00:00:42    700
2018-11-01 00:00:47    nan
</code></pre>
</blockquote>

<p><strong>code :</strong></p>

<pre><code>my_list = [""2018-11-01 00:00:02"", ""2018-11-01 00:00:07"", ""2018-11-01 00:00:12"", ""2018-11-01 00:00:17"", ""2018-11-01 00:00:22"", ""2018-11-01 00:00:27"", ""2018-11-01 00:00:32"", ""2018-11-01 00:00:37"", ""2018-11-01 00:00:42"", ""2018-11-01 00:00:47""]
df[""date_time""] = pd.Series(my_list).astype(str)
</code></pre>

<p>when i execute above code it produce following output:</p>

<blockquote>
<pre><code>   date_time           value
2018-11-01 00:00:02    100
2018-11-01 00:00:07    150
2018-11-01 00:00:12    56
2018-11-01 00:00:17    95
2018-11-01 00:00:22    700
2018-11-01 00:00:27    nan
2018-11-01 00:00:32    nan
2018-11-01 00:00:37    nan
2018-11-01 00:00:42    nan
2018-11-01 00:00:47    nan
</code></pre>
</blockquote>
","11599412","","","","","2019-08-28 14:19:37","How to update column values ignore if exist","<python><python-3.x><pandas><append>","1","0","","","","CC BY-SA 4.0","1"
"56656364","1","","","2019-06-18 20:08:55","","0","90","<p>I am sending 15 minute audio files of 2 person conversations to a transcription/speaker diarization service. Circumstances require me chunk 15 minute files into three 5 minute files. Unfortunately, speaker labels are not consistent across chunks, but I need them to be for analysis.</p>

<p>For example, in the first file, speakers are labeled '0' and '1'. However, in the second file, they are labeled '1' and '2'. In the third file, they may be labeled '1' and '0' respectively. This is a problem as I need consistent labeling.</p>

<p>My current approach is to represent data from each chunk in a dataframe. To have a reference for labels across dataframes, I overlapped each dataframe by 10 seconds. I want to merge each dataframe where 'transcript', 'start', and/or 'start' columns match.</p>

<p>Then, I want to modify the speaker labeling scheme on the newly merged dataframe to match the previous dataframe based on the overlapping values.</p>

<p>This is what dataframe 1 looks like:</p>

<blockquote>
  <p>df</p>
</blockquote>

<pre><code>                transcript  start  stop  speaker_label
0              hello world    1.2   2.2              0
1  why hello, how are you?    2.3   4.0              1
2          fine, thank you    4.1   5.0              0
</code></pre>

<p>This is what dataframe 2 looks like. Note how the first row matches the last row in the previous dataframe because of the overlapping, but now the speaker_label scheme is different.</p>

<blockquote>
  <p>df1</p>
</blockquote>

<pre><code>                          transcript  start  stop  speaker_label
0                    fine, thank you    4.1   5.0              1
1          you?(should be speaker 0)    5.1   6.0              1
2  good, thanks(should be speaker 1)    6.1   7.0              2
</code></pre>

<p>This is what I want, dataframes vertically merged where 'start' values match, and having the 'df1' 'speaker_label' scheme match the scheme of 'df'.</p>

<blockquote>
  <p>ideal_df</p>
</blockquote>

<pre><code>                          transcript  start  stop  speaker_label
0                        hello world    1.2   2.2              0
1            why hello, how are you?    2.3   4.0              1
2                    fine, thank you    4.1   5.0              0
3          you?(should be speaker 0)    5.1   6.0              0
4  good, thanks(should be speaker 1)    6.1   7.0              1
</code></pre>
","11666502","","8121719","","2019-06-18 20:12:51","2019-06-19 14:10:27","How to vertically merge dataframes on matching column value","<python><python-3.x><pandas>","2","2","","2019-06-24 18:34:22","","CC BY-SA 4.0","1"
"57902772","1","57903182","","2019-09-12 08:31:39","","0","89","<p>I have below dataframe, want to separate date and text from the field ""Indicator_NameB"" and I have written function ""String_Year_Pick_V2""</p>

<p>Data:</p>

<pre><code>    Indicator_NameB
       2011-12 (RE)
       2012-13 (BE)
        2007-08
    Approved Outlay
Total for 11th Plan

Data['Indicator_NameB'].map(str).apply(String_Year_Pick_V2)
</code></pre>

<p><code>KN.String_Year_Pick_V2</code> - This function separates Text and Date from Each value of ""Indicator_NameB"" and returns as series value like <code>(RE), 2011-12</code></p>

<p>Getting below error:</p>

<pre><code>AttributeError: 'builtin_function_or_method' object has no attribute 'get_indexer'
</code></pre>

<p>Then I update to Dataframe:</p>

<pre><code>DataT[['Indicator_NameB', 'Year']] = DataT['Indicator_NameB'].map(str).apply(String_Year_Pick_V2)
</code></pre>

<p>I suspect, </p>

<pre><code>String_Year_Pick_V2('2007-08') gives '', '2007-08'
String_Year_Pick_V2('Approved Outlay') gives 'Approved Outlay', ''
</code></pre>

<p>may be above results causing the error</p>
","2717063","","2717063","","2019-09-12 08:40:24","2019-09-12 08:57:47","Apply user defined function to DataFrame fails, Python 3.6","<python><python-3.x><pandas><dataframe><apply>","1","4","","","","CC BY-SA 4.0","1"
"49627146","1","","","2018-04-03 09:54:57","","2","89","<p>I have a column in the dataframe that has PT7M37S type values and i want to convert them all into seconds in duration.I'm thinking of matching strings using regular expression and then parsing the integers.</p>

<p>This is my code so far:<code>match = re.match('PT(\d+H)?(\d+M)?(\d+S)?', duration).groups()</code></p>

<pre><code>hours = _js_parseInt(match[0]) if match[0] else 0
minutes = _js_parseInt(match[1]) if match[1] else 0
seconds = _js_parseInt(match[2]) if match[2] else 0
</code></pre>

<p>Is there any other way to do it?</p>
","6683406","","","","","2018-04-03 12:40:40","how can i convert PT7M37S iso 8601 format to duration in seconds using regular expressions","<regex><python-3.x><pandas>","1","4","","","","CC BY-SA 3.0","1"
"57701334","1","57716121","","2019-08-28 23:56:42","","2","89","<p>I have this python code </p>

<pre><code>def get_yahoo(tickers, startdate, enddate):
    from pandas_datareader import data as pdr
    import pandas as pd
    def data(ticker):
        return(pdr.get_data_yahoo(ticker, start=startdate, end=enddate))
    datas=map(data,tickers)
    return(pd.concat(datas,keys=tickers,names=['Ticker','Date']))



tickers_example=['MSFT', 'AAPL']
yahoo=get_yahoo(tickers_example, ""2017-01-01"", ""2018-01-01"")
</code></pre>

<p>That is returning a pandas dataframe with this ""composed (ticker,date)"" index:</p>

<p><a href=""https://i.stack.imgur.com/XI4lN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XI4lN.png"" alt=""enter image description here""></a></p>

<p>But I need my pandas Dataframe with the index and columns with a format like that:
<a href=""https://i.stack.imgur.com/PsK1E.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PsK1E.png"" alt=""enter image description here""></a></p>

<p>How can I get that?</p>
","2132478","","6361531","","2019-11-01 17:44:39","2019-11-01 17:44:39","How can I modify the index of my pandas dataframe?","<python><python-3.x><pandas><dataframe>","1","1","","","","CC BY-SA 4.0","1"
"57802524","1","57802912","","2019-09-05 09:35:16","","3","88","<p>My dataframe is as below,</p>

<pre><code> _dict = {'t_head': ['H1', 'H2', 'H3', 'H4', 'H5','H6'], 
            'r_head': ['Revenue', 'Revenue', 'Income', 'Income', 'Cash', 'Expenses'], 
            '3ME__ Q219': [159.9, '', 45.6, '', '', ''], 
            '3ME__ Q218': [112.3, '', 27.2, '', '', ''], 
            '3ME__ Q119': [121.0, '', 23.1, '', '', ''], 
            '3ME__ Q18': [85.7, '', 15.3, '', '', ''], 
            '3ME__ Q418': [160.5, '', 51.1, '', '', ''], 
            '9ME__ Q417': [102.6, '', 24.2, '', '', ''], 
            '9ME__ Q318': [118.8, '', 30.2, '', '', ''], 
            '9ME__ Q317': [79.4, '', 15.3, '', '', ''], 
            '6ME__ Q219': ['', 280.9, '', 68.7, '', ''], 
            '6ME__ Q218': ['', 198.0, '', 42.6, '', ''], 
            'Q219': ['', '', '', '', 1305, 1239], 
            'Q418': ['', '', '', '', 2072, 1117]
            }
df = pd.DataFrame.from_dict(_dict)
print(df)  

  t_head    r_head 3ME__ Q219 3ME__ Q218 3ME__ Q119 3ME__ Q18 3ME__ Q418 9ME__ Q417 9ME__ Q318 9ME__ Q317 6ME__ Q219 6ME__ Q218  Q219  Q418
0     H1   Revenue      159.9      112.3        121      85.7      160.5      102.6      118.8       79.4                                  
1     H2   Revenue                                                                                             280.9        198            
2     H3    Income       45.6       27.2       23.1      15.3       51.1       24.2       30.2       15.3                                  
3     H4    Income                                                                                              68.7       42.6            
4     H5      Cash                                                                                                               1305  2072
5     H6  Expenses                                                                                                               1239  1117
</code></pre>

<p>I want to split this dataframe into multiple dtaframes base on column heading. Here column headings <strong>can</strong> start with <code>3ME__</code>,<code>6ME__</code>,<code>9ME__</code> (<em>all/any/none can be present</em>) or other values. i want to all columns starting with <code>3ME__</code> to be in one dataframe,<code>6ME__</code> to another...etc. and the all of the rest to be in a fourth dataframe.<br>
What i had tried is as below,</p>

<pre><code>df1 = df.filter(regex='3ME__')
if not df1.empty:
    df1 = df1[df1.iloc[:,0].astype(bool)]
df2 = df.filter(regex='6ME__')
if not df2.empty:
    df2 = df2[df2.iloc[:,0].astype(bool)]
df3 = df.filter(regex='9ME__')
if not df3.empty:
    df3 = df3[df3.iloc[:,0].astype(bool)]
</code></pre>

<p>Here i am able to filter out column names starting with<code>3ME__</code>,<code>6ME__</code> &amp; <code>9ME__</code> to different dataframes, but <strong>not able to get the rest of column headings to one dataframe</strong>.   </p>

<p>1.) <em>How to get the rest of column headings to one dataframe?</em><br>
2.) <em>Is there any simpler method to split into dictionary with a key and dataframes as values?</em>  </p>

<p>Please Help. </p>
","10419999","","10419999","","2019-09-05 09:57:28","2019-09-05 12:34:45","How to split a dataframe to multiple dataframes bases on column names","<python><python-3.x><pandas><dataframe><dictionary>","4","5","","","","CC BY-SA 4.0","1"
"56646019","1","56701854","","2019-06-18 09:33:40","","0","88","<p>I have a huge Dask Dataframe which has a series dtype Object that contains a string that I want to convert into a timestamp (epoch unix time) so I can save it as an integer. Of the date, I am only interested in Day, Month and Year cause in my case, all the time is 00:00:00.</p>

<p>This is my series to convert:</p>

<p><a href=""https://i.stack.imgur.com/PwnhR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PwnhR.png"" alt=""Series to convert""></a></p>

<p>This is what I tried at the end:</p>

<pre class=""lang-py prettyprint-override""><code>df_leavetimes['DAYOFSERVICE'] = df_leavetimes['DAYOFSERVICE'].map_partitions(pd.to_datetime,format='%d-%b-%y %H:%M:%S',meta = ('datetime64[ns]'))
</code></pre>

<p>Now I have this kind of data:</p>

<pre><code>2018-01-01
</code></pre>

<p>How do I convert this into timestamp for the whole series?</p>
","9944937","","1000551","","2019-06-18 09:53:36","2019-06-21 10:42:12","Convert Dask Series object into timestamp","<python><python-3.x><pandas><timestamp><dask>","1","0","","","","CC BY-SA 4.0","1"
"56656229","1","","","2019-06-18 19:58:32","","3","88","<p>I am looking for a <strong>faster and more elegant</strong> way to solve the following problem: </p>

<p>Given a pandas data frame, I want to combine the current row and the previous k (<code>prev_len</code>) rows into a new row (of a new data frame). I want to do this for every valid old row, i.e. every row that has k previous rows. That is, every new row will consist of <code>prev_len + 1</code> old rows that were horizontally appended next to each other. Hence, the resulting data frame will have <code>prev_len</code> fewer rows than the old data frame and its number of columns will be <code>prev_len + 1</code> *  <code>number_of_columns_in_old_data_frame</code>. Please see the example below with <code>prev_len=2</code>. Thanks a lot in advance!</p>

<hr>

<p>Given data frame:</p>

<pre><code>    x1  x2         y
0  166   9 -2.426679
1  192   6 -0.428913
2  198   1  1.265936
3  117   0 -0.866740
4  183   1 -0.678886
</code></pre>

<p>Desired data frame:</p>

<pre><code>   00_x1  00_x2      00_y  01_x1  01_x2      01_y  02_x1  02_x2      02_y
0  166.0    9.0 -2.426679  192.0    6.0 -0.428913  198.0    1.0  1.265936
1  192.0    6.0 -0.428913  198.0    1.0  1.265936  117.0    0.0 -0.866740
2  198.0    1.0  1.265936  117.0    0.0 -0.866740  183.0    1.0 -0.678886
</code></pre>

<p>My solution:</p>

<pre><code>import numpy as np
import pandas as pd
import random 

# given data ----------------------------------------------------------
np.random.seed(seed=123)
df = pd.DataFrame({'x1': np.random.randint(100, 200, 5), 
                       'x2': np.random.randint(0,10,5), 
                       'y': np.random.randn(5)})
print(df)

# desired data  -------------------------------------------------------
prev_len = 2

lag = []

for i in range(prev_len + 1):
    lag += [i] * len(df.columns.to_list())

col = df.columns.to_list() * (prev_len + 1)
colnames = [""{:02}_{}"".format(lag_, col_) for lag_, col_ in zip(lag, col)]

df_new = pd.DataFrame(columns = colnames)

for i_new, i_old in zip(range(df.shape[0] - prev_len), range(prev_len, df.shape[0])):

    obs = pd.Series()

    print(i_old)

    for j in range(i_old - 2, i_old + 1):

        obs = obs.append(df.iloc[j, :])

    df_new.loc[i_new] = obs.to_list()

print(df_new)
</code></pre>
","6140832","","3483203","","2019-06-18 20:32:23","2019-06-19 03:58:49","How to convert several rows into 1 row in a pandas data frame in a rolling way and fast?","<python><python-3.x><pandas>","3","7","1","","","CC BY-SA 4.0","1"
"57456578","1","57456761","","2019-08-12 06:33:26","","2","87","<p>I am trying to filter a dataframe by column values, but I do not get it. Let suppose I have the following dataframe:</p>

<pre><code>Index Column1 Column2
1      path1   ['red']
2      path2   ['red' 'blue']
3      path3   ['blue']
</code></pre>

<p>My dataframe has exactly that format. I want to create a sub-dataframe with the rows containing only <code>['red']</code> in <code>Column2</code>. That would be just the first row.</p>

<p>What I tried so far, among other approaches, is:</p>

<pre><code>classes = ['red']
df=df.loc[df['Column2'].isin(classes)]
</code></pre>

<p>But it does not work. I get this warning and just remains unchanged:</p>

<blockquote>
  <p>FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
    f = lambda x, y: htable.ismember_object(x, values)</p>
</blockquote>

<p>How could it done correctly? Thanks.</p>

<p>Edit: I think I did not explain myself very good.</p>

<p>My data, for example <code>['red' 'blue']</code> does not have comma in the middle. Is type 'object'. I would like to filter the original dataframe in such a way, it shows the rows with the column 'Column2' containing, for example, <code>red</code>. In that case, it would show me rows <code>1</code> and <code>2</code>. Is that possible?</p>
","9903362","","9903362","","2019-08-12 07:07:18","2019-08-12 07:53:52","I cannot filter my dataframe by column values","<python-3.x><pandas><dataframe>","2","0","","","","CC BY-SA 4.0","1"
"56583831","1","","","2019-06-13 15:24:30","","2","87","<p>I am new to Python , and I know is somehow a basic question but has taken my time for a while.</p>

<p>I have table with four columns as <code>date, time, flow, dayType</code> which show how many cars pass some area in specific times in two days(<code>2018-10-05</code> ,<code>2018-10-12</code>) . </p>

<p>Now I want to plot the corresponding flow of each day base on the time on same plot(axes) to compare these days , but the only things I get is an empty plot without showing any diagram!</p>

<pre><code>import matplotlib
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from numpy import split
%matplotlib inline
sns.set()
data = pd.read_excel('friday_frame.xlsx', names=['date', 'time', 'flow', 'dayType'])
fig, ax = plt.subplots(figsize=(12, 4))
dataPivot = pd.pivot_table(data, values='flow', index='time', columns='date').plot()
plt.ylabel('flows');
</code></pre>
","4516219","","7505395","","2019-06-13 15:27:53","2019-06-13 16:34:15","What's the problem with my code , that returns empty plot?","<python><python-3.x><pandas><matplotlib>","1","0","","2019-06-13 16:46:38","","CC BY-SA 4.0","1"
"57375252","1","","","2019-08-06 11:41:24","","0","87","<p>I'm learning python at the moment and I've had success with using CSV's together with Pandas dataframes but I'm now trying to use XML, however I'm not figuring out how 'select' data from elements in my XML.</p>

<p>My XML file looks like this:</p>

<pre class=""lang-xml prettyprint-override""><code>&lt;DATABASE&gt;
    &lt;OBJECT&gt;
        &lt;PROPERTY NAME=""__type"" VALUE="".com.infoblox.dns.bind_a""/&gt;
        &lt;PROPERTY NAME=""address"" VALUE=""192.168.10.1""/&gt;
    &lt;/OBJECT&gt;
    &lt;OBJECT&gt;
        &lt;PROPERTY NAME=""__type"" VALUE="".com.infoblox.dns.bind_a""/&gt;
        &lt;PROPERTY NAME=""address"" VALUE=""192.168.20.1""/&gt;
    &lt;/OBJECT&gt;
    &lt;OBJECT&gt;
        &lt;PROPERTY NAME=""__type"" VALUE="".com.infoblox.dns.bind_ptr""/&gt;
        &lt;PROPERTY NAME=""address"" VALUE=""1.20.168.192.in-addr.arpa""/&gt;
    &lt;/OBJECT&gt;
&lt;/DATABASE&gt;
</code></pre>

<p>I'm trying to figure out how to search my root for PROPERTY elements with NAME __type and VALUE .com.infoblox.dns.bind_a, when I find a OBJECT ELEMENT that contains this NAME and VALUE I want to save some other NAME/VALUE pairs in a dictionary. </p>

<p>My end goal is a dictionary looking like:
{'__type' : ['.com.infoblox.dns.bind_a','.com.infoblox.dns.bind_a'], 'address' : ['192.168.10.1', '192.168.20.1']} </p>

<p>This was achieved using the following code but also includes the .com.infoblox.dns.bind_ptr __type, now I was wondering if anyone can help me with the logic needed to only iterate through PROPERTY elements if the attribute __type == .com.infoblox.dns.bind_a and then if that is the case only put all property name/value pairs under that object in the dictionary.</p>

<pre class=""lang-py prettyprint-override""><code>import xml.etree.ElementTree as et
from  collections import defaultdict

xml_data = open('onedb.xml').read()
root = et.XML(xml_data)
dict = defaultdict(list)

for child in root:
    for newchild in child:
        property = newchild.get('NAME')
        value = newchild.get('VALUE')
        dict[property].append(value)
</code></pre>

<p>So my resulting dictionary now = </p>

<pre><code>print(dict)
defaultdict(&lt;class 'list'&gt;, {'__type': ['.com.infoblox.dns.bind_a', '.com.infoblox.dns.bind_a', '.com.infoblox.dns.bind_ptr'], 'address': ['192.168.10.1', '192.168.20.1', '1.20.168.192.in-addr.arpa']})
</code></pre>

<p>And I want it to look like:</p>

<pre><code>{'__type' : ['.com.infoblox.dns.bind_a','.com.infoblox.dns.bind_a'], 'address' : ['192.168.10.1', '192.168.20.1']}
</code></pre>

<p>The end result will be to put the data in a dataframe from pandas</p>
","5331484","","","","","2019-08-06 15:27:03","XML to Dictionary/Dataframe","<python><xml><python-3.x><pandas><dictionary>","2","0","","","","CC BY-SA 4.0","1"
"57693483","1","","","2019-08-28 13:22:00","","0","87","<p>I am using python spyder, Python v. 3, I need to color format the <code>pandas</code> <code>DataFrame</code> table and export it into HTML or image file, in case if export to image, how to make the image height and width of the table and set appropriate font size.
Could someone please help me on this.</p>
","9697751","","6573902","","2019-08-28 13:24:06","2019-08-28 13:26:11","how to color the pandas table and export it into html","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57904817","1","57904980","","2019-09-12 10:30:18","","0","87","<p>I have a two dictionaries like:</p>

<pre><code>d1 = {'new_list1':['a', 'b', 'c', 'd'], 'new_list2':['a', 'b', 'd', 'e']}
d2 = {'new_list1': [1,2,3,4], 'new_list2': [1,2,4,5]}
</code></pre>

<p>I want output like:</p>

<pre><code>d3 = {'new_list1':[['a',1],['b',2],['c',3],['d',4]], 'new_list2':[['a',1],['b',2],['d',4],['e',5]]}
</code></pre>

<p>Points to See:
1. both the dictionaries will have same number of keys
2. values present in form of list can have different length, so padding as 0 will be required in case of mismatch</p>
","11753555","","11753555","","2019-09-12 10:44:41","2019-09-12 11:08:33","How to merge two dictionaries in a specific way?","<python><python-3.x><pandas>","3","7","0","","","CC BY-SA 4.0","1"
"57105552","1","57105696","","2019-07-19 04:30:10","","1","87","<p>i wondering about something expression of string list in dataframe.
how to split string value using python?
I'm using replace method. 
But, i can't find a way to delete only the node number.</p>

<p>dataframe</p>

<pre><code>index    article_id
0      ['@abc_172', '@abc_249', '@abc-32', '@def-1']
1      ['@az3_2', '@bwc_4', '@xc-34', '@xc-1']   
2      ['@ac_12']
3      ['#ea457870a2d32453609f52e50f84abdc_15', '@bb_3']
4       ...
...     ...
</code></pre>

<p>I want to get like this</p>

<pre><code>index       article_id                      article_id_unique_count
0      ['abc', 'abc', 'abc', 'def']                   2
1      ['az3', 'bwc', 'xc', 'xc']                     3
2      ['ac']                                         1
3      ['#ea457870a2d32453609f52e50f84abdc', 'bb']    2

 ...
</code></pre>
","10411085","","10411085","","2019-07-19 05:44:09","2019-07-19 06:05:19","how to split string data using criterion in python?","<python><python-3.x><pandas><dataframe>","4","0","","","","CC BY-SA 4.0","1"
"57511904","1","57512258","","2019-08-15 15:15:41","","1","86","<p>Just looking forward a solution to remove empty values from a column which has values as a list in a sense where we are already replacing some strings beforehand, where it's a column of string representation of lists.</p>

<p>In <code>df.color</code> we are Just replacing <code>*._Blue</code> with empty string:</p>

<h2>Example DataFrame:</h2>

<pre><code>df = pd.DataFrame({ 'Bird': [""parrot"", ""Eagle"", ""Seagull""], 'color': [ ""['Light_Blue','Green','Dark_Blue']"", ""['Sky_Blue','Black','White', 'Yellow','Gray']"", ""['White','Jet_Blue','Pink', 'Tan','Brown', 'Purple']""] })

&gt;&gt;&gt; df
      Bird                                              color
0   parrot                 ['Light_Blue','Green','Dark_Blue']
1    Eagle      ['Sky_Blue','Black','White', 'Yellow','Gray']
2  Seagull  ['White','Jet_Blue','Pink', 'Tan','Brown', 'Pu...
</code></pre>

<h2>Result of above DF:</h2>

<pre><code>&gt;&gt;&gt; df['color'].str.replace(r'\w+_Blue\b', '')
0                                 ['','Green','']
1           ['','Black','White', 'Yellow','Gray']
2    ['White','','Pink', 'Tan','Brown', 'Purple']
Name: color, dtype: object
</code></pre>

<p>Usually in python it easily been done as follows..</p>

<pre><code>&gt;&gt;&gt; lst = ['','Green','']
&gt;&gt;&gt; [x for x in lst if x]
['Green']
</code></pre>

<p>I'm afraid if something like below can be done.</p>

<pre><code>df.color.mask(df == ' ')
</code></pre>
","5704863","","5704863","","2019-08-15 15:37:11","2019-08-15 15:42:10","How to remove empty values from the pandas DataFrame from a column type list","<regex><python-3.x><pandas><numpy>","3","4","","","","CC BY-SA 4.0","1"
"56978793","1","","","2019-07-10 21:19:22","","-1","86","<p>I'm trying to convert the below dataset into the right format to then plot it into a chord diagram.</p>

<pre><code>    a   b   c   d   e   f   g   h
0   1   0   0   0   0   1   0   0
1   1   0   0   0   0   0   0   0
2   1   0   1   1   1   1   1   1
3   1   0   1   1   0   1   1   1
4   1   0   0   0   0   0   0   0
5   0   1   0   0   1   1   1   1
6   1   1   0   0   1   1   1   1
7   1   1   1   1   1   1   1   1
8   1   1   0   0   1   1   0   0
9   1   1   1   0   1   0   1   0
10  1   1   1   0   1   1   0   0
11  1   0   0   0   0   1   0   0
12  1   1   1   1   1   1   1   1
13  1   1   1   1   1   1   1   1
14  0   1   1   1   1   1   1   0
</code></pre>

<p>The result would be a chord diagram showing all the possible combinations between the variables, with each stream width being the count of a particular combination occurrences within the dataset - for example a + b count is 7 in the dataset above (where both are 1).</p>
","8542692","","8542692","","2019-07-11 22:31:29","2019-07-12 18:51:19","Create a dataset for chord diagram and plot","<python><python-3.x><pandas><numpy><holoviews>","1","0","1","","","CC BY-SA 4.0","1"
"49246790","1","","","2018-03-13 01:13:16","","0","86","<p>I am trying to extract data out of a csv file and output the data to another csv file. </p>

<pre><code>relate task perform     
0   avc asd     
1   12  24      
2   34  54      
3   22  33      
4   11  11      
5   335 534     
Time    A   B   C   D
0   0.334   0.334   0.334   0.334
1   0.543   0.543   0.543   0.543
2   0.752   0.752   0.752   0.752
3   0.961   0.961   0.961   0.961
4   1.17    1.17    1.17    1.17
5   1.379   1.379   1.379   1.379
</code></pre>

<p>I am writing a python script to read the above table. I want all the data from Time, A, B,C, and D onwards in a separate file. </p>

<pre><code>import csv
import pandas as pd
import os

read_file = False


with open ('xyz.csv', mode = 'r', encoding = 'utf-8') as f_read:
reader = csv.reader(f_read)

for row in reader:
    if 'Time' in row
</code></pre>

<p>I am stuck here. I read all the data in 'reader'. All the rows should have been parsed inside 'reader'. Now, how can I extract the data from line with Time and onwards into a separate file? </p>

<p>Is there a better method to achieve the above objective?
Should I use pandas instead of regular python commands?</p>

<p>I read many similar answers on stackoverflow but I am confused on how to finish this problem. Your help is appreciated. </p>

<p>Best</p>
","5428772","","","","","2018-03-13 01:13:16","Extract data out of a csv file","<python><python-3.x><pandas><coding-style>","0","2","","","","CC BY-SA 3.0","1"
"49616277","1","","","2018-04-02 18:00:38","","-2","86","<p>This is the dataset I am working on right now.
<a href=""https://i.stack.imgur.com/vlgj6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vlgj6.jpg"" alt=""Dataset""></a></p>

<p>I want to find the maximum value of the cycle column for a single value of the id column. For ex, I want to find out the max cycle value for id-1, id-2 and so on but I can't really figure out the loop logic. How do I get the code to ouptut maximum cycle value for particular id? My dataset has id values from 1-100 so the code should give me a 100 max cycle values.</p>
","6073158","","9209546","","2018-04-02 21:07:24","2018-04-02 21:07:24","How to iterate over rows of a single column based on value of another column?","<python><python-3.x><pandas>","1","5","","2018-04-04 14:15:01","","CC BY-SA 3.0","1"
"57225782","1","","","2019-07-26 19:06:51","","3","86","<p>I have a CSV file with 100K+ lines of data in this format:</p>

<pre><code>""{'foo':'bar' , 'foo1':'bar1', 'foo3':'bar3'}""


""{'foo':'bar' , 'foo1':'bar1', 'foo4':'bar4'}""
</code></pre>

<p>The quotes are there before the curly braces because my data came in a CSV file.</p>

<p>I want to extract the key value pairs in all the lines to create a dataframe like so:</p>

<pre><code>Column Headers: foo, foo1, foo3, foo...


Rows:           bar, bar1, bar3, bar...
</code></pre>

<p>I've tried implementing something similar to what's explained here ( <a href=""https://stackoverflow.com/questions/29270031/python-error-parsing-strings-from-text-file-with-ast-module"">Python: error parsing strings from text file with Ast module</a>).</p>

<p>I've gotten the ast.literal_eval function to work on my file to convert the contents into a dict but now how do I get the DataFrame function to work? I am very much a beginner so any help would be appreciated.</p>

<pre><code>import pandas as pd
import ast

with open('file_name.csv') as f:
        for string in f:
            parsed = ast.literal_eval(string.rstrip())
            print(parsed)


pd.DataFrame(???)
</code></pre>
","11842999","","10035985","","2019-07-26 19:07:53","2019-07-26 21:38:32","Converting a string representation of dicts to an actual dict","<python><python-3.x><pandas>","2","2","","","","CC BY-SA 4.0","1"
"57160788","1","57174671","","2019-07-23 09:19:59","","0","85","<p>I've created a web scraper that scrapes the Yahoo Finance Summary and Statistics page of a stock for Python programming educational purposes only. It reads from the '1stocklist.csv' in the programs directory which looks like this:</p>
<pre><code>Symbols
SNAP
KO
</code></pre>
<p>From there, it adds the new information to new columns in the dataframe as it should. There are a lot of 'for' loops in there and I'm still tweaking it as it's not grabbing some data correctly, but it's fine for now.</p>
<p>My problem is trying to save the dataframe to a new .csv file. The way it outputs right now as you'll see is something like this:</p>
<p><a href=""https://i.stack.imgur.com/XDSCw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XDSCw.png"" alt=""wrong output"" /></a></p>
<p>The SNAP row should begin with the 14.02 and everything right, and the next row should be KO beginning with the 51.39 and over.</p>
<p>Any ideas? Just create a 1stocklist.csv file that looks like the above and try it. Thanks!</p>
<pre><code># Import dependencies
from bs4 import BeautifulSoup
import re, random, time, requests, datetime, csv
import pandas as pd
import numpy as np


# Use Pandas to read the &quot;1stocklist.csv&quot; file. We'll use Pandas so that we can append a 'dataframe' with new
# information we get from the Zacks site to work with in the program and output to the 'data(date).csv' file later
maindf = pd.read_csv('1stocklist.csv', skiprows=1, names=[
# The .csv header names
    &quot;Symbols&quot;
    ]) #, delimiter = ',')

# Setting a time delay will help keep scraping suspicion down and server load down when scraping the Zacks site
timeDelay = random.randrange(2, 8)


# Start scraping Yahoo
print('Beginning to scrape Yahoo Finance site for information ...')
tickerlist = len(maindf['Symbols']) # for progress bar


# Create a progress counter to display how far along in the zacks rank scraping it is
zackscounter = 1

# For every ticker in the stocklist dataframe
for ticker in maindf['Symbols']:

# Print the progress
    print(zackscounter, ' of ', tickerlist, ' - ', ticker) # for seeing which stock it's currently on

# The list of URL's for the stock's different pages to scrape the information from
    summaryurl = 'https://ca.finance.yahoo.com/quote/' + ticker
    statsurl = 'https://ca.finance.yahoo.com/quote/' + ticker + '/key-statistics'

# Define the headers to use in Beautiful Soup 4
    headers = requests.utils.default_headers()
    headers['User-Agent'] = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'

# Employ random time delay now before starting with the (next) ticker
    time.sleep(timeDelay)





# Use Beautiful Soup 4 to get the info from the first Summary URL page
    page = requests.get(summaryurl, headers=headers)
    soup = BeautifulSoup(page.text, 'html.parser')

    counter = 0 # used to tell which 'td' it's currently looking at
    table = soup.find('div', {'id' :'quote-summary'})
    for i in table.find_all('span'):
        counter += 1
        if counter % 2 == 0: # All Even td's are the metrics/numbers we want
            data_point = i.text
            #print(data_point)
            maindf[column_name] = data_point # Add the data point to the right column
        else:                # All odd td's are the header names
            column_name = i.text
            #print(column_name)





# Use Beautiful Soup 4 to get the info from the second stats URL page
    page = requests.get(statsurl, headers=headers)
    soup = BeautifulSoup(page.text, 'html.parser')
    time.sleep(timeDelay)
# Get all the data in the tables
    counter = 0 # used to tell which 'td' it's currently looking at
    table = soup.find('section', {'data-test' :'qsp-statistics'})
    for i in table.find_all('td'):
        counter += 1
        if counter % 2 == 0: # All Even td's are the metrics/numbers we want
            data_point = i.text
            #print(data_point)
            maindf[column_name] = data_point # Add the data point to the right column
        else:                # All odd td's are the header names
            column_name = i.text
            #print(column_name)





    file_name = 'data_raw.csv'
    if zackscounter == 1:
        maindf.to_csv(file_name, index=False)
    else:
        maindf.to_csv(file_name, index=False, header=False, mode='a')

    zackscounter += 1
    continue
</code></pre>
<p>UPDATE:</p>
<p>I know it’s something to do with how I’m trying to append the dataframe to the .csv file at the end. My beginning dataframe is just one column with all the ticker symbols in it, then it’s trying to add each new column to the dataframe as the program goes along, and fills down to the bottom of the ticker list. What I’m wanting to happen is to just add the column_name header as it should, and then append the appropriate data specific to the one ticker and do that for each ticker in the “Symbols” column of my dataframe. Hope that provides some clarity to the issue?</p>
<p>I’ve tried using .loc in various ways but to no success. Thanks!</p>
","10018602","","-1","","2020-06-20 09:12:55","2019-07-24 03:08:29","Appending to next row in dataframe, from within a for loop","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"42282103","1","","","2017-02-16 18:38:49","","1","85","<p>I'm trying to pass a variable into a <code>DataFrame</code> (ultimately to copy the variable to the system clipboard)</p>

<pre><code>import sys
import dropbox
import pandas as pd
dbx = dropbox.Dropbox('zzzzzzzzzzz')

def getSharedLink(full_path):
    try:
        link = dbx.sharing_create_shared_link(full_path).url
    except dropbox.exceptions.ApiError as err:
        print('*** API error', err)
        return None
df=pd.DataFrame(""link"")
df.to_clipboard(index=False,header=False)

getSharedLink(""/z/z"")
</code></pre>

<p>I know text should be formatted like (['text']) , I`m missing how I can use the variable ""link"" instead.</p>
","5224728","","1040092","","2017-02-16 19:29:03","2017-02-16 19:43:22","Python - pass variable to DataFrame","<python><python-3.x><pandas>","0","0","","","","CC BY-SA 3.0","1"
"57555217","1","57555644","","2019-08-19 10:47:53","","0","84","<p>Is there perhaps another function in Python I can use to group customers' transactions? Let's say a specific word is contained in a transaction, and there are multiple transactions that have the same name, then group them together.</p>

<p>I used this code, but it will be too long, because I have thousands of unique transactions from different merchants.</p>

<pre><code>temp=tranx.TRANX.fillna(""0"")
tranx['Activity_2'] = pd.np.where(temp.str.contains(""PNP ""),""PICKNPAY"",
               pd.np.where(temp.str.contains(""CHECKERS""), ""CHECKERS"",
               pd.np.where(temp.str.contains(""MRPRICE""), ""MRPRICE"",
               pd.np.where(temp.str.contains(""FOOD LOVER""), ""FOODLOVERMARKET"",
               pd.np.where(temp.str.contains(""DISCHEM""), ""DISCHEM"",
                pd.np.where(temp.str.contains(""DIS-CHEM""), ""DISCHEM"",           
                pd.np.where(temp.str.contains(""OK FOODS""), ""OKFOODS"",
                pd.np.where(temp.str.contains(""DISCHEM""), ""DISCHEM"",
                pd.np.where(temp.str.contains(""FASHION EXPRESS""), ""FASHIONEXPRESS"",
                pd.np.where(temp.str.contains(""MTC""), ""MTC"",
                pd.np.where(temp.str.contains(""TELECOM""), ""TELECOM"",
                pd.np.where(temp.str.contains(""KFC""), ""KFC"",
                pd.np.where(temp.str.contains(""ACKERMANS""), ""ACKERMANS"",
                pd.np.where(temp.str.contains(""SHOPRITE""), ""SHOPRITE"",
                pd.np.where(temp.str.contains(""USAVE""), ""SHOPRITE"",            
                pd.np.where(temp.str.contains(""S/STATION""), ""SERVICESTATION"",
                pd.np.where(temp.str.contains(""SERVICE STATION""), ""SERVICESTATION"",
                pd.np.where(temp.str.contains(""SOULSTICE DAY SPA""), ""SOULSTICESPA"",
                pd.np.where(temp.str.contains(""CLICKS"" ), ""CLICKS"",
                pd.np.where(temp.str.contains(""JET ""), ""JET"",
                pd.np.where(temp.str.contains(""PEP ""), ""PEP"",           
               pd.np.where(temp.str.contains(""WOERMANN""), ""WOERMANN"", ""OTHER""))))))))))))))))))))))
</code></pre>

<p>Is there no way where I can create a list that contains all the merchants and then come up with a loop which loops in every row to identify if the merchant name appears in that row, then IF YES OUTPUT MERCHANT NAME, IF NO CLASSIFY THE TRANSACTION AS OTHER?</p>

<p>Below is a sample of the data:</p>

<p><a href=""https://i.stack.imgur.com/96lO4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/96lO4.png"" alt=""Transactions Sample""></a></p>
","11841670","","6395052","","2019-08-19 12:19:59","2019-08-19 12:19:59","Is there a python function I can use to group retail banking transactions?","<python><python-3.x><pandas><numpy>","2","3","","","","CC BY-SA 4.0","1"
"57553084","1","57553232","","2019-08-19 08:35:06","","1","84","<p>with columns as L, W, H, D. Each of them have range from 10 to 100, if it goes beyond this it is fail else it is pass</p>

<p>How can i simplify this code. I tried using if condition which failed</p>

<pre><code>def Target(FM):

     if (df['L'] &lt; 10 or df['L'] &gt; 120):
        return 'L-Fail'
     else:
        return 'Pass'


     elif (df['W'] &lt; 10 or df['W'] &gt; 120):
        return 'W-Fail'
     else:
        return 'Pass'


     elif (df['H'] &lt; 10 or df['H'] &gt; 120):
        return 'H-Fail'
     else:
        return 'Pass'


     elif(df['D'] &lt; 10 or df['D'] &gt; 120):
        return 'D-Fail'
     else:
        return 'Pass'


     df['Remarks_Target'] = df.apply(Target, axis = 1)

      L       W       H       D     Remarks

      1       20      30      40    L-Fail
      10      40      0       50    Pass
      15      30      30      60    Pass
      60      90      80      300   D-Fail
      50      30      30      120   Pass
      10      10      120     120   Pass
      30      20      9       80    H-Fail
      14      5       85      34    W-Fail
</code></pre>
","7905329","","7905329","","2019-08-20 08:14:13","2019-08-20 08:14:13","How to simplify code to check ranges in pandas","<python><python-3.x><pandas><dataframe><if-statement>","3","0","","","","CC BY-SA 4.0","1"
"56669290","1","","","2019-06-19 13:49:29","","0","84","<p>I've a pandas DataFrame with Multi-Level index of 4 levels (say Name, Year, Month, Date) and i want to make a nested JSON from this DataFrame which has 2 columns namely, col1 &amp; col2, with arbitrary numbers.</p>

<p>I've tried <code>df.to_json()</code> with different 'orient' values but it doesn't give json in required nested structure.</p>

<p>The structure of the nested JSON should be like:
'Name' within should be 'Year' within which should be 'Month' within which should be 'Date' and then the column values in a list or dictionary. So, for the key of 'Year' we will have 12 months objects, and so on.</p>
","11670600","","9995930","","2019-06-19 14:05:02","2019-06-19 14:05:02","How to get a nested JSON from a pandas dataframe which has multi-level index of 4 levels?","<json><python-3.x><pandas>","0","2","","","","CC BY-SA 4.0","1"
"56755075","1","","","2019-06-25 13:21:29","","0","84","<p>In my Python Code, I would also like Dakota with Hurricane, display appearances to show, in the Data Table, when run in Jupyter Notebook. </p>

<p>I typed the following modification to the Code, aiming to achieve this :-</p>

<pre><code>(df['Spitfire'].str.contains('S', na=True))
</code></pre>

<p>Now the Dakota with Hurricane Display booking, i.e. in this case for Worthing - Display, that Data Displays, as does the Dakota Spitfire and Hurricane, and Dakota with Spitfire Display Bookings. But also the Solo Dakota Display bookings, which I don't want to display. What do I type to enable, that when Dakota = 'D' and 'Spitfire' = 'NaN' and 'Hurricane' = 'NaN', that Row is not displayed ? </p>

<p>I have almost managed, to sort out what I need to, in my Python code, for the 2007 Url, I just need, the Dakota with Hurricane bookings issue, sorting out Here is my Code, containing the relevant Url :-</p>

<pre><code>import pandas as pd
import requests
from bs4 import BeautifulSoup

res = requests.get(""http://web.archive.org/web/20070701133815/http://www.bbmf.co.uk/june07.html"")
soup = BeautifulSoup(res.content,'lxml')
table = soup.find_all('table')[0]

df = pd.read_html(str(table))
df = df[1]
df = df.rename(columns=df.iloc[0])
df = df.iloc[2:]
df.head(15)

display = df[(df['Location'].str.contains('- Display')) &amp; (df['Dakota'].str.contains('D')) &amp; (df['Spitfire'].str.contains('S', na=True)) &amp; (df['Lancaster'] != 'L')]     
display
</code></pre>

<p>Any help would be much appreciated.</p>

<p>Regards</p>

<p>Eddie</p>
","9813876","","9813876","","2019-06-26 15:18:04","2019-08-31 12:21:29","I Need Assistance With Data Sorting In Python Code","<python><python-3.x><pandas><jupyter-notebook><code-cleanup>","1","0","","","","CC BY-SA 4.0","1"
"56768874","1","56768914","","2019-06-26 08:53:38","","1","84","<p>From a df column 'Desc' I want to extract substrings that start with n or N followed by a digit, here's a test df with my code and result:</p>

<pre><code>import pandas as pd
testdf = pd.DataFrame({'Desc': ['n1.2A Full Version', 'N5.0.0 Bridge', 'N5.35A Automatic', 'n2 Bridge']})
testdf['Version'] = testdf['Desc'].str.extract(r'([nN]\d.+?[\s])', expand=False)
</code></pre>

<p>How to fix the regex so that it doesn't show NaN for the last record? Thanks </p>
","9144386","","9144386","","2019-06-26 08:56:40","2019-06-26 08:56:40","Pandas extract substring with optional in pattern","<regex><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57462434","1","57462514","","2019-08-12 13:47:10","","0","84","<p>I've created a list of dataframes for which I've to fetch the names of the same dataframes to use it to name the newly generated dataframes. I want to name the newly generated dataframes from the 'for-loop' similar to (signifying) their respective dataframe.</p>

<pre class=""lang-py prettyprint-override""><code>list_containing_dataframes = [Furnishings, Labels, Accessories, Binders, Supplies, Phones, Tables]

for i in list_containing_dataframes:
  # some operations done on 'i' to generate new dataframes !
  # let's save the newly generated dataframes! 
  newly_generated_dataframes.to_csv(str(i)+'_forecast' , index = False, encoding = 'utf-8')

#  .to_csv() function is being used to save the dataframes in csv format in google colab!
# Expected - The name of newly generated dataframes to be 'name of dataframe #operated on'+'forecast'.
# example- Tables_forecast, where df_Tables is the name of Dataframe operated on!
# Error - str(i) in '.to_csv()' function prints the name along with the index column which becomes so long that it throws error 'name too long!'
# I just want the name of the dataframe along with string '_forecast' attached to it!
</code></pre>
","7120445","","","","","2019-08-12 13:51:50","how to fetch/extract the names of dataframes from a list of dataframes in a for-loop to use it inside the same for-loop?","<python><python-3.x><pandas><pandas-groupby>","1","3","","","","CC BY-SA 4.0","1"
"50421585","1","50421659","","2018-05-19 03:49:45","","0","83","<p>Please, considere the dataframe df generated below:</p>

<pre><code>import pandas as pd

def creatingDataFrame():

    raw_data = {'code': [1, 2, 3, 2 , 3, 3],                
                'var1': [10, 20, 30, 20 , 30, 30],
                'var2': [2,4,6,4,6,6],
                'price': [20, 30, 40 , 50, 10, 20],
                'sells': [3, 4 , 5, 1, 2, 3]}
    df = pd.DataFrame(raw_data, columns = ['code', 'var1','var2', 'price', 'sells'])
    return df


if __name__==""__main__"":

    df=creatingDataFrame()

    setCode=set(df['code'])


    listDF=[]
    for code in setCode:
        dfCode=df[df['code'] == code].copy()
        print(dfCode)
        lenDfCode=len(dfCode)
        if(lenDfCode==1):
            theData={'code': [dfCode['code'].iloc[0]],                
                'var1': [dfCode['var1'].iloc[0]],
                'var2': [dfCode['var2'].iloc[0]],
                'averagePrice': [dfCode['price'].iloc[0]],
                'totalSells': [dfCode['sells'].iloc[0]]
            }
        else:
            dfCode['price*sells']=dfCode['price']*dfCode['sells']
            sumSells=np.sum(dfCode['sells'])
            sumProducts=np.sum(dfCode['price*sells'])
            dfCode['totalSells']=sumSells
            av=sumProducts/sumSells
            dfCode['averagePrice']=av
            theData={'code': [dfCode['code'].iloc[0]],                
                'var1': [dfCode['var1'].iloc[0]],
                'var2': [dfCode['var2'].iloc[0]],
                'averagePrice': [dfCode['averagePrice'].iloc[0]],
                'totalSells': [dfCode['totalSells'].iloc[0]]
            }
        dfPart=pd.DataFrame(theData, columns = ['code', 'var1','var2', 'averagePrice','totalSells'])
        listDF.append(dfPart)
    newDF = pd.concat(listDF)
    print(newDF)
</code></pre>

<p>I have this dataframe</p>

<pre><code>   code  var1  var2  price  sells
0     1    10     2     20      3
1     2    20     4     30      4
2     3    30     6     40      5
3     2    20     4     50      1
4     3    30     6     10      2
5     3    30     6     20      3
</code></pre>

<p>I want to generate the following dataframe:</p>

<pre><code>   code  var1  var2  averagePrice  totalSells
0     1    10     2          20.0           3
0     2    20     4          34.0           5
0     3    30     6          28.0          10
</code></pre>

<p>Note that this dataframe is created from the first by evaluating the average price and total sells for each code. Furthermore, var1 and var2 are the same for each code. The python code above does that, but I know that it is inefficient. I believe that a desired solution can be done using groupby, but I am not able to generate it.</p>
","2065691","","","","","2018-05-19 04:24:54","Efficient evaluation of weighted average variable in a Pandas Dataframe","<python-3.x><pandas><dataframe><pandas-groupby>","1","6","","","","CC BY-SA 4.0","1"
"57605863","1","57626617","","2019-08-22 09:13:22","","0","83","<p><strong>Background</strong>:
Excel sheet with mapping. General appearance like this:</p>

<pre><code>                Req1     Req2     Req3 ..... Req10
                A  B     A  B     A  B       A   B
Id     Text      
1      abc         x     x                       x
2      def               x
3      ghi                  x
4      jkl                                       x
5      mno      x                     
</code></pre>

<p>Edit: <a href=""https://i.stack.imgur.com/2j5p2.png"" rel=""nofollow noreferrer"">Screenshot of Excel File</a></p>

<p><strong>Question</strong>: How to extract only the columns with the marked 'x'? That is, Column Req3 would not be considered. </p>

<p><strong>Challenge</strong>: Column headers are merged cells in the original excel - and at a different level than the ""Text"" column. </p>

<p><strong>Already Tried</strong>: Looking into .groupby() function and for loops. But not sure how to proceed with either options (if they are applicable). </p>

<p>IDEA: (update) Splitting the file into two separate dataframes (Text + Reqs). Deleting the rows that cause the difference between the placing of the headers. Then rejoining the separate dataframes ---- But how?</p>

<p>Already have the entire excel file converted to .csv and opened as dataframe in jupyternotebooks code. Can also extract separate columns, but because of the difference between the column titles, cannot get ""ReqN"" to be part of dataframe. </p>

<p><strong>Expected Result</strong>: Pandas dataframe with column ""Text"" and the column ""ReqN"" where the ""x"" occurs. (Not interested in whether it is A or B that is marked.)</p>

<p><strong>Next Step</strong>: (in case relevant for solution above) to store the ""Text""+ corresponding ""ReqN"" somehow as a csv file. </p>

<p>Newbie to python and pandas. Would really appreciate some guidance for the code. Found several examples on SO, but they fell short somewhere or the other.</p>
","11900800","","11900800","","2019-08-22 10:08:34","2019-08-23 12:55:41","Extracting dataframe columns for mapped data","<python-3.x><pandas><dataframe><artificial-intelligence>","1","5","","","","CC BY-SA 4.0","1"
"57744563","1","57744655","","2019-09-01 08:52:42","","0","83","<p>My question is on how to create a list of dictionaries from a list of lists, and is a deviation from this question <a href=""https://stackoverflow.com/questions/46612230/dict-comprehension-python-from-list-of-lists"">Dict Comprehension python from list of lists</a> and this question <a href=""https://stackoverflow.com/questions/15380073/list-of-lists-to-list-of-dictionaries"">List of Lists to List of Dictionaries</a></p>

<p>I have a very long pandas data frame with latitudes and longitudes as such</p>

<pre><code>     LAT        LON
     40         5
     40         6
     41         5
     42         8
     42         9
</code></pre>

<p>I managed to transform it into a list of lists with towers.values(towers is the name of the dataframe) in which each list is of the format [lat,lon]</p>

<p>My goal is to have</p>

<pre><code>list_dict = [{'lat': 40, 'lon': 5}, 
             {'lat': 40, 'lon': 6}, 
             {'lat': 41, 'lon': 5},
             {'lat': 41, 'lon': 5},
             {'lat': 42, 'lon': 8},
             {'lat': 42, 'lon': 9}]
</code></pre>

<p>How can I do this?</p>
","11053294","","9698684","","2019-09-01 09:05:41","2019-09-01 09:45:50","Creating a list of dictionaries from a list of lists in Python","<python><python-3.x><pandas><list><dictionary>","2","1","","","","CC BY-SA 4.0","1"
"57550432","1","57550503","","2019-08-19 04:18:40","","0","82","<p>My pd.df looks like this:</p>

<pre><code>                          open     high      low     close   volume
timestamp  expiry                                                  
2018-09-10 2018-09-21  2885.25  2888.25  2876.50  2880.250   999262
           2018-12-21  2889.75  2893.00  2881.25  2885.000    15999
2018-09-11 2018-09-21  2871.25  2893.00  2867.25  2889.750   973957
           2018-12-21  2876.00  2897.75  2872.25  2894.500    25031
2018-09-12 2018-09-21  2888.00  2895.25  2879.50  2888.375  1252385
           2018-12-21  2893.00  2900.25  2884.50  2893.375    54971
2018-09-13 2018-09-21  2899.75  2907.00  2896.50  2905.250   978670
           2018-12-21  2905.00  2912.00  2901.75  2910.250   226989
2018-09-14 2018-09-21  2907.00  2909.50  2896.75  2906.250   507802
           2018-12-21  2912.25  2914.50  2902.00  2911.375   703911
2018-09-17 2018-09-21  2904.50  2905.25  2887.25  2891.000   311638
           2018-12-21  2909.50  2910.25  2892.25  2896.000   894660
2018-09-18 2018-09-21  2892.25  2912.50  2892.00  2906.500   206016
           2018-12-21  2897.75  2917.75  2897.25  2911.750   946741
2018-09-19 2018-09-21  2906.00  2913.75  2904.75  2909.875   130272
           2018-12-21  2911.50  2918.75  2910.00  2915.125   828194
2018-09-20 2018-09-21  2922.50  2935.75  2921.25  2934.125   132722
           2018-12-21  2927.75  2940.75  2926.25  2939.375  1063115
2018-09-21 2018-12-21  2945.25  2946.50  2932.00  2933.750  1181406
</code></pre>

<p>I need to select the rows where the volume is bigger. Not sure how to use <code>groupby()</code> or possibly <code>drop_duplicates()</code> for this. </p>

<p>Desired output should look like this:</p>

<pre><code>                          open     high      low     close   volume
timestamp  expiry                                                  
2018-09-10 2018-09-21  2885.25  2888.25  2876.50  2880.250   999262
2018-09-11 2018-09-21  2871.25  2893.00  2867.25  2889.750   973957
2018-09-12 2018-09-21  2888.00  2895.25  2879.50  2888.375  1252385
2018-09-13 2018-09-21  2899.75  2907.00  2896.50  2905.250   978670
2018-09-14 2018-12-21  2912.25  2914.50  2902.00  2911.375   703911
2018-09-17 2018-12-21  2909.50  2910.25  2892.25  2896.000   894660
2018-09-18 2018-12-21  2897.75  2917.75  2897.25  2911.750   946741
2018-09-19 2018-12-21  2911.50  2918.75  2910.00  2915.125   828194
2018-09-20 2018-12-21  2927.75  2940.75  2926.25  2939.375  1063115
2018-09-21 2018-12-21  2945.25  2946.50  2932.00  2933.750  1181406
</code></pre>

<p>Grateful for your help!</p>
","8166819","","","","","2019-08-19 04:34:59","Use groupby() and condition to select rows in pd.df","<python><python-3.x><pandas><pandas-groupby>","2","0","","2019-08-20 10:17:18","","CC BY-SA 4.0","1"
"57747028","1","","","2019-09-01 14:56:21","","1","82","<p>I have following dataframe in pandas</p>

<pre><code>data = {'call_put':['C', 'C', 'P','C', 'P'],'price':[10,20,30,40,50], 'qty':[11,12,11,14,9]}
df['amt']=df.price*df.qty
df=pd.DataFrame(data)


call_put    price   qty amt
0   C   10  11  110
1   C   20  12  240
2   P   30  11  330
3   C   40  14  560
4   P   50  9   450
</code></pre>

<p>I want output something like following based on call_put value is 'C' or 'P' count, median and calculation as follows</p>

<pre><code>call_put price  qty amt      cummcount    cummmedian               cummsum           

C   10  11  110      1            110                       110
C   20  12  240      2            175    ((110+240)/2 )     350  
P   30  11  330      1            330                       680
C   40  14  560      3            303.33 (110+240+560)/3   1240
P   50  9   450      2            390 ((330+450)/2)        1690
</code></pre>

<p>Can it be done in some easy way without creating additional dataframes and functions?</p>
","9389773","","4420967","","2019-09-01 17:42:02","2019-09-03 08:52:23","How to do cumulative mean and count in a easy way","<python-3.x><pandas>","3","0","","","","CC BY-SA 4.0","1"
"57079500","1","","","2019-07-17 15:31:02","","0","82","<p>Let me define a simple dataframe:</p>

<pre><code>In  [1]: df = pd.DataFrame({'a': [True, False], 'b': [1, 2]})
</code></pre>

<p>The data type of the <code>'a'</code> column is stored as <code>bool</code>:</p>

<pre><code>In  [2]: df['a'].dtype
Out [2]: dtype('bool')
</code></pre>

<p>If I then set <code>'a'</code> as the index column of the dataframe:</p>

<pre><code>In  [3]: df.set_index('a', inplace=True)
</code></pre>

<p>The dtype of the index column is now <code>object</code>:</p>

<pre><code>In  [4]: df.index
Out [4]: Index([True, False], dtype='object', name='a')
</code></pre>

<p>If I use the integer column <code>'b'</code> as the index, the dtype of the index is as expected:</p>

<pre><code>In  [5]: df.reset_index(inplace=True)
         df.set_index('b', inplace=True)
         df.index
Out [5]: Int64Index([1, 2], dtype='int64', name='b')
</code></pre>

<p>Any ideas why this is?</p>
","1750612","","","","","2019-07-17 15:36:21","Why does pandas store a boolean index with an object dtype?","<python><python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"57603132","1","57603222","","2019-08-22 06:24:51","","3","82","<p>I am new to pandas, I am getting a result in reverse order of my expected result.</p>

<p>What I have tried is:</p>

<p>o_rg,o_gg,a_rg,a_gg are arrays</p>

<pre><code>    df1=pd.DataFrame({'RED':o_rg,'GREEN':o_gg})
df2=pd.DataFrame({'RED':a_rg,'RED':a_gg})
df=df1-(df2)
print(df)
pop_complete = pd.concat([df.T,
                          df1.T,
                          df2.T],
                          keys=[""O-A"", ""O"", ""A""])
df = pop_complete.swaplevel()
df.sort_index(inplace=True)

print(df)
df.to_csv(""OUT.CSV"")
</code></pre>

<p>What I get the output as:</p>

<pre><code>             0      1       2
RED        A        14.0    12.0    15.0
           O        14.0    12.0    15.0
           O-A      0.00    0.00    0.00
GREEN      A        12.0    10.0    12.0
           O        14.0    9.0     12.0
           O-A      -2.0    1.0     0.0
</code></pre>

<p>What I actually want is:</p>

<pre><code>                    RED     GREEN       

        A1 O        14.0     14.0
           A        14.0     12.0
           O-A      0.0      2.0

        A3 O        12.0     9.0
           A        12.0     10.0
           O-A      0.0      -1.0

        A8 O        15.0     12.0
           A        15.0     12.0
           O-A      0.0      0.0

 where 'A1','A3','A8' ... can be stored in array cases=[]
</code></pre>

<p>How to get the actual output?</p>
","11591582","","11591582","","2019-08-27 02:35:22","2019-08-27 02:35:22","How to transpose row and columns using pandas?","<python><python-3.x><pandas><rows>","1","3","","","","CC BY-SA 4.0","1"
"40781795","1","","","2016-11-24 08:57:50","","0","82","<p>I have a table in pandas df</p>

<pre><code>id_x  id_y
a      b
b      c
a      c
d      a
x      a
m      b
c      z
a      k
b      q
d      w
a      w
q      v
</code></pre>

<p>How to read this table is :</p>

<p>the combinations for a is, a-b,a-c,a-k,a-w, similarly for b(b-c,b-q) and so on..
I want to write a function which takes id_x from the df <code>def test_func(id)</code></p>

<p>and check whether the occurrences of that id is greater than 3 or not, which may be done by <code>df['id_x'].value_counts</code> .</p>

<p>for eg.</p>

<pre><code>def test_func(id):
    if id_count &gt;= 3:
       print 'yes'
       ddf = df[df['id_x'] == id]
       ddf.to_csv(id+"".csv"")
    else:
       print 'no'
       while id_count &lt;3:
           # do something.(I've explained below what I have to do when count&lt;3)
</code></pre>

<p>Say for b the occurrence is only 2(i.e b-c, and b-q) which is less than 3.</p>

<p>so in such case, look if 'c'(from id_y) has any combinations.</p>

<p>c has 1 combination(c-z) and similarly q has 1 combination(q-v)</p>

<p>thus b should be linked with z and v.</p>

<pre><code>id_x   id_y
b       c
b       q
b       z
b       v
</code></pre>

<p>and store it in ddf2 like we stored for >10.</p>

<p>Also for particular id,if I could have csv saved with the name of id. 
I hope I explained my question correctly, I am very new to python and I don't know to write functions, this was my logic.</p>

<p>Can anyone help me with the implementation part.
Thanks in advance.</p>
","6803114","","6803114","","2016-11-24 09:16:42","2016-11-25 07:28:12","Complex function for getting combinations of one column with other","<python><python-2.7><python-3.x><pandas>","2","2","","","","CC BY-SA 3.0","1"
"33504026","1","","","2015-11-03 16:18:46","","2","82","<p>About a year ago I wrote a script that took a single column of datetime values and ran a window through the series to determine the greatest ""lumping"" of values based on an adjustable dimension of time. For example, given a million date time values what is the maximum value of entries that exist within 1 second, or 1 minute, or 1 hour of each other.</p>

<p>The problem is that I had a machine blow up on me and lost some of the documentation, specifically the versions of packages that I was working with. I think I've updated the code to execute within 3.x but am now getting errors that seem to suggest that pandas no longer supports the packages I'm trying to use. I've tried just installing a few random versions, updating pip, etc., but am not having much luck.</p>

<p>The exact error states, 'UserWarning: Installed openpyxl is not supported at this time. Use >=1.61 and &lt;2.0.0' -- I'm not seeing a version history in their repository. Might just try installing older versions of Python and trying to bash this into place.</p>

<p>Here is the code: </p>

<pre><code>import numpy as np
import pandas as pd

# Your original code was correct here. I assumed there will be a data column along with the timestamps.
df = pd.read_csv(""ET.txt"", parse_dates=[""dt""])

# Construct a univariate `timeseries` instead of a single column dataframe as output by `read_csv`.
# You can think of a dataframe as a matrix with labelled columns and rows. A timeseries is more like
# an associative array, or labelled vector. Since we don't need a labelled column, we can use a simpler
# representation.
data = pd.Series(0, df.dt)  
print(data)
window_size = 1
buckets_sec = data.resample(""1S"", how=""count"").fillna(0)

# We have to shift the data back by the same number of samples as the window size. This is because `rolling_apply`
# uses the timestamp of the end of the period instead of the beginning. I assume you want to know when the most
# active period started, not when it ended. Finally, `dropna` will remove any NaN entries appearing in the warmup
# period of the sliding window (ie. it will output NaN for the first window_size-1 observations).
rolling_count = pd.rolling_apply(buckets_sec, window=window_size, func=np.nansum).shift(-window_size).dropna()
print(rolling_count.describe())

# Some interesting data massaging
# E.g. See how the maximum hit count over the specified sliding window evolves on an hourly
# basis:
seconds_max_hits = rolling_count.resample(""S"", how=""max"").dropna()

# Plot the frequency of various hit counts. This gives you an idea how frequently various
# hit counts occur.
seconds_max_hits.hist()

# Same on a daily basis
daily_max_hits = rolling_count.resample(""S"", how=""max"").dropna()
</code></pre>

<p>Screen cap of the error: <a href=""http://i.imgur.com/uSv29I5.png"" rel=""nofollow"">http://i.imgur.com/uSv29I5.png</a></p>
","5520568","","","","","2015-11-03 21:51:02","Error with openpyxl after converting a script to Python 3.x that utilizes pandas and numpy","<python-3.x><pandas><openpyxl>","1","0","","","","CC BY-SA 3.0","1"
"57262369","1","57262414","","2019-07-29 23:09:17","","0","81","<p>I have the following <code>.json</code> response that I have sourced from the <code>spreadsheets.values.get</code> method within the google sheets API</p>

<pre><code>{'majorDimension': 'ROWS',
 'range': 'Sheet1!A1:D5',
 'values': [['Item', 'Cost', 'Stocked', 'Ship Date'],
            ['Wheel', '$20.50', '4', '3/1/2016'],
            ['Door', '$15', '2', '3/15/2016'],
            ['Engine', '$100', '1', '3/20/2016'],
            ['Totals', '$135.50', '7', '3/20/2016']]}
</code></pre>

<p>Within the json response above, the obvious column headers are <code>['Item','Cost','Stocked','Ship Date']</code> but the <code>.json</code> response appears to have these column headers included within the actual data records under the ""values"" label.</p>

<p>My intention is to read this .json response into a pandas df with the columns names sourced from the top row of ""values"" <code>['Item','Cost','Stocked','Ship Date']</code> . I am aware that you can use the pandas <code>json_normalize</code> method to flatten the ""values"" records but I haven't been able to separate the column headers as yet - can anyone help me filter the top row of ""values"" into a column header?</p>

<p>Suggested that I do the following:</p>

<pre><code>response = {'majorDimension': 'ROWS',
 'range': 'Sheet1!A1:D5',
 'values': [['Item', 'Cost', 'Stocked', 'Ship Date'],
            ['Wheel', '$20.50', '4', '3/1/2016'],
            ['Door', '$15', '2', '3/15/2016'],
            ['Engine', '$100', '1', '3/20/2016'],
            ['Totals', '$135.50', '7', '3/20/2016']]}

sheet_values = response.get('values', [])

df = pd.DataFrame(sheet_values['values'],columns=sheet_values['values'][0]).drop(0)

</code></pre>

<p>The above code gets me the following error..</p>

<pre><code>TypeError: list indices must be integers or slices, not str
</code></pre>
","7638546","","7638546","","2019-07-30 01:10:29","2019-07-30 01:13:29","Convert json response (from google sheets API's 'spreadsheet.value.get ') into a pandas dataframe with correct column headers","<python-3.x><pandas><google-sheets-api>","1","0","","","","CC BY-SA 4.0","1"
"48799882","1","48799935","","2018-02-15 04:17:46","","1","81","<p>The first column in the Pandas dataframe below is in Unix time format:</p>

<pre><code>                Open      High       Low     Close
Timestamp                                         
1417411980    100.00    105.00     98.00     98.00
1417412040     98.00    105.00     96.00    105.00
1417412100    105.00    106.00    102.00    103.00
</code></pre>

<p>I want to convert it to two columns: date (say in format yyyymmdd), and time (hhmm). 
The resulting dataframe should then be saved to a CSV file.</p>
","8794133","","","","","2018-02-15 04:25:45","Convert Unix timestamp to two dataframe columns","<python-3.x><pandas><csv>","1","0","","","","CC BY-SA 3.0","1"
"57377894","1","","","2019-08-06 14:03:39","","0","81","<p>I want to group values, if they are within the same x amount of seconds.
e.g. I got this by doing this:</p>
<pre><code>m_failed = df[(df[&quot;Signal&quot;] == &quot;Alarm&quot;) &amp; (df[&quot;State&quot;] == &quot;Active&quot;)]
dd_failed = m_failed.groupby(['Country', 'Lane', 'Unit', 'Datetime']).size().to_frame('count').reset_index()
</code></pre>
<p>UPDATE:
Sorry, but my question was very vague, and I even forgot to include important data, so I have updated the question and added part of a log.
I have changed city to lane, as that it is more true to the real data. (Sorry for the obscurity)</p>
<pre><code>Sign Descr  State   Country Lane    Unit    Datetime
Alarm   Active  USA Lane1   00003   2019-08-03 13:32:43
Alarm   Active  USA Lane1   00005   2019-08-03 13:32:43
Alarm   Active  USA Lane1   00006   2019-08-03 13:32:43
Alarm   Active  USA Lane1   00004   2019-08-03 13:32:43
Alarm   Active  USA Lane1   00002   2019-08-03 13:32:43
Alarm   Active  USA Lane1   00007   2019-08-03 13:32:43
Alarm   Active  Spain   Lane1   00003   2019-08-03 07:47:54
Alarm   Active  Spain   Lane1   00002   2019-08-03 07:47:54
Alarm   Active  Spain   Lane1   00005   2019-08-03 07:47:54
Alarm   Active  Spain   Lane1   00007   2019-08-03 07:47:54
Alarm   Active  Spain   Lane1   00004   2019-08-03 07:47:53
Alarm   Active  Spain   Lane1   00006   2019-08-03 07:47:53
Alarm   Active  Spain   Lane1   00004   2019-08-03 07:26:16
Alarm   Active  Spain   Lane1   00003   2019-08-03 07:26:16
Alarm   Active  Italy   Lane2   00002   2019-08-03 12:09:34
Alarm   Active  Italy   Lane2   00004   2019-08-03 09:50:32
Alarm   Active  Italy   Lane2   00006   2019-08-03 09:50:32
Alarm   Active  Italy   Lane2   00002   2019-08-03 09:50:32
Alarm   Active  Italy   Lane1   00007   2019-08-03 07:58:43
Alarm   Active  Italy   Lane2   00002   2019-08-03 07:58:01
Alarm   Active  Germany Lane1   00007   2019-08-03 12:36:48
Alarm   Active  Germany Lane1   00007   2019-08-03 12:31:19
Alarm   Active  Sweden  Lane1   00007   2019-08-03 12:27:33
Alarm   Active  Norway  Lane1   00007   2019-08-03 12:35:21
Alarm   Active  Norway  Lane1   00005   2019-08-03 12:35:21
Alarm   Active  Norway  Lane1   00002   2019-08-03 12:35:21
Alarm   Active  Norway  Lane1   00007   2019-08-03 12:28:50
Alarm   Active  Norway  Lane2   00007   2019-08-03 12:27:31
Alarm   Active  Norway  Lane2   00003   2019-08-03 12:27:31
Alarm   Active  Norway  Lane2   00006   2019-08-03 12:27:31
Alarm   Active  Norway  Lane2   00005   2019-08-03 09:24:53
Alarm   Active  Denmark Lane2   00003   2019-08-03 09:46:23
Alarm   Active  UK  Lane2   00003   2019-08-03 09:56:08
Alarm   Active  UK  Lane2   00004   2019-08-03 09:56:08
Alarm   Active  Brazil  Lane2   00002   2019-08-03 09:47:19
Alarm   Active  Brazil  Lane2   00003   2019-08-03 09:47:19
</code></pre>
<p>and I want the results to be like this:</p>
<pre><code>Sign Descr  State   Country Lane    Unit    Datetime    Count
Alarm   Active  USA Lane1       2019-08-03 13:32:43 1
Alarm   Active  Spain   Lane1       2019-08-03 07:47:54 1
Alarm   Active  Spain   Lane1   00004   2019-08-03 07:26:16 1
Alarm   Active  Spain   Lane1   00003   2019-08-03 07:26:16 1
Alarm   Active  Italy   Lane2   00002   2019-08-03 12:09:34 3
Alarm   Active  Italy   Lane2   00004   2019-08-03 09:50:32 1
Alarm   Active  Italy   Lane2   00006   2019-08-03 09:50:32 1
Alarm   Active  Italy   Lane1   00007   2019-08-03 07:58:43 1
Alarm   Active  Germany Lane1   00007   2019-08-03 12:36:48 2
Alarm   Active  Sweden  Lane1   00007   2019-08-03 12:27:33 1
Alarm   Active  Norway  Lane1   00007   2019-08-03 12:35:21 1
Alarm   Active  Norway  Lane1   00005   2019-08-03 12:35:21 1
Alarm   Active  Norway  Lane1   00002   2019-08-03 12:35:21 1
Alarm   Active  Norway  Lane2   00007   2019-08-03 12:27:31 2
Alarm   Active  Norway  Lane2   00003   2019-08-03 12:27:31 1
Alarm   Active  Norway  Lane2   00006   2019-08-03 12:27:31 1
Alarm   Active  Norway  Lane2   00005   2019-08-03 09:24:53 1
Alarm   Active  Denmark Lane2   00003   2019-08-03 09:46:23 1
Alarm   Active  UK  Lane2   00003   2019-08-03 09:56:08 1
Alarm   Active  UK  Lane2   00004   2019-08-03 09:56:08 1
Alarm   Active  Brazil  Lane2   00002   2019-08-03 09:47:19 1
Alarm   Active  Brazil  Lane2   00003   2019-08-03 09:47:19 1
</code></pre>
<p>The units can be from 00002 to 00007
The lanes can be either lane 1 or lane 2, while the &quot;country&quot; can be -anything-
Log created is from 00:00 -&gt; 23:59</p>
<p>If the country and lane are the same, and if all units failed within the same 1-2 minutes, then group them and count them as 1, as it's the lane that failed.
If the same lane fails several times during the day, then count the amount of times the whole lane failed.</p>
<p>while if not all units failed, then show the unit and count the amount of times this unit failed during the day.</p>
<h2>??What is the best way to add tables in stack overflow??</h2>
","7062510","","-1","","2020-06-20 09:12:55","2019-08-07 06:34:15","Pandas, count if time difference is within x seconds","<python-3.x><pandas>","2","1","","","","CC BY-SA 4.0","1"
"56835463","1","56835661","","2019-07-01 12:10:47","","3","81","<p>I'm trying to assign one of 8 labels to my data based on the strings in an existing column. However, with the method I'm using I get this error: </p>

<blockquote>
  <p><em>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</em></p>
</blockquote>

<p>I have 144 different strings I'm looking for, that I want to assign to 8 labels.</p>

<p>Here is a simplified example of what I mean. If A is the existing column in my dataframe, I want to create B with the strings assigned depending on the value of A.</p>

<p>Dataframe:</p>

<pre><code>   A     B
0  1   low
1  1   low
2  2   mid
3  3   mid
4  5  high
5  4   mid
6  2   mid
7  5  high
</code></pre>

<p>The code I'm using currently is something like:</p>

<pre><code>for index, row in df.iterrows():
    if df['A'] == 1:
        df['Label'] = 'low'
    elif any([df['A'] == 2, df['A'] == 3, df['A'] == 4]):
        df['Label'] = 'mid'
    elif df['A'] == 5:
        df['Label'] = 'high'
</code></pre>

<p>I think it is the use of any() that is giving me the error. 
As I understand it, this is because of how pandas works, but I don't really understand it. Is there any easier way to do this? </p>

<p>Any help or pointers would be appreciated :)</p>
","11196704","","11196704","","2019-07-01 12:20:08","2019-07-01 12:43:01","Use multiple conditions on a column to assign values of new column","<python><python-3.x><pandas><any>","4","5","","","","CC BY-SA 4.0","1"
"56656780","1","56656841","","2019-06-18 20:44:12","","1","80","<p>I have a simple Python dictionary. I'd like to add a new column to a Pandas Dataframe where each row in that column is equal to the dictionary.</p>

<pre><code>import pandas as pd

df = pd.DataFrame(data=[[1,2,3],[4,5,6]],columns=['A','B','C'])

df['D'] = {'AA': 'BB', 'CC': 'DD'}
</code></pre>

<p>Desired output</p>

<pre><code>   A  B  C                         D
0  1  2  3  {'AA': 'BB', 'CC': 'DD'}
1  4  5  6  {'AA': 'BB', 'CC': 'DD'}
</code></pre>

<p>The actual output only keeps the keys if the dictionary length equals the row length, otherwise it just errors.</p>
","2242044","","","","","2019-06-18 22:14:15","Set a the same dictionary for each Pandas cell","<python><python-3.x><pandas>","3","0","","","","CC BY-SA 4.0","1"
"56637127","1","56637306","","2019-06-17 18:43:26","","1","80","<p>I want to know how many items/documents get matched by a regular expression. The final result should look like this format:</p>

<pre><code>data = [['Regular Expression 1', 10], ['Regular Expression 2', 15]]
final = pd.DataFrame(data, columns = ['Regular Expression', 'Count']) 
</code></pre>

<p>Here is the example dataset of text documents</p>

<pre><code>foo = pd.DataFrame({'a' : [1,2,3,4,5], 
                    'b' : ['hi whatever something',
                           'foo', 'fat a a a foo', 'cat', 'fat']})
</code></pre>

<p>here are the regular expressions that count up the data</p>

<pre><code>g1 = foo['b'][foo['b'].str.contains(r'(?=.*foo)(?=.*fat)|(\bwhatever\b)',regex=True)].count()
g2 = foo['b'][foo['b'].str.contains(r'\bfat\W+(?:\w+\W+){0,5}?foo\b',regex=True)].count()
g1  # 2 
g2  # 1
</code></pre>

<p>How do I concatenate (the pandas equivalent <code>rbind</code> in R) the data rows? So I can get the data to look like object ""Final""?</p>

<p>I tried <code>pd.concat([g1,g2], axis=0)</code> but I get this error message from using concat: <code>cannot concatenate a non-NDFrame object</code></p>
","7620499","","202229","","2019-06-17 20:14:30","2019-06-17 20:14:30","Match multiple regular expressions against a single column (and tabulate the matches)","<python><regex><python-3.x><pandas><dataframe>","1","6","","","","CC BY-SA 4.0","1"
"57165674","1","","","2019-07-23 13:49:23","","-2","80","<p>I have a single <code>df</code> with two columns <code>df['A']</code> and <code>df['B']</code> (<code>df['C'])</code> is timestamp). <code>A</code>'s data is a username, and in <code>B</code> is a number. </p>

<p>I want to extract where the username+number values are A) the same, and B) different, i.e. so to show where username has >1 (different) numbers.</p>

<p>Is that possible?</p>

<p>I tested with <code>set(df.A+df.B)</code> to get unique values, but I can't do anything with this.</p>

<p>EDIT: </p>

<p>I need to make this more clear....</p>

<p>I'm picturing a loop whereby I start at index <code>0</code>, grab its value in <code>df['A']</code> and <code>df['B']</code>, then I iterate through index n+1...nth row looking for a match on index <code>0</code>'s <code>df['B']</code>, if match exists then check if the matches <code>df['A']</code> != <code>df['A']</code> of index <code>0</code> and if it does not then print both index's data, then move to index n+1 and repeat the process. Does that make sense?</p>

<p>So this will basically only print data from a dataframe <code>df</code> where the username string (in <code>df['A']</code>) is associated with different numbers (<code>df['B']</code> values).</p>
","808946","","808946","","2019-07-23 15:19:58","2019-07-23 15:19:58","Pandas df how to show where df['col']+df['col2'] don't match","<python><python-3.x><pandas>","1","5","","","","CC BY-SA 4.0","1"
"57669675","1","57670015","","2019-08-27 07:28:29","","1","79","<p>Consider the following table:</p>

<pre><code>              date       mainMode       freq
  7     1560327222            CAR          3
165     1560327508        WALKING          1
 28     1560327306            CAR          1
 35     1560326894            CAR          1
184     1560327408            CAR          2
</code></pre>

<p>I would like to keep the table ordered by the <code>freq</code> column in descending order, but the first row must always be the most recent one based on the <code>date</code>column. The idea would be to move the most recent one to the first row, not copy it or inserting the same duplicated file, but move it to the first position, avoiding to duplicate it.</p>

<p>The expected result would be:</p>

<pre><code>              date       mainMode       freq
 35     1560326894            CAR          1
  7     1560327222            CAR          3
184     1560327408            CAR          2
165     1560327508        WALKING          1
 28     1560327306            CAR          1
</code></pre>

<p><strong>EDIT</strong></p>

<p>The goal would be to sort the table based on <code>freq</code>, and then take the most recent record (a single row) and move it to the first row position of the dataframe without duplicating it. I hope this helps understanding the issue.</p>

<p>Thank you very much in advance</p>
","9542954","","9542954","","2019-08-27 07:48:25","2019-08-27 07:51:44","How to move a row in a dataframe based on a condition","<python><python-3.x><pandas><dataframe>","2","5","","","","CC BY-SA 4.0","1"
"57466190","1","57466603","","2019-08-12 18:12:14","","0","78","<p>this one is a bit of a doozy.</p>
<p>At a high level, I'm trying to figure out how to run a nested for loop. I'm essentially trying to iterate through columns and rows, and perform a computational check to make sure outcomes match a specified requirement - if so, they loop to the next row, if not, they are kicked out and the loop moves onto the next user.</p>
<p>Specifically, I want to perform a T-Test between a control/treatment group of users, and make sure the result is less than a pre-determined value.</p>
<h2>Example:</h2>
<p>I have my table of values - <strong>&quot;DF&quot;</strong> - there are 7 columns. The user_id column specifies the user's unique identifier. The user_type column is a binary classifier, users can be of either T (treatment) or C (control) types. The 3 &quot;hour&quot; columns are dummy number columns, values that I'll perform computation on. The mon column is the month, and tval is the number that the computation will have to be less than to be accepted.</p>
<p>In this case, the month is all January data. Each month can have a different tval.</p>
<h3>DF</h3>
<pre><code>| user_id | user_type | hour1 | hour2 | hour3 | mon | tval |
|---------|-----------|-------|-------|-------|-----|------|
| 4       | T         |   1   |   10  |  100  |  1  | 2.08 |
| 5       | C         |   2   |   20  |  200  |  1  | 2.08 |
| 6       | C         |   3   |   30  |  300  |  1  | 2.08 |
| 7       | T         |   4   |   40  |  400  |  1  | 2.08 |
| 8       | T         |   5   |   50  |  500  |  1  | 2.08 |

</code></pre>
<p>My goal is to iterate through each T user - and for each, loop through each C user. For each &quot;Pair&quot;, I want to perform computation (t-test) between their hour 1 values. If the value is less than the tval, move to hour2 values, etc. If not, it gets kicked out and the loop moves to the next C user without completing that C user's loop. If it passes all value checks, the user_ids of each would be appended to a list or something external.</p>
<p>The output would hopefully look like a table of pairs. The T user and C user that have successfully iterated through all hour columns, and the month that passed (as each set of users have data for all 12 months).</p>
<h3>Output:</h3>
<pre><code>| t_userid |  c_userid | month | 
|--------- |-----------|-------|
| 4        | 5         |   1   | 
| 8        | 6         |   1   |
</code></pre>
<p>To sum it all up:</p>
<pre><code>For each T user:
    For each C user:
        If t-test on t.hour1 and c.hour1 is less than X number (passing test):
            move to next hour (hour2) and repeat
        If all hours pass, add pair (T user_id, c_user_id) to separate list/series/df,etc
        else: skip following hours and move to next C user. 
</code></pre>
<p>I'm wondering if my data format is also incorrect. Would this be easier if I unpivoted my hourly data and iterated over each row? Any help is greatly appreciated. Thanks, and let me know if any clarification is necessary.</p>
<h3>EDIT:</h3>
<p>So far I've split the data between Treat and Control groups, and calculated average and standard deviation for a users monthly data (which is normally broken down by day) and added them as columns, hour1_avg and hour1_stdev. I've attempted another for loop, but am getting a ValueError.</p>
<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
</code></pre>
<p>I know this is due to the fact that I cant compare a pandas Series to a float, int, str, etc. I will make another post addressing this question.</p>
<p>Here's what I have so far:</p>
<pre class=""lang-py prettyprint-override""><code>for i in treatment.user_id:
    for i in control.user_id:
        if np.absolute((treatment['hour1_avg'] - control['hour1_avg'])/np.sqrt((treatment['hour1_stdev']**2/31)+(control['hour1_stdev']**2/31))) &gt; treatment.tval:
            &quot;End loop, move to next control user&quot;
        else:
            &quot;copy paste if statement above, but for hour2, etc etc&quot;
</code></pre>
","10071640","","-1","","2020-06-20 09:12:55","2019-08-13 14:57:32","How would i iterate through a list of lists and perform computational filtering?","<python><python-3.x><pandas><list><numpy>","1","2","","","","CC BY-SA 4.0","1"
"49372083","1","49372599","","2018-03-19 20:44:34","","0","78","<p>I have a couple of csv files that have as title yyyymmdd: <img src=""https://i.stack.imgur.com/HR9hR.png"" alt=""&#39;yyyymmdd&#39;"">. I also have a several csv files that have 'yyyymm' and one with 'yyyy' which I <strong>don't</strong> want to import.</p>

<p>Problem is, the csv files themselves only have time and not date: </p>

<p><img src=""https://i.stack.imgur.com/6kDgU.png"" alt=""datetimeformat"">.</p>

<p>Here is my code:</p>

<pre><code>import pandas as pd
from glob import glob

photolist = glob('********.csv')

dataframes = [pd.read_csv(Tage, delimiter=';',
                    skiprows=2,
                    encoding='cp1252',
                    parse_dates = True,
                    index_col = 0) for Tage in photolist]


print(dataframes)
</code></pre>

<p>The index for the dataframes returns the right time but not the right date.</p>

<p>So my question is: How can I have the same date as index as the csv filname?</p>
","9519112","","9519112","","2018-03-21 11:25:02","2018-03-21 11:25:02","CSV filename in date-form as Index","<python><python-3.x><pandas><csv><datetime>","1","2","","","","CC BY-SA 3.0","1"
"57231723","1","","","2019-07-27 11:34:48","","0","78","<p>Value from pandas:</p>

<pre><code>Index   Bra Obr Zal Uto Str Nah Tec Ryc Hla BestP_A BestP_Amin  BestP_minA
0       461 38  44  46  49  137 324 322 162 Bra     Obr         137.0
1       442 32  30  35  39  180 322 325 180 Bra     Obr         180.0
2       152 28  23  23  30  175 335 355 206 Bra     Obr         175.0
3       38  33  68  33  49  119 223 46  46  Zal     Obr         46.0
4       36  33  203 36  46  217 253 166 170 Zal     Obr         166.0
5       35  84  38  41  54  49  175 57  141 Obr     Nah         49.0
6       34  45  71  45  59  72  207 57  60  Zal     Obr         57.0
</code></pre>

<p><a href=""https://i.stack.imgur.com/zldZr.png"" rel=""nofollow noreferrer"">Value from Pandas</a></p>

<p>I need return column name to column ""BestP_Amin"", Python return bad column name. Example value index 0:</p>

<pre><code>BestP_A = Bra, Bra = 461, Nah = 137, Tec = 324, Ryc = 322, Hla = 162
</code></pre>

<p>code:</p>

<pre class=""lang-py prettyprint-override""><code>if data.at[i,'BestP_A'] == 'Bra':
    data['BestP_Amin'] = data[['Bra','Nah','Tec','Ryc','Hla']].idxmin(axis=1) 
</code></pre>

<p>Returned value <code>Obr</code>, where is problem?
Block code:</p>

<pre><code># nalezení nejleší pozice, přepočet podle výše atributů
for i in range(0,len(data.index)):
# nalezení pozice
    data['BestP_A'] = data[['Bra','Obr','Zal','Uto']].idxmax(axis=1)
#nalezení nejmenšího atributu
# VRACI CHYBNE HODNOTY
    if data.at[i,'BestP_A'] == 'Bra':
         data['BestP_Amin'] = data[['Bra','Nah','Tec','Ryc','Hla']].idxmin(axis=1)
    elif data.at[i,'BestP_A'] == 'Obr':
         data['BestP_Amin'] = data[['Obr','Nah','Tec','Ryc','Hla']].idxmin(axis=1)
    elif data.at[i,'BestP_A'] == 'Zal':
         data['BestP_Amin'] = data[['Zal','Nah','Tec','Ryc','Hla']].idxmin(axis=1)
    elif data.at[i,'BestP_A'] == 'Uto':
         data['BestP_Amin'] = data[['Uto','Nah','Tec','Ryc','Hla']].idxmin(axis=1)

    data.at[i,'BestP_minA'] = min(data.at[i,'Nah'],data.at[i,'Tec'],data.at[i,'Ryc'],data.at[i,'Hla'])
    data.at[i,'BestA_U'] = int(round(data.at[i,'BestA']*(data.at[i,'Ene']/100)*(1+(.25*data.at[i,'Sou']/100)+(.2*data.at[i,'Zku']/100)),0))
</code></pre>

<p><a href=""https://i.stack.imgur.com/Uukpg.png"" rel=""nofollow noreferrer"">Block code in python:</a></p>
","9822593","","9822593","","2019-07-27 11:45:16","2019-07-29 09:34:25","Pandas .idxmin(axis=1) returns bad column name values","<python><python-3.x><pandas>","3","0","","","","CC BY-SA 4.0","1"
"56908438","1","","","2019-07-05 19:36:28","","0","78","<p>I have applied a performance rank per participant in my survey. 
I now want to apply their respective rank on their question response (+1 to -1)
I have 2 data frames</p>

<p>&gt; df1</p>

<pre><code>SetAScore   rank_A
689964  0.90    1.000000
689966  0.65    0.771845
689967  0.65    0.771845
689968  0.55    0.286408
689970  0.55    0.286408
</code></pre>

<p>&gt; df2</p>

<pre><code>Participant response-1-quantised response-101-quantised 

689964 1 -1
689966 1 -1
</code></pre>

<p>The end result I am seeking is:</p>

<p>Participant response-1-quantised response-101-quantised </p>

<p>689964 1 -1 <em>(edited)</em>
689966 0.771845 -0.771845 <em>(edited)</em></p>

<p>Maintaining the structure of df2, which has 120 question response columns and 113 rows (1 row per participant).</p>

<p>I see the ""key"" being the 'Participant' number</p>

<p>I have tried to also bring the 'rank_A' column into df2 and perform a simple multiply but to no success. </p>

<p>Any help would be greatly appreciated for my MSc</p>

<p>(sorry about the format, I'm a noob) I can share images if easier. Thanks!</p>

<p>EDIT ----</p>

<p>I think I have solved it with @joe's help (thanks Joe!)</p>

<pre><code>for col in b_test:
    result_df[col] = [a*b for a,b in zip(set_rank['rank_A'].values.tolist(),b_test[col].values.tolist())]
    result_df[col] = [a*b for a,b in zip(set_rank['rank_A'].values.tolist(),b_test[col].values.tolist())]
</code></pre>

<p>This seems to have worked as I had to loop through n columns</p>
","6166573","","6166573","","2019-07-06 12:08:16","2019-07-06 12:08:16","How to multiply columns by first column in DataFrame?","<python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"57465348","1","","","2019-08-12 17:01:28","","0","78","<p>I have to copy the excel sheets from different source files to destination sheets with same format(Headers, Borders, Fonts, Formulas, Background color, Spacing, ... etc)</p>

<p>I have tried below script but it is copying only data also, header is not exactly matched.</p>

<pre><code>import pandas as pd

df = pd.read_excel(src, sheet_name=""Sheet1"")

df.to_excel(des, sheet_name='new_tab')
</code></pre>

<p>Any help is appreciated. Preferable Python.</p>
","11463582","","11463582","","2019-08-13 05:02:52","2019-08-13 05:02:52","How to copy excel sheet to another workbook with same format using python","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57745829","1","57745849","","2019-09-01 12:02:05","","0","77","<p>How can I get the value with the smaller absolute value from a panda Series?</p>

<p>Example code:</p>

<pre><code>import pandas as pd

def get_min_absvalue(values):
    ¿?

lst = [-2.1,2.2,-1.5]

ser = pd.Series(lst)
min_absvalue = get_min_absvalue(ser)
#min_absvalue must be -1.5 here
</code></pre>
","2132478","","","","","2019-09-01 12:17:29","How to get the value with the smaller absolute value from a panda Series","<python><python-3.x><pandas><series>","1","1","","","","CC BY-SA 4.0","1"
"57512096","1","","","2019-08-15 15:29:02","","0","77","<p>I have looked up on the site and saw numerous answers and tried to modify them to do what I want to do but for some reason I am not getting the result.</p>

<p>I have two dataframes, one with millions of rows called Big_df with 10 columns and one with 1000 rows with 6 columns called Small_df.  </p>

<p><code>Small_df</code> has column 'Print_ID' and <code>Big_df</code> has 'Job_ID' that are in common to the dataframes.  I need to retrieve column 'Status' from <code>Big_df</code> and add it to the <code>Small_df</code>.  This is the code:</p>

<pre class=""lang-py prettyprint-override""><code>NewSmall_df = pd.merge(Small_df, Big_df, left_on = 'Print_ID', right_on ='Job_ID')
</code></pre>

<p>I get a result of 2000+ rows which is not correct as <code>Small_df</code> has 1000 rows and the maximum would be 1000 or less depending if there is a <code>NaN</code> in one of the IDs.  I have looked at the answers for all the similar type questions on Stackoverflow and elsewhere, I should be getting the result that I am looking for.  </p>

<p>The result that I am looking for is to have an extra column added to <code>small_df</code> that has the 'Status' from <code>Big_df</code> matched by the Print_ID &amp; Job_ID.</p>

<p>What am I doing wrong? Any help is greatly appreciated.</p>

<p>Below is a sample data:</p>

<p>Small_df:
<a href=""https://i.stack.imgur.com/scEZf.png"" rel=""nofollow noreferrer"">Smal_df</a></p>

<p>Big_df:
<a href=""https://i.stack.imgur.com/TrCvX.png"" rel=""nofollow noreferrer"">Big_df</a></p>

<p>Result_df:
<a href=""https://i.stack.imgur.com/xeoZO.png"" rel=""nofollow noreferrer"">Result_df after matching Print_ID and Job_ID to return Status as the last column</a></p>
","11917874","","11917874","","2019-08-15 17:40:26","2019-08-15 17:40:26","Using Pandas merge to perform lookup","<python-3.x><pandas>","0","4","","","","CC BY-SA 4.0","1"
"56954569","1","56955021","","2019-07-09 14:13:27","","1","76","<p>I'm trying to categorize latin/non-latin data through Python. I want the output to be 'columnname: Latin' if it's Latin, 'columnname: Non-Latin' if it's non-latin. Here's the data set I'm using:</p>

<pre><code>name|company|address|ssn|creditcardnumber

Gauge J. Wiley|Crown Holdings|1916 Central Park Columbus|697-01-963|4175-0049-9703-9147

Dalia G. Valenzuela|Urs Corporation|8672 Cottage|Cincinnati|056-74-804|3653-0049-5620-71

هاها|Exide Technologies|هاها|Washington|139-09-346|6495-1799-7338-6619
</code></pre>

<p>I tried adding the below code. I don't get any error, but I get 'Latin' all the time. Is there any issue with the code?</p>

<pre><code>if any(dataset.name.astype(str).str.contains(u'[U+0000-U+007F]')):
    print ('Latin')
else:
    print('Non-Latin')
</code></pre>

<p>And also I'd be happy if someone could tell me how to display the output as ""column name: Latin"", column name being iterated from dataframe</p>
","11027302","","2573061","","2019-07-09 14:27:06","2019-07-09 14:38:06","Categorize the dataframe column data into Latin/Non-Latin","<python><python-3.x><pandas>","1","5","","","","CC BY-SA 4.0","1"
"57370026","1","57370385","","2019-08-06 06:22:58","","1","76","<p>I want to generate unique records with the combination of two columns and that value must be the same all the time.
For example, I want to concatenate <code>Col1</code> ,<code>Col2</code> and create <code>C12</code>
I want to generate numeric valuse as shown below for the concatenated column.</p>

<p><a href=""https://i.stack.imgur.com/b6xem.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/b6xem.jpg"" alt=""enter image description here""></a></p>

<p>my code is this:</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.DataFrame({'Col1' : ['Bob', 'Joe', 'Bill', 'Mary', 'Joe'],
              'Col2' : ['Joe', 'Steve', 'Bob', 'Bob', 'Steve'],
               'Col3' : np.random.random(5)})
df['C12'] = df['Col1'] + df['Col2']
df
</code></pre>
","10808871","","6925185","","2019-08-06 08:01:12","2019-08-06 08:01:12","How to combine multiple columns and create unique numeric values","<python><python-3.x><pandas>","4","3","","","","CC BY-SA 4.0","1"
"57603199","1","57606806","","2019-08-22 06:29:52","","1","76","<p>I have 1 Dataframe with some rows of <code>Sold-To Country Name</code> column in the value of <code>Not: XX XX XX</code>,which means the rest of <code>Sold-To Country Codes</code> except <code>XX XX XX</code> will be reporting to the mapped <code>Reporting Country</code>.</p>

<p>Another requirement is that if <code>Sold-To Country Code</code> is <code>null</code> (or <code>NaN</code>), it will capture all revenue from all country codes in that SalesOrg.</p>

<pre><code>df_mapping = pd.DataFrame({'SalesOrg Code':['0001','0002','0002','0002','0002'],
                           'Reporting Country':['Spain','UK','UK','UK','Netherlands'],
                           'Sold-To Country Code':[np.nan,'IE','FR','IT','Ex:'],
                           'Sold-To Country Name':[np.nan,'Ireland','France','Italy','NOT: FR IE IT']})
</code></pre>

<pre><code>SalesOrg Code   Reporting Country   Sold-To Country Code    Sold-To Country Name
0001            Spain                null                   null
0002            UK                   IE                     Ireland
0002            UK                   FR                     France
0002            UK                   IT                     Italy
0002            Netherlands          Ex:                    NOT: FR IE IT
.......
</code></pre>

<p>There will be another Dataframe with a full list of global country codes, where we can search for the rest of the country codes. </p>

<p>Example of the Dataframe:</p>

<pre><code>df_countrylist = pd.DataFrame([""AF"", ""AX"", ""AL"", ""DZ"", ""AS"", ""AD"", ""AO"", ""AI"", ""AQ"", ""AG"", ""AR"",
""AM"", ""AW"", ""AU"", ""AT"", ""AZ"", ""BS"", ""BH"", ""BD"", ""BB"", ""BY"", ""BE"",
""BZ"", ""BJ"", ""BM"", ""BT"", ""BO"", ""BQ"", ""BA"", ""BW"", ""BV"", ""BR"", ""IO"",
""BN"", ""BG"", ""BF"", ""BI"", ""CV"", ""KH"", ""CM"", ""CA"", ""KY"", ""CF"", ""TD"",
""CL"", ""CN"", ""CX"", ""CC"", ""CO"", ""KM"", ""CG"", ""CD"", ""CK"", ""CR"", ""CI"",
""HR"", ""CU"", ""CW"", ""CY"", ""CZ"", ""DK"", ""DJ"", ""DM"", ""DO"", ""EC"", ""EG"",
""SV"", ""GQ"", ""ER"", ""EE"", ""ET"", ""FK"", ""FO"", ""FJ"", ""FI"", ""FR"", ""GF"",
""PF"", ""TF"", ""GA"", ""GM"", ""GE"", ""DE"", ""GH"", ""GI"", ""GR"", ""GL"", ""GD"",
""GP"", ""GU"", ""GT"", ""GG"", ""GN"", ""GW"", ""GY"", ""HT"", ""HM"", ""VA"", ""HN"",
""HK"", ""HU"", ""IS"", ""IN"", ""ID"", ""IR"", ""IQ"", ""IE"", ""IM"", ""IL"", ""IT"",
""JM"", ""JP"", ""JE"", ""JO"", ""KZ"", ""KE"", ""KI"", ""KP"", ""KR"", ""KW"", ""KG"",
""LA"", ""LV"", ""LB"", ""LS"", ""LR"", ""LY"", ""LI"", ""LT"", ""LU"", ""MO"", ""MK"",
""MG"", ""MW"", ""MY"", ""MV"", ""ML"", ""MT"", ""MH"", ""MQ"", ""MR"", ""MU"", ""YT"",
""MX"", ""FM"", ""MD"", ""MC"", ""MN"", ""ME"", ""MS"", ""MA"", ""MZ"", ""MM"", ""NA"",
""NR"", ""NP"", ""NL"", ""NC"", ""NZ"", ""NI"", ""NE"", ""NG"", ""NU"", ""NF"", ""MP"",
""NO"", ""OM"", ""PK"", ""PW"", ""PS"", ""PA"", ""PG"", ""PY"", ""PE"", ""PH"", ""PN"",
""PL"", ""PT"", ""PR"", ""QA"", ""RE"", ""RO"", ""RU"", ""RW"", ""BL"", ""SH"", ""KN"",
""LC"", ""MF"", ""PM"", ""VC"", ""WS"", ""SM"", ""ST"", ""SA"", ""SN"", ""RS"", ""SC"",
""SL"", ""SG"", ""SX"", ""SK"", ""SI"", ""SB"", ""SO"", ""ZA"", ""GS"", ""SS"", ""ES"",
""LK"", ""SD"", ""SR"", ""SJ"", ""SZ"", ""SE"", ""CH"", ""SY"", ""TW"", ""TJ"", ""TZ"",
""TH"", ""TL"", ""TG"", ""TK"", ""TO"", ""TT"", ""TN"", ""TR"", ""TM"", ""TC"", ""TV"",
""UG"", ""UA"", ""AE"", ""GB"", ""US"", ""UM"", ""UY"", ""UZ"", ""VU"", ""VE"", ""VN"",
""VG"", ""VI"", ""WF"", ""EH"", ""YE"", ""ZM"", ""ZW""])
</code></pre>

<p>Ultimately, I want to have like this:</p>

<pre><code>SalesOrg Code   Reporting Country   Sold-To Country Code    Sold-To Country Name
0001            Spain                null (all)             null
0002            UK                   IE                     Ireland
0002            UK                   FR                     France
0002            UK                   IT                     Italy
0002            Netherlands          AT                     Austria 
0002            Netherlands          DK                     Denmark 
0002            Netherlands          NL                     Netherlands 
0002            Netherlands          BE                     Belgium 
0002            Netherlands          LT                     Lithuania 
0002            Netherlands          LX                     Latvia      
.......
</code></pre>

<p>For SalesOrg #0002, if the <code>Sold-To Country Code</code> are not <code>FR IE IT</code>, the rest will be reporting to Netherlands. So I want to create rows for the rest of the country codes. </p>

<p>Is there any better way to create rows and expand into the existing Dataframe?</p>
","9852051","","9852051","","2019-08-22 08:41:49","2019-08-26 06:19:32","How to create rows by 'Exclude' condition and to expand into the existing Dataframe?","<python><python-3.x><pandas><dataframe>","2","1","1","","","CC BY-SA 4.0","1"
"49613734","1","","","2018-04-02 15:06:00","","0","76","<p>I'm facing some issues with my recently ""python"" configuration, i've worked with Python before in Linux based OS's and my older Mac, althou i never had faced those issues before, i've tried to google some alternatives, applied their possible fixes but it have not fixed the issue. </p>

<p>I recently installed Python on my Mac (El Captain 10.11.6) and it supposed to be Python 3.6x or so, althou when i execute <code>easy_install</code> pandas is show's </p>

<blockquote>
  <p>Using /Users/XXX/Library/Python/2.7/lib/python/site-packages</p>
</blockquote>

<p>I'm not sure if this is alright or not, to be honest. </p>

<p>Another thing is that i've been trying to install Pandas thru homebrew, pip, and a series of alternative methods and i have not been able to so far, a series of different errors have been happening. </p>

<pre><code>pip install pandas 
</code></pre>

<blockquote>
  <p>OSError: [Errno 1] Operation not permitted:
  '/var/folders/5d/49fysh254dj8__50svgj2bmw0000gn/T/pip-q4dR7r-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy-1.8.0rc1-py2.7.egg-info'</p>
</blockquote>

<pre><code>sudo pip install pandas 
</code></pre>

<blockquote>
  <p>Exception: Traceback (most recent call last):   File
  ""/Library/Python/2.7/site-packages/pip-9.0.3-py2.7.egg/pip/basecommand.py"",
  line 215, in main</p>
  
  <p>...</p>
  
  <blockquote>
    <p>OSError: [Errno 1] Operation not permitted:
    '/tmp/pip-e5bgac-uninstall/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy-1.8.0rc1-py2.7.egg-info'</p>
  </blockquote>
</blockquote>

<p>or some other issues like </p>

<blockquote>
  <p>Command ""python setup.py egg_info"" failed with error code 1 in
  /private/tmp/pip-build-c2a9i3g4/pandas/</p>
</blockquote>

<p>Using some other methods </p>

<p>I've also noticed that the Pandas are been installed (trying to be installed) at the 2.7 Python path, which i'm not sure if it's relatable to the issue what so ever. </p>

<p>I've also tried to install pandas in my project thru PyCharm and i got this error: </p>

<blockquote>
  <p>distutils.errors.DistutilsError: Could not find suitable distribution
  for Requirement.parse('numpy>=1.9.0')</p>
</blockquote>

<p>I have xCode so for the sake of information. </p>
","2784143","","","","","2018-04-02 15:32:39","Weird (or newbie) Python Version behavior and issues installing Pandas","<python><python-3.x><python-2.7><pandas><pip>","0","6","","","","CC BY-SA 3.0","1"
"50067418","1","","","2018-04-27 17:24:29","","0","76","<p>I have a query in Python pandas.</p>

<p>I have a two data frames:</p>

<pre><code> df1:
Process ID,room no,IP Address,port,status
2553,4,192.168.1.9,2,started    
2556,4,192.168.1.49,0,started
2556,2,192.168.1.25,25,started
3056,8,192.168.1.49,0,started

df2:
Process ID,room no,IP Address,port,status
2553,4,192.168.1.9,2,completed
2556,4,192.168.1.49,0,completed
2556,2,192.168.1.25,25,completed
2556,2,192.168.1.46,25,completed
3056,8,192.168.1.49,0,completed
</code></pre>

<p>I would like to remove rows from df2 based on a matching criteria with df1.</p>

<p>For every df2 rows that has multiple matches with df1 (based on two columns value (process ID, port)) the excess rows with (same process ID &amp; port) in df2 should be removed.</p>

<p>Hope my explanation is understandable.</p>

<p>In short My modified df2 should look like this:</p>

<pre><code>df2:
Process ID,room no,IP Address,port,status
2553,4,192.168.1.9,2,completed
2556,4,192.168.1.49,0,completed
2556,2,192.168.1.25,25,completed
3056,8,192.168.1.49,0,completed
</code></pre>

<p>Any help in this regard is highly appreciated.</p>
","9614582","","","","","2018-04-27 17:24:29","Matching two dataframes based on a criteria in Pandas","<python-3.x><pandas>","0","4","","","","CC BY-SA 3.0","1"
"58378716","1","","","2019-10-14 14:18:20","","1","75","<p>I am attempting to speed up dozens of calls I make to pandas groupby using cython optimised functions. These incldue straight groupby, groupby with ranking and others. I have one that does a groupby that runs in my notebook, but not when called I get a NameError.</p>

<p>Here is the test code from my notebook (in 3 cells there)</p>

<pre><code>%%cython
def _set_indices(keys_as_int, n_keys):
    import numpy

    cdef int i, j, k
    cdef object[:, :] indices = [[i for i in range(0)] for _ in range(n_keys)]

    for j, k in enumerate(keys_as_int):
        indices[k].append(j)
    return [([numpy.array(elt) for elt in indices])]



def group_by(keys):
    _, first_occurrences, keys_as_int = np.unique(keys, return_index=True, return_inverse=True)
    n_keys = max(keys_as_int) + 1
    indices = [[] for _ in range(max(keys_as_int) + 1)]
    print(str(keys_as_int) + str(n_keys) + str(indices))
    indices = _set_indices(keys_as_int, n_keys)
    return indices



%%timeit
result = group_by(['1', '2', '3', '1', '3'])
print(str(result))
</code></pre>

<p>The error I get is:</p>

<pre><code>&lt;ipython-input-20-3f8635aec47f&gt; in group_by(keys)
  4     indices = [[] for _ in range(max(keys_as_int) + 1)]
  5     print(str(keys_as_int) + str(n_keys) + str(indices))
 ----&gt; 6     indices = _set_indices(keys_as_int, n_keys)
      7     return indices

NameError: name '_set_indices' is not defined
</code></pre>

<p>Can someone explain if this is due to notebook or if I have done something wrong with the way cython is used, I am new to it. </p>

<p>Also any hints to get a strongly type, with minimum cache hits solution are most welcome.</p>
","1453007","","100297","","2019-10-14 15:44:32","2019-10-14 15:44:32","cython implementation of groupby failing with NameError","<python-3.x><pandas-groupby><cython>","1","2","","","","CC BY-SA 4.0","1"
"56835149","1","56835292","","2019-07-01 11:48:45","","-2","75","<p>I have a dataframe like this:</p>

<pre><code>value1       value2
aa7bbc       aaaa
ss           ss0
qqq          wwww
nn77         qqee
</code></pre>

<p>I want to remove the row that :</p>

<ul>
<li>has digit in value</li>
<li>begin with <code>nn</code></li>
<li>has less than two characters</li>
</ul>

<p>I've tried this:</p>

<pre><code>df[~df.value1.str.contains(r'\d')]
</code></pre>

<p>but that doesn't cover everything I need. what is the most efficient way to solve this?</p>

<p>Thank you so much</p>
","10907221","","10907221","","2019-07-01 11:57:57","2019-07-01 12:05:35","Remove row with spesific value in pandas dataframe","<python><python-3.x><pandas><dataframe>","4","1","","2019-07-01 14:03:28","","CC BY-SA 4.0","1"
"41074264","1","","","2016-12-10 09:59:06","","0","75","<p>The code to read the CSV file:</p>

<pre><code>import pandas as pd
l = pd.read_csv('ex.csv', header=0, sep=',')
print(l)
pivoted = l.pivot('date', 'item', 'value')
</code></pre>

<p>Error:</p>

<blockquote>
  <p>KeyError: 'value'</p>
</blockquote>

<p>The CSV file:</p>

<pre><code>date, item, value
0, 1959-03-31 00:00:00, realgdp, 2710.349
1, 1959-03-31 00:00:00, infl, 0.000
2, 1959-03-31 00:00:00, unemp, 5.800
3, 1959-06-30 00:00:00, realgdp, 2778.801
4, 1959-06-30 00:00:00, infl, 2.340
5, 1959-06-30 00:00:00, unemp, 5.100
6, 1959-09-30 00:00:00, realgdp, 2775.488
7, 1959-09-30 00:00:00, infl, 2.740
8, 1959-09-30 00:00:00, unemp, 5.300
9, 1959-12-31 00:00:00, realgdp, 2785.204
</code></pre>
","4925130","","","","","2016-12-10 10:17:35","Cannot assign the first row as header in a CSV file","<python-3.x><csv><pandas>","1","0","","","","CC BY-SA 3.0","1"
"49376339","1","49376557","","2018-03-20 04:25:25","","0","74","<p>I'm trying to replace the commas with decimal points and then replace the semi-colons with commas in a CSV file.</p>

<p>The CSV file:</p>

<pre><code># 2018-03-16: ECarbix Prices/Volumes for Emission Spot Market
#  
# Data type(ST);Trading Date;Creation Time
# Data type(IL);Index;Unit;Price;Volume
# Data type(AL);Number of Lines
#  
ST;2018-03-16;2018-03-19T08:39:48+01:00
IL;Day;EUR/tCO2;10,97;4533000
AL;9
</code></pre>

<p>The code I tried:</p>

<pre><code>import pandas as pd
from os import walk
import csv

import xml.dom.minidom
from xml.etree import ElementTree
with open('some.csv', 'w', newline='') as fw:
 writer = csv.writer(fw)

 for filenames in walk(""D:\EEX_EMS\CSV""):
    (filenames)
    fname= list(filenames)
    for f in fname[2]:
     if ""Auction""  not in f:
      #print(f)
      with open('D:/EEX_EMS/CSV/'+f, 'r') as csvfile:
       spamreader = csv.reader(csvfile,  quotechar='|')
       for fg in spamreader:
        fi =str(fg)
        print(fi)
        fi1 = str(fi.replace(',','.'))
        fi2 = str(fi1.replace(';',','))
        fi_list =str(fi2.split(','))
        print (str(fi_list))

        writer.writerow(fi_list)
fw.close()
</code></pre>

<p>This is the output:</p>

<pre><code>['# 2018-03-16: ECarbix Prices/Volumes for Emission Spot Market']
['#  ']
['# Data type(ST),Trading Date,Creation Time']
['# Data type(IL),Index,Unit,Price,Volume']
['# Data type(AL),Number of Lines']
['#  ']
['ST,2018-03-16,2018-03-19T08:39:48+01:00']
['IL,Day,EUR/tCO2,10'. '97,4533000']
['AL,9']
</code></pre>

<p>How can I get the correct output?</p>
","8527475","","1961728","","2018-03-20 10:24:47","2018-03-20 10:24:47","Replacing ',' with '.' and then ';' with ',' in a csv file","<python><python-3.x><pandas><csv>","1","2","","","","CC BY-SA 3.0","1"
"49281512","1","49291620","","2018-03-14 15:20:02","","0","74","<pre><code>I'm trying to convert wiki page table to dataframe. Headings are shifted to the  
right, 'Launches' should be there were it is now 'Successes'.  
</code></pre>

<p>I have used skiprows option, but it did not work. </p>

<pre><code>df = pd.read_html(r'https://en.wikipedia.org/wiki/2018_in_spaceflight',skiprows=[1,2])[7]

df2 = df[df.columns[1:5]]

               1          2         3                 4
0       Launches  Successes  Failures  Partial failures
1          India          1         1                 0
2          Japan          3         3                 0
3    New Zealand          1         1                 0
4         Russia          3         3                 0
5  United States          8         8                 0
6             24         23         0                 1
</code></pre>
","5171169","","","","","2018-03-15 04:30:07","pandas pd.read_html heading shifted to the right","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 3.0","1"
"56928506","1","56928866","","2019-07-08 04:40:02","","1","74","<p>I have following dataframe</p>

<pre><code>df=pd.DataFrame({'column1_T1':[1,0,0,1,1],'column1_issues':     ['Comment1','abc','pqr','Comment2','Comment1'],'column2_T2':[0,0,1,0,1],'column2_issues':['OK','abc','Comment3','efg','Comment3']}) 

</code></pre>

<p>it will look like the following
df</p>

<pre><code>column1_T1  column1_issues  column2_T2    column2_issues
1                Comment1    0                 OK
0                  abc       0                 abc
0                  pqr       1                 Comment3
1              Comment2      0                 efg
1              Comment1      1                 Comment3
</code></pre>

<p>Columns with suffixes T1,T2 and so on contain either 1 or 0.</p>

<p>Columns with suffixes ""issues"" contain comments about the corresponding issues.
I only have to consider 1s in columns with suffixes T1/T2 and so on, and the corresponding issues in column1_issues,column2_issues and so on.</p>

<p>Now I want to count the number of 1s in column1_T1,column2_T2, and unique comments in column1_issues,column2_issues corresponding to the 1s in column1_T1,column2_T2 respectively, and get it in the following format</p>

<pre><code>column_labels     count   issue1     issue2
column1_issues     3      comment1   commen2
column2_issues     2      comment3
</code></pre>

<p>I have tried groupby and crosstab,but I am not able to get it</p>

<pre><code>df3=df.groupby(['column1_T1', 'column1_issues'])['column1_T1'].count().unstack().fillna(0)

df3['Total'] =df3.loc[[1]].sum(axis=1)

</code></pre>

<p>but this is far from what I want. I am really stuck here.</p>

<p>I want my final dataframe in the following format as mentioned above
in the following format</p>

<pre><code>column_labels     count   issue1     issue2
column1_issues     3      comment1   commen2
column2_issues     2      comment3
</code></pre>
","10481744","","6573902","","2019-07-08 04:48:20","2019-07-08 05:27:07","How to filter, summarize and reshape the dataframe based on criteria","<python><python-3.x><pandas><dataframe>","1","0","1","","","CC BY-SA 4.0","1"
"57376012","1","57376065","","2019-08-06 12:24:23","","1","73","<p>I want to fetch some free data from <a href=""https://coinmetrics.io/"" rel=""nofollow noreferrer"">https://coinmetrics.io/</a> for academic assignment.</p>

<p>I was trying to retrieve only the <code>id</code> and list of <code>metrics</code> for each <code>id</code> from this <a href=""https://community-api.coinmetrics.io/v2/asset_info"" rel=""nofollow noreferrer"">URL</a></p>

<p>I want to save results in DataFrame with columns <code>id</code> and <code>metrics</code></p>

<pre class=""lang-py prettyprint-override""><code>Response = requests.get('https://community-api.coinmetrics.io/v2/asset_info')
Data_API = json.loads(Response.content.decode('utf-8'))
</code></pre>
","2717063","","7440787","","2019-08-06 12:31:26","2019-08-06 12:31:26","Filter- required values from nested Dictionary in Python 3.6","<python><python-3.x><pandas><dataframe><dictionary>","1","1","","","","CC BY-SA 4.0","1"
"49646670","1","49647700","","2018-04-04 08:51:29","","4","73","<p>I have df like:</p>

<pre><code>material plant  Order                
24990   89952   4568789,5098710     
24990   89952   9448609,1007081     
166621  3062    18364103            
166621  3062    78309139            
240758  3062    55146035            
276009  3062    38501581,857542     
</code></pre>

<p>and df1 like:</p>

<pre><code>material plant   Order      m1     m2      m3   m4   m5
24990     89952 4568789     0.123  0.214   0.0  0.0  0.0
24990     89952 5098710     1.000  0.363   0.0  0.0  0.0
24990     89952 9448609     0.0    0.345   0.0  1.0  0.0
24990     89952 1007081     0.0    0.756   0.0  1.0  0.0
166621    3062  18364103    0.0    0.0     0.0  0.0  0.0
166621    3062  78309139    0.0    1.0     0.0  0.0  0.0
240758    3062  55146035    1.0    1.0     1.0  0.0  0.0
276009    3062  38501581    1.0    1.0     1.0  0.0  0.0
276009    3062  38575428    1.0    1.0     1.0  0.0  0.0
</code></pre>

<p>I want to iterate through Order in df1 and when there is a match of Order in df2 then find average from m1 to m5.
I want to achieve df2 like:</p>

<pre><code>material plant  Order              avg m1 avgm2 avgm3 avgm4 avgm5
24990   89952   4568789,5098710    0.5615 0.2885 0.0   0.0   0.0
24990   89952   9448609,1007081     
166621  3062    18364103            
166621  3062    78309139            
240758  3062    55146035            
276009  3062    38501581,857542 
</code></pre>

<p>I am trying different ways to achieve df2 such as:</p>

<pre><code>df2 = (df.groupby(df1, sort=False)['Order'].apply(lambda x: ','.split(x.astype(str)))
   .mean() 
   .reset_index()
.reindex(columns=df.columns))
 print (df2)
</code></pre>

<p>second:</p>

<pre><code>group = df.columns[np.r_[0:3, 3:len(df.columns)]]
 res = df1.groupby(group)['Order'].apply(list).mean().reset_index()
</code></pre>

<p>But I am not sure if this is the correct way to get it.</p>
","7465783","","9209546","","2018-04-04 09:48:58","2018-04-04 12:13:45","Average from two different dataframes","<python><python-3.x><pandas><dataframe>","2","3","","","","CC BY-SA 3.0","1"
"56929606","1","56929637","","2019-07-08 06:45:55","","0","72","<p>I know how to replace values in specific columns value.
Following example is how to replace value '[NULL]' to blank in 'col01'.</p>

<pre><code>df['col01'] = df['col01'].str.replace('[NULL]', '')
</code></pre>

<p>However I have no idea to replace value without column names.
I would like to make all columns as a replacement targets.</p>

<p>How can I make a code? Thanks.</p>
","1155578","","","","","2019-07-08 06:54:27","How to replace values in all columns in Dataframe","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"56978492","1","","","2019-07-10 20:54:21","","0","72","<p>I am trying to create a new column based on values of a different column to essentially label the values in a different column into two buckets.</p>

<p>so if the dates of 4/24, 4/26 I wanted labels of 1 and 4/25 label of 2,</p>

<p>expected df would look like:</p>

<pre><code>date | label 
4/24     1
4/25     2
4/26     1
4/24     1
4/26     1
</code></pre>

<p>where label would be the new column</p>

<p>Thanks for looking</p>
","8797830","","8797830","","2019-07-10 21:02:02","2019-07-10 22:52:03","Create a new column with labels bases on a different columns values","<python><python-3.x><pandas><data-science>","4","3","","","","CC BY-SA 4.0","1"
"57192499","1","","","2019-07-24 23:35:22","","1","72","<p>I am trying to read values from a txt file using pandas and plotting a scatter diagram with matploitlib but i keep getting all sort of errors while trying different methods</p>

<p>a summery of my txt file...</p>

<pre><code>Brain   Body         
0.37    0.117182754
73  1.349981613
70  0.925010921
0.8 0.007620352
0.15    0.001406136
50  0.419981176


from pandas import*
import pandas as pd
from sklearn import linear_model
import matplotlib.pyplot as plt

#Read_data

dataframe = pd.read_csv('./brain_body.txt' , header = None);
x_values = dataframe[['Brain']]
y_values = dataframe[['Body']]

#Training Model on data

body_reg = linear_model.LinearRegression()
body_reg.fit(x_values, y_values)


plt.scatter(x_values, y_values)
plt.plot(x_values, body_reg.predict(x_values))
plt.show()
</code></pre>

<p>with this code i et the error message below</p>

<pre><code>Traceback (most recent call last):
  File ""C:/Users/User/Documents/body_brain_prediction.py"", line 9, in &lt;module&gt;
    x_values = dataframe[['Brain']]
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pandas\core\frame.py"", line 2981, in __getitem__
    indexer = self.loc._convert_to_indexer(key, axis=1, raise_missing=True)
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pandas\core\indexing.py"", line 1271, in _convert_to_indexer
    return self._get_listlike_indexer(obj, axis, **kwargs)[1]
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pandas\core\indexing.py"", line 1078, in _get_listlike_indexer
    keyarr, indexer, o._get_axis_number(axis), raise_missing=raise_missing
  File ""C:\Users\User\AppData\Local\Programs\Python\Python37-32\lib\site-packages\pandas\core\indexing.py"", line 1163, in _validate_read_indexer
    key=key, axis=self.obj._get_axis_name(axis)
KeyError: ""None of [Index(['Brain'], dtype='object')] are in the [columns]""
</code></pre>
","11833185","","","","","2019-07-25 00:27:15","issues with pandas txt read and ploting","<python><python-3.x><pandas><dataframe>","3","1","","","","CC BY-SA 4.0","1"
"57553554","1","57553644","","2019-08-19 09:06:04","","3","70","<p>I have a dataframe in which the index is a datetime and column A and B are objects. I need to see the unique values of A and B per week. 
I managed to get the unique value count per week (I am using the pd.grouper function for that) but I am struggling to get the unique values per week.</p>

<p>This code gives me the unique value counts per week</p>

<pre><code>df_unique = pd.DataFrame(df.groupby(pd.Grouper(freq=""W""))['A', 'B'].nunique())
</code></pre>

<p>However, the code below does not give me the unique values itself per week </p>

<pre><code>df_unique_list = pd.DataFrame(df.groupby(pd.Grouper(freq=""W""))['A', 'B'].unique())
</code></pre>

<p>This code gives me te following error message</p>

<pre><code>AttributeError: 'DataFrameGroupBy' object has no attribute 'unique'
</code></pre>
","11945555","","10035985","","2019-08-19 09:07:11","2019-08-19 09:18:47","List unique values using grouper","<python><python-3.x><pandas>","1","0","1","","","CC BY-SA 4.0","1"
"48707700","1","","","2018-02-09 14:06:55","","0","70","<p>How to store the dataframe from the output from pandas group by as something like output 2 given below without displaying both category in the first column (i think it is the index). </p>

<p>The below code output is given in the output 1 but I need to create something like output 2</p>

<pre><code>df1 = (df.groupby(['a_category', 'b_category' ]). size(). unstack ()
</code></pre>

<p>Output1:</p>

<pre><code>df1:

b_category        no     yes
a_category
A1                10     15
A2                20     22
A3                11     16
A4                12     26
</code></pre>

<p>Output :</p>

<p>df1: </p>

<pre><code>a_category        no     yes
A1                10     15
A2                20     22
A3                11     16
A4                12     26
</code></pre>
","3762120","","-1","","2018-02-09 18:03:47","2018-02-09 18:03:47","How to store the dataframe from the output from group by","<python><python-3.x><pandas><pandas-groupby>","0","2","","2018-02-09 14:14:10","","CC BY-SA 3.0","1"
"57750303","1","","","2019-09-02 00:09:25","","1","70","<p>I have html tables with multiple lines of text and data per cell that I'm trying to eventually extract and they use breaks for readability from site they were scraped from. 
Here is an example of one such cell:
<div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;td class=""cell""&gt;-&lt;br&gt;21%&lt;br&gt;1&lt;br&gt;
&lt;font color=""red""&gt;5001&lt;/font&gt;&lt;br&gt;12%
                &lt;/td&gt;</code></pre>
</div>
</div>
</p>

<p><strong>How can I convert these breaks into newlines that are compatible with Pandas (i.e there will be a 4 line string seperated by \n)?</strong></p>

<p>Using this snippet:</p>

<pre><code>for cell in soup.find_all('td'):
    cell.replace_with(cell.get_text('\n',strip=True))
</code></pre>

<p>Results in NaN values for every entry in the table.</p>
","507974","","7311767","","2019-09-02 00:37:47","2019-09-02 01:17:33","BeautifulSoup convert Breaks into python newlines","<python><python-3.x><pandas><beautifulsoup><newline>","1","2","","","","CC BY-SA 4.0","1"
"56889497","1","56889876","","2019-07-04 13:54:54","","1","70","<p>I am trying to find the location of a regex in a dataframe series and assign it to another series. I can do this for string with</p>

<pre><code>df['text'].str.lower().str.find('hello')
</code></pre>

<p>This function gives the index of the match. like</p>

<pre><code>text
World Hello
Hello WOrld
WOW
</code></pre>

<p>using this</p>

<pre><code>df['match_ind'] = df['text'].str.lower().str.find('hello')
</code></pre>

<p>it gives</p>

<pre><code>text            match_ind
World Hello     6
Hello WOrld     0
WOW             -1
</code></pre>

<p>but instead of <code>hello</code>, I want to use a regex say <code>'hello|world'</code>. Currently, it's giving me -1</p>

<pre><code>df['text'].str.lower().str.find('hello|world')
</code></pre>

<p>I am using python3</p>

<p>Does pandas find supports regex or is there some pandas way to do this.</p>
","9099959","","9099959","","2019-07-04 14:05:08","2019-07-04 14:18:05","How to use regex with pandas series.find function","<python><python-3.x><pandas>","1","2","","","","CC BY-SA 4.0","1"
"41319035","1","41319080","","2016-12-25 06:01:33","","1","70","<p>I am trying to read a content of a Wikipedia table in a pandas DataFrame. </p>

<pre><code>In [110]: import pandas as pd

In [111]: df = pd.read_html(""https://en.wikipedia.org/wiki/List_of_cities_by_GDP"")[0]
</code></pre>

<p>However, this dataframe contains gibberish values in certain columns:</p>

<pre><code>                        0                            1                     2  \
0  City/Metropolitan area                      Country  Geographical zone[1]   
1                Aberdeen               United Kingdom       Northern Europe   
2                 Abidjan  Côte d'Ivoire (Ivory Coast)                Africa   
3               Abu Dhabi         United Arab Emirates          Western Asia   
4             Addis Ababa                     Ethiopia                Africa   

                                    3  \
0     Official est. Nominal GDP ($BN)   
1  7001113000000000000♠11.3 (2008)[5]   
2                                 NaN   
3         7002119000000000000♠119 [6]   
4                                 NaN   

                                                   4  \
0  Brookings Institution[2] 2014 est. PPP-adjuste...   
1                                                NaN   
2                                                NaN   
3                          7002178300000000000♠178.3   
4                                                NaN   

                                         5  \
0  PwC[3] 2008 est. PPP-adjusted GDP ($BN)   
1                                      NaN   
2                   7001130000000000000♠13   
3                                      NaN   
4                   7001120000000000000♠12   

                                         6                             7  
0  McKinsey[4] 2010 est. Nominal GDP ($BN)  Other est. Nominal GDP ($BN)  
1                                      NaN                           NaN  
2                                      NaN                           NaN  
3                 7001671009999900000♠67.1                           NaN  
4                                      NaN                           NaN 
</code></pre>

<p>For example, in the above dataframe in the column for <code>Official est. Nominal GDP</code>, the first entry is <code>11.3(2008)</code> but we see some big number before that. I thought that this must be problem with encoding and I tried passing <code>ASCII</code> as well as <code>UTI</code> encodings:</p>

<pre><code>In [113]: df = pd.read_html(""https://en.wikipedia.org/wiki/List_of_cities_by_GDP"", encoding = 'ASCII')[0]
</code></pre>

<p>However, even this doesn't solve the problem. Any ideas?</p>
","2396502","","","","","2016-12-25 06:14:41","Improper rendering of numerical values while reading Wikipedia table in pandas","<python><python-3.x><pandas><beautifulsoup>","2","0","","","","CC BY-SA 3.0","1"
"57132310","1","57132345","","2019-07-21 10:30:39","","-2","69","<p>I've a data frame and one of its columns contains a list.</p>

<pre><code>             A         B
0            5    [3, 4]
1            4    [1, 1]
2            1    [7, 7]
3            3    [0, 2]
4            5    [3, 3]
5            4    [2, 2]
</code></pre>

<p>The output should look like this:</p>

<pre><code>             A      x    y
0            5      3    4
1            4      1    1
2            1      7    7
3            3      0    2
4            5      3    3
5            4      2    2
</code></pre>

<p>I have tried these options that I found <a href=""https://stackoverflow.com/a/35491399/10160150"">here</a> but its not working.</p>
","10160150","","","","","2019-07-21 10:51:39","How to convert a column containing list into separate column in pandas data-frame?","<python-3.x><pandas><list><dataframe><multiple-columns>","1","2","","2019-07-21 10:33:10","","CC BY-SA 4.0","1"
"57105740","1","57106293","","2019-07-19 04:53:12","","0","69","<p>I have a dataframe with 2 columns. </p>

<pre><code>id          data


135790075   job done, pay by card 4444-5555-6666-7777
</code></pre>

<p>I have 25k such rows where id is unique. Next I pass each card number to Luhn check and mask the card numbers that passed the check.</p>

<pre><code>data = pd.read_csv(""sample.csv"")
summ = data['summary']
creditcards = []
regex_match_index_list =[]
Validcardsfound = 0
regex_count = 0
for i in range(2):
    temp = re.findall(r'(\d\B(?:\d[ -]*?){13,16}\b)',str(values[i]))

    if temp:
        for each in temp:

            regex_count = regex_count + 1
            if doLuhn(str(each)) is True:


                creditcards.append(each)
                Validcardsfound = Validcardsfound + 1
                regex_match_index_list.append(i)
                #else:
                #    pass

            elif doLuhn(str(temp)) is False:
                pass




    else:
        pass
rows =[]


for each in regex_match_index_list:

    changed = data.iloc[each].str.replace(r'(\d\B(?:\d[ -]*?){13,16}\b)', r'(xxxx-xxxx-xxxx-xxxx')
   # print(""Changed"", changed)
    rows.append(changed)
</code></pre>

<p>When I try to replace the card numbers using str.replace function, I am losing the id column. The id field is empty when I create a new csv after replacing. If remove str.replace the id field is not gone. What am I doing wrong?</p>
","11645232","","","","","2019-07-19 05:55:44","str.replace() deletes column in python dataframe","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57298835","1","57299231","","2019-07-31 21:06:11","","0","69","<p>Please see my code below. The code works as expected but the output DAT file is missing the þ (LATIN SMALL LETTER THORN) character.</p>

<pre><code>import pandas as pd

inputdat = ""C:/Downloads/Python/SAMPLEDATA.dat""
outputdat = ""C:/Downloads/Python/Output.dat""

colnames = [""ID"", ""Company"", ""Employee"", ""Salary"", 'Years']
df = pd.read_csv(inputdat, names = colnames, header = None, skiprows = 1, encoding='UTF-8', quotechar='\xfe', sep='\x14', engine='python')
filter = ['UID_001', 'UID_002']

df.loc[df.ID.isin(filter), ['Salary', 'Years']] = """"

df.to_csv(outputdat, quotechar='\xfe', sep='\x14', index=False, encoding='UTF-8')
</code></pre>

<p>How do I make sure the thorn symbol is included in the output dat? </p>

<p>SAMPLE INPUT: <a href=""https://imgur.com/vUWCRfF"" rel=""nofollow noreferrer"">https://imgur.com/vUWCRfF</a></p>

<p>CURRENT OUTPUT: <a href=""https://imgur.com/sUlsy87"" rel=""nofollow noreferrer"">https://imgur.com/sUlsy87</a></p>

<p>DESIRED OUTPUT: <a href=""https://imgur.com/UnX0j43"" rel=""nofollow noreferrer"">https://imgur.com/UnX0j43</a></p>

<p>Note: I apologize for posting the sample input and outputs in image form. The delimiter and quote characters were not showing up correctly otherwise.</p>
","11228769","","240490","","2019-08-01 15:14:32","2019-08-01 15:14:32","How do I read and write the thorn character from a DAT file using quotechar in Pandas?","<python><python-3.x><pandas><csv>","1","5","","","","CC BY-SA 4.0","1"
"49703470","1","","","2018-04-07 03:12:47","","0","69","<p><strong>Aim:</strong></p>

<p>1.Populate single col DataFrame of random numbers based on poisson distribution</p>

<p>2.Depending on the random number, populate that quantity of random dates between Inception &amp; Expire Dates boundary.</p>

<p>3.Append into another DataFrame the random numbers &amp; dates
***if there are 1 random date then there will be 1 column, if 2 random dates then there will be 2 column of dates etc****</p>

<p><strong>Problem: I keep getting all ZEROs..I've tried alternative random no# methods and converting date into integer then randomise it but still cant rectify it...Please help!!!</strong></p>

<pre><code>import numpy as np
import pandas as pd
import random

InceptionDate = pd.to_datetime(dfMerged[""Inception Date""])
ExpireDate = pd.to_datetime(dfMerged[""Expire Date""])

#A extract of the dates for your reference if need
Name: Inception Date, Length: 40081, dtype: datetime64[ns]
0       2000-01-02
1       2000-01-08
2       2000-01-03
3       2000-01-08
4       2000-01-11
5       2000-01-05
6       2000-01-07
7       2000-01-04
...      ......
#A extract of the dates for your reference if need
Name: Expire Date, Length: 40081, dtype: datetime64[ns]
0       2001-01-02
1       2001-01-08
2       2001-01-03
3       2001-01-08
4       2001-01-11
5       2001-01-05
6       2001-01-07
7       2001-01-04
...      ......

L=0.08
PolicyCount = 40081
Claim_Random_Date = []
Claim_Random_Number = []

for row in range(1, PolicyCount):
    ClaimCount = np.random.poisson(L, 1)

    if ClaimCount == 0:
        Claim_Random_Date.append(ClaimCount)
        Claim_Random_Number.append(ClaimCount)

    else:
        for x in range(1, ClaimCount.shape[0]):
            Claim_Date_Random = random.uniform(InceptionDate, ExpireDate)

            Claim_Random_Date.append(Claim_Date_Random)
            Claim_Random_Number.append(ClaimCount)
########################
###Current Result###
print(pd.DataFrame(Claim_Random_Date))    
       0
0      0
1      0
2      0
3      0
4      0
5      0
6      0
7      0
print(sum(pd.DataFrame(Claim_Random_Number)))
0
########################
###Desired Result###
print(pd.DataFrame(Claim_Random_Date)) 
             0             1
0       2000-01-02         0
1       2000-12-03    2000-09-03
2       2000-07-03         0
3       2000-08-12         0
4       2000-03-11    2000-01-29
5       2000-08-20         0
6       2000-09-30         0
7       2000-12-04         0

print(sum(pd.DataFrame(Claim_Random_Number)))
9
</code></pre>
","9590985","","9590985","","2018-04-07 17:44:21","2018-04-07 17:44:21","Random Date & Number into DataFrame. But results are all 0s?","<python-3.x><pandas><for-loop><dataframe><random>","0","3","","","","CC BY-SA 3.0","1"
"57557788","1","57566493","","2019-08-19 13:30:16","","2","68","<p>I have df1 (key_col, cola, colb, colc) and df2(key_col, colz) I want to do the SQL equivalent of:</p>

<pre><code>UPDATE df1
SET df1.colc = df2.colz
WHERE df1.key = df2.key
</code></pre>

<p>I've tried various incarnations of merge and join without success. Not looking for loops but rather using Pandas</p>

<pre><code>df1: key_col  cola  colb  colc
     50       'foo' 'bar' 'foo'
     49       'foo' 'bar' 'bla'
     23       'bar' 'foo' 'bla'

df2: key_col  colz
     23      'something'
     50      'something else'
</code></pre>

<p>Note, the key col is not a pandas index rather a normal column</p>

<p>Resulting df1 desired:</p>

<pre><code>df1: key_col  cola  colb  colc
     50   'foo' 'bar' 'something else'
     49   'foo' 'bar' 'bla'
     23   'bar' 'foo' 'something'
</code></pre>

<p>EDIT:</p>

<p>Have added row key_col = 49 to make clear that a straight copy over of colc c will not work as it will overwrite existing values.</p>
","8297764","","8297764","","2019-08-19 14:29:52","2019-08-20 03:52:19","Pandas Update dataframe with join on common column","<python-3.x><pandas><sql-update>","2","0","0","","","CC BY-SA 4.0","1"
"56835963","1","56836112","","2019-07-01 12:44:08","","-1","68","<p>I would like to divide the number of sales by the number of sales opportunities in order to get average sales by opportunities.</p>

<p>Here is an example dataframe with mixed types:</p>

<pre><code>df = pd.DataFrame({'Opportunity':['AB122','AB122','AB123', 'AB124'],
           'Quantity': [2, 3, 4, 1],
           'Member': [""AACC"", ""AACC"", ""AACC"", 'DDEE']})


print (df)
  Opportunity  Quantity Member
0       AB122         2   AACC
1       AB122         3   AACC
2       AB123         4   AACC
3       AB124         1   DDEE
</code></pre>

<p>I was able to get the sum of the sales with this one</p>

<pre><code>df.pivot_table('Quantity', 'Member', aggfunc=np.sum)
</code></pre>

<p>But if I do the same for the Opportunity, I only get the Opportunity names glued together. Also, the duplicate opportunities are still included.</p>

<pre><code>df.pivot_table('Opportunity','Member', aggfunc=np.sum)
</code></pre>

<p>What I need instead is that the opportunities are counted, but only the unique ones (AACC should only have 2 opportunities). The outcome of the counting should be:</p>

<pre><code>print (df2)
AACC 2
DDEE 1
</code></pre>

<p>So then I could get the average member sales by dividing the sales quantity by number of opportunities:</p>

<pre><code>print (df3)
AACC 4.5 
DDEE 1
</code></pre>

<p>Note on the calculation. AACC gets 2 as 9 divided by 2 is 4.5, DDEE gets 1 as 1 divided by 1 is 1.</p>
","10748447","","10748447","","2019-07-01 15:34:23","2019-07-01 15:42:20","Count number of unique occurrences in a dataframe","<python><python-3.x><pandas><dataframe>","2","4","","","","CC BY-SA 4.0","1"
"56768938","1","56769085","","2019-06-26 08:57:28","","2","68","<p>Given the following list:</p>

<pre><code>a = ['1', '2', '12', '5']
</code></pre>

<p>I am trying to delete all values from the Dataframes cells which don't match the elements in the list. I know for sure that every value from the list appears only once in each row of the Dataframe.</p>

<pre><code>test = pd.DataFrame({'0' : ['1','4','5','5'],
           '1' : ['4','1','12','10'],
           '2' : ['10','12','4','2'],
           '3' : ['2','10','10','4'],
           '4' : ['5','2','2','1'],
           '5' : ['12','5','1','12']})


    0   1   2   3   4   5
0   1   4   10  2   5   12
1   4   1   12  10  2   5
2   5   12  4   10  2   1
3   5   10  2   4   1   12
</code></pre>

<p>The important aspect is to maintain the order of the matching numbers from the list in the Dataframe. </p>

<p>The result should look like this:</p>

<pre><code>    0   1   2   3
0   1   2   5   12
1   1   12  2   5
2   5   12  2   1
3   5   2   1   12
</code></pre>

<p>Thanks in advance!</p>
","11181726","","","","","2019-06-26 09:47:03","Delete pandas cells values not included in list","<python-3.x><pandas>","3","0","","","","CC BY-SA 4.0","1"
"57748843","1","","","2019-09-01 19:23:50","","0","68","<p>I have a column &quot;data&quot; which has json object as values. I would like to add a key-value pair inside nested json</p>
<pre><code>source = {'my_dict':[{'_id': 'SE-DATA-BB3A'},{'_id': 'SE-DATA-BB3E'},{'_id': 'SE-DATA-BB3F'}],  'data': [ {'bb3a_bmls':[{'name': 'WAG 01', 'id': '105F', 'state': 'available', 'nodes': 3,'volumes-': [{'state': 'available', 'id': '330172', 'name': 'q_-4144d4e'}, {'state': 'available', 'id': '275192', 'name': 'p_3089d821ae', }]}]}
, {'bb3b_bmls':[{'name': 'FEC 01', 'id': '382E', 'state': 'available', 'nodes': 4,'volumes': [{'state': 'unavailable', 'id': '830172', 'name': 'w_-4144d4e'}, {'state': 'unavailable', 'id': '223192', 'name': 'g_3089d821ae', }]}]}
, {'bb3c_bmls':[{'name': 'ASD 01', 'id': '303F', 'state': 'available', 'nodes': 6,'volumes': [{'state': 'unavailable', 'id': '930172', 'name': 'e_-4144d4e'}, {'state': 'unavailable', 'id': '245192', 'name': 'h_3089d821ae', }]}]}
] }

input_df = pd.DataFrame(source)
</code></pre>
<p>input_df looks like below:
<a href=""https://i.stack.imgur.com/gapUc.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gapUc.png"" alt=""enter image description here"" /></a></p>
<p>Now I need to add the &quot;my_dict&quot; column values as a 1st element inside the nested json values of &quot;data&quot; column</p>
<p>My Target dataframe should look like below ( I have highlighted the changes in bold)
<a href=""https://i.stack.imgur.com/gEBT2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gEBT2.png"" alt=""enter image description here"" /></a></p>
<p>I tired using dict.update() but it doesn't seem to help. I'm stuck here and not getting any idea how to take this forward. Appreciate your help.</p>
","9274726","","-1","","2020-06-20 09:12:55","2019-09-03 12:52:33","How to add a column value to another column's dictionary using python , pandas","<python><json><python-3.x><pandas>","3","0","","","","CC BY-SA 4.0","1"
"57695288","1","57695723","","2019-08-28 15:00:43","","0","67","<p>I have a dataframe like as given below</p>

<pre><code>df = pd.DataFrame({
'date' :['2173-04-03 12:35:00','2173-04-03 17:00:00','2173-04-03 20:00:00','2173-04-04 11:00:00','2173-04-04 12:00:00','2173-04-04 11:30:00','2173-04-04 16:00:00','2173-04-04 22:00:00','2173-04-05 04:00:00'],
'subject_id':[1,1,1,1,1,1,1,1,1],
'val' :[5,5,5,10,10,5,5,8,8]
 })
</code></pre>

<p>I would like to apply couple of logic (<code>logic_1</code> on <code>val</code> column and <code>logic_2</code> on <code>date</code> column) to the code. Please find below the logic</p>

<pre><code>logic_1 = lambda x: (x.shift(2).ge(x.shift(1))) &amp; (x.ge(x.shift(2).add(3))) &amp; (x.eq(x.shift(-1)))
logic_2 = lambda y: (y.shift(1).ge(1)) &amp; (y.shift(2).ge(2)) &amp; (y.shift(-1).ge(1)) 
</code></pre>

<p>credit to SO users for helping me with logic</p>

<p>This is what I tried below</p>

<pre><code>   df['label'] = ''
   df['date'] = pd.to_datetime(df['date'])
   df['tdiff'] = df['date'].shift(-1) - df['date']
   df['tdiff'] = df['tdiff'].dt.total_seconds()/3600
   df['lo_1'] = df.groupby('subject_id')['val'].transform(logic_1).map({True:'1',False:''})
   df['lo_2'] = df.groupby('subject_id')['tdiff'].transform(logic_2).map({True:'1',False:''}) 
</code></pre>

<p>How can I make both the <code>logic_1</code> and <code>logic_2</code> be part of one logic statement? Is it even possible? I might have more than 2 logics as well. Instead of writing one line for each logic, is it possible to couple all logics together in one logic statement.</p>

<p>I expect my output to be with <code>label</code> column being filled with <code>1</code> when both <code>logic_1</code> and <code>logic_2</code> are satisfied</p>

<p><a href=""https://i.stack.imgur.com/Kf2yU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Kf2yU.png"" alt=""enter image description here""></a></p>
","10829044","","10829044","","2019-08-28 15:27:25","2019-08-28 15:29:08","How to include two lambda operations in transform function?","<python><python-3.x><pandas><dataframe><pandas-groupby>","1","11","","","","CC BY-SA 4.0","1"
"49662291","1","49662369","","2018-04-05 00:16:24","","2","67","<p>I have an excel spreadsheet that has four column names: ""Column One, Column Two, Jun-17, and Column Three""</p>

<p>When I display my column names after reading in the data I get something very different from the ""Jun-17"" text I was hoping to receive.  What should I be doing differently?</p>

<pre><code>import pandas as pd

df = pd.read_excel('Sample.xlsx', sheet_name='Sheet1')

print(""Column headings:"")
print(df.columns.tolist())

Column headings:
['Column One', 'Column Two', datetime.datetime(2018, 6, 17, 0, 0), 'Column Three']
</code></pre>
","9599072","","9209546","","2018-04-05 00:27:03","2018-04-05 08:06:25","How can I force Pandas dataframe column query to return only str and not interpret what it is seeing?","<python><python-3.x><pandas><datetime><dataframe>","1","0","","","","CC BY-SA 3.0","1"
"57413403","1","","","2019-08-08 13:08:15","","0","67","<p>I'm writing a csv-file by reading out values from a binary file.
If I want to open the csv-file with pandas</p>

<pre class=""lang-py prettyprint-override""><code>df=pd.read_csv(file)
</code></pre>

<p>im getting an error.</p>

<pre class=""lang-py prettyprint-override""><code>Unable to open 'parsers.pyx': Unable to read file
(Error: File not found (c:\users\sascha\test_files\pandas\_libs\parsers.pyx)).
</code></pre>

<p>The file is definitely at the given path and I am able to open it by</p>

<pre class=""lang-py prettyprint-override""><code>open(file, 'w')
</code></pre>

<p>The file seems to have a line with unreadable characters at line 25 which I found out by using nrows in the read_csv.</p>

<pre class=""lang-py prettyprint-override""><code>df_data = pd.read_csv('y-Values.csv', nrows=24) --&gt; Works!
df_data = pd.read_csv('y-Values.csv', nrows=25) --&gt; Doesn't work!
</code></pre>

<p>Im writing the file as follows:</p>

<pre class=""lang-py prettyprint-override""><code>with open(name, ""rb"") as binary_file:
            # Values of byte 437 String: Batchnname
            binary_file.seek(436, 0)
            chargenname_bytes = binary_file.read(331)
            try: # fängt fehler beim decoden ab
                chargenname_bytes = chargenname_bytes.decode('ascii')
            except:
                continue
            chargenname_bytes = re.match(r""[\x20-\x7E]+"", chargenname_bytes).group() # match printable characters using regex
            chargenname_bytes = str(chargenname_bytes.replace('  ', '_').replace(' ', '_').replace('/', '#')) # replace forbidden characters
            chargen_nummer = chargenname_bytes.split('_', 1)[0]
            try:
                chargen_typ = chargenname_bytes.split('_', 1)[1].rstrip('_')
            except:
                chargen_typ = chargenname_bytes[1].rstrip('_')
            # Path and filename to write
            output_filename_y_values = str('../csv_file/y-Values.csv')

            # Open the file with writing permission
            myfile_data = open(output_filename_y_values, 'at+')
            myfile_data.write(chargenname_bytes+',') # Write Batchname to csv
            # Write values to csv
            for i in range(0,x_anzahl_steps,1000):
                binary_file.seek(768+2*i, 0)
                y_values = int.from_bytes(binary_file.read(2), byteorder='little',signed=True)*2**y_exponent
                myfile_data.write(str(y_values)+',')
            myfile_data.write('\n')
            chargenname_bytes = ''
myfile_data.close()
binary_file.close()
</code></pre>

<p>May be there is a smarter way to write the csv. But anyway I can open it in text editor or excel but I am not able to load it into a dataframe. Any ideas?</p>

<p>I just recently found out that VSCode shows me also this error:</p>

<pre class=""lang-py prettyprint-override""><code>ERROR TOKENIZING DATA. C ERROR: EXPECTED 10 FIELDS IN LINE 27, SAW 11
</code></pre>
","11568146","","11568146","","2019-08-08 14:08:20","2019-08-08 14:08:20","I have problems reading a csv file. There seem to be some non readable characters in the file but I can not find them","<python-3.x><sklearn-pandas>","0","3","","","","CC BY-SA 4.0","1"
"48541018","1","","","2018-01-31 11:25:38","","0","67","<p>I am reading multiple .csv files and writing them to a new .csv file.</p>

<p>I am trying to use an if/else statement in conjunction with <code>pd.insert()</code> to add a new column to the beginning of a dataframe as such:</p>

<pre><code>path = r'.../Model_AMs'

allFiles = glob.glob(path + ""/*.csv"")

for file_ in allFiles:

    df = pd.read_csv(file_, header=None).astype(int).transpose().melt()

    if 'slot' in file_:
        df.insert(loc=0, column='label', value=0)
        print('slot file!')
    else:
        df.insert(loc=0, column='label', value=1)
        print('boss file!')

    pd.DataFrame(dict(zip(df.index, df.value)), index=[0]).to_csv(
      'trainingdata.csv', index=False, header=None, mode='a')
</code></pre>

<p>Currently the new csv file looks something like this:</p>

<pre><code>pixel0, pixel1, pixel2,...
0, 1, 1,...
</code></pre>

<p>I want it to look like this:</p>

<pre><code>label, pixel0, pixel1, pixel2,...
1, 0, 1, 1,...
</code></pre>

<p>Writing the csv files is working as I want it to, however I want to add a new column to the beginning of each dataframe based on what the file name contains. Not sure if I am using the <code>pd.insert()</code> function correctly or maybe the if/else statement is wrong.</p>

<p>Any help would be appreciated</p>
","9261499","","","","","2018-01-31 11:35:02","Correct usage of pd.insert() function","<python><python-3.x><pandas><csv>","1","1","","","","CC BY-SA 3.0","1"
"57864240","1","57864537","","2019-09-10 04:53:46","","2","67","<p>I have the following DataFrame:</p>

<pre><code>df = pd.DataFrame({
'subject_id':[1,1,1,1,1,1,1,2,2,2,2,2],
'time_1' :['2173/04/01 12:35:00','2173/04/01 12:50:00','2173/04/02 12:59:00','2173/04/02 13:14:00','2173/04/04 13:37:00','2173/04/06 13:39:00','2173/04/06 11:30:00','2173/04/08 16:00:00','2173/04/09 22:00:00','2173/04/11 04:00:00','2173/04/13 04:30:00','2173/04/14 08:00:00'],
 'val' :[0,0,0,0,1,0,0,0,0,0,0,0]
})
df['time_1'] = pd.to_datetime(df['time_1'])
df['day'] = df['time_1'].dt.day
</code></pre>

<p><a href=""https://i.stack.imgur.com/2C1b7.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2C1b7.png"" alt=""enter image description here""></a></p>

<p>What I would like to do is, <code>for each subject, get two days</code>, based on a condition given below</p>

<p><code>val = 1</code>- <strong>Fetch previous two days (from val = 1)</strong></p>

<p><code>val = 0</code>- <strong>Fetch middle two days of his duration</strong></p>

<p>This is what I tried, but it isn't elegant or accurate.</p>

<pre><code>con = lambda x: (x.eq(1))
con_1 = lambda x: (x.eq(0))
df.loc[df.groupby('subject_id')['val'].transform(con)]['time_1'] - timedelta(days = 2)
</code></pre>

<p>Following is my expected output:</p>

<p><a href=""https://i.stack.imgur.com/PXINY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/PXINY.png"" alt=""enter image description here""></a></p>

<p><code>subject = 1</code> has <code>val = 1</code>, so we get <code>previous two days from val = 1</code> for him and for <code>subject = 2</code>, as there is no <code>val = 1</code>, we get <code>middle 2 days of his duration</code> (8th - 14th)</p>
","10829044","","7758804","","2019-09-10 05:11:21","2019-09-10 06:16:36","How to get previous and middle dates in each group using pandas","<python><python-3.x><pandas><dataframe><pandas-groupby>","1","0","","","","CC BY-SA 4.0","1"
"49065937","1","","","2018-03-02 09:10:35","","-2","67","<p>I want to create a dataset with 300 features and instances which are combinations of 0 or 1(boolean).I have to specify the 1's using some id's.How can I do it with python.
for eg: one instance should be like the columns 4,45,213,6,48 should be 1 and the combinations of those id's</p>
","7765871","","","","","2018-03-21 22:37:33","How to create a Supervised dataset?","<python-3.x><machine-learning><data-science><sklearn-pandas>","1","0","","","","CC BY-SA 3.0","1"
"57561581","1","57561918","","2019-08-19 17:40:30","","0","67","<p>I am reading in some excel data that contains datetime values stored as '8/13/2019  4:51:00 AM' and formatted as '4:51:00 AM' in excel.  I would like to have a data frame that converts the value to a timestamp formatted as '4:51 AM' or H%:M% p%.</p>

<p>I have tried using datetime strptime but I don't believe I have been using it correctly.  None of my attempts have worked so I have left it out of the code below.  The two columns I would like to convert are 'In Punch' and 'Out Punch'</p>

<pre><code>import pandas as pd
import pymssql
import numpy as np
import xlrd
import os
from datetime import datetime as dt    

rpt = xlrd.open_workbook('OpenReport.xls', logfile=open(os.devnull,'w'))
rpt = pd.read_excel(rpt, skiprows=7)[['ID','Employee','Date/Time','In Punch','Out Punch',
                                      'In Punch Comment','Out Punch Comment', 'Totaled Amount']]
rpt
</code></pre>

<p>Any suggestions will be greatly appreciated.  Thanks</p>

<p>EDIT:<br>
Working with the following modifications now.</p>

<pre><code>rpt['In Punch'] = pd.to_datetime(rpt['In Punch']).dt.strftime('%I:%M %p')
rpt['Out Punch'] = pd.to_datetime(rpt['Out Punch']).dt.strftime('%I:%M %p')
</code></pre>
","11072812","","11072812","","2019-08-19 18:41:45","2019-08-19 18:41:45","Data Frame column to extract time with AM/PM fromat from datetime value","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57657119","1","57657319","","2019-08-26 11:31:51","","1","67","<p>Assume the dataframe with column 'A' and column 'condition' as reproduced by the code below.</p>

<pre><code>example = pd.DataFrame({'A': range(10), 'condition': [0,1,0,1,2,0,1,2,2,1]})
</code></pre>

<p>I want to multiply by 2 the values in column 'A' if the values in column 'B' are 0 or 2. So I tried these:</p>

<pre><code>example['A']=example['A'].apply(lambda x: x*2 \
             if example['condition']==0 or example['condition']==2)

example['A']=np.where(example.condition==0 or example.condition==2, \
             lambda x: x*2, example.A)
</code></pre>

<p>but none of these work in order to get the desired output as below:</p>

<pre><code>    output:                 desired output:
    example                 example
       A  B                          A  B
    0  0  0                      0   0  0
    1  1  1                      1   1  1
    2  2  0                      2   4  0
    3  3  1                      3   3  1
    4  4  2                      4   8  2
    5  5  0                      5  10  0
    6  6  1                      6   6  1
    7  7  2                      7  14  2 
    8  8  2                      8  16  2 
    9  9  1                      9   9  1  
</code></pre>

<p>If I get the desired output, I want to groupby 'condition' and calculate the absolute summation of 'A' values  if the 'A' values are bigger than 2.5. I have this in mind, but I if I do not get the desired output from above I am not sure if it works.</p>

<pre><code>group1=example.groupby([example[condition')['A'].\
       agg([ ('A sum' , lambda x : x[x&gt;=2.5].abs(sum()) ])
</code></pre>

<p>Any suggestions please?</p>
","10750541","","","","","2019-08-27 03:55:51","Apply a function on a column of a dataframe depending on the value of another column and then groupby","<python-3.x><pandas><dataframe><lambda>","4","0","","","","CC BY-SA 4.0","1"
"57026783","1","","","2019-07-14 11:01:08","","0","66","<p>The date is in separate columns</p>

<pre><code>Month   Day Year
8   12  1993
8   12  1993
8   12  1993
</code></pre>

<p>I want to merge it in one column</p>

<pre><code>Date
8/12/1993
8/12/1993
8/12/1993
</code></pre>

<p>I tried </p>

<pre><code>df_date = df.Timestamp((df_filtered.Year*10000+df_filtered.Month*100+df_filtered.Day).apply(str),format='%Y%m%d')
</code></pre>

<p>I get this error</p>

<pre><code>AttributeError: 'DataFrame' object has no attribute 'Timestamp'
</code></pre>
","11571791","","10679134","","2019-07-14 11:44:42","2019-07-14 14:32:13","how to merge month day year columns in date column?","<python-3.x><pandas>","3","2","","","","CC BY-SA 4.0","1"
"49700647","1","","","2018-04-06 20:35:21","","2","66","<p>i have data frame that looks like this </p>

<pre><code>    value
0   A067-M4FL-CAA-020
1   MRF2-050A-TFC,60 ,R-12,HT
2   moreinfo
3   MZF8-050Z-AAB
4   GoCats
5   MZA2-0580-TFD,60 ,R-669,LT
</code></pre>

<p>i want to be able to strip <code>,60 ,R-12,HT</code> using regex and also deletes the <code>moreinfo</code> and <code>GoCats</code> rows from the df. </p>

<p>My expected Results:</p>

<pre><code>     value
0   A067-M4FL-CAA-020
1   MRF2-050A-TFC
2   MZF8-050Z-AAB
3   MZA2-0580-TFD
</code></pre>

<p>I first removed the strings</p>

<pre><code>del = ['hello', 'moreinfo']
for i in del:
   df = df[value!= i]
</code></pre>

<p>Can somebody suggest a way to use regex to match and delete all case that do match <code>A067-M4FL-CAA-020</code> or <code>MZF8-050Z-AAB</code> pattern so i don't have to create a list for all possible cases?</p>

<p>I was able to strip a single line like this but i want to be able to strip all matching cases in the dataframe</p>

<pre><code>pattern = r',\w+ \,\w+-\w+\,\w+ *'
line = 'MRF2-050A-TFC,60 ,R-12,HT'
for i in re.findall(pattern, line):
   line = line.replace(i,'')

&gt;&gt;&gt; MRF2-050A-TFC
</code></pre>

<p>I tried adjusting my code but it prints out the same output for each row </p>

<pre><code>pattern = r',\w+ \,\w+-\w+\,\w+ *'
for d in df:
   for i in re.findall(pattern, d):
     d = d.replace(i,'')
</code></pre>

<p>Any suggestions will be greatly appreciated. Thanks</p>
","8862527","","1222951","","2018-04-06 20:36:19","2018-04-07 12:18:21","Using Reg Ex to Match Strings in a Data Frame and Replace - python","<python><regex><python-3.x><pandas><strip>","3","1","0","","","CC BY-SA 3.0","1"
"57197860","1","57198095","","2019-07-25 08:46:46","","0","66","<p>I have a dataframe like shown below:</p>

<pre><code>df = pd.DataFrame({'person_id' :[1,1,1,2,2,2,2,2,2],'level_1': ['L1FR','L1Date','L1value','L1FR','L1Date','L1value','L2FR','L2Date','L2value'], 'val3':['Fasting','11/4/2005',1.33,'Random','18/1/2007',4.63,'Fasting','18/1/2017',8.63]})
</code></pre>

<p><a href=""https://i.stack.imgur.com/ERZtl.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ERZtl.png"" alt=""Image.""></a></p>

<p>But I would like to have my output dataframe as shown below:</p>

<p><a href=""https://i.stack.imgur.com/hOEYu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hOEYu.png"" alt=""Image.""></a></p>

<p>Please note that concept_id = 123 indicates ""Fasting"" and 456 represents ""Random"". I have this information (keys) in another csv file. How do I link all this?</p>

<p>However, I managed to get till here:</p>

<pre class=""lang-py prettyprint-override""><code>d1 = s1[s1['level_1'].str.contains('Date')]
d2 = s1[~s1['level_1'].str.contains('Date')]


d1['g'] = d1.groupby('person_id').cumcount()
d2['g'] = d2.groupby('person_id').cumcount()

d3 = pd.merge(d1,d2,on=[""person_id"",'g'],how='left').drop(['g','level_1_x','level_1_y'], axis=1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/yaKrY.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yaKrY.png"" alt=""Image.""></a></p>

<p>Please note that what I have shown is for a single csv data file and single hash/key file which contains concept_ids. In real time, I have more than 30 csv files and 1 hash/key file.</p>

<p>So the hash file remains the same, but data file changes frequently. </p>

<p>For example, this file had two concepts ""Fasting"" and ""Random"" , which could be replaced as 123,456 by looking at the hash file. Similarly other data file might contain terms like ""Sick"", ""Healthy"" which should be replaced as 135,579 etc.</p>

<p>But the data format remains the same. Can you help me achieve this?</p>

<p>** Update screenshot for output **</p>

<p><a href=""https://i.stack.imgur.com/Pr93e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pr93e.png"" alt=""enter image description here""></a></p>

<p>** Mismatch in group by cum count **</p>

<p><a href=""https://i.stack.imgur.com/Cd3W1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Cd3W1.png"" alt=""enter image description here""></a></p>

<p>I am expecting all these values should be of same numbers for each group (ex: 1,1,1 or 10,10,10) based on the number of occurrence the value is. Am I right? But no idea why it is different. Moreover my input dataframe has no NA's</p>
","10829044","","10829044","","2019-07-26 07:05:49","2019-07-26 07:05:49","Apply rules to transpose dataframe in Python","<python><python-3.x><pandas><list><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"56783275","1","56788923","","2019-06-27 02:56:22","","0","66","<p>I'm working on excel file to show the data on new tkinter window. I want to sort the rows using the Agent column values and ditsplay it on tkinter window</p>

<p>I tried using condition and iteration but i can't do it properly.</p>

<p>Here is the data:</p>

<pre><code>  Country       Port incoterm Capacity    Date Agent  $ value      Total
0   Japan   Yokohama      FOB    20ton  2019/5   Sam   2650.6  2650600.0
1   China     Ningbo      DAT    40ton  2019/1    Li   2650.6  2385540.0
2     USA  Baltimore      FOB    Other  2018/9  John   2650.6  4240960.0
3  Russia     Moscow      EXW    20ton  2019/1  Vlad   2650.6  2120480.0
4   Japan      Tokyo      FOB    20ton  2019/1   Sam   2650.6  2915660.0
5   Japan      Tokyo      FOB    20ton  2019/1  Dave   2650.6  3180720.0
6   China   Shanghai      EXW    40ton  2019/1    Li   2500.6  3128250.6
</code></pre>

<p>Here is the code:</p>

<pre><code>data = pd.read_excel(""example.xlsx"")
df = pd.DataFrame(data)
a = df.loc[(df.Country == 'Japan') &amp; (df.incoterm == 'FOB') &amp; (df.Capacity 
== '20ton') &amp; (df.Port == 'Tokyo')]
_a = pd.DataFrame(a)
root = Tk()

for agent in _a.itertuples():
    if agent.Agent is agent.Agent:
        temp_agent = '{0}'.format(agent.Agent)
        ttk.Label(root, text=""Agent:""+temp_agent).pack()
        for data in _a.itertuples():
            temp_text = '{0} {1} - ({2})'.format(data.Country, 
            data.incoterm, data.Total)
            ttk.Label(root, text=temp_text).pack()
            print (temp_text)
mainloop()
</code></pre>

<p>Output:</p>

<pre><code>Sam
Japan FOB - (2915660.0)
Japan FOB - (3180720.0)
Dave
Japan FOB - (2915660.0)
Japan FOB - (3180720.0)
</code></pre>

<p>Expected output:</p>

<pre><code>Sam
Japan FOB - (2915660.0)
Dave
Japan FOB - (3180720.0)
</code></pre>
","11623819","","11623819","","2019-06-27 03:49:37","2019-06-27 10:20:58","How to sort dictionary by its values","<python><python-3.x><pandas><tkinter>","2","2","","","","CC BY-SA 4.0","1"
"48932379","1","48932411","","2018-02-22 16:23:10","","1","66","<p>I have the below code that basically performs a group by operation, followed by a sum.</p>

<pre><code>grouped = df.groupby(by=['Cabin'], as_index=False)['Fare'].sum()
</code></pre>

<p>I then rename the columns</p>

<pre><code>grouped.columns = ['Cabin', 'testCol']
</code></pre>

<p>And I then merge the ""grouped"" dataframe with my original dataframe to calculate aggregate.</p>

<pre><code>df2 = df.merge(grouped, on='Cabin')
</code></pre>

<p>What this does is to populate my initial dataframe with the 'testCol' from my ""grouped"" dataframe.</p>

<p>Can this code be optimized to fit in one line or something similar?</p>
","6638903","","","","","2018-02-22 16:24:36","Code Optimization for groupby","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"57704995","1","57705507","","2019-08-29 07:20:01","","2","65","<p>I have a dataframe like as shown below</p>

<pre><code>df1 = pd.DataFrame({
     'subject_id':[1,1,1,1,1,1,1,1,1,1,1],
     'time_1' :['2173-04-03 12:35:00','2173-04-03 12:50:00','2173-04-03 
           12:59:00','2173-04-03 13:14:00','2173-04-03 13:37:00','2173-04-04 
           11:30:00','2173-04-05 16:00:00','2173-04-05 22:00:00','2173-04-06 
           04:00:00','2173-04-06 04:30:00','2173-04-06 08:00:00']
       })
</code></pre>

<p>I would like to create another column called <code>tdiff</code> to calculate the time difference</p>

<p>This is what I tried</p>

<pre><code>df1['time_1'] = pd.to_datetime(df1['time_1'])
df['time_2'] = df['time_1'].shift(-1)
df['tdiff'] = (df['time_2'] - df['time_1']).dt.total_seconds() / 3600
</code></pre>

<p>But this produces an output like as shown below. As you can see, it subtracts from the next date. Instead I would like to restrict the time difference only to the same day. Ex: if <code>Jan 15th 20:00:00 PM</code> is the last record for that day, then I expect the <code>tdiff</code> to be <code>4:00:00</code> (<code>24:00:00: - 20:00:00</code>)</p>

<p>I understand it is happening because I am shifting the values of time to subtract and it's obvious that the highlighted rows are picking records from next date. But is there a way to avoid this but calculate the time difference between records in a same day? </p>

<p><a href=""https://i.stack.imgur.com/X2RPr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/X2RPr.png"" alt=""enter image description here""></a></p>

<p>I expect my output to be like this. Here NaN should be replaced by the current date (<code>23:59:00</code>). if you check the difference, you will get an idea</p>

<p><a href=""https://i.stack.imgur.com/0o6Sw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0o6Sw.png"" alt=""enter image description here""></a></p>

<p>Is there any existing method or pandas function that can help us do this <code>datewise timedelta</code>? How can I shift the values datewise?</p>
","10829044","","10829044","","2019-08-29 08:05:32","2019-08-29 08:15:13","How to restrict time difference to same day?","<python><python-3.x><pandas><datetime><python-datetime>","2","2","","","","CC BY-SA 4.0","1"
"49211471","1","49211547","","2018-03-10 16:37:19","","0","65","<p>I have a DataFrame with customer_id, date, product_id that they bought. I want to convert this DataFrame to 2 dictionary</p>

<pre><code>customer_id    date     product_id
1            10/3/2017  1234
2            11/3/2017  4321
1            10/3/2017  7384
2            10/3/2017  1234
</code></pre>

<p>I want the output like: </p>

<pre><code>{'10/3/2017': {1 : 1234, 1: 7384, 2: 1234}, '11/3/2017': {2 : 4321}}
</code></pre>

<p>I tried to use</p>

<pre><code>df.set_index(['date','customer_number']).T.to_dict('record')
</code></pre>

<p>But it won't give me 2 dictionaries.</p>

<pre><code>{('10/3/2017', 1): 1234', .....}
</code></pre>
","8792365","","8792365","","2018-03-10 17:13:57","2018-03-10 17:55:52","Converting pandas DF into special dictionary?","<python-3.x><pandas>","2","4","","","","CC BY-SA 3.0","1"
"57262864","1","57262914","","2019-07-30 00:36:01","","0","65","<p><strong>Background</strong></p>

<p>The following is a minor change from <a href=""https://stackoverflow.com/questions/57242540/modification-of-skipping-empty-list-and-continuing-with-function"">modification of skipping empty list and continuing with function</a></p>

<pre><code>import pandas as pd
Names =    [list(['ann']),
               list([]),
               list(['elisabeth', 'lis']),
               list(['his','he']),
               list([])]
df = pd.DataFrame({'Text' : ['ann had an anniversery today', 
                                       'nothing here', 
                                       'I like elisabeth and lis 5 lists ',
                                        'one day he and his cheated',
                                        'same here'
                            ], 

                          'P_ID': [1,2,3, 4,5], 
                          'P_Name' : Names

                         })

#rearrange columns
df = df[['Text', 'P_ID', 'P_Name']]
df
                  Text                P_ID  P_Name
0   ann had an anniversery today        1   [ann]
1   nothing here                        2   []
2   I like elisabeth and lis 5 lists    3   [elisabeth, lis]
3   one day he and his cheated          4   [his, he]
4   same here                           5   []
</code></pre>

<p>The code below works </p>

<pre><code>m = df['P_Name'].str.len().ne(0)
df.loc[m, 'New'] = df.loc[m, 'Text'].replace(df.loc[m].P_Name,'**BLOCK**',regex=True) 
</code></pre>

<p>And does the following</p>

<p>1) uses the name in <code>P_Name</code> to block the corresponding text in the <code>Text</code> column by placing <code>**BLOCK**</code> </p>

<p>2) produces a new column <code>New</code> </p>

<p>This is shown below</p>

<pre><code>   Text  P_ID P_Name  New
0                     **BLOCK** had an **BLOCK**iversery today
1                     NaN
2                     I like **BLOCK** and **BLOCK** 5 **BLOCK**ts
3                     one day **BLOCK** and **BLOCK** c**BLOCK**ated
4                     NaN
</code></pre>

<p><strong>Problem</strong></p>

<p>However, this code works a little ""too well."" </p>

<p>Using <code>['his','he']</code> from <code>P_Name</code>  to block <code>Text</code>:</p>

<p>Example:  <code>one day he and his cheated</code> becomes <code>one day **BLOCK** and **BLOCK** c**BLOCK**ated</code></p>

<p>Desired:  <code>one day he and his cheated</code> becomes <code>one day **BLOCK** and **BLOCK** cheated</code> </p>

<p>In this example, I would like <code>cheated</code> to stay as <code>cheated</code> and not become <code>c**BLOCK**ated</code></p>

<p><strong>Desired Output</strong></p>

<pre><code>    Text P_ID P_Name  New
0                     **BLOCK** had an anniversery today
1                     NaN
2                     I like **BLOCK** and **BLOCK**5 lists
3                     one day **BLOCK** and **BLOCK** cheated
4                     NaN
</code></pre>

<p><strong>Question</strong></p>

<p>How do I achieve my desired output?</p>
","6598999","","6598999","","2019-08-25 15:26:52","2019-10-12 00:23:14","including word boundary in string modification to be more specific","<python-3.x><string><pandas><text><replace>","2","0","","","","CC BY-SA 4.0","1"
"57413410","1","57413439","","2019-08-08 13:08:25","","0","65","<p>I have a pandas dataframe like:</p>

<pre><code>   _Ab  _Bc   C    D   _Ef  _Fg
0   a    4    7    1    5    a
1   b    5    8    3    3    a
2   c    4    9    5    6    a
3   d    5    4    7    9    b
4   e    5    2    1    2    b
5   f    4    3    0    4    b
</code></pre>

<p>I want to remove the starting <code>_</code> present in some of the column names. Obviously I can replace them one at a time but that would be very inefficient as I have a lot of columns. So how can I do this efficiently?</p>
","5181947","","9835872","","2019-08-08 13:09:51","2019-08-08 13:20:00","How to rename multiple columns in pandas dataframe in one go?","<python-3.x><pandas><dataframe>","2","1","","","","CC BY-SA 4.0","1"
"57688183","1","","","2019-08-28 08:35:54","","0","65","<p>I want to make a pie chart over Europe and some specific countries, i need to groupe and sum some countries or companies in a group call ""Others"", for example: all the companies that have the budget less than 10000 euros.</p>

<pre><code>import pandas as     pd
from   pandas import Series, DataFrame
import numpy  as     np
import matplotlib.pyplot as plt

    Year    Project Entity  Participation   Country Budget
0   2015    671650 - MMMAGIC - 5G   FUNDACION IMDEA NETWORK*    Participant Spain   € 304,000
1   2015    671650 - MMMAGIC - 5G   ROHDE &amp; SCHWARZ GMBH*   Participant Germany € 120,000
2   2015    671650 - MMMAGIC - 5G   SAMSUNG ELECTRONICS (UK) LIMITED    Coordinator UnitedKingdom   € 797,500
3   2015    671650 - MMMAGIC - 5G   HUAWEI TECHNOLOGIES DUESSELDORF GMBH    Participant Germany € 648,000
4   2015    671650 - MMMAGIC - 5G   TELEFONICA INVESTIGACION Y DESARROLLO SA*   Participant Spain   € 531,976
5   2015    671650 - MMMAGIC - 5G   CHALMERS TEKNISKA HOGSKOLA AB*  Participant Sweden  € 233,000

datos1 = (df[(df['Project'].str.contains('5G-MOBIX')) &amp; (df.Country.str.count('Spain'))])
datos2 = 'La cantidad de proyectos es ='
datos4= 'El presupuesto total en Euros es ='
print(df[(df['Project'].str.contains('5G-MOBIX')) &amp; (df.Country.str.count('Spain'))&amp; (df['Country'].replace(!=''NOKIA SPAIN SA'','Other')]) 
print(datos2, datos1.Country.str.count('Spain').sum())
print(datos4, datos1.Budget.sum())
</code></pre>

<p>I have an error with the command ""replace"".</p>

<pre><code>datos1 = (df[(df['Project'].str.contains('5G-MOBIX')) &amp; (df.Country.str.count('Spain'))])
datos2 = 'La cantidad de proyectos es ='
datos4= 'El presupuesto total en Euros es ='
print(df[(df['Project'].str.contains('5G-MOBIX')) &amp; (df.Country.str.count('Spain'))&amp; (df['Country'].replace(!=''NOKIA SPAIN SA'','Other')]) 
print(datos2, datos1.Country.str.count('Spain').sum())
print(datos4, datos1.Budget.sum())

explode = (0.1, 0, 0, 0)
datos1.groupby(""Entity"")[""Budget""].sum().plot(kind=""pie"", explode=explode, counterclock=False, autopct='%1.1f%%', shadow=True, startangle=90, figsize=(10, 8))
plt.axis('equal')
plt.title(""SPAIN in the project 5G-MOBIX"", bbox={""facecolor"":""0.5"", ""pad"":5}, color='b')
plt.ylabel('')
plt.show()
</code></pre>

<p>i expect this result in the pie chart:</p>

<pre><code>     Year       Project     Entity  Country   Budget                                                                             
742  2018   825496 - 5G-MM  'NOK'   Spain     € 556815.21                                                                       
743  2018   825496 - 5G-MM  'TDE'   Spain     € 747879.88                                                              
778  2018   825496 - 5G-MM  'ALS'   Spain     € 152868.86                                                                        
780  2018   825496 - 5G-MM  'OTHER' Spain     € 300000 
</code></pre>
","11987873","","11987873","","2019-12-02 08:15:17","2019-12-02 08:15:17","How to group and sum some results in others ( Style format in Euros)","<python><python-3.x><pandas><matplotlib><pandas-groupby>","0","3","","","","CC BY-SA 4.0","1"
"48359692","1","","","2018-01-20 18:29:19","","0","64","<p>I m trying to modify the same dataframe that I pass to a function but the changes are not visible</p>

<p>when I apply changes to the dataframe outside the function the expected results is achieved</p>

<pre><code>test_df = test_df.apply(lambda x: x.str.strip(""\t"") if x.dtype == ""object"" else x)
</code></pre>

<p>printing out unique values - </p>

<pre><code>print (""{0}--&gt;{1}"".format(val,pd.unique(test_df[val])))
</code></pre>

<p>O/P-</p>

<pre><code>htn--&gt;['yes' 'no']   
dm--&gt;['yes' 'no' '  yes']  
cad--&gt;['no' 'yes']
appet--&gt;['good' 'poor']  
pe--&gt;['no' 'yes']  
ane--&gt;['no' 'yes']  
classification--&gt;['ckd' 'notckd']
</code></pre>

<p>However if I pass the Dataframe to a function and apply the above same functions the changes are not observed</p>

<pre><code>def FillMissing(dataFrame):
    dataFrame = dataFrame.apply(lambda x: x.str.strip(""\t"") if x.dtype == ""object"" else x)


FillMissing(test_df)
</code></pre>

<p>O/P-</p>

<pre><code>htn--&gt;['yes' 'no']  
dm--&gt;['yes' 'no' ' yes' '\tno' '\tyes']  
cad--&gt;['no' 'yes' '\tno']  
appet--&gt;['good' 'poor']  
pe--&gt;['no' 'yes']  
ane--&gt;['no' 'yes']  
classification--&gt;['ckd' 'ckd\t' 'notckd']
</code></pre>

<p>How can I modify the same existing dataframe without declaring it a global variable.</p>

<p>Also I have tried inplace flag with the lambda function , it does not work</p>
","9108912","","9108912","","2018-01-20 18:31:39","2018-01-20 18:45:45","How to Modify the existing dataframe passed to a function","<python><python-3.x><pandas>","1","8","","","","CC BY-SA 3.0","1"
"56668526","1","56668700","","2019-06-19 13:13:17","","-1","64","<p>i have a pandas dataframe with 2 column.1st column has datatype timestamp and 2nd column consist of values and datatype is  int.</p>

<pre><code>df1 frame
1st column            2nd column
2019-06-15 00:00:00     520
2019-06-15 02:00:00     263
2019-06-15 04:00:00     756
2019-06-16 14:00:00     264
2019-06-16 17:00:00     1254
</code></pre>

<p>i want output like this</p>

<pre><code>1st column            2nd column 
2019-06-15 00:00:00     756
2019-06-15 01:00:00     0
2019-06-15 02:00:00     263
2019-06-15 03:00:00     0
2019-06-15 04:00:00     756
         ...
2019-06-16 00:00:00     0
2019-06-16 01:00:00     0
         ...
2019-06-16 14:00:00      264
2019-06-16 15:00:00     0
2019-06-16 16:00:00     0
2019-06-16 01:00:00     1254
         ...
2016-06-16 23:00:00      0
2019-06-15 01:00:00      0
</code></pre>

<p>that is i want to fill the missing hours values to zero. </p>
","11211647","","","","","2019-06-19 13:22:31","i have a pandas dataframe and i want to fill missing value","<python><python-3.x><pandas><dataframe>","1","1","","2019-06-19 13:22:11","","CC BY-SA 4.0","1"
"56955091","1","56955187","","2019-07-09 14:41:53","","0","64","<p>I have a data-frame like:</p>

<pre><code>          a                     b                  
 [[35.6113, -95.855]]    [[[36.028, -95.93], [36.10, -95.82] .... ]]]
</code></pre>

<p>How can i convert it into like this:</p>

<pre><code>      a                     b                  
 [35.6113, -95.855]  [36.028, -95.93]
 [35.6113, -95.855]  [36.10,  -95.82]
  ....                      ....
  ....                      ....
</code></pre>

<p>I tried with <code>df['b'].apply(pd.Series)</code> but it's throwing error as it must be 1 d. Is there a simple process to do it?</p>

<p>Edit:   </p>

<pre><code>df['b'][0] 

array([[[ 36.0285042 , -95.938618  ],
       [ 36.108158  , -95.8241013 ],
       [ 36.1544989 , -95.9931978 ],
       [ 36.515477  , -96.139459  ],
       [ 36.1330019 , -95.8372588 ],
       [ 36.0405703 , -96.0028027 ],
       [ 35.850413  , -96.072558  ],
       [ 36.0814011 , -95.7677365 ],
       [ 36.40235964, -95.97271572],
       [ 36.0299891 , -95.7085335 ],
       [ 36.328715  , -95.488263  ],
       [ 36.781739  , -95.99514   ],
       [ 36.05102591, -95.72716157],
       [ 35.866565  , -95.523982  ],
       [ 35.8321172 , -96.049276  ],
       [ 35.9691288 , -95.8382827 ],
       [ 36.0872146 , -96.1264932 ],
       [ 35.9682564 , -96.1106048 ],
       [ 36.1375913 , -95.8341317 ],
       [ 35.7914036 , -95.8728959 ]]])
</code></pre>
","6759267","","6759267","","2019-07-09 14:57:09","2019-07-09 15:06:41","How to convert 2d into row","<python><python-3.x><pandas><python-2.7>","2","2","","","","CC BY-SA 4.0","1"
"41244748","1","41245249","","2016-12-20 14:34:37","","1","64","<p>I have the following pandas dataframe:</p>

<p>In: </p>

<pre><code>df
</code></pre>

<p>out:</p>

<pre><code>         A    B        C                                             D
0  0938320  usa   amazon              orange: $ 8.00| pineapple: $2.00
1  0938320  usa  alibaba                  orange: $ 8.00| apple: $2.00
2  0938320  usa     ebay  mint: $ 8.00| watermelon: $2.00| mint: $2.00
...
n  0938320  usa   amazon                  pear: $ 8.00| bannana: $2.00
</code></pre>

<p>I would like to split by <code>|</code> and stack it into (*):</p>

<pre><code>         A    B        C                  D
0  0938320  usa   amazon     orange: $ 8.00
1  0938320  usa   amazon   pineapple: $2.00
2  0938320  usa  alibaba     orange: $ 8.00
3  0938320  usa  alibaba       apple: $2.00
4  0938320  usa      bay       mint: $ 8.00
5  0938320  usa     ebay  watermelon: $2.00
6  0938320  usa     ebay        mint: $2.00
7  0938320  usa   amazon       pear: $ 8.00
...
8  0938320  usa   amazon     bannana: $2.00
</code></pre>

<p>So, I tried the following:</p>

<p>In:</p>

<pre><code>s = df2.D.str.split(""|"").apply(pd.Series, 1).stack()
s.index = s.index.droplevel(-1)
del df2['D']
df.join(s)
</code></pre>

<p>out:</p>

<pre><code>ValueError: Other Series must have a name
</code></pre>

<p>And:</p>

<pre><code>b = pd.DataFrame(df2.D.str.split('|').tolist(), index=df2['A','B','C']).stack()
b = b.reset_index()[[0, 'D']] 
b.columns = ['A','B','C']
b
</code></pre>

<p>However, is not working. How can I modify the last approach in order to get (*)?. I guess that my main problem is that I do not know how to take all the columns in <code>index=df2['A','B','C']).stack()</code>.</p>
","4114372","","6336881","","2016-12-20 14:51:32","2016-12-20 17:02:27","Problems while spliting pandas dataframe row?","<python><python-3.x><pandas><dataframe>","2","0","","","","CC BY-SA 3.0","1"
"33709983","1","33710079","","2015-11-14 15:28:26","","1","64","<p>I am trying to create a data frame in Python with Pandas that involves nested dictionaries and lists of lists. I looked through other questions about converting nested dictionaries, but I couldn't find a sufficient answer.</p>

<p>I have a dictionary, which for example, is an activity book that keeps track of extracurricular school lessons. In this case, there are two lessons, and each of the lessons is its own dictionary nested under the activity book dictionary. Each lesson dictionary contains a list of lists of the activities by each person, organized by month. The amount of students performing an activity each month is variable, but the structure is always Student-Activity-Minutes. For example:</p>

<pre><code>activity_dict = {

'lesson1' : {  'january' : [['Todd', 'Running', 30],['Christy', 'Studying', 25],['Alex','Soccer', 10]],
               'february' : [['Jim', 'Bobsledding', 5],['Frank', 'Jogging',8]]},

'lesson2' : {'february' : [['Todd', 'Running', 18],['John', 'Studying', 3],['Don','Soccer', 40]],
              'march' : [['Tom', 'Bobsledding', 10],['Sam', 'Yoga', 42]],
              'april' : [['Julie', 'Biking', 20],['Chris', 'Baseball', 10]]}
}  
</code></pre>

<p>I am trying to get an output that for each student's activity, ColA = Lesson #, ColB = Month, ColC = Student, ColD = Activity, and ColE = Minutes. Sample output would be:</p>

<pre><code>Lesson # Month Student Activity Minutes
Lesson 1 February Jim Bobsledding 5
Lesson 1 February Frank Jogging 8
Lesson 2 February Todd Running 18
</code></pre>

<p>I have found a way to create a dataframe of Columns C through E, but I am unable to include Columns A and B.</p>

<p>My code right now is the following:</p>

<pre><code>import pandas

activity_log = []

for lesson, all_activities in activity_dict.items():
    for month, month_activities in all_activities.items():
        activity_log.append(pandas.DataFrame(month_activities))
</code></pre>

<p>How can I update this to include the dictionary keys (lesson and month) as Columns A and B? I'm not sure if changing the list of lists to a dictionary would help, but I have kept it as a list since that is how I received the data.</p>
","5405592","","4805990","","2015-11-14 16:32:20","2015-11-14 16:32:20","Converting list nested under two dictionaries to DataFrame","<python><python-3.x><dictionary><pandas>","1","0","","","","CC BY-SA 3.0","1"
"57023508","1","","","2019-07-13 23:27:06","","0","64","<p>I have a dataframe of coefficients for countries, where each coefficient looks like: </p>

<p><code>s = ""C(Country)[T.China]""</code></p>

<p><code>s2 = ""C(Country)[T.Italy]""</code></p>

<p><code>s3 = ""C(Country)[T.United States]""</code></p>

<p>How would I go about extracting just the country name (i.e: ""China"" or ""Italy""?)</p>

<p>And can this be done with a ""strip"" command instead of regex?</p>
","","user5813071","","user5813071","2019-07-14 00:10:59","2019-07-14 04:26:59","Expression to extract country name?","<regex><python-3.x><pandas><dataframe>","2","2","","","","CC BY-SA 4.0","1"
"57510591","1","57521710","","2019-08-15 13:42:51","","0","64","<p>(sorry about the title I realise it isn't very descriptive)</p>

<p>Given a data set such of the following:</p>

<pre><code>       word  entity
0   Charlie      1
1        p.      1
2    Nelson      1
3     loves   None
4      Dana      2
5        c.      2
6  anderson      2
7       and   None
8     james      3
</code></pre>

<p>I want to apply a function (e.g. get_gender()) to first element of each entity (I would imagine I groupby of some sort)</p>

<p>as to get something like this:</p>

<pre><code>       word entity gender
0   Charlie      1      m
1        p.      1   None
2    Nelson      1   None
3     loves   None   None
4      Dana      2      f
5        c.      2   None
6  anderson      2   None
7       and   None   None
8     james      3      m
</code></pre>

<p>and lastly populate the missing rows of each entity to get</p>

<pre><code>       word entity gender
0   Charlie      1      m
1        p.      1      m
2    Nelson      1      m
3     loves   None   None
4      Dana      2      f
5        c.      2      f
6  anderson      2      f
7       and   None   None
8     james      3      m
</code></pre>

<p>Here is some code for generating the above data frame</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd
df  = pd.DataFrame([(""Charlie"", ""p."", ""Nelson"", ""loves"", ""Dana"", ""c."", ""anderson"", ""and"", ""james""), (1,1,1, None, 2,2,2, None, 3)]).transpose()
df.columns = [""word"", ""entity""]
</code></pre>

<p>The current 'solution' I am using is:</p>

<pre><code>import gender_guesser.detector as gender
d = gender.Detector() 
# Detect gender in of the names in word. However this one if applied to all of the entity (including last names, furthermore one entity can be multiple genders (depending on e.g. their middle name)
df['gender'].loc[(df['entity'].isnull() == False)] = df['word'].loc[(df['entity'].isnull() == False)].apply(lambda string: d.get_gender(string.lower().capitalize()))
</code></pre>
","11723688","","11723688","","2019-08-16 07:41:31","2019-08-16 09:19:07","Apply function to first element in a group by and then remerging","<python><python-3.x><pandas>","1","4","","","","CC BY-SA 4.0","1"
"56910537","1","56910663","","2019-07-06 01:11:23","","0","64","<p><strong>Background</strong></p>

<p>I have the following sample <code>df</code>:</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'Before' : [['there, are, many, different'], 
                               ['i, like, a, lot, of, sports '], 
                               ['the, middle, east, has, many']], 
                   'After' : [['in, the, bright, blue, box'], 
                               ['because, they, go, really, fast'], 
                               ['to, ride, and, have, fun'] ],

                  'P_ID': [1,2,3], 
                  'Word' : ['crayons', 'cars', 'camels'],
                  'N_ID' : ['A1', 'A2', 'A3']

                 })
</code></pre>

<p><strong>Output</strong></p>

<pre><code>      After                          Before                       N_ID  P_ID  Word
0 [in, the, bright, blue, box]    [there, are, many, different]     A1  1   crayons
1 [because, they, go, really,fast] [i, like, a, lot, of, sports ]   A2  2   cars
2 [to, ride, and, have, fun]        [the, middle, east, has, many]  A3  3   camels
</code></pre>

<p><strong>Desired Output</strong></p>

<pre><code>      After                          Before               N_ID  P_ID  Word
0 in the bright blue box        there are many different  A1    1   crayons
1 because they go really fast   i like a lot of sports    A2    2   cars
2 to ride and have fun         the middle east has many   A3    3   camels
</code></pre>

<p><strong>Question</strong></p>

<p>How do I get my desired output which is <strong>1)</strong> unlisted and <strong>2)</strong> has the commas removed?</p>

<p>I tried <a href=""https://stackoverflow.com/questions/45086985/removing-lists-from-each-cell-in-pandas-dataframe"">Removing lists from each cell in pandas dataframe</a> to no avail  </p>
","6598999","","","","","2019-07-06 01:51:11","Removing commas and unlisting a dataframe","<python-3.x><pandas><list><dataframe><nlp>","1","11","","","","CC BY-SA 4.0","1"
"56626982","1","56627556","","2019-06-17 07:54:21","","0","63","<p>I've got two dataframes. First one is empty but with columns defined:</p>

<pre><code>Empty DataFrame
Columns: [ID, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157]
Index: []
</code></pre>

<p>Second dataframe is:</p>

<pre><code>    3123    3124    3125    3126    3127
0   A       B       C       D       
</code></pre>

<p>Later, I will have another dataframe that will be:</p>

<pre><code>    3146    3147    3148    3149    3150
0   X       Y       Z           
</code></pre>

<p>And so on. What I want is to put all this little dataframes in the first one to get something like:</p>

<pre><code>ID  3120    3121    3122    3123    3124    3125    3126    3127    3128    3129    3130    3131    3146    3147    3148    3149    3150    3151    3152    3153    3154    3155    3156    3157
1                           A       B       C       D                                               X       Y       Z
</code></pre>

<p>So what I am doing in my loop is:</p>

<pre><code>df_main.merge(df_i, how='inner', on=df_i.columns)
</code></pre>

<p>Where, when i=1:</p>

<pre><code>df_main.columns:

Index(['ID', '3120', '3121', '3122', '3123', '3124', '3125', '3126',
       '3127', '3128', '3129', '3130', '3131', '3146', '3147', '3148', '3149',
       '3150', '3151', '3152', '3153', '3154', '3155', '3156', '3157'],
      dtype='object')


df_i.columns:

Index(['3123', '3124', '3125', '3126', '3127'], dtype='object')
</code></pre>

<p>And code is raising this KeyError:</p>

<pre><code>    raise KeyError(key)
KeyError: Index(['3123', '3124', '3125', '3126', '3127'], dtype='object')
</code></pre>

<p>How is this possible? <code>df_i.columns</code> is contained and exist in <code>df_main.columns</code></p>

<p>Thank you in advance!</p>
","9369118","","","","","2019-06-17 10:18:22","Pandas giving KeyError when merging on df2.columns","<python><python-3.x><pandas><dataframe><merge>","1","3","","","","CC BY-SA 4.0","1"
"49282312","1","","","2018-03-14 15:54:12","","0","63","<p>I have a problem using numpy. Basically I am converting data from pandas data-frame to numpy.
data simple :</p>

<pre><code>[1503532800000, 4147.0, 4371.68, 4085.01, 4316.01, 787.418753]]
</code></pre>

<p>to convert to numpy I use the code :</p>

<pre><code>Df= df.values
</code></pre>

<p>the out put is :</p>

<pre><code>[1.52098560e+12 9.15192000e+03 9.33378000e+03 8.57000000e+03
 8.69998000e+03 2.39326870e+04]
</code></pre>

<p>I dont understand what is the problem I using python3.5</p>
","5011551","","","","","2018-03-14 16:06:53","numpy printing numeric values in e+12","<python-3.x><pandas><numpy>","1","1","","","","CC BY-SA 3.0","1"
"41935637","1","","","2017-01-30 12:11:45","","0","63","<p>I have a csv I've imported as a pandas dataframe which looks like this:</p>

<pre><code>TripId,  DeviceId, StartDate,                EndDate
817d0e7, dbf69e23, 2015-04-18T13:54:27.000Z, 2015-04-18T14:59:06.000Z
817d0f5, fkri449g, 2015-04-18T13:59:21.000Z, 2015-04-18T14:50:56.000Z
8145g5g, dbf69e23, 2015-04-18T15:12:26.000Z, 2015-04-18T16:21:04.000Z
4jhbfu4, fkigit95, 2015-04-18T14:23:40.000Z, 2015-04-18T14:59:38.000Z
8145g66, dbf69e23, 2015-04-20T11:20:24.000Z, 2015-04-20T16:22:41.000Z
...
</code></pre>

<p>I want to add a new column, with an indicator value based on whether the DeviceId reappears in my dataframe, with a StartDate 1hour after the current EndDate.
So my new dataframe should look like:</p>

<pre><code>TripId,  DeviceId, StartDate,                EndDate,                  newcol
817d0e7, dbf69e23, 2015-04-18T13:54:27.000Z, 2015-04-18T14:59:06.000Z, 1
817d0f5, fkri449g, 2015-04-18T13:59:21.000Z, 2015-04-18T14:50:56.000Z, 0
8145g5g, dbf69e23, 2015-04-18T15:12:26.000Z, 2015-04-18T16:21:04.000Z, 0
4jhbfu4, fkigit95, 2015-04-18T14:23:40.000Z, 2015-04-18T14:59:38.000Z, 0
8145g66, dbf69e23, 2015-04-20T11:20:24.000Z, 2015-04-20T16:22:41.000Z, 0
...
</code></pre>

<p>I've started to write some code, but I'm unsure how to proceed.</p>

<pre><code>df['newcol'] = np.where(df['DeviceId'].isin(df['DeviceId']) and , 1, 0) 
</code></pre>

<p>One problem is that I'm not sure how to find device id in dataframe excluding current row, and another is that I don't know how to tackle the time issue. </p>

<p>EDIT: I've been working on it a bit, and my new code is now:</p>

<pre><code>df['UniqueId'] = range(0, 14571, 1)

df['StartDate'] = pd.to_datetime(df['StartDate'])
df['EndDate'] = pd.to_datetime(df['EndDate'])

df2 = df.loc[df.duplicated(subset=['DeviceId'],keep=False)] 
#Returns list of trips with repeated deviceid
DeviceIds = df2['DeviceId'].tolist()
DeviceIds = list(set(DeviceIds))
for ID in DeviceIds:
    temp = df2.loc[df2['DeviceId'] == ID]
    temp.sort_values(by='StartDate')
    temp['PreviousEnd'] = temp['EndDate'].shift(periods=1)
    temp['Difference'] = temp['StartDate'] - temp['PreviousEnd']
    temp['Difference'] = [1 if x &lt; pd.Timedelta('1H')
                      else 0 for x in temp['Difference']]
    temp = temp[['UniqueId','Difference']]
    df.join(temp, on='UniqueId', how='left',rsuffix='2')
</code></pre>

<p>The it creates the right temp dataframe, but I can't seem to join the values in Difference to the original dataframe</p>
","6637269","","3930542","","2017-01-30 19:28:30","2017-01-31 09:57:45","Create new column in pandas dataframe based on whether a value in the row reappears in dataframe","<python><python-3.x><pandas>","2","2","","","","CC BY-SA 3.0","1"
"49137671","1","49137887","","2018-03-06 18:29:31","","3","63","<p>Is there a way to use the ternary operator inside a groupby condition?Apparently this syntax is invalid.</p>

<pre><code>d = {'name':['bil','bil','bil','jim'],
     'col2': ['acct','law', 'acct2','law'],
     'col3': [1,2,3,55],
     'col4': [1,1,1,2]

    }
df2 = pd.DataFrame(data=d)

df2[['col4']] = df2[['col4']].apply(pd.to_numeric)
df2.groupby(['name','col2'])['col4']\
    .max() if (.max()&gt;30) else ''
</code></pre>
","1634753","","1126841","","2018-03-06 18:36:26","2018-03-06 18:46:56","Python using ternary operator in groupby","<python><python-3.x><pandas>","2","2","","","","CC BY-SA 3.0","1"
"57507531","1","57507722","","2019-08-15 09:22:53","","2","63","<p>Here I have a dataset with time and three inputs. Here I calculate the time difference using panda. </p>

<p>code is :</p>

<pre><code>data['Time_different'] = pd.to_timedelta(data['time'].astype(str)).diff(-1).dt.total_seconds().div(60)
</code></pre>

<p>This is reading the difference of time in each row. But I want to write a code for find the time difference only specific rows which are having X3 values. </p>

<p>I tried to write the code using for loop. But it's not working properly. Without using for loop can we  write the code.?</p>

<p><a href=""https://i.stack.imgur.com/AVzMH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AVzMH.png"" alt=""enter image description here""></a></p>

<p>As you can see in my image I have three inputs, X1,X2,X3. Here when I used that code it is showing the time difference of X1,X2,X3. </p>

<p>Here what I want to write is getting the time difference for X3 inputs which are having a values.</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>time      X3

6:00:00		0
7:00:00		2
8:00:00		0
9:00:00		50
10:00:00	0
11:00:00	0
12:00:00	0
13:45:00	0
15:00:00	0
16:00:00	0
17:00:00	0
18:00:00	0
19:00:00	20</code></pre>
</div>
</div>
</p>

<p>Then here I want to skip the time of having 0 values of X3 and want to read only time difference of values of X3.</p>

<pre><code>time             x3

7:00:00          2(values having)
9:00:00          50
</code></pre>

<p>So the time difference is <code>2hrs</code></p>

<p>Then second:</p>

<pre><code>9:00:00          50
19:00:00         20
</code></pre>

<p>Then time difference is <code>10 hrs</code></p>

<p>Like wise I want write the code or my whole column. Can anyone help me to solve this?</p>

<p>While putting the code then get the error with time difference in minus value.</p>

<p><a href=""https://i.stack.imgur.com/gGrWM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gGrWM.png"" alt=""enter image description here""></a></p>
","11781947","","11781947","","2019-08-15 10:56:55","2019-08-15 10:56:55","How to get time difference in specifc rows include in one column data using python","<python><python-3.x><pandas><time>","1","0","","","","CC BY-SA 4.0","1"
"57461422","1","57461452","","2019-08-12 12:40:39","","2","63","<p>I have Dataframe ""Report"" with Date column ""Next release date"" and I want to calculate no. of days between today and the 'Next release date'</p>

<pre><code>&gt;&gt;&gt; Report['Next release date']
1    2020-02-11
2    2019-08-12
3    2019-08-13
</code></pre>

<p>Column Type of 'Next release date' is 'str'</p>

<pre><code>Report['Next release date'] = pd.to_datetime(Report['Next release date'], format='%Y-%m-%d')
Report['Date_Diff'] = Report['Next release date'] - datetime.date.today().strftime('%Y-%m-%d')
</code></pre>

<p>Above script gives zero for date '2019-08-13' - today(), actual difference is 1 day, but it gives zero
and for ""2019-08-12"" gives '-1' result instead of zero</p>

<p>Please help on this.</p>
","2717063","","","","","2019-08-12 12:42:44","Day difference, Python 3.6","<python><python-3.x><pandas><dataframe><datetime>","1","0","","","","CC BY-SA 4.0","1"
"57669326","1","","","2019-08-27 07:06:36","","0","62","<p>I had a question earlier which is deleted and now modified to a less verbose form for you to read easily.</p>

<p>I have a dataframe as given below</p>

<pre><code>df = pd.DataFrame({'subject_id' :[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2],'day':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20] , 'PEEP' :[7,5,10,10,11,11,14,14,17,17,21,21,23,23,25,25,22,20,26,26,5,7,8,8,9,9,13,13,15,15,12,12,15,15,19,19,19,22,22,15]})
df['fake_flag'] = ''
</code></pre>

<p>I would like to fill values in column <code>fake_flag</code> based on the below rules</p>

<p>1) if preceding two rows are constant (ex:5,5) or decreasing (7,5), then pick the highest of the two rows. In this case, it is 7 from (7,5) and 5 from (5,5)</p>

<p>2) Check whether the current row is greater than the output from rule 1 by 3 or more points (>=3) and it repeats in another (next) row (2 occurrences of same value). It can be 8/gt 8(if rule 1 output is 5). ex: (8 in row <code>n</code>,8 in row <code>n+1</code> or 10 in row <code>n</code>,10 in row <code>n+1</code>) If yes, then key in <code>fake VAC</code> in the <code>fake_flag column</code></p>

<p>This is what I tried</p>

<pre><code>for i in t1.index:
if i &gt;=2:
    print(""current value is  "", t1[i])
    print(""preceding 1st (n-1) "", t1[i-1])
    print(""preceding 2nd (n-2) "", t1[i-2])
    if (t1[i-1] == t1[i-2] or t1[i-2] &gt;= t1[i-1]): # rule 1 check
        r1_output = t1[i-2] # we get the max of these two values (t1[i-2]), it doesn't matter when it's constant(t1[i-2] or t1[i-1]) will have the same value anyway
        print(""rule 1 output is "", r1_output)
        if t1[i] &gt;= r1_output + 3:
            print(""found a value for rule 2"", t1[i])
            print(""check for next value is same as current value"", t1[i+1])
            if (t1[i]==t1[i+1]): # rule 2 check
                print(""fake flag is being set"")
                df['fake_flag'][i] = 'fake_vac'
</code></pre>

<p>This check should happen for all records (one by one) for each subject_id. I have a dataset which has million records. Any efficient and elegant solution is helpful. I can't run a loop over million records.</p>

<p>I expect my output to be like as shown below</p>

<p><strong>subject_id = 1</strong></p>

<p><a href=""https://i.stack.imgur.com/tkSJu.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/tkSJu.png"" alt=""enter image description here""></a></p>

<p><strong>subject_id = 2</strong></p>

<p><a href=""https://i.stack.imgur.com/vHYhK.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vHYhK.png"" alt=""enter image description here""></a></p>
","10829044","","10829044","","2019-08-27 14:00:34","2019-08-27 14:00:34","Compare preceding two rows with subsequent two rows of each group till last record","<python><python-3.x><pandas><dataframe><pandas-groupby>","1","4","","","","CC BY-SA 4.0","1"
"49460878","1","","","2018-03-24 03:20:55","","0","62","<p>I'm getting an assertion error saying 20 columns passed, but passed data has 50 columns. I kind of know what is causing this error, but it is late and I'm not exactly sure how to fix it - the issue is that there are truly 20 column headers, but the 50 figure comes from the number of rows. I'm thinking it might have something to do with the loop as well, but any help would be appreciated as I imagine this is simple but I'm not quite sure how to fix it.</p>

<pre><code>from bs4 import BeautifulSoup
import requests
import pandas as pd
import time

playerData = []

for i in range(6):
    initialURL = 'https://www.fangraphs.com/leaders.aspx?pos=all&amp;stats=sta&amp;lg=all&amp;qual=0&amp;type=8&amp;season=2017&amp;month=0&amp;season1=2017&amp;ind=0&amp;team=0&amp;rost=0&amp;age=0&amp;filter=&amp;players=0&amp;sort=7,d&amp;page=' + str(i) +'_50'
    r = requests.get(initialURL)
    soup = BeautifulSoup(r.text, 'html.parser')
    statistics = soup.find(""table"", {""class"" : ""rgMasterTable""})
    statistics.findAll('th')
    column_headers = [th.getText() for th in soup.findAll('th')]
    data = statistics.findAll('tr')[3:]
    pitcherStatistics = [[td.text.strip() for td in data[a].findAll('td')]
                          for a in range(len(data))]
    playerData.append(pitcherStatistics)


print(playerData)

df = pd.DataFrame(playerData, columns=column_headers)
df.to_csv(""Starting Pitchers.csv"", index=False)
</code></pre>
","8121824","","6573902","","2020-02-06 13:11:52","2020-02-06 13:11:52","Assertion Error related to number of columns","<python><python-3.x><pandas><for-loop><web-scraping>","1","1","","","","CC BY-SA 3.0","1"
"50419159","1","","","2018-05-18 20:56:49","","0","62","<p>Using this code:</p>

<pre><code>import pandas

df = pandas.DataFrame()

df['id']     = list('aaabbbcccdddeee')
df['gender'] = list('mmfmfmmffmfmfff') 
</code></pre>

<p>I get a DataFrame like:</p>

<pre><code>   id gender
0   a      m
1   a      m
2   a      f
3   b      m
4   b      f
5   b      m
6   c      m
7   c      f
8   c      f
9   d      m
10  d      f
11  d      m
12  e      f
13  e      f
14  e      f
</code></pre>

<p>How can I split <code>df</code> by <code>id</code> and then count the number of <code>m</code> and <code>f</code> in each category so that I can get a result that looks something like:</p>

<pre><code>    m    f
a   2    1
b   2    1
c   1    2
d   2    1
e   0    3
</code></pre>
","1367204","","9209546","","2018-05-18 22:31:30","2018-05-18 22:31:30","Pandas how groupby and then count the number of unique items in each column?","<python><python-3.x><pandas><numpy>","1","4","","2018-05-18 21:01:08","","CC BY-SA 4.0","1"
"57465412","1","57465564","","2019-08-12 17:06:44","","0","62","<p>I have a pandas column where I would like to remove the last character if character equals to 'F'.</p>
","9585960","","","","","2019-08-12 17:22:43","How to remove character in pandas column from specific index if character exists","<python><python-3.x><pandas>","2","1","","","","CC BY-SA 4.0","1"
"57503071","1","57503485","","2019-08-14 23:04:06","","0","62","<p>To help categorize a large dataset by months, I am trying to convert all actual dates of an entry into the end of the month.</p>

<p>I saw similar questions to this and used the code I found, but it does not seem to work if the date happened to already be the end of the month (that would result in the next month's end being calculated instead).</p>

<pre><code>df['CalcEnd'] = pd.to_datetime(df['ActualDate'], format=""%m/%d/%Y"") + MonthEnd(1)
</code></pre>

<p>That leads to:</p>

<pre><code>    ActualDate    CalcEnd
    7/1/2019      7/31/2019
    7/2/2019      7/31/2019
    7/31/2019     8/31/2019
</code></pre>

<p>The third entry should be returning 7/31/2019.</p>

<p>I tried to use numpy to only use the CalcEnd if the date is not already the end of the month to avoid this issue, but for some reason CalcEnd resulted in a weird series of numbers if it wasn't already the end of month.</p>

<p>Specifically I tried:</p>

<pre><code>def isMonthEnd(date):
    return date + pd.offsets.MonthEnd(0) == date

df['EndCheck'] = isMonthEnd(pd.to_datetime(df['ActualDate'], format=""%m/%d/%Y""))
df['CalcEnd'] = pd.to_datetime(df['ActualDate'], format=""%m/%d/%Y"") + MonthEnd(1)
df['End'] = np.where(df['EndCheck']==False, df['CalcEnd'], df['ActualDate'])
</code></pre>

<p>When EndCheck is False, instead of showing 7/31/2019, it shows 1564531200000000000. </p>

<p>But when it is True, it correctly shows ActualDate as 7/31/2019.</p>

<p>Any advice on:</p>

<p>1) How to convert all given dates in a dataframe to the end of the month, even when the given date is already the end of the month; and</p>

<p>2) Why the np.where statement is not working when attempting to use the calculated column</p>

<p>.... would be greatly appreciated!</p>

<p>I was able to find a workaround by just writing the dataframe to a csv, and reading that new csv back into the dataframe before creating End; it seems to resolve the issue with the np.where statement returning 1564531200000000000. However, I'm hoping there is a more elegant solution.</p>

<p>Thank you!</p>
","6799378","","6799378","","2019-08-14 23:20:14","2019-08-15 00:13:37","Finding end of a month in a column of pandas dataframe","<python-3.x><pandas><numpy>","2","0","","","","CC BY-SA 4.0","1"
"49649706","1","49649849","","2018-04-04 11:22:36","","0","62","<p>I have a data set which contains keywords,rank_organic and document column. For every keyword there are documents for rank_organic = 1,2,3,4 or 5. But for some keywords I have some rank_organic field missing.</p>

<p>For example: For keyword A, I have rank_organic = 1,2,4,5 and 3 is missing. I want to create a list of documents of length 5 where for rank_organic=3, null or space should come and for rest rank the documents should come.
Below is the code which I am using but it giving error. Please help me how to achieve it.</p>

<pre><code>def key_doc(data):
    lis=[]
    for i in pd.unique(data['keyword']):
        a = data.loc[data['keyword'].isin([i])]
        j = i.replace("" "",""_"")

        j =  Node(i, parent= Testing,
                   documents=[(a.loc[(a['rank_organic']==1)])['vocab'].tolist()[0]
                            ,(a.loc[(a['rank_organic']==2)])['vocab'].tolist()[0]
                            ,(a.loc[(a['rank_organic']==3)])['vocab'].tolist()[0]
                            ,(a.loc[(a['rank_organic']==4)])['vocab'].tolist()[0]
                            ,(a.loc[(a['rank_organic']==5)])['vocab'].tolist()[0]])

#        print j.name, len(j.documents)
        lis.append(j)
    return lis
</code></pre>

<p>ERROR:</p>

<pre><code>,(a.loc[(a['rank_organic']==3)])['vocab'].tolist()[0]

IndexError: list index out of range
</code></pre>
","9460458","","9209546","","2018-04-04 11:38:58","2018-04-04 11:44:02","Creating a list of documents using if-else in python","<python><python-3.x><pandas><for-loop><if-statement>","1","0","","","","CC BY-SA 3.0","1"
"56819559","1","","","2019-06-29 17:36:41","","0","62","<p>I have a very big data contains nnumerical values mostly. I want to filter multiple columns that each is between different range. The problem is columns and range will be selected by user which means that filtered columns and ranges can be changed each time.</p>

<p>e.g <code>0&lt;df[a]&lt;5 &amp; 0&lt;df[b]&lt;10</code>. It can be ""a"" and ""b"" and ""c"" also, totaly depend on input. </p>

<p>I want to see how many rows in a range such that for example; for each column; col.a is between ""0"" and ""1"", ""1"" and ""2"" etc. until 5 and same for col.b or any other until e.g ""10""</p>

<p>Because of my code is very long , tried to explain the attached some part inside strings:</p>

<pre><code># -*- coding: utf-8 -*-
""""""
excel_file: readed excel file dataframe
entered_parameters: (list) to be filtered columns typed by user
parameters: readed columns of excel_file
limits: (list) upper_limits inputted by user for each entered_parameters
ranges: range or incrementation list for each entered parameters
boolean_frame: Boolean dataframe returned for filtering each entered_parameters(columns) upto limits in each cycle
total_boolean_frame:appended boolean_frame(shows ranges up to limits for each parameter)
total_frame: concat of total_boolean_frame (shows all filtered boolean values by range for all param)

""""""


total_frame=pd.DataFrame()
parameters=[i for i in excel_file.columns if type(i)==str]

totalrownumberlist=[]
for i,v in enumerate(limits):
    if i==0:
        totalrownumberlist.append(len(excel_file)*v)
    else:
        totalrownumberlist.append(totalrownumberlist[i-1]*v)
totalrownumber=totalrownumberlist[-1]
for i,param in enumerate(entered_parameters):

    total_boolean_frame=pd.DataFrame()
    appended_row_num=totalrownumberlist[i]
    if param in parameters:
        while appended_row_num&lt;=totalrownumber:

            boolean_frame=pd.DataFrame()
            initial=0


            while initial&lt;limits[i]:                          

                boolean_frame[param]=(excel_file[param]&gt;=initial) &amp; (excel_file[param]&lt;=initial+ranges[i])

                boolean_frame[""aralik-%s""%param]=""%s-%s""%(initial,initial+ranges[i])


                initial=initial+ranges[i]

                total_boolean_frame=total_boolean_frame.append(boolean_frame,sort=False,ignore_index=True)

            appended_row_num=appended_row_num+totalrownumberlist[i]
        total_frame=pd.concat([total_frame,total_boolean_frame],axis=1)`
</code></pre>

<p>Edit: Output should be like this; count(range[0-1] col.a and range[0-1] col.b)=2 (avg. through axis=1 if all cells in the row is True which means avg of rows for excel_file[total_frame.all(axis=1)]. count(range[1-2] col.a and range[0-1] col.b)=3 with avg. again, count(range[2-3] col.a and range[0-1] col.b)=6 and avg. and goes on...
Thnks</p>
","11655159","","11655159","","2019-06-29 19:35:38","2019-06-29 19:35:38","Filter Multiple Columns between multiple range","<python-3.x><pandas>","0","2","","","","CC BY-SA 4.0","1"
"56657449","1","","","2019-06-18 21:43:49","","0","62","<p>I want to analyze a database hosted in Mongo. Therefore, I want to connect the mongo URI to pandas, so I can run all my python queries freely in my Jupyter Lab environment.</p>

<p>Let's day that this is my mongo connection: </p>

<p>""mongoURI"": ""mongodb+srv://test:test12345@cluster0-ze0tw.mongodb.net/databasetest?retryWrites=true&amp;w=majority""</p>

<p>I have read other answer, and they shared this code that supposedly might help me out with this connection. I am not quite if I am filling out the fields correctly. Can you guys walk me through using as an example the connection shown above. </p>

<pre><code>import pandas as pd
from pymongo import MongoClient


def _connect_mongo(host, port, username, password, db):
   """""" A util for making a connection to mongo """"""

   if username and password:
       mongo_uri = 'mongodb://%s:%s@%s:%s/%s' % (username, password, host, port, db)
       conn = MongoClient(mongo_uri)
   else:
       conn = MongoClient(host, port)


   return conn[db]


def read_mongo(db, collection, query={}, host='localhost', port=27017, 
username=None, password=None, no_id=True):
   """""" Read from Mongo and Store into DataFrame """"""

   # Connect to MongoDB
   db = _connect_mongo(host=host, port=port, username=username, password=password, 
   db=db)

   # Make a query to the specific DB and Collection
   cursor = db[collection].find(query)

   # Expand the cursor and construct the DataFrame
   df =  pd.DataFrame(list(cursor))

   # Delete the _id
   if no_id:
       del df['_id']

   return df

</code></pre>
","10694017","","10273354","","2019-06-19 01:18:45","2019-06-19 01:18:45","How can I import data from MongoDB to Panda?","<python><python-3.x><mongodb><pandas>","1","1","","","","CC BY-SA 4.0","1"
"40580161","1","","","2016-11-13 23:58:13","","0","62","<p>I'm trying to read in a data set and dropping the first two columns of the data set, but it seems like it is dropping the wrong column of information.  I was looking at <a href=""https://stackoverflow.com/questions/26347412/drop-multiple-columns-pandas"">this</a> thread, but their suggestion is not giving the expected answer.  My data set starts with 6 columns, and I need to remove the first two.  Elsewhere in threads it has the option of dropping columns with labels, but I would prefer not to name columns only to drop them if I can do it in one step.</p>

<pre><code>df= pd.read_excel('Data.xls', header=17,footer=246)
df.drop(df.columns[[0,1]], axis=1, inplace=True)
</code></pre>

<p>But it is dropping columns 4 and 5 instead of the first two.  Is there something with the drop function that I'm just completely missing?  </p>
","4039994","","-1","","2017-05-23 12:25:10","2017-03-06 17:02:01","Issue with dropping columns","<python-3.x><pandas>","1","4","","","","CC BY-SA 3.0","1"
"57025601","1","57033855","","2019-07-14 08:07:07","","1","62","<p>Assume I have a dataset with three inputs:</p>

<pre><code>   x1   x2  x3  
0  a    b   c
1  d    e   f
2  g    h   i
3  j    k   l
4  m    n   o
5  p    q   r
6  s    t   u 
      :
      :
</code></pre>

<p>0,1,2,3 are times, x1, x2, x3 are inputs that are measured. So here x1 inputs are measured at every one hour. x2 and x3 will be measured at different time. What I need to do , I want write that what ever the measured in x1, x2, x3 it will add and subtract the values are equal to the x1 input next time value 
So here what I want to do is:</p>

<pre><code>    x1   x2   x3   y
 0  a    b    c   a+b-c=d
 1  d    e    f   d+e-f=g
 2  g    h    i   g+h-i=j
 3  j    k    l   j+k-l=m
 4  m    n    o   m+n-o=p
 5  p    q    r   p+q-r=s
 6  s    t    u   s+t-u=v
         :
         :
</code></pre>

<p>Here with my actual data according to my csv file:</p>

<pre><code>             X1     x2    x3    y
   0         63      0     0    63+0-0=63
   60(min)   63      0     2    63+0-2 =104
   120       104     11    0    104+11-0=93
   180       93      0    50    93+0-50=177
   240       177     0     2    177+0-2=133
   300       133     0     0    133+0-0=next value of x1
</code></pre>

<p>I tried shift method and it didn't work for me what I want exactly. I tried another method and it worked, but didn't came as I want. Here I upload the code.</p>

<pre><code>Code :
 data = pd.read_csv('data6.csv')
 i=0
 j=1
 while j &lt; len(data):
   j=data['x1'][i] - data['x2'][i] + data['x3'][i] 
   i+=1 
   j!=i 
  print(j)
</code></pre>

<p>This is works , but it is just showing only one data </p>

<blockquote>
  <p>63</p>
</blockquote>

<p>In my csv file this is second input value of x1 input.
I want to write this code contonously happened and read the value as I shown above.
Can anyone help me to solve this problem?</p>

<p><a href=""https://docs.google.com/spreadsheets/d/1WWq1qhqi4bGzNir_svQV7VstBkGbocToipPCY83Cclc/edit#gid=1512153575"" rel=""nofollow noreferrer"">My csv file</a> </p>
","11781947","","11781947","","2019-07-15 05:47:35","2019-07-15 05:51:04","Adding and subtract inbetween row inputs and value equal to the first column next row using pandas","<python-3.x><pandas><time>","2","13","","","","CC BY-SA 4.0","1"
"56800219","1","56800239","","2019-06-28 02:20:43","","2","62","<p><code>(not a duplicate question)</code></p>

<p>I have the following datasets:</p>

<pre><code>GMT TIME, Value
2018-01-01 00:00:00,    1.2030   
2018-01-01 00:01:00,    1.2000 
2018-01-01 00:02:00,    1.2030   
2018-01-01 00:03:00,    1.2030   
.... , ....
2018-12-31 23:59:59,    1.2030   
</code></pre>

<p>I am trying to find a way to remove the following:</p>

<ul>
<li><code>hh:mm:ss</code> form the datetime</li>
<li>After removing the <code>time (hh:mm:ss)</code> section, we will have duplicate <code>date</code> entry like multiple <code>2018-01-01</code> and so on... so I need to remove the duplicate date data and only keep the last date, before the next date, eg <code>2018-01-02</code> and similarly keep the last <code>2018-01-02</code> before the next date <code>2018-01-03</code> and repeat...</li>
</ul>

<p>How can I do it with <code>Pandas</code>?</p>
","9161607","","","","","2019-06-28 02:35:17","Pandas: Remove duplicate dates but keeping the last","<python><python-3.x><pandas><dataframe>","2","0","1","","","CC BY-SA 4.0","1"
"48551028","1","","","2018-01-31 20:36:40","","1","61","<p>When using the python TA-Lib library with large arrays (length 2000) I am finding a TA-Lib function (for example BBands) only produces results up til index 1100 then the rest of the result is NaN's.</p>

<p>So for a array of 2000 passed to BBands the final 900 values are NaN's. </p>

<p>I am finding this result consistent across all functions I have tried.</p>

<p>Is anyone else experiencing this issue?</p>
","7151326","","6451573","","2018-01-31 21:06:48","2018-01-31 21:06:48","TA-Lib producing NaN's for large inputs (length 1100+)","<python><python-3.x><pandas><ta-lib>","0","0","","","","CC BY-SA 3.0","1"
"49567036","1","49567343","","2018-03-30 00:52:01","","3","61","<p>let's say you have the following data frame:</p>

<pre><code>item_a item_b
1       123
7       32   
4       18
</code></pre>

<p>and then you have a constant `PERIODS = 3', how do I repeat the above data frame by 3 times while adding each repetition as a counter. </p>

<p><strong>The desired outcome is:</strong> </p>

<pre><code>counter item_a item_b
1       1       123
1       7       32   
1       4       18
2       1       123
2       7       32   
2       4       18
3       1       123
3       7       32   
3       4       18
</code></pre>
","104071","","","","","2018-03-30 14:32:30","Repeat Pandas data frame based on a value and add the counter back to the data frame","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"42281426","1","42282701","","2017-02-16 17:59:43","","0","61","<p>I have a dataframe that measure the student performance <code>student</code> as below:</p>

<pre><code>ID  TestDate    PerformanceStatus (PS)
1   15/03/2016  0
1   01/04/2016  2
1   05/05/2016  1
1   07/06/2016  1
2   15/03/2016  0
2   01/04/2016  2
2   05/05/2016  1
2   07/06/2016  3
2   23/08/2016  1
</code></pre>

<p>I want to update my table to have a new column PreviousPerformanceStatus. 
This PreviousPerformanceStatus is calculated based on the performanceStatus monitored, as below:
Note: If there is not performanceStatus recorded before the TestDate, I want to make the <code>PreviousPerformanceStatus = PerformanceStatus</code></p>

<pre><code>ID  TestDate    PS  PreviousPerformanceStatus
1   15/03/2016  0   0
1   01/04/2016  2   0
1   05/05/2016  1   2
1   07/06/2016  1   1
2   15/03/2016  0   0
2   01/04/2016  2   0
2   05/05/2016  1   2
2   07/06/2016  3   1
2   23/08/2016  1   3
</code></pre>

<p>I can do it with an SQL-statement, but how do I update my Dataframe with pandas.
Thanks.</p>

<p>Example:
(for with ID=1) The previousPerformanceStatus is calculated based on the PerformanceStatus from the ""earlier"" test date., so when TestDate=01/04/2016, I want to use the data from TestDate=15/03/2016. However, if I can't find any previous data, I will default the PreviousPerformanceStatus with the value in the PerformanceStatus</p>
","4394770","","","","","2017-02-16 19:12:50","How to update Pandas dataframe based on the other row?","<python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"56666517","1","56666835","","2019-06-19 11:21:32","","1","60","<p>I have the following <code>df</code>,</p>

<pre><code>year    code    col1   col2
2019    1       2      3
2019    1       3      5
2019    1       2      4
2018    2       1      4
2018    2       2      6
</code></pre>

<p>I want to <code>groupby</code> <code>df</code> by <code>year</code> and <code>code</code>, then sum the differences between <code>col2</code> and <code>col1</code>, and then averages the sum over the group size;</p>

<pre><code>df.apply(lambda row: (row['col_2'] - row['col_1']).mean(level=[0, 1]).reset_index(name='avg_num')
</code></pre>

<p>this code seems to calculate the mean of differences rather than summing the differences and divided by the group size, so how to fix this? </p>

<pre><code>year    code    col1   col2    avg_num
2019    1       2      3       1.66
2019    1       3      5       1.66
2019    1       2      4       1.66
2018    2       1      4       3.5
2018    2       2      6       3.5
</code></pre>
","766708","","","","","2019-06-19 11:42:23","pandas groupby sums differences between two columns and get the average for each group","<python><python-3.x><pandas><dataframe><group-by>","3","0","","","","CC BY-SA 4.0","1"
"49461988","1","","","2018-03-24 06:37:53","","0","60","<p>I am making a generic tool which can take up any csv file.The file contains a city column which needs to be geocoded to latitudes and Longitudes. I have a csv file which looks something like this. The first row is the column name and the second row is the type of variable.</p>

<pre><code>Time,M1,M2,M3,CityName
temp,num,num,num,loc
20-May-13,19,20,0,delhi
20-May-13,25,42,7,agra
20-May-13,23,35,4,mumbai
20-May-13,21,32,3,delhi
20-May-13,17,27,1,mumbai
20-May-13,16,40,5,delhi
</code></pre>

<p>First of all, I find the unique values in the City column and form a list of it. </p>

<pre><code>filename = 'data_file.csv'
data_date = pd.read_csv(filename)
column_name = data_date.ix[:, data_date.loc[0] == ""city""]
column_work = column_name.iloc[1:]
column_unique = column_work.iloc[:,3].unique().tolist()
</code></pre>

<p>Secondly, I have written code for geocoding my cities.</p>

<pre><code>def geocode(address):
    i = 0
    try:
        while i &lt; len(geocoders):
            # try to geocode using a service
            location = geocoders[i].geocode(address)

            # if it returns a location
            if location != None:

                # return those values
                return [location.latitude, location.longitude]
            else:
                # otherwise try the next one
                i += 1
    except:
        print (sys.exc_info()[0])
        return ['null','null']

    # if all services have failed to geocode, return null values
    return ['null','null']

list = ['delhi', 'agra', 'mumbai']
j = 0
lat = []
for row in list:
    print ('processing #',j)
    j+=1
    try:
        state = row
        address = state
        result = geocode(address)
        # add the lat/lon values to the row
        lat.extend(result)
    except:
       # print 'Unsuccessful'
       to_print = 'Unsuccessful'
       # row.extend(to_print)
       dout.append(row)
print(lat)
</code></pre>

<p>This gives me a list of latitudes and longitudes <code>[28.7040592, 77.10249019999999, 27.1766701, 78.00807449999999, 19.0759837, 72.8776559]</code>. I want to write this onto my CSV file as</p>

<pre><code>Time,M1,M2,M3,CityName,Latitude,Longitude
temp,num,num,num,loc,lat,lng
20-May-13,19,20,0,delhi,28.7040592,77.10249019999999
20-May-13,25,42,7,agra,27.1766701,78.00807449999999
20-May-13,23,35,4,mumbai,19.0759837, 72.8776559
20-May-13,21,32,3,delhi,28.7040592,77.10249019999999
20-May-13,17,27,1,mumbai,19.0759837, 72.8776559
20-May-13,16,40,5,delhi,28.7040592,77.10249019999999
</code></pre>

<p>I tried making a separate list of latitudes and longitudes <code>latitude = lat[0::2] longitude = lat[1::2]</code> or convert it to into a dictionary <code>{'delhi': [28.7040592, 77.10249019999999], 'agra': [27.1766701, 78.00807449999999], 'mumbai': [19.0759837, 72.8776559]}</code> but somehow could not figure out how to write it on a csv file.</p>
","5433480","","6666231","","2018-03-24 08:48:34","2018-03-24 08:48:34","Mapping values into two additional DataFrame columns by an existing one in Python","<python><python-3.x><pandas><csv>","1","1","","","","CC BY-SA 3.0","1"
"56873664","1","56873715","","2019-07-03 15:37:22","","1","60","<p>I have two data frames with similar data in different formats</p>

<p>df1:</p>

<pre><code>Nodo      X          Y          Z
CTB3901   CTBX3901   CTBY3901   CTBZ3901
MTR5331   MTRX5331   MTRY5331   MTRZ5331
ADC3451   ADCX3451   ADCY3451   ADCZ3451
</code></pre>

<p>df2:</p>

<pre><code>Site_x     Site_y
CTBX3901E  CTBX3901
CTB3901    CTB3901E
CTBZ3901E  CTBZ3901
CTBY3901E  CTB3901
MADX6379E  MADX6379
</code></pre>

<p>I want to check if any entries from <code>df2['Site_x', 'Site_y']</code> is in any of  the columns <code>df1['Nodo','X','Y','Z']</code>. The data need not be in the same row in both frames.</p>

<p>The final output after the check shud be as below</p>

<pre><code>Site_x     Site_y   Checked
CTBX3901E  CTBX3901  True
CTB3901    CTB3901E  True
CTBZ3901E  CTBZ3901  True
CTBY3901E  CTB3901   True
MADX6379E  MADX6379  False
</code></pre>

<p>Pardon me for the clumsy dataset. In desperation of getting this part right, I had to paste the same data I was working with. </p>

<p>I've tried isin method with the below syntax but the output has False in the entire 'Checked' column.</p>

<pre><code>df2['Checked'] = df2[['Site_x','Site_y']].isin(df1[['Nodo','X','Y','Z']]).any(axis=1)
</code></pre>
","11600323","","","","","2019-07-03 16:43:35","Check if any value ( multiple columns) of one data frame exists in any values (multiple columns) of another data frame","<python><python-3.x><pandas>","4","0","","","","CC BY-SA 4.0","1"
"57559935","1","57561956","","2019-08-19 15:38:59","","0","60","<p>I am developing code for searching a keyword in the given data.
    for example, I have a data in column A &amp; I want to find if 
    the substring is present in the row if yes give me that keyword against 
    the data, if that keyword is not present then give me 'blank'.</p>

<pre><code>import pandas as pd
data = pd.read_excel(""C:/Users/606736.CTS/Desktop/Keyword.xlsx"")

# dropping null value columns to avoid errors 
data.dropna(inplace = True)
# Converting the column to uppercase
data[""Uppercase""]= data[""Skill""].str.upper()

# Below is the keywords I want to search in the data
sub =['MEMORY','PASSWORD','DISK','LOGIN','RESET']
# I have used the below code, which is creating multiple columns &amp; 
giving me the boolean output
for keyword in sub:
data[keyword] = data.astype(str).sum(axis=1).str.contains(keyword)
 # what I want is, search the keyword if it exits give me the keyword 
  name else blank
</code></pre>
","9571412","","","","","2019-08-21 11:09:03","change the column value based on the multiple columns","<python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"49823263","1","49823873","","2018-04-13 18:36:14","","2","60","<p>I have a financial dataset that contains measures for some markets during some time periods.</p>

<pre><code>market  date    metric1    metric2    metric3
ASX     2000            
ASX     2001            
ASX     2002            
ASX     2003            
TSX     2000            
TSX     2001            
TSX     2002            
TSX     2003            
TSX     2004            
NYSE    2000            
NYSE    2001            
NYSE    2002            
NYSE    2003            
NYSE    2004    
</code></pre>

<p>Metric1 to metric3 contains numerical values. I would like to draw some line or bar graphs that group by market and between some date values, say 2000 and 2002 in this example. my date variable could be yearmonth (e.g. 200101, 200102). Is there a way to tell python that it is yearmonth so there is no gap between 200112 and 200201?
I am using matplotlib and pandas.</p>
","6059024","","6059024","","2018-04-13 18:46:14","2018-04-13 19:26:29","drawing graphs from a dataframe by group and between certain values","<python><python-3.x><pandas><dataframe><matplotlib>","2","1","","","","CC BY-SA 3.0","1"
"41441360","1","41441525","","2017-01-03 10:27:06","","0","60","<p>I have a dataframe bunch of categorical variables, each row corresponds to a product.I wanted to find the number of rows for every combination of attribute levels and decided to run the following: </p>

<pre><code>att1=list(frame_base.columns.values)
f1=att.groupby(att1,as_index=False).size().rename('counts').to_frame()
</code></pre>

<p><strong><em>att1</em></strong> is the list of all attributes, <strong><em>f1</em></strong> does not seem to provide the correct value as <strong><em>f1.counts.sum()</em></strong>  is not equal to <strong><em>len(f1)</em></strong> before the group by.Why doesn't this work?</p>
","5779797","","","","","2017-01-03 10:35:01","Get number of rows for all combinations of attribute levels in Pandas","<python-3.x><pandas>","1","1","","","","CC BY-SA 3.0","1"
"57001725","1","","","2019-07-12 07:02:14","","1","60","<p>How can I add the count of string present in target column. </p>

<pre><code>data = [{'target': ['Aging','Brain', 'Neurons', 'Genetics']}, 
        {'target': ['Dementia', 'Genetics']}, 
        {'target': ['Brain','Dementia', 'Genetics']}]

df = pd.DataFrame(data)
</code></pre>

<p>Dataframe </p>

<pre><code>target
0   [Aging, Brain, Neurons, Genetics]
1   [Dementia, Genetics]
2   [Brain, Dementia, Genetics]
</code></pre>

<p>Unique labels</p>

<pre><code>target = []
for sublist in df['target'].values:
    tmp_list = [x.strip() for x in sublist]
    target.extend(tmp_list)

target = list(set(target))

# ['Brain', 'Neurons', 'Aging', 'Genetics', 'Dementia']
</code></pre>

<p>The expected output is here
<a href=""https://i.stack.imgur.com/TjJqk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TjJqk.png"" alt=""enter image description here""></a></p>
","8700970","","","","","2019-07-12 11:26:24","Building dataframe with label count","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"56836996","1","56837016","","2019-07-01 13:58:06","","3","60","<p>Let's say that I have a dataset that contains 4 binary columns for 2 rows.</p>

<p>It looks like this:</p>

<pre><code>    c1 c2 c3 c4 c5
r1  0   1  0  1 0
r2  1   1  1  1 0
</code></pre>

<p>I want to create a matrix that gives the number of occurrences of a column, given that it also occurred in another column. Kinda like a confusion matrix</p>

<p>My desired output is:</p>

<pre><code>   c1 c2  c3  c4 c5
c1  -  1   1   1  0
c2  1  -   1   2  0
c3  1  1   -   1  0
c4  1  2   1   -  0
</code></pre>

<p>I have used pandas crosstab but it only gives the desired output when using 2 columns. I want to use all of the columns</p>
","11724419","","13302","","2019-07-09 16:04:52","2019-07-09 16:04:52","How can I create Frequency Matrix using all columns","<python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"57688695","1","","","2019-08-28 09:02:47","","1","59","<p>I have a dates column with timezone offset of +01:00 and +00:00 so the time offset is different but the same format should stay.
E.G </p>

<pre><code>2019-05-21T00:00:00+01:00
2018-12-10T00:00:00+00:00
</code></pre>

<p>I have tried to parse this by letting pandas automatically define the time by using:</p>

<pre><code>pd.to_datetime(df['Effective_Date'])
</code></pre>

<p>which just changes the column from</p>

<pre><code>2019-05-06T00:00:00+01:00
to
2019-05-06 00:00:00+01:00
</code></pre>

<p>Yet my column when I run <code>df.dtypes</code>is still of type object - which I believe is due to the different time offset differences?</p>

<p>Have also tried </p>

<p><code>pd.to_datetime(df['Effective_Date'], format='%Y-%m-%dT%H:%M:%S%z')</code></p>

<p>But this didn't work either</p>

<p>How do I let pandas convert my times and then return a type of datetime, and not type object?</p>

<p>Any help much appreciated, thanks!</p>
","11158895","","","","","2019-08-29 07:24:13","Pandas parsing dates with timezone offset not returning dtype of datetime, just object?","<python-3.x><pandas><timezone-offset>","2","0","","","","CC BY-SA 4.0","1"
"58322385","1","58322594","","2019-10-10 11:57:13","","1","59","<p>I have a dataset df(250,3) 250 raws and three columns. I want to write a loop that merges the content of each column in my dataframe to have one single series(250,1) of 250 raws and 1 columns 'df_single'. The manual operation is the following: </p>

<p>df_single = df['colour']+"" ""+df['model']+"" ""+df['size']</p>

<p>How can I create df_single with a for loop, or non-manually?</p>

<p>I tried to write this code with TypeError</p>

<pre><code>df_conc=[]
for var in cols:
    cat_list=df_code_part[var]
    df_conc = df_conc+"" ""+cat_list
</code></pre>

<p>TypeError: can only concatenate list (not ""str"") to list</p>
","9327426","","","","","2019-10-10 13:35:44","Transforming multiple data frame columns into one series","<python-3.x><pandas><for-loop>","3","5","","","","CC BY-SA 4.0","1"
"48818190","1","","","2018-02-15 23:56:44","","0","59","<p>I am trying to figure out how to code the following problem using python. Suppose we have the following data set in a .txt file:</p>

<pre><code>datatype1 designator1 3:45:14AM
datatype1 designator1 3:45:19AM
datatype1 designator1 3:45:26AM
datatype1 designator1 3:45:31AM
datatype1 designator1 4:10:05AM
datatype1 designator1 4:10:21AM
datatype1 designator1 4:10:30AM
datatype1 designator1 4:10:46AM
</code></pre>

<p>Note the time break. I need my code to read through the text file and, where there is a break in the time intervals, split the file up and write the following to another text file:</p>

<pre><code>datatype1 designator1 3:45:14AM 3:45:31AM
datatype1 designator1 4:10:05AM 4:10:46AM
</code></pre>

<p>In other words, I want to condense the original data to individual ""sessions"" represented by single lines with start and end times.</p>

<p>Thanks for your help!</p>
","9367408","","7386332","","2018-02-17 01:06:02","2018-02-17 01:06:02","Extracting data from .txt and writing to .txt with Python","<python><python-3.x><pandas>","3","1","","","","CC BY-SA 3.0","1"
"49610910","1","","","2018-04-02 12:00:54","","0","59","<p>I have a huge Dataframe with 8 miliion rows and a algorithm, which works with it in an recursive module. As I would like to prevent the algorithm to load every single time the whole dataframe, I would like to store it in hdf-format, so that I can preselect the information I need, to save memory-space. The problem is, that one column consists of a list in every row, which is way saving leads to the following error:
Exception: cannot find the correct atom type -> [dtype->object,items->Index(['Herkunft', 'ID', 'Objekttyp', 'ObjekttypNr', 'Staat', 'aktueller Name',
       'art', 'ldName', 'population', 'neuername', 'neuername2', 'kphdist'],
      dtype='object')] </p>

<p>This leads to my question: is there a way to store a columns out of lists in a hdf-table? It should be available right after loading, without any costly reformatations.</p>

<p>edit:
hear are my columns with 10 entities as lists:</p>

<pre><code>Herkunft ['1Wiki', '1Wiki', '1Wiki', '1Wiki']
ID ['http://www.wikidata.org/entity/Q1917863', 'http://www.wikidata.org/entity/Q7165355', 'http://www.wikidata.org/entity/Q7165354', 'http://www.wikidata.org/entity/Q7165337']
Objekttyp [nan, nan, nan, nan]
ObjekttypNr ['nan', 'Dorf', 'Dorf', 'Dorf']
Staat ['ES', 'MY', 'IN', 'CA']
aktueller Name [nan, nan, nan, nan]
art ['http://www.wikidata.org/entity/Q2074737', 'http://www.wikidata.org/entity/Q486972', 'http://www.wikidata.org/entity/Q486972', 'http://www.wikidata.org/entity/Q486972']
latitude [38.840555555, 1.56667, 13.3667, 44.3008]
ldName ['carrícola', 'penunus', 'penumuru', 'pentz, nova scotia']
longitude [-0.471388888, 111.45, 79.1833, -64.3819]
population ['95', nan, nan, nan]
lakurz [38.840555555, 1.56667, 13.3667, 44.3008]
lokurz [-0.471388888, 111.45, 79.1833, -64.3819]
neuername ['carrícola', 'penunus', 'penumuru', 'pentz , nova scotia']
neuername2 ['carricola', 'penunus', 'penumuru', 'pentz , nova scotia']
lettermass [5, 5, 6, 11]
kphdist [['4745'], ['1668'], ['1667'], ['168', '', '', '63', '82']]
</code></pre>
","7022574","","7022574","","2018-04-17 15:23:35","2018-04-17 15:23:35","Is there a way to save Pandas Dataframe to hdf, if one column consists of lists?","<python-3.x><pandas><dataframe><hdf>","0","3","","","","CC BY-SA 3.0","1"
"33694602","1","33694774","","2015-11-13 14:07:32","","2","59","<p>With this script/module, XRateDKKUSD_test.py, I can successfully fetch the exchange rate DKK pr USD.</p>

<pre><code>import pandas as pd
import pandas.io.data as web
import datetime

def xRate_pd(years,modus,start=datetime.datetime(2000,1,1),end=pd.Timestamp.utcnow()):
    global xrate, xratedate, df_xrate

    days = int(252 * years)  # ant. arb. dage pr år = 252

    if modus == 'sim':
        start = datetime.datetime(2014,1,1)  # indstil manuelt
        end   = datetime.datetime(2015,5,18) # indstil manuelt

    if modus == 'trading':
        end   = pd.Timestamp.utcnow()
        start = end - days * pd.tseries.offsets.BDay()

    df_xrate = web.DataReader('DEXDNUS', 'fred',
                       start = start, end = end)
    print('df_xrate \n',df_xrate)

    # Selecting only last day from df, saving to xrate, xratedate
    xrate = df_xrate.ix[-1, 'DEXDNUS']
    xratedate = df_xrate.index[-1]

    return xrate, xratedate, df_xrate

if __name__ == '__main__':
#    xrate_lookup()
    xRate_pd(modus='trading',years=0.25)
</code></pre>

<p>However, when I try to run this script from my main program with this function...</p>

<pre><code>def xRate(start, end, years, modus): 
    global xrate, xratedate, df_xrate

    xrate, xratedate, df_xrate = XRateDKKUSD_test.xRate_pd(start, end, modus) 

    return xrate, xratedate, df_xrate
</code></pre>

<p>Run with this call</p>

<pre><code>import XRateDKKUSD_test
xRate_pd(start, end)
</code></pre>

<p>Obviously I have set the 'start' &amp; 'end' parameters in a preceeding function.</p>

<p>when the script is run imported as a module I suddenly run into this problem that I do not get when the script is run stand-alone:</p>

<pre><code>  File ""z:/python/crystallball/git - crystalball/_crystalball_main.py"", line 277, in &lt;module&gt;
    xRate_pd(start, end)

  File ""Z:/python/CrystallBall/Git - CrystalBall/XRateDKKUSD.py"", line 55, in xRate_pd
    days = int(252 * years)

TypeError: unsupported operand type(s) for *: 'int' and 'Timestamp'
</code></pre>

<p>Anyone who knows why there is this difference, and error when I import and run the script?</p>
","4498798","","100297","","2015-11-13 14:18:44","2015-11-13 14:18:44","Script is fine, but will not run as imported module","<python><python-3.x><pandas><currency-exchange-rates>","1","0","","","","CC BY-SA 3.0","1"
"58376427","1","","","2019-10-14 12:05:12","","1","59","<p>I need to concatenate all the non-blank columns from a set of columns in a dataframe</p>

<pre><code>my_df = pd.DataFrame({
    'Imp':  ['1', '2', '3'],
    'Apple':  ['a', 'b', 'c'],
    'Pear':   ['d',    ,    ],
    'Cherry': ['h',    , 'j']})
</code></pre>

<p>My desired output is the dataframe with the column 'Concat' in it</p>

<p>I only want to concatenate certain columns in my dataframe (Apple, Pear and Cherry)</p>

<pre><code>Imp Apple  Pear Cherry  Concat
  1   a      d    h     a,d,h
  2   b                 b
  3   c           j     c,j
</code></pre>
","9848572","","","","","2019-10-14 12:55:07","How to concatenate certain columns with a condition of blank","<python><python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"57194839","1","","","2019-07-25 05:23:28","","2","59","<p>I want to use fuzzywuzzy package on the following table</p>

<pre><code>x   Reference   amount
121 TOR1234        500
121 T0R1234        500
121 W7QWER         500
121 W1QWER         500
141 TRYCATC        700
141 TRYCATC        700
151 I678MKV        300
151 1678MKV        300
</code></pre>

<ol>
<li>I want to group the table where the columns 'x' and 'amount' match.</li>
<li>for each reference in the group
i. Compare(fuzzywuzzy) with other references in that group.
       a. where the match is 100%, delete them
       b. where the match is 90-99.99%, keep them
       c. delete anything below 90% match for that particular row
the expected output-</li>
</ol>

<pre><code> x   y     amount
151 I678MKV 300
151 1678MKV 300
121 TOR1234 500
121 T0R1234 500
121 W7QWER  500
121 W1QWER  500

</code></pre>

<p>This is to detect the fraud entries, Like in the tables, '1' is replaced by 'I' and '0' is replaced by 'O'. If you any alternative solution, please suggest.</p>
","9168092","","11607986","","2019-07-25 07:11:19","2019-07-25 07:11:19","I have been trying to apply fuzzywuzzy package to solve a problem to find fraud entries. How do i apply the same in the following problem?","<python><python-3.x><pandas><nlp><fuzzy-logic>","1","0","1","","","CC BY-SA 4.0","1"
"57454037","1","57454342","","2019-08-11 22:26:24","","1","59","<p>I have a list of dicts like this:</p>

<pre class=""lang-py prettyprint-override""><code>sample = [
    {'title': title, 'description': description, 'category': category, 'URLS': [1, 3, 4]},
    {'title': title, 'description': description, 'category': category, 'URLS': [1, 3, 4, 5, 6, 7]},
    {'title': title, 'description': description, 'category': category, 'URLS': [1]}
]
</code></pre>

<p>This is loaded into the dataframe without any problems:</p>

<pre class=""lang-py prettyprint-override""><code>dataframe = pandas.DataFrame(data)
</code></pre>

<p>There are a lot of such dictionaries, about 200,000 and a lot of categories, I want to save files sorted by categories into different csv files.</p>

<p>At first, I just tried to display all the elements for each category:</p>

<pre class=""lang-py prettyprint-override""><code>for item in range(len(dataframe['category'])):
    dataframe['category'][item]
</code></pre>

<p>But i have output like this:</p>

<pre><code>'Games &amp; Hobbies'
'Video Games'
'Business'
...
</code></pre>

<p>Just to save all this is not a problem, but there are a lot of records and I would like to separate them.
Thanks in advance for your help.</p>
","11436357","","11436357","","2019-08-11 22:40:58","2019-08-11 23:29:03","How to save dataframe as separated csv files?","<python-3.x><pandas>","2","1","","","","CC BY-SA 4.0","1"
"57079203","1","57079265","","2019-07-17 15:17:00","","1","59","<p>I am trying to add a column to the left of the dataframe. By default it seems to add to the right. Is there a way to add the columns to the left?</p>

<p>Here is my code:</p>

<pre><code>import pandas as pd
import numpy as np
df = pd.read_csv(""/home/Sample Text Files/sample5.csv"", delimiter = ""\t"")
df=pd.DataFrame(df)
df['Creation_DT']=pd.to_datetime('today')
print(df)
</code></pre>

<p>Here is the output:</p>

<pre><code> ID,Name,Age                Creation_DT
0  1233,Maliva,15 2019-07-17 11:11:37.145194
</code></pre>

<p>I want the output to be like this:</p>

<pre><code>Creation_DT, ID, Name, Age
[value], [Value], [Value], [Value]
</code></pre>
","10537095","","","","","2019-07-17 15:27:45","How to add a column to the left of a datafra,e","<python-3.x><pandas><numpy><dataframe>","2","0","","2019-07-17 15:27:27","","CC BY-SA 4.0","1"
"57052669","1","57052792","","2019-07-16 08:01:27","","3","59","<p>I have a dataset with <em>time</em> in the epoch. I need to extract the epoch <em>time</em> and convert it into normal <em>DD MM YYYY</em> format along with time details in <em>HH: MM</em> format.
The column is like this:-</p>

<pre><code>Index       Date                                                                  
0        {'$date': {'$numberLong': '1562005805010'}}   
</code></pre>

<p>I have tried using regex, extract and replace methods but they convert the date column to NaN</p>

<pre><code>df1['date'] = df1['date'].str.extract('(\d+)', expand=False)
</code></pre>

<p>I want only epochs to be displayed so that they can be converted to date and time.
<a href=""https://i.stack.imgur.com/vX2D7.png"" rel=""nofollow noreferrer"">Here is the column that I have</a></p>
","10800593","","2901002","","2019-07-16 08:17:13","2019-07-16 08:17:13","How to extract only the epoch details and leave other things out in pandas dataframe?","<python><python-3.x><pandas><datetime><epoch>","1","0","1","","","CC BY-SA 4.0","1"
"57658802","1","57658948","","2019-08-26 13:23:37","","1","59","<p>First off, I realize that this question has been asked a ton of times in many different forms, but a lot of the answers just give code that solves the problem without explaining what the code actually does or why it works. </p>

<p>I have an enormous data set of phone numbers and area codes that I have loaded into a dataframe in python to do some processing with. Before I do that processing, I need to split the single dataframe into multiple dataframes that contain phone numbers in certain ranges of area codes that I can then do more processing on. For example:</p>

<pre><code>+---+--------------+-----------+
|   | phone_number | area_code |
+---+--------------+-----------+
| 1 | 5501231234   | 550       |
+---+--------------+-----------+
| 2 | 5051231234   | 505       |
+---+--------------+-----------+
| 3 | 5001231234   | 500       |
+---+--------------+-----------+
| 4 | 6201231234   | 620       |
+---+--------------+-----------+
</code></pre>

<p>into</p>

<pre><code>area-codes (500-550)
+---+--------------+-----------+
|   | phone_number | area_code |
+---+--------------+-----------+
| 1 | 5501231234   | 550       |
+---+--------------+-----------+
| 2 | 5051231234   | 505       |
+---+--------------+-----------+
| 3 | 5001231234   | 500       |
+---+--------------+-----------+
</code></pre>

<p>and </p>

<pre><code>area-codes (600-650)
+---+--------------+-----------+
|   | phone_number | area_code |
+---+--------------+-----------+
| 1 | 6201231234   | 620       |
+---+--------------+-----------+
</code></pre>

<p>I get that this should be possible using pandas (specifically groupby and a Series object I think) but the documentation and examples on the internet I could find were a little too nebulous or sparse for me to follow. Maybe there's a better way to do this than the way I'm trying to do it?</p>
","7535670","","","","","2019-08-26 16:00:29","How do I split a single dataframe into multiple dataframes by the range of a column value?","<python><python-3.x><pandas>","2","1","","","","CC BY-SA 4.0","1"
"57559817","1","57560096","","2019-08-19 15:31:47","","3","59","<p>I have a data frame that looks like </p>

<pre><code>d = {'col1': ['a,a,b', 'a,c,c,b'], 'col2': ['a,a,b', 'a,b,b,a']}
pd.DataFrame(data=d)
</code></pre>

<p>expected output</p>

<pre><code>d={'col1':['a,b','a,c,b'],'col2':['a,b','a,b,a']}
</code></pre>

<p>I have tried like this : </p>

<pre><code>arr = ['a', 'a', 'b', 'a', 'a', 'c','c']
print([x[0] for x in groupby(arr)])
</code></pre>

<p>How do I remove the duplicate entries in each row and column of dataframe?</p>

<p><code>a,a,b,c</code> should be <code>a,b,c</code></p>
","11921705","","9840637","","2019-08-19 16:19:33","2019-08-19 16:19:33","Remove consecutive duplicate entries from pandas in each cell","<python-3.x><pandas><pandas-groupby>","2","0","1","","","CC BY-SA 4.0","1"
"56617993","1","","","2019-06-16 10:31:00","","1","58","<p>I am using python/pandas on Windows 10 from last month &amp; did not face the below issue that suddenly came into being. I have a csv file that is read with pandas. However, the dataframe is arbitrarily joining the comma separated heading into one &amp; while doing this abruptly leaving off last few characters, as a result of this, the code though very simple, is failing. Has anyone seen this kind of problem? Suggestions to overcome this would be of great help</p>

<p>Was trying to check the date format to be in 'yyyy-mm-dd'. Since I got the error, put a print statement to check column names,
Reinstalled python 3.6.8, pandas etc, but that did not help.</p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd

df = pd.read_csv('Data.csv','r')

print(df.columns)
for pdt in df.PublicDate:
    try:
        dat = pdt[0:10]
        if dat[4] != '-' or dat[7] != '-':
            print('\nPub Date Format Error',dat)
    except TypeError as e:
        print(e)
</code></pre>

<p>Test Data csv file has:</p>

<pre><code>PIC,PublicDate,Version,OriginalDate,BPD
ABCD,2019-06-15T19:25:22.000000000Z,1,2019-06-1519.25.22.0000000000,15-06-2019
EFGH,06/15/2019T19:26:22.000000000Z,,2019-06-1519.26.22.0000000000,15-06-2019
IJKL,2019-06-15T20:26:22.000000000Z,1,2019-06-1520.26.22.0000000000,6/25/2019
MNOP,,,2019-06-1520.26.22.0000000000,6/25/2019
QRST,2019-06-15T22:26:22.000000000Z,1,,6/25/2019
</code></pre>

<p>Expected: </p>

<blockquote>
  <p>dates of the format 6/25/2019 should be pointed out for not being in the format 2019-06-25</p>
</blockquote>

<p>Actual Result: Below Error</p>

<p>=============== RESTART: H:\Python\DateFormat.py ===============</p>

<pre><code>    Index(['PIC,PublicDate,Ve', 'sion,O', 'iginalDate,BPD'], dtype='object')
    Traceback (most recent call last):
      File ""H:\Program Files\Python\DateFormat.py"", line 8, in &lt;module&gt;
        for pdt in df.PublicDate:
      File ""G:\Program Files\lib\site-packages\pandas\core\generic.py"", line 5067, in __getattr__
        return object.__getattribute__(self, name)
    AttributeError: 'DataFrame' object has no attribute 'PublicDate'
</code></pre>
","11654543","","2172752","","2019-06-16 13:58:17","2019-07-26 09:16:14","Facing Wierd Issue using pandas-0.24.2","<python><python-3.x><pandas><csv>","1","0","","","","CC BY-SA 4.0","1"
"57227739","1","57227890","","2019-07-26 22:41:53","","0","58","<p>I am trying to create a new column based on condition in one column and assigning the value from multiple columns in the same data frame.</p>

<p>Below is the code that i tried.</p>

<pre><code>data[""Associate""]= data.apply(lambda x: np.where(x.BU=='SBS',x.SBS,x.MAS_NAS_HRO),axis=1)
</code></pre>

<p><a href=""https://i.stack.imgur.com/gBQFX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gBQFX.png"" alt=""enter image description here""></a></p>

<pre><code>BU    SBS       MAS_NAS_HRO   Associate
SBS   Ren       Sunil         Ren
MAS   Uma       Majid         Majid
NAS   Sumit     Uma           Uma
</code></pre>

<p>Above image is what i am trying to achieve i get this error: </p>

<pre><code>ValueError: Cannot set a frame with no defined index and a value that cannot be converted to a Series
</code></pre>

<p>I tried also</p>

<pre><code>data['Associate']=''
data.loc[data['BU'] == 'SBS',Associate]=data['SBS']
</code></pre>

<p>I tried this and as well it did not work.</p>

<pre><code>associate_details=['SBS']
associate1=data[data.BU.isin(associate_details)]
choice_assoc1=efile_data['SBS']
associate2=data[~data.BU.isin(associate_details)]
choice_assoc2=efile_data['MAS_NAS_HRO']
efile_data['Associate']=np.select([associate1,associate2],[choice_assoc1,choice_assoc2],default=np.nan)

i get this message [0 rows x 4 columns]
Empty DataFrame
</code></pre>

<p>How do i change these errors.</p>

<p>Regards,
Ren.</p>
","9542169","","9542169","","2019-07-26 22:48:13","2019-07-26 23:27:50","create a new pandas column based on condition in one column and assigning the value from multiple columns in the same data frame","<python-3.x><pandas><numpy>","1","3","","","","CC BY-SA 4.0","1"
"57266132","1","57266682","","2019-07-30 07:10:23","","0","58","<p>I have an excel sheet which contains more than 30 sheets for different parameters like BP, Heart rate etc. </p>

<p>One of the dataframe (df1 - created from one sheet of excel) looks like as shown below</p>

<pre><code>df1= pd.DataFrame({'person_id':[1,1,1,1,2,2,2,2,3,3,3,3,3,3],'level_1': ['H1Date','H1','H2Date','H2','H1Date','H1','H2Date','H2','H1Date','H1','H2Date','H2','H3Date','H3'],
               'values': ['2006-10-30 00:00:00','6.6','2006-08-30 00:00:00','4.6','2005-10-30 00:00:00','6.9','2016-11-30 00:00:00','6.6','2006-10-30 00:00:00','6.6','2006-11-30 00:00:00','8.6',
                       '2106-10-30 00:00:00','16.6']})
</code></pre>

<p><a href=""https://i.stack.imgur.com/WNfhj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WNfhj.png"" alt=""enter image description here""></a></p>

<p>Another dataframe (df2) from another sheet of excel file can be generated using the code below</p>

<pre><code>df2= pd.DataFrame({'person_id':[1,1,1,1,2,2,2,2,3,3,3,3,3,3],'level_1': ['GluF1Date','GluF1','GluF2Date','GluF2','GluF1Date','GluF1','GluF2Date','GluF2','GluF1Date','GluF1','GluF2Date','GluF2','GluF3Date','GluF3'],
               'values': ['2006-10-30 00:00:00','6.6','2006-08-30 00:00:00','4.6','2005-10-30 00:00:00','6.9','2016-11-30 00:00:00','6.6','2006-10-30 00:00:00','6.6','2006-11-30 00:00:00','8.6',
                       '2106-10-30 00:00:00','16.6']})
</code></pre>

<p>Similarly there are more than 30 dataframes like this with values of the same format (Date &amp; measurement value) but column names (H1, GluF1, H1Date,H100,H100Date, GluF1Date,P1,PDate,UACRDate,UACR100, etc) are different</p>

<p>What I am trying to do based on SO search is as shown below</p>

<pre><code>g = df1.level_1.str[-2:] # Extracting column names
    df1['lvl'] = df1.level_1.apply(lambda x: int(''.join(filter(str.isdigit, x)))) # Extracting level's number
    df1= df1.pivot_table(index=['person_id', 'lvl'], columns=g, values='values', aggfunc='first')
    final = df1.reset_index(level=1).drop(['lvl'], axis=1)
</code></pre>

<p>The above code gives an output like this which is not expected</p>

<p><a href=""https://i.stack.imgur.com/MbMvU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MbMvU.png"" alt=""enter image description here""></a></p>

<p>This doesn't work as <code>g</code> doesn't result in same string output (column names) for all records. My code would work if the substring extract has resulted in same output but since the data is like sequence, I am not able to make it uniform</p>

<p>I expect my output to be like as shown below for each dataframe. Please note that a person can have 3 records (H1..H3)/10 records (H1..H10) / 100 records (ex: H1...H100). It is all possible.</p>

<p><a href=""https://i.stack.imgur.com/7xFQk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7xFQk.png"" alt=""enter image description here""></a></p>

<p><strong>updated screenshot</strong></p>

<p><a href=""https://i.stack.imgur.com/kSyeX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kSyeX.png"" alt=""enter image description here""></a></p>
","10829044","","10829044","","2019-07-30 08:20:51","2019-07-30 08:36:21","Create common columns and transform time series like data","<python><python-3.x><pandas><list><dataframe>","2","2","","","","CC BY-SA 4.0","1"
"57664445","1","57664533","","2019-08-26 20:18:20","","0","58","<p>I am trying to convert all items in my dataframe to a float. The types are varies at the moment. The following error persist -> ValueError: could not convert string to float: '116,584.54'</p>

<p>The file can be found at <a href=""https://www.imf.org/external/pubs/ft/weo/2019/01/weodata/WEOApr2019all.xls"" rel=""nofollow noreferrer"">https://www.imf.org/external/pubs/ft/weo/2019/01/weodata/WEOApr2019all.xls</a></p>

<p>I checked the value in excel, it is a Number. I tried .replace, .astype, pd.to_numeric. </p>

<pre class=""lang-py prettyprint-override""><code>for i in weo['1980']:
    if i == float:
        print(i)
        i.replace("","",'')
        i.replace(""--"",np.nan)
    else:
        continue
</code></pre>

<p>Also, I have tried:</p>

<pre class=""lang-py prettyprint-override""><code>weo['1980'] = weo['1980'].apply(pd.to_numeric)
</code></pre>
","11729425","","4647152","","2019-08-26 20:40:33","2019-08-26 21:40:10","Convert All Items in a Dataframe to Float","<python-3.x><pandas>","2","1","","","","CC BY-SA 4.0","1"
"48631231","1","","","2018-02-05 20:49:22","","1","58","<p>I have code that creates a bokeh datatable from an oracle sql query converted into a dataframe <code>df</code>. The original query result looks like this:</p>

<pre><code>User | Entry | feature1 | feature2
jim    JAN       1           22
jones  JAN       1           93
ted    FEB       0           93
</code></pre>

<p>I want to return the count of <code>Entry</code> for each <code>User</code> only. Then I want to sum entries for each user in a column called 'YEARLY_TOTAL' So I do:</p>

<pre><code>del df['feature1']
del df['feature2']
df=df.pivot_table(index=['User'], columns=['Entry'], aggfunct=len)
df=df.reset_index()
df['YEARLY_TOTAL']=df.sum(axis=1)
source=ColumnDataSource(data=dict())
</code></pre>

<p>Now it looks like:</p>

<pre><code>User     JAN    FEB      YEARLY_TOTAL
jim    1                    1
jones  1                    1
ted           1             1
</code></pre>

<p>I attempt to render it in a bokeh server datatable with adjustable slider via:</p>

<pre><code>def update():
    current=df[(df[""YEARLY_TOTAL""]&gt;=slider.value[0]) &amp; (df[""YEARLY_TOTAL""]&lt;=slider.value[1])]
    source.data={
        'User': current.User,
        'January':current.Jan,
        'February':current.Feb,
        'Total':current.YEARLY_TOTAL,}

slider=RangeSlider(title=""Yearly Total"", start=0, end=1000000, value=(0,5000), step=10000)
slider.on_change('value', lambda attr, old, new: update())

columns=[TableColumn(field=""User"", title=""User""),
    TableColumn(field=""Jan"", title=""January""), 
    TableColumn(field=""Feb"", title=""February""),
    TableColumn(field=""Total"", title=""Total"")]

data_table=DataTable(source=source, columns=columns, width=800)

controls=widgetbox(slider)
table=widgetbox(data_table)

curdoc().add_root(row(controls, table))

update()
</code></pre>

<p>The table renders in the browser, but there is no data in the table. Interestingly enough, if I save the pivoted dataframe to csv, and pass the csv to render the table (after commenting out the pandas transformations as they are no longer necessary), the datatable renders fine. What is happening?</p>
","6480215","","","","","2018-02-06 19:24:21","Bokeh server datatable renders no records from Oracle sql output (incl. panda transforms)","<python-3.x><pandas><bokeh>","1","0","","","","CC BY-SA 3.0","1"
"57506792","1","","","2019-08-15 08:19:31","","0","58","<p>I have a dataframe that I need to randomise in a very specific way with a particular rule, and I'm a bit lost. A simplified version is here:</p>

<pre><code>idx type    time
1   a   1
2   a   1
3   a   1
4   b   2
5   b   2
6   b   2
7   a   3
8   a   3
9   a   3
10  b   4
11  b   4
12  b   4
13  a   5
14  a   5
15  a   5
16  b   6
17  b   6
18  b   6
19  a   7
20  a   7
21  a   7
</code></pre>

<p>If we consider this as containing seven ""bunches"", I'd like to randomly shuffle by those bunches, i.e. retaining the time column. However, the constraint is that after shuffling, a particular bunch type (<code>a</code> or <code>b</code> in this case) cannot appear more than n (e.g. 2) times in a row. So an example correct result looks like this:</p>

<pre><code>idx type    time
21  a   7
20  a   7
19  a   7
7   a   3
8   a   3
9   a   3
17  b   6
16  b   6
18  b   6
6   b   2
5   b   2
4   b   2
2   a   1
3   a   1
1   a   1
14  a   5
13  a   5
15  a   5
12  b   4
11  b   4
10  b   4
</code></pre>

<p>I was thinking I could create a separate ""order"" array from 1 to 7 and <code>np.random.shuffle()</code> it, then sort the dataframe by <code>time</code> in that order, which will probably work - I can think of ways to do that part, but I'm especially struggling with the rule of restricting the number of repeats.</p>

<p>I know roughly that I should use a while loop, shuffle it in that way, loop over the frame and track the number of consecutive <code>type</code>s, if it exceeds my n then break out and start the while loop again until it completes without breaking out, in which case set a value to end the while loop. But this got so messy and didn't work.</p>

<p>Any ideas?</p>
","7728410","","","","","2019-08-15 11:43:22","pandas random shuffling dataframe with constraints","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"32825774","1","","","2015-09-28 14:41:49","","1","58","<p>I have the following snippet:</p>

<pre><code>import pandas as pd
pd.set_option('display.width', 165)
</code></pre>

<p>on my startup file:</p>

<pre><code>/Users/avazquez/.ipython/profile_default/startup/00-first.py
</code></pre>

<p>I can see that importing pandas worked (<code>pd</code> is already defined when I start IPython). However, the command <code>pd.set_option('display.width', 165)</code> didn't work (it does work though if I invoke it manually once IPython has started).</p>

<p>It could be a bug, but I wanted to check here just in case. How can I change my defaults for pandas for IPython?</p>

<p>This is with <code>IPython 4.0.0</code> on <code>Python 3.4.3 :: Anaconda 2.1.0 (x86_64)</code>. </p>
","283296","","","","","2015-10-01 18:40:31","IPython defaults for pandas","<python-3.x><pandas><ipython>","1","0","","","","CC BY-SA 3.0","1"
"57553191","1","57553402","","2019-08-19 08:42:05","","0","58","<p>I have a file which I read in as a string. In sublime the file looks like this:</p>

<pre><code>Filename
Dataset
Level
Duration
Accuracy
Speed Ratio
Completed
file_001.mp3
datasetname_here
value
00:09:29
0.00%
7.36x
2019-07-18
file_002.mp3
datasetname_here
value
00:22:01
...etc.
</code></pre>

<p>in Bash:</p>

<pre><code>['Filename\n', 'Dataset\n', 'Level\n', 'Duration\n', 'Accuracy\n', 'Speed Ratio\n', 'Completed\n', 'file_001.mp3\n', 'datasetname_here\n', 'value\n', '00:09:29\n', '0.00%\n', '7.36x\n', '2019-07-18\n', 'file_002.mp3\n', 'datasetname_here\n', 'L1\n', '00:20:01\n', ...etc.
</code></pre>

<p>I want to split this into a 7 column csv. As you can see, the values repeat every 8th line. I know I can use a for loop and modulus to read each line. I have done this successfully before. </p>

<p>How can I use pandas to read things into columns?</p>

<p>I don't know how to approach the Pandas library. I have looked at other examples and all seem to start with csv.</p>

<pre><code>import sys 

parser = argparse.ArgumentParser()
parser.add_argument('file' , help = ""this is the file you want to open"")

args = parser.parse_args()
print(""file name:"" , args.file)

with open(args.file , 'r') as word:
    print(word.readlines())  ###here is where i was making sure it read in properly

###here is where I will start to manipulate the data
</code></pre>

<p><strong>This is the Bash output:</strong></p>

<pre><code>['Filename\n', 'Dataset\n', 'Level\n', 'Duration\n', 'Accuracy\n', 'Speed Ratio\n', 'Completed\n', 'file_001.mp3\n', 'datasetname_here\n', 'value\n', '00:09:29\n', '0.00%\n', '7.36x\n', '2019-07-18\n', 'file_002.mp3\n', 'datasetname_here\n', 'L1\n', '00:20:01\n', ...]
</code></pre>
","5250909","","5770501","","2019-08-19 09:48:24","2019-08-19 09:48:24","sort one giant string into 7 columns","<python><python-3.x><pandas>","3","1","","","","CC BY-SA 4.0","1"
"56888627","1","","","2019-07-04 13:05:34","","0","57","<p>I am working on a JSON data of shape as below.Its a list of documents and i am showing the first one </p>

<pre><code>info=[
   {id: 34,
    log:[
         {ttp : 80,
          segm :[
                 {
                  dr: '2 hours'
                  room: 'F32'
                  },
                 {
                   dr: '48 hours',
                   room: 'G45'
                  }
                 ],
           },
           {
            ttp : 45,
            segm:[
                  {
                  dr: '4 hours'
                  room: 'F32'
                  },
                 {
                   dr: '8 hours',
                   room: 'G45'
                  }
             ]
            }
           ]
        },
        {document},
        {document}
 ]
</code></pre>

<p>I have created this very simple document just for understanding of data structure. </p>

<p>Now the problem is i want to access dr and room which are in segm. To access them i am using three nested loops first loop to iterate over info. Second loop to iterate over log and third loop on segm.</p>

<pre><code>for loop -&gt; info -&gt; for loop-&gt; log -&gt; for loop -&gt; segm
</code></pre>

<p>But i am compromising on performance here as the data is growing it's becoming slower.
I need a better solution (an optimized solution) for this that does not depend upon the data size. Also If we can avoid loops that would be better i think. </p>
","4654001","","","","","2019-07-04 13:05:34","avoid nested loops or optimize the performance","<python-3.x><pandas><numpy><optimization><itertools>","0","8","","","","CC BY-SA 4.0","1"
"57076955","1","","","2019-07-17 13:24:26","","0","57","<p>I am trying to scrape off titles of news articles from the newspaper website Khaleej times (<a href=""https://www.khaleejtimes.com"" rel=""nofollow noreferrer"">https://www.khaleejtimes.com</a>) and store them in a CSV file</p>

<p>The order is as follows:
I scrape them (theoretically), 
 Print them out (redundant and a waste of time, I know),
 Append them to a list,
 Print out the list (just to check if the list actually has anything),
 Conver the list to a Pandas Data Frame,
 and finally, convert the Data Frame to a CSV file </p>

<p>However, every time I run the code, nothing gets appended to the list thus leaving my DataFrame and CSV file empty. Sometimes one or two article titles appear, but most of the time it is empty. </p>

<p>Note: My only interest is article titles, and not the text</p>

<p>I've got more questions about this task, but I'll save them for a separate post</p>

<p>The one thing I did try was to remove HTTPS from the URL and make it HTTP as well as getting rid of www. That didn't really do much if anything at all</p>

<pre><code>from newspaper import Article
import newspaper
import csv
import pandas as pd
url = 'http://khaleejtimes.com'

a = newspaper.build(url)

titles = []  # initialize an empty list of titles

for article in a.articles:
    article.download()  # download the article
    article.parse()  # parse the article
    # print(article.title) print the title
    titles.append(article.title)  # append the title to the list


for i in range(len(titles)):  # initialize a for loop 
    print(titles[i])  # to print the titles within the titles list

TitleDF = pd.DataFrame({'Title': titles})  # convert the titles list to a 
pandas DataFrame
print(TitleDF)  # print the DataFrame
TitleDF.to_csv('KhaleejArticles.csv', encoding='utf-8', sep=""\n"")  
# Convert the DataFrame to csv format
</code></pre>

<p>I expect the output of print(TitleDF) to be two columns: one with the indices (0, 1, 2... n) and Titles (article titles exclusively). </p>

<p>However, on several occasions, it was either several titles repeated, very few titles, and for the most no titles at all.</p>

<p>What am I doing wrong? This is not a syntax error as far as I can tell</p>
","7247603","","","","","2019-07-17 13:24:26","Why is my data frame empty even after I append all the article titles to the list and convert the list to a DF?","<python-3.x><pandas><csv><web-scraping><python-newspaper>","0","3","","","","CC BY-SA 4.0","1"
"57659544","1","57670121","","2019-08-26 14:07:56","","1","57","<p>Following my initial question <a href=""https://stackoverflow.com/questions/57657119/apply-a-function-on-a-column-of-a-dataframe-depending-on-the-value-of-another-co"">here</a> with very constructive answers, I want to customize it for my original dataframe.</p>

<p>The dataframe upon which I need to make changes has accrued from: <code>nightframe=new_df.between_time('22:00','04:00')</code> and the first few rows look like this:</p>

<pre><code>                           date      time  diffs criteria 1
datetime                                                   
2018-01-05 22:00:00  2018-01-05  22:00:00    0.0       True
2018-01-05 23:00:00  2018-01-05  23:00:00   -1.0      False
2018-01-06 00:00:00  2018-01-06  00:00:00    1.0       True
2018-01-06 01:00:00  2018-01-06  01:00:00   -2.0      False
2018-01-06 02:00:00  2018-01-06  02:00:00   -1.0       True
2018-01-06 03:00:00  2018-01-06  03:00:00    1.0       True
2018-01-06 04:00:00  2018-01-06  04:00:00    1.0      False
2018-01-06 22:00:00  2018-01-06  22:00:00   -1.0       True
</code></pre>

<p>I need to assign the date to the previous date if the time is from 00:00 to 04:00. I have tried these codes for my condition and they do not work:</p>

<pre><code>condition = nightframe['time'].isin([0,1,2,3,4])
condition = nightframe['time'].dt.time.isin(\
                      ['00:00','01:00','02:00','03:00','04:00'])
condition = nightframe['time'](['00:00','01:00','02:00','03:00','04:00'])
</code></pre>

<p>If the condition works, I suppose that the dataframe I need can be given from:  <code>nightframe['date']=np.where(condition, nightframe['date']-pd.Timedelta('1 day'), nightframe['date'])</code> and should give this view:</p>

<pre><code>                           date      time  diffs criteria 1
datetime                                                   
2018-01-05 22:00:00  2018-01-05  22:00:00    0.0       True
2018-01-05 23:00:00  2018-01-05  23:00:00   -1.0      False
2018-01-06 00:00:00  2018-01-05  00:00:00    1.0       True
2018-01-06 01:00:00  2018-01-05  01:00:00   -2.0      False
2018-01-06 02:00:00  2018-01-05  02:00:00   -1.0       True
2018-01-06 03:00:00  2018-01-05  03:00:00    1.0       True
2018-01-06 04:00:00  2018-01-05  04:00:00    1.0      False
2018-01-06 22:00:00  2018-01-06  22:00:00   -1.0       True
2018-01-06 23:00:00  2018-01-06  23:00:00    1.0       True
2018-01-07 00:00:00  2018-01-06  00:00:00    0.0      False
2018-01-07 01:00:00  2018-01-06  01:00:00    1.0       True
2018-01-07 02:00:00  2018-01-06  02:00:00    0.0      False
2018-01-07 03:00:00  2018-01-06  03:00:00   -1.0      False
2018-01-07 04:00:00  2018-01-06  04:00:00    1.0       True
2018-01-07 22:00:00  2018-01-07  22:00:00    1.0       True
</code></pre>

<p>Note: the 'datetime' is the index of my dataframe and the types of the columns of nightframe are:</p>

<pre><code>print(nightframe.dtypes)
date           object
time           object
diffs         float64
criteria 1     object
dtype: object

print(nightframe.index.dtype)
datetime64[ns]
</code></pre>
","10750541","","10750541","","2019-08-27 07:57:17","2019-08-27 07:58:56","Change the date column when the time column satisfies a certain condition","<python-3.x><pandas><datetime><timedelta>","3","2","0","","","CC BY-SA 4.0","1"
"40519308","1","40519915","","2016-11-10 02:59:49","","3","56","<p>This is a weird one: I have 3 dataframes, ""prov_data"" with contains a provider id and counts on regions and categories (ie. how many times that provider interacted with those regions and categories). </p>

<pre><code>prov_data = DataFrame({'aprov_id':[1122,3344,5566,7788],'prov_region_1':[0,0,4,0],'prov_region_2':[2,0,0,0],
                  'prov_region_3':[0,1,0,1],'prov_cat_1':[0,2,0,0],'prov_cat_2':[1,0,3,0],'prov_cat_3':[0,0,0,4],
                   'prov_cat_4':[0,3,0,0]})
</code></pre>

<p><a href=""https://i.stack.imgur.com/4pjhb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4pjhb.png"" alt=""enter image description here""></a> </p>

<p>""tender_data"" which contains the same but for tenders.</p>

<pre><code>tender_data = DataFrame({'atender_id':['AA12','BB33','CC45'],
                     'ten_region_1':[0,0,1,],'ten_region_2':[0,1,0],
                  'ten_region_3':[1,1,0],'ten_cat_1':[1,0,0],
                     'ten_cat_2':[0,1,0],'ten_cat_3':[0,1,0],
                   'ten_cat_4':[0,0,1]})
</code></pre>

<p><a href=""https://i.stack.imgur.com/Iy1cA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Iy1cA.png"" alt=""enter image description here""></a></p>

<p>And finally a ""no_match"" DF wich contains forbidden matches between provider and tender.</p>

<pre><code>no_match = DataFrame({ 'prov_id':[1122,3344,5566], 
            'tender_id':['AA12','BB33','CC45']})
</code></pre>

<p><a href=""https://i.stack.imgur.com/SkhVg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SkhVg.png"" alt=""enter image description here""></a></p>

<p>I need to do the following: create a new df that will append the rows of the prov_data &amp; tender_data DataFrames if they (1) match one or more categories (ie the same category is > 0) AND (2) match one or more regions AND (3) are not on the no_match list.</p>

<p>So that would give me this DF:</p>

<pre><code>df = DataFrame({'aprov_id':[1122,3344,7788],'prov_region_1':[0,0,0],'prov_region_2':[2,0,0],
                  'prov_region_3':[0,1,1],'prov_cat_1':[0,2,0],'prov_cat_2':[1,0,0],'prov_cat_3':[0,0,4],
                   'prov_cat_4':[0,3,0], 'atender_id':['BB33','AA12','BB33'],
                     'ten_region_1':[0,0,0],'ten_region_2':[1,0,1],
                  'ten_region_3':[1,1,1],'ten_cat_1':[0,1,0],
                     'ten_cat_2':[1,0,1],'ten_cat_3':[1,0,1],
                   'ten_cat_4':[0,0,0]})
</code></pre>
","6896217","","","","","2016-11-10 07:54:32","Trying to merge DataFrames with many conditions","<python><python-3.x><pandas><dataframe>","2","0","","","","CC BY-SA 3.0","1"
"56584672","1","","","2019-06-13 16:13:54","","0","56","<p>I'm trying to create a scatter plot using column data from a dataframe: when I run a one-line command in the shell, I am able to achieve the desired result (the values in a column of the dataframe define the resulting plot points' size). However, using similar code in a script (in the same environment, on the same machine) does not have the same effect.</p>

<p>I can achieve the desired results in the shell:</p>

<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; import matplotlib
&gt;&gt;&gt; matplotlib.use('Agg')
&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; df = pd.read_csv('CTRL_to_M5.csv', sep=',')
&gt;&gt;&gt; df.reset_index().plot.scatter('index','logFC',s=df['BenjHoch'],c='FCGroup',colormap='jet')
&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f3ffa238438&gt; 
</code></pre>

<p>When translating these commands and running the script I cannot achieve the same results:</p>

<pre class=""lang-py prettyprint-override""><code>def dictToPlots(formattedTopTagsDict):
    """"""Docstring description of the dictToPlots function.
    """"""
    for tabName in formattedTopTagsDict.keys():
        if tabName != 'Experimental Design':

            indexedDF = formattedTopTagsDict[tabName].reset_index(drop=True)

            indexedDF.reset_index().plot.scatter('index',
                                                 'logFC',
                                                 s=indexedDF['BenjHoch'],
                                                 c='FCGroup',
                                                 colormap='jet')
            plt.savefig(tabName+'_COLOR_SHAPE.pdf')

    return None
</code></pre>

<p>Traceback:</p>

<pre><code>  File ""my_script.py"", line 916, in &lt;module&gt;
    main()
  File ""my_script.py"", line 906, in main
    exactTest(reads, expDes, parseArgs.numTopTags, parseArgs.refGenome)
  File ""my_script.py"", line 386, in exactTest
    dictToPlots(formattedTopTagsDict, 'genes')
  File ""my_script.py"", line 832, in dictToPlots
    colormap='jet')
  File ""/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py"", line 3516, in scatter
    return self(kind='scatter', x=x, y=y, c=c, s=s, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py"", line 2942, in __call__
    sort_columns=sort_columns, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py"", line 1973, in plot_frame
    **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py"", line 1801, in _plot
    plot_obj.generate()
  File ""/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py"", line 251, in generate
    self._make_plot()
  File ""/usr/local/lib/python3.6/dist-packages/pandas/plotting/_core.py"", line 894, in _make_plot
    label=label, cmap=cmap, **self.kwds)
  File ""/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py"", line 1717, in inner
    return func(ax, *args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py"", line 4032, in scatter
    alpha=alpha
  File ""/usr/local/lib/python3.6/dist-packages/matplotlib/collections.py"", line 880, in __init__
    self.set_sizes(sizes)
  File ""/usr/local/lib/python3.6/dist-packages/matplotlib/collections.py"", line 853, in set_sizes
    scale = np.sqrt(self._sizes) * dpi / 72.0 * self._factor
AttributeError: 'int' object has no attribute 'sqrt'
</code></pre>
","11546183","","11546183","","2019-06-13 16:34:31","2019-06-13 16:34:31","pandas.DataFrame.plot.scatter: passing dataframe column to define size works in shell but not in code","<python-3.x><pandas><matplotlib>","0","8","","","","CC BY-SA 4.0","1"
"57742659","1","","","2019-09-01 01:41:43","","0","56","<p>I'm adding a new column to a dataframe it includes bins and labels.</p>

<p>It won't accept the column:</p>

<pre><code># set up bins
bins = [0, 585, 615, 645, 675]
group_name = ['0 - 584', '585 - 614', '615 - 644', '645 +']

# add a new column: spending range
score_by_school_spending[""Spending Range""] = pd.cut(score_by_school_spending[""Per Student Budget""],bins,labels=group_name)
score_by_school_spending
</code></pre>

<h3>the error i received is as followed</h3>

<hr>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-56-7e730a291b70&gt; in &lt;module&gt;
      1 # add a new column: spending range
----&gt; 2 score_by_school_spending[""Spending Range""]= pd.cut(score_by_school_spending[""Per Student Budget""],bins,labels=group_name)
      3 score_by_school_spending

~\Anaconda3\lib\site-packages\pandas\core\reshape\tile.py in cut(x, bins, right, labels, retbins, precision, include_lowest, duplicates)
    239                               include_lowest=include_lowest,
    240                               dtype=dtype,
--&gt; 241                               duplicates=duplicates)
    242 
    243     return _postprocess_for_cut(fac, bins, retbins, x_is_series,

~\Anaconda3\lib\site-packages\pandas\core\reshape\tile.py in _bins_to_cuts(x, bins, right, labels, precision, include_lowest, dtype, duplicates)
    342 
    343     side = 'left' if right else 'right'
--&gt; 344     ids = ensure_int64(bins.searchsorted(x, side=side))
    345 
    346     if include_lowest:

TypeError: '&lt;' not supported between instances of 'int' and 'str'
</code></pre>

<p>everywhere else it works but here its a wall</p>
","11721215","","6925185","","2019-09-01 16:52:30","2019-09-01 16:52:30","I'm getting an error while adding a new column with set up bins --TypeError: '<' not supported between instances of 'int' and 'str'","<python-3.x><pandas>","1","2","","","","CC BY-SA 4.0","1"
"57332119","1","","","2019-08-02 19:01:11","","0","55","<p>I have dataframe cannot show the whole data(confidential) which have a longitudes &amp; latitudes points all  around the world but I need to filter for just one country</p>

<p>suppose this is data</p>

<p><a href=""https://i.stack.imgur.com/7af6n.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7af6n.png"" alt=""enter image description here""></a></p>

<p>Now I have to filter rows data for a specific country which have co-ordinates ranges this</p>

<pre><code>`latitude` :  8°4′ N to 37°6′ N
`longitude` : 68°7′ E to 97°25′ E
</code></pre>

<p>so I tried this</p>

<pre><code>df_filtered = df[38 &gt; df['LATITUDE'] &gt; 8 and 98 &gt; df['LATITUDE'] &gt; 68]  
</code></pre>

<p>but this is giving this weird error</p>

<blockquote>
  <p>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</p>
</blockquote>
","6482145","","","","","2019-08-02 19:15:51","Multi condition filtering in pandas dataframe","<python><python-3.x><pandas><data-science>","1","4","","2019-08-02 19:03:43","","CC BY-SA 4.0","1"
"56853397","1","","","2019-07-02 13:17:12","","0","55","<p>I'm trying to make a web API to save a csv into an template excel file. I have made a function which works perfect when I perform it. But once I call it from the function
(def on_get(self, req, resp): )
the file is getting corrupted. What are the changes I can make to prevent the file from getting corrupt?</p>

<p>Server Code:</p>

<pre><code>class Test:
def on_get(self, req, resp):
    ct.save_df_into_excel(df_template, file_location, list_of_code_columns, new_location)
    """"""Handles GET requests""""""
    response = {
        'status': 'success'
    }
    resp.media = response
</code></pre>

<p>function:</p>

<pre><code>def save_df_into_excel(df_template, file_location, list_of_code_columns, new_location = None):
if new_location == None:
    new_location = file_location
if df_template.columns.nlevels &gt; 1:
    df_template.columns = df_template.columns.droplevel()

df_new_template = df_template.reindex(columns=list_of_code_columns)
workbook = openpyxl.load_workbook(file_location)
writer = pd.ExcelWriter(file_location, engine='openpyxl')
writer.book = workbook
writer.sheets = dict((ws.title, ws) for ws in workbook.worksheets)
worksheet = workbook.active
df_new_template.to_excel(writer, sheet_name=worksheet.title,
                     header=None, startrow=2, index=False)
writer.book.save(new_location)
</code></pre>
","8875463","","8875463","","2019-07-02 13:38:30","2020-06-27 21:41:33","When I make an falcon API my excel is corrupted","<python><python-3.x><pandas><openpyxl><falcon>","1","1","0","","","CC BY-SA 4.0","1"
"57295458","1","57296857","","2019-07-31 16:39:59","","0","55","<p>I m using Pandas with Python 3.6.
My script loads an excel file which contains multiple worksheets.
In some sheets, rows have either numeric values, or string values on two columns.
After running the script, the numeric values are splitted on two columns, but I cannot duplicate on the second column the string value of the first column.</p>

<p>For the numeric values, I am using :</p>

<pre><code>df=df[['ID_Test']].join(df[pd_column].str.split(':',expand=True)).rename(columns={0: pd_column, 1: ''})
</code></pre>

<p>The second column for the string values remains blank (None), and must be updated with the same value than the first column</p>

<p>If I use : 
<code>df[''] = df[pd_column]</code>, the second column [''] is entirely updated with the values of the first one (overwriting numeric values), and I did not find any solution specific to my concern.</p>

<p>Data Input:</p>

<pre><code>ID_Test_1   Test_1
Indicator_1 AAAAAAA
Indicator_2 2.745 : 2.03
Indicator_3 BBBBBBBB
Indicator_4 -5.013 : -5.013
Indicator_5 CCCCCCCC
</code></pre>

<p>Actual Output : (Wrong)</p>

<pre><code>ID_Test_1   Test_1  
Indicator_1 AAAAAAA      None
Indicator_2 2.745        2.03
Indicator_3 BBBBBBBB     None
Indicator_4 -5.013       -5.013
Indicator_5 CCCCCCCCC    None
</code></pre>

<p>Desired Output :</p>

<pre><code>ID_Test_1   Test_1  
Indicator_1 AAAAAAA      AAAAAAA
Indicator_2 2.745        2.03
Indicator_3 BBBBBBBB     BBBBBBBB
Indicator_4 -5.013       -5.013
Indicator_5 CCCCCCCCC    CCCCCCCCC
</code></pre>

<p>The second column must not have a label</p>
","10710874","","10710874","","2019-07-31 17:12:39","2019-07-31 18:24:41","how to fill an empty dataframe column in pandas python","<python-3.x><pandas>","1","3","","","","CC BY-SA 4.0","1"
"49613287","1","","","2018-04-02 14:36:47","","1","55","<p>I was wondering what is the common and the better way to manage data, that I recieve from MySQL.</p>

<pre><code>db = MySQLdb.connect(host=""####"", user=""####"", passwd=""####"", db=""####"", charset='utf8')
db.query(sql)
result = db.use_result()
</code></pre>

<p>I use use_result() because data is pretty big (about 9000000 lines) and I don't have much RAM.</p>

<p>After that I'm working with this data line per line by</p>

<pre><code>line = result.fetch_row(maxrows=1, how=1)[0]
#creating objects from lines 
</code></pre>

<p>But I found another way to manage MySQL data by using a pandas DataFrame</p>

<pre><code>df = pd.read_sql_query('SQL QUERY', con= db)
</code></pre>

<p>So the question is simple - what is better?</p>

<p>P.S. I can't really test what is faster on my cheap laptop, because the execution time this program is fluctuating from 170 to 250 sec. </p>
","9442469","","","","","2018-04-02 14:40:08","What is a fastest way to manage MySQL data? pandas vs fetch_row","<python><mysql><python-3.x><pandas><mysql-python>","1","0","","","","CC BY-SA 3.0","1"
"57230109","1","57230230","","2019-07-27 07:33:37","","2","55","<p>I have data like below:</p>

<pre><code>   id  attribute1  attribute2  attribute3  attribute4  attribute5   new  otherattri
0   1           1           0           0           0           0     1           2
1   2           1           1           1           1           0  1234          12
2   3           0           0           0           0           1     5          21
3   4           1           0           1           0           0    13          93
4   5           0           0           0           1           0     4           2
</code></pre>

<p>The column called <code>new</code> is what I want to calculate. What should I do to achieve this in pandas?  The new column simply combines the previous 5 columns in the dataframe into a new column, e.g. for line 4, attribute1 == 1 and attribute3 == 1, so the new column for line4 is 13.<br>
Thanks in advance!</p>
","9710270","","6395052","","2019-07-27 11:47:18","2019-07-27 11:47:18","how to get multiple column indexes that satisfy a condition","<python><python-3.x><pandas><numpy>","2","2","","","","CC BY-SA 4.0","1"
"57225758","1","57238889","","2019-07-26 19:04:45","","2","55","<p>In Pandas we can generate a correlation matrix with <a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html"" rel=""nofollow noreferrer""><code>.corr()</code></a>. My question is quite simple: is the column order of the original dataframe preserved? From my testing it seems the case, but I want to be sure.</p>

<p>I am asking because I am on Python 3.7.3 where dictionaries maintain insertion order. I don't know if the current question is related to that, but if Pandas uses dictionaries behind the scenes, then it might very well be that <code>corr()</code> is ordered as expected in Python 3.6+ but not in lower versions.</p>
","1150683","","","","","2019-07-28 07:41:15","Is the ordered of df.corr() columns ensured","<python><python-3.x><pandas><correlation>","1","0","","","","CC BY-SA 4.0","1"
"41105595","1","41107258","","2016-12-12 16:47:44","","0","55","<p>I have dataframe (df) of 12 rows x 5 columns. I sample 1 row from each label and create a new dataframe (df1) of 3 rows x 5 columns. I need that the next time I sample more rows from df I will not choose the same ones that are already in df1. So how can I delete the already sampled rows from df? </p>

<pre><code>import pandas as pd
import numpy as np

# 12x5
df = pd.DataFrame(np.random.rand(12, 5))
label=np.array([1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3])
df['label'] = label


#3x5
df1 = pd.concat(g.sample(1) for idx, g in df.groupby('label'))


#My attempt. It should be a 9x5 dataframe
df2 = pd.concat(f.drop(idx) for idx, f in df1.groupby('label'))
</code></pre>

<p>df</p>

<p><a href=""https://i.stack.imgur.com/sT34j.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sT34j.png"" alt=""enter image description here""></a></p>

<p>df1</p>

<p><a href=""https://i.stack.imgur.com/cinMk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cinMk.png"" alt=""enter image description here""></a></p>

<p>df2</p>

<p><a href=""https://i.stack.imgur.com/nO936.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nO936.jpg"" alt=""enter image description here""></a></p>
","6106842","","2285236","","2016-12-12 18:39:24","2016-12-12 18:40:37","How to delete the randomly sampled rows of a dataframe, to avoid sampling them again?","<python><python-3.x><pandas><numpy>","1","0","","","","CC BY-SA 3.0","1"
"41314967","1","41315006","","2016-12-24 16:25:40","","4","55","<p>I have a dataframe like this</p>

<pre><code>import pandas as pd
df = pd.DataFrame({'item': [1, 1,2,2], 
                   'user': [1,2,2,1], 
                   'appraisal': [4,2,1,3],
                   'feedback' : ['good', 'bad', 'bad', 'well']
                  })
names = ['item', 'user',  'appraisal', 'feedback' ]
df = df[names]
df
</code></pre>

<p>I want to get a dataframe as below</p>

<pre><code>  item  appraisal   feedback
0   1      3       good bad
1   2      2       bad well
</code></pre>

<p>Where 'item' is 'item' from df, 'appraisal' is average of df.appraisal for items and 'feedback' is combined cells from df.feedback for items
I can get two variales </p>

<pre><code>df1 = df.groupby('item')['appraisal'].mean()
</code></pre>

<p>But how to combine text cells? I can make pivot_table item / user and ""feedback"" as a value and then add cells user1+user2.....
but the real dataset has many unique values and i don't think it's a best decision
thanx for help</p>
","5998425","","5998425","","2018-12-09 21:05:28","2018-12-09 21:05:28","How to combine multiple cells into a single text сell","<python-3.x><pandas><text>","1","0","","","","CC BY-SA 3.0","1"
"57691107","1","57691178","","2019-08-28 11:14:38","","2","55","<p>I have a data frame with search traffic and write a code to get the first and last request per day and the time difference between it.</p>

<pre><code>df = pd.read_csv(""Testordner2/""+i, parse_dates=True)
df['new_time'] = pd.to_datetime(df['new_time'])
df['dates']= df['new_time'].dt.date
df['time'] = df['new_time'].dt.time
out = df.groupby(df['dates']).agg({'time': ['min', 'max']}) \
                                 .stack(level=0).droplevel(1)
out['min_as_time_format'] = pd.to_datetime(out['min'], format=""%H:%M:%S"")
out['max_as_time_format'] = pd.to_datetime(out['max'], format=""%H:%M:%S"")
out['wh'] = out['max_as_time_format'] - out['min_as_time_format']
out['wh'].astype(str).str[-18:-10]
</code></pre>

<p>This works well and I get a dataframe <code>out</code> that looks like:</p>

<pre><code>                 min       max       wh
dates                                  
2005-09-06  07:41:18  21:59:57 14:18:39
2005-09-12  14:49:22  14:49:22 00:00:00
2005-09-19  11:08:56  11:24:05 00:15:09
2005-09-21  21:19:21  21:20:15 00:00:54
2005-09-22  19:41:52  19:41:52 00:00:00
2005-10-13  11:22:07  21:05:41 09:43:34
2005-11-22  11:53:12  21:21:22 09:28:10
2005-11-23  00:07:01  14:08:50 14:01:49
2005-11-30  13:42:48  23:59:19 10:16:31
2005-12-01  00:05:16  10:24:12 10:18:56
2005-12-21  17:38:43  19:26:03 01:47:20
2005-12-22  09:20:07  11:25:40 02:05:33
2006-01-23  07:46:20  08:01:52 00:15:32
2006-04-27  16:27:54  19:29:52 03:01:58
2006-05-11  12:48:34  23:10:44 10:22:10
2006-05-15  10:14:59  22:28:12 12:13:13
2006-05-16  01:14:07  23:55:51 22:41:44
2006-05-17  01:12:45  23:57:56 22:45:11
2006-05-18  02:42:08  21:48:49 19:06:41
2006-05-22  00:00:29  23:07:12 23:06:43
2006-05-23  02:14:55  22:35:04 20:20:09
2006-05-24  11:53:08  21:25:39 09:32:31
2006-05-25  01:20:38  22:14:55 20:54:17
2006-05-29  01:34:09  23:53:33 22:19:24
</code></pre>

<p>The problem is I need a column <code>dates</code> in my dataframe <code>out</code> but this does not exist. I don't know why the column name ""dates"" is not in the same height as the other column names ""min"", ""max"" and ""wh"".. 
I never had this problem before by using groupby but never use the <code>egg</code>function before. Do not know if this is a reason for the problem ..</p>

<p>And the second question: I want to build the mean of the workhours in <code>wh</code> per month.
I use:</p>

<pre><code>out['month']= pd.PeriodIndex(out.dates, freq='M')
out2=out.groupby('month')['wh'].mean().reset_index(name='wh2')
</code></pre>

<p>But the values in <code>wh</code> are no numeric data, so I can't use <code>mean</code>.
How can I convert the whole column?</p>
","11988801","","11988801","","2019-08-28 11:37:24","2019-08-29 14:55:19","How to keep column name using group-by function in python?","<python><python-3.x><pandas><dataframe><group-by>","1","3","","2019-08-28 11:19:30","","CC BY-SA 4.0","1"
"57661475","1","57666892","","2019-08-26 16:14:39","","0","54","<p>I am trying to balance my highly imbalanced data using ADASYN library. 
After I balance my data I have to join the features and target label numpy arrays into one single data frame.</p>

<p>Here is my Python code for balancing data:</p>

<pre><code>from imblearn.over_sampling import ADASYN
ada = ADASYN()
# X is feature set and y is the label
X_resampled, y_resampled = ada.fit_sample(X, y)
# Add X_resampled, y_resampled into one dataframe
</code></pre>

<p>How would I do this?</p>
","493829","","","","","2019-08-27 02:12:10","joining the ADASYN balanced features and target label","<python><python-3.x><pandas><numpy>","1","7","","","","CC BY-SA 4.0","1"
"56803435","1","56803508","","2019-06-28 08:26:26","","0","54","<p>I have a dataframe which can be generated from the code given below</p>

<pre><code>df = pd.DataFrame({'person_id' :[1,2,3],'date1': ['12/31/2007','11/25/2009','10/06/2005'],'date1derived':[0,0,0],'val1':[2,4,6],'date2': ['12/31/2017','11/25/2019','10/06/2015'],'date2derived':[0,0,0],'val2':[1,3,5],'date3':['12/31/2027','11/25/2029','10/06/2025'],'date3derived':[0,0,0],'val3':[7,9,11]})
</code></pre>

<p>The dataframe looks like as shown below</p>

<p><a href=""https://i.stack.imgur.com/TVhUN.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TVhUN.png"" alt=""enter image description here""></a></p>

<p>I would like to retain the rows of each person as seperate rows and not as columns like shown in screenshot above.In addition, I want the date1derived,date2derived columns to be dropped. </p>

<p>I did try below approaches but they didn't provide the expected output</p>

<pre><code>1) df.set_index(['person_id']).stack()/unstack
2) df.set_index(['person_id','date1','date2','date3']).stack()/unstack()
3) df.set_index('person_id').unstack()/stack
</code></pre>

<p>How can I get an output to be like this? I have more than 600 columns, so I don't think writing the column names manually would help me.</p>

<p><a href=""https://i.stack.imgur.com/vVFF1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vVFF1.png"" alt=""enter image description here""></a></p>
","10829044","","10829044","","2019-06-28 11:29:34","2019-06-28 11:29:34","How to Transpose multiple columns into multiple rows but retain primary keys as is using Pandas","<python><python-3.x><pandas><dataframe><transpose>","2","0","2","","","CC BY-SA 4.0","1"
"57371855","1","","","2019-08-06 08:26:20","","0","54","<p>I'm searching for solution how to replace several strings within one cell inside of data frame in Python-Pandas.</p>

<p>Each column has unique elements to be replaced based on legend that is already defined.</p>

<p>I've already find solution how to replace values within column, but result that I get replace only one string at time, and removing other. For example cell value: AA, BB, CC
legend: AA - Level 1, BB - Level 2, CC - Level 3, DD - Level 4
result: Level 1.</p>

<pre class=""lang-py prettyprint-override""><code>Data set:
Field Name | Category 1 | Category 2
Test1        AA BB CC      LD DD
Test2        BB CC         DD
Test3        AA            LD
Test4        AA BB DD      LD DD

Legend:
AA - Level 1, BB - Level 2, CC - Level 3, DD - Level 4
LD - High, DD - Low
</code></pre>

<p>I expect result to be combined with one cell, for example: Level 1; Level 2 while cell value was AA, BB</p>
","11442973","","9081267","","2019-08-06 08:26:43","2019-08-06 09:17:39","Replacing multiple value within cell in dataframe - Python/Pandas","<python-3.x><pandas><dataframe>","1","6","","","","CC BY-SA 4.0","1"
"57664811","1","","","2019-08-26 20:53:13","","0","54","<p>I'm working with a large flight delay dataset trying to predict the flight delay based on multiple new features. Based on a plane's tailnumber, I want to count the number of flights and sum the total airtime the plane has done in the past X (to be specified) hours/days to create a new ""usage"" variable. </p>

<p>Example of data (excluded airtime:</p>

<pre><code>ID                     tail_num deptimestamp     dep_delay distance air_time
2018-11-13-1659_UA2379 N14118    13/11/2018 16:59       -3   2425    334
2018-11-09-180_UA275   N13138    09/11/2018 18:00       -3   2454    326
2018-06-04-1420_9E3289 N304PQ    04/06/2018 14:20       -2    866    119
2018-09-29-1355_WN3583 N8557Q    29/09/2018 13:55       -5    762    108
2018-05-03-815_DL2324  N817DN    03/05/2018 08:15        0   1069    138
2018-01-12-1850_NK347  N635NK    12/01/2018 18:50      100    563     95
2018-09-16-1340_OO4721 N242SY    16/09/2018 13:40       -3    335     61
2018-06-06-1458_DL2935 N351NB    06/06/2018 14:58        1    187     34
2018-06-25-1030_B61    N965JB    25/06/2018 10:30       48   1069    143
2018-12-06-1215_MQ3617 N812AE    06/12/2018 12:15       -9    427     76
</code></pre>

<p>Example output for give = 'all' (not based on example data):</p>

<pre><code>2018-12-31-2240_B61443    (1, 152.0, 1076.0, 18.0)
</code></pre>

<p>I've written a function to be applied to each row that filters the dataframe for flights with the same tail number and within the specified time frame and then gives back either the number of flights/total airtime or a dataframe containing the flights in question. It works but take a long time (around 3 hours calculating for a subset of 400k flights but filtering the entire dataset of over 7m rows). Is there a way to speed this up?</p>

<pre class=""lang-py prettyprint-override""><code>
def flightsbefore(ID, 
                  give = 'number',
                  direction = 'before',
                  seconds = 0, 
                  minutes = 0, 
                  hours = 0, 
                  days = 0, 
                  weeks = 0, 
                  months = 0, 
                  years = 0):

    """""" Takes the ID of a flight and a time unit to return the flights of that plane within that timeframe""""""

    tail_num = dfallcities.loc[ID,'tail_num']
    date = dfallcities.loc[ID].deptimestamp

    #dfallcities1 = dfallcities[(dfallcities.a != -1) &amp; (dfallcities.b != -1)]

    if direction == 'before':
        timeframe = dfallcities.loc[ID].deptimestamp - datetime.timedelta(seconds = seconds, 
                                                        minutes = minutes, 
                                                        hours = hours, 
                                                        days = days, 
                                                        weeks = weeks)

        output = dfallcities[(dfallcities.tail_num == tail_num) &amp; \
                    (dfallcities.deptimestamp &gt;= timeframe) &amp; \
                    (dfallcities.deptimestamp &lt; date)]

    else:
        timeframe = dfallcities.loc[ID].deptimestamp + datetime.timedelta(seconds = seconds, 
                                                        minutes = minutes, 
                                                        hours = hours, 
                                                        days = days, 
                                                        weeks = weeks)

        output = dfallcities[(dfallcities.tail_num == tail_num) &amp; 
                             (dfallcities.depTimestamp &lt;= timeframe) &amp; 
                             (dfallcities.deptimestamp &gt;= date)]

    if give == 'number':
        return output.shape[0]

    elif give == 'all':

        if output.empty:
            prev_delay = 0

        else: 
            prev_delay = np.max((output['dep_delay'].iloc[-1],0))


        return (output.shape[0], output['air_time'].sum(),output['distance'].sum(), prev_delay)

    elif give == 'flights':
        return output.sort_values('deptimestamp')
    else:
        raise ValueError(""give must be one of [number, all, flights]"")
</code></pre>

<p>No errors but simply very slow</p>
","11345137","","","","","2019-08-26 20:53:13","Efficient way to count/sum rows in dataframe based on conditions","<python><python-3.x><pandas><dataframe><apply>","0","4","1","","","CC BY-SA 4.0","1"
"57074587","1","","","2019-07-17 11:15:39","","1","54","<p>I need to create a data frame for 100 customer_id along with their expenses for each day starting from 1st June 2019 to 31st August 2019. I have customer id already in a list and dates as well in a list. How to make a data frame in the format shown.      </p>

<pre><code>CustomerID  TrxnDate

1        1-Jun-19
1        2-Jun-19
1        3-Jun-19
1        Upto....
1        31-Aug-19
2        1-Jun-19
2        2-Jun-19
2        3-Jun-19
2        Upto....
2        31-Aug-19
and so on for other 100 customer id
</code></pre>

<p>I already have customer_id data frame using pandas function now i need to map each customer_id with the date ie assume we have customer id as 1 then 1 should have all dates from 1st June 2019 to 31 aug 2019 and then customerId 2 should have the same dates. Please see the data frame required.</p>
","9337568","","9337568","","2019-07-17 11:20:24","2019-07-17 13:52:06","Data frame formation","<python><python-3.x><pandas><data-processing>","1","3","","","","CC BY-SA 4.0","1"
"57505034","1","57505261","","2019-08-15 04:59:55","","0","54","<p>I have defined a list that reads the contents of a number of files and stores all of them.
How do I create a dataframe, with each filename in a row, and the corresponding columns count the occurrence of each word and output it.</p>

<p>For the sake of example, assume this is all well-defined (but I can provide original code if needed):</p>

<pre><code>#define list
words = [ file1_contents, file2_contents ]

file1_contents = ""string with dogs, cats and my pet sea turtle that lives in my box with my other turtles.""
file2_contents = ""another string about my squirrel, box turtle (who lives in the sea), but not my cat or dog"".

filter_words = [""cat"", ""dog"", ""box turtle"", ""sea horse""]
</code></pre>

<p>Output would be something like this:</p>

<pre><code>output = {'file1'{'cat': 1, 'dog':1, 'box turtle': 1, 'sea horse': 0}, 'file2'{ ...}} 

</code></pre>

<p>I have attached an image of my end goal. I am just beginning to use python, so I'm not too sure about what package/module I would use here? I know pandas lets you work with dataframes.</p>

<p>I had the idea of using <code>Counter</code> from <code>collections</code></p>

<pre><code>from collections import Counter
z = ['blue', 'red', 'blue', 'yellow', 'blue', 'red']
Counter(z)
Counter({'blue': 3, 'red': 2, 'yellow': 1})
</code></pre>

<p>BUT, here is where I am stuck. How do I organise a table in python which would look like the attached image?</p>

<p>Example output:
<a href=""https://i.stack.imgur.com/EFgQv.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EFgQv.jpg"" alt=""enter image description here""></a></p>
","6571276","","","","","2019-08-16 02:53:15","Is there a module that can count occurrences of a list of strings in Python?","<python-3.x><pandas><nltk><counter>","3","0","","","","CC BY-SA 4.0","1"
"56907552","1","56907666","","2019-07-05 18:03:10","","1","54","<p>With a dataframe like</p>

<pre><code>import pandas as pd

df = pd.DataFrame(
    [""2017-01-01 04:45:00"", ""2017-01-01 04:45:00removeMe""], columns=[""col""]
)
</code></pre>

<p>why do I get a SettingWithCopyWarning here</p>

<pre><code>def test_fun(df):
    df = df[~df[""col""].str.endswith(""removeMe"")]
    df.loc[:, ""col""] = pd.to_datetime(df[""col""])
    return df

df = test_fun(df)
</code></pre>

<p>but not if I run it without the function?</p>

<pre><code>df = df[~df[""col""].str.endswith(""removeMe"")]
df.loc[:, ""col""] = pd.to_datetime(df[""col""])
</code></pre>

<p>And how is my function supposed to look like?</p>
","5380431","","5380431","","2019-07-05 18:05:54","2019-07-05 18:15:18","pandas SettingWithCopyWarning only inside function","<python-3.x><pandas>","1","4","","","","CC BY-SA 4.0","1"
"58373542","1","58379053","","2019-10-14 09:12:31","","1","54","<p>I am using an embedded bokeh app in jupyter to label sections of time series.
Lets say we have to following example dataframe:</p>

<pre><code>    Time                Y           Label
0   2018-02-13 13:14:05 0.401028    a
1   2018-02-13 13:30:46 0.900101    a
2   2018-02-13 13:40:06 -0.648143   a
3   2018-02-14 16:33:27 1.111675    a
4   2018-03-13 11:43:16 -0.986025   a
</code></pre>

<p>where Time is datetime64[ns], Y is float64 and Label is from type object.</p>

<p>Now I use the following bokeh app to change the entries of Label by using a user input and trigger the callback by a button click.</p>

<pre><code>def modify_doc(doc):
    p = figure(tools=[""pan, box_zoom, wheel_zoom, reset, save, xbox_select""])
    source=ColumnDataSource(df_test)
    p.line(x=""index"", y=""Y"", source=source)
    p.circle(x=""index"", y=""Y"", source=source, alpha=0)
    def callback():
        global list_new
        list_new = []
        inds = source.selected.indices
        for j in inds:
            source.data[""Label""][j] = label_input.value.strip()
        list_new.append(pd.DataFrame(source.data))
    label_input = TextInput(title=""Label"")
    button = Button(label=""Label Data"")
    button.on_click(callback)
    layout = column(p, label_input, button)
    doc.add_root(layout)
show(modify_doc)
</code></pre>

<p><a href=""https://i.stack.imgur.com/8kky2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8kky2.png"" alt=""enter image description here""></a></p>

<p>Do not wonder about list_new, it is a needed approach as I use multiple time series plots and ColumnDataSource objects.</p>

<p>After the callback I get the accepted Label output:</p>

<pre><code>    Label   Time        Y           index
0   a   1.518528e+12    0.401028    0
1   a   1.518529e+12    0.900101    1
2   b   1.518529e+12    -0.648143   2
3   b   1.518626e+12    1.111675    3
4   b   1.520941e+12    -0.986025   4
</code></pre>

<p>But why does Time get converted to float? I know how to reconstruct the timestamps by using datetime.datetime.utcfromtimestamp() or matching the indices but how can I change the callback to keep the original entries in Time?</p>
","11888869","","11888869","","2019-10-14 10:03:43","2019-10-14 14:39:52","Why does the bokeh app callback convert variable type? (ColumnDataSource to pandas df)","<python><python-3.x><pandas><bokeh><pandas-bokeh>","1","0","","","","CC BY-SA 4.0","1"
"57100992","1","57101080","","2019-07-18 18:53:40","","0","53","<p>My dataset <code>df</code> looks like this:</p>

<pre><code>time                    Open
2017-01-01 00:00:00     5.2475
2017-01-01 01:00:00     5.2180
2017-01-01 02:00:00     5.2128
...., ....
2017-12-31 23:00:00     5.7388
</code></pre>

<p>This is an <code>hourly</code> dataset.</p>

<p>I want to remove any data between <code>10 PM Friday</code> to <code>10 AM Sunday</code></p>

<p><strong>What did I do?</strong></p>

<ul>
<li><p>I am able to remove the entire day like this: </p>

<pre><code>df = df[df['time'].dt.dayofweek != 5] # Removes Saturday
</code></pre></li>
<li>But I want to remove couple of hours of <code>Friday</code> and <code>Sunday</code> aswell.</li>
</ul>

<p>How do I do this? </p>
","9161607","","","","","2019-07-18 19:00:06","How to remove specific time-range data using Pandas","<python><python-3.x><pandas><dataframe>","1","5","1","","","CC BY-SA 4.0","1"
"57332727","1","57332822","","2019-08-02 19:55:49","","1","53","<p>I'm looking for merging 2 dataframes with items and some numbers. Let's pretend 1st column is the name, 2nd is how much money the name gets and the 3rd is how many hours they worked. And I'm looking for merging them in a way I'll show you in the example. Hopefully it is going to be all clear. I didn't try anything special here, I guess there should be a function which is going to make it in a spectacular way. </p>

<p>Thank you guys! </p>

<p>1st Dataframe</p>

<pre><code>name  money hours 
------------------
name0 1234  50
name1 4321  50
name2 1234  40
name3 1234  50
name4 4321  50
name5 1234  40
name6 1234  50
name7 4321  50
name8 1234  40
</code></pre>

<p>2nd Dataframe</p>

<pre><code>name  money hours 
------------------
name1 200   4
name4 100   5
name6 300   6
name7 300   10
name8 400   14
name1 100   10
name7 200   5
name7 100   3
</code></pre>

<p>OUTPUT should look like this:</p>

<pre><code>name  money hours 
------------------
name0 1234  50
name1 4621  64
name2 1234  40
name3 1234  50
name4 4421  55
name5 1234  40
name6 1534  56
name7 4921  68
name8 1634  54
</code></pre>
","10306224","","3483203","","2019-08-02 20:06:21","2019-08-02 20:17:58","Merging 2 dataframes with only few same items","<python><python-3.x><pandas><dataframe><merge>","2","0","","","","CC BY-SA 4.0","1"
"57664171","1","57664256","","2019-08-26 19:53:37","","-2","53","<p>In my df I have a field called <strong>day</strong> for day and month such as, <code>2019/07/29</code> if I want to create another field in the df that states the actual day of week this is such as Sunday or Monday is that possible?</p>

<p>Thanks!</p>
","8797830","","8797830","","2019-08-26 19:58:08","2019-08-26 20:01:00","Converting day field in a df into actual day of week?","<python><python-3.x><pandas><datetime><data-science>","1","1","","2019-08-26 20:01:23","","CC BY-SA 4.0","1"
"57605471","1","57607992","","2019-08-22 08:49:15","","1","53","<p>I am trying to parse a CSV file, but somehow pandas does not recognize the separator/delimiter. I have look at the similar replies around, but I still have not managed to parse my file correctly (only the header is parsed properly). </p>

<p>Each line of the file looks like this: <code>https://drive.google.com/a/company.com/uc?export=download&amp;id=10p-c0i2xtWBSvJ3OJV5pgEUarE1X,-1,""{""""type"""":""""F03""""}"",0,0,""{}"",""{}""</code></p>

<p>The code I have tried is following:</p>

<pre><code>In  [0]: import pandas as pd

In  [1]: data = pd.read_csv('file.csv', sep=',')
         data.head()
Out [1]: 

    filename          file_size   file_attributes    region_count    region_id   region_shape_attributes  region_attributes
0   https://drive...        NaN               NaN             NaN          NaN                       NaN                NaN
1   https://drive...        NaN               NaN             NaN          NaN                       NaN                NaN
2   https://drive...        NaN               NaN             NaN          NaN                       NaN                NaN
3   https://drive...        NaN               NaN             NaN          NaN                       NaN                NaN
4   https://drive...        NaN               NaN             NaN          NaN                       NaN                NaN

In  [2]: data['filename'][0]
Out [2]: 

'https://drive.google.com/a/company.com/uc?export=download&amp;id=10p-c0i2xtWBSvJ3OJV5pgEUarE1X,-1,""{""""type"""":""""F03""""}"",0,0,""{}"",""{}""'
</code></pre>
","7762646","","7122272","","2019-08-22 14:13:51","2019-08-22 14:13:51","Pandas unable to parse comma separated file correctly","<python-3.x><pandas><csv>","1","2","","","","CC BY-SA 4.0","1"
"57000785","1","57001067","","2019-07-12 05:43:16","","1","53","<p>I'm in the middle of cleaning a data set that has this:</p>

<p>[IN]</p>

<pre><code>my_Series = pd.Series([""-"",""ASD"", ""711-AUG-M4G"",""Air G2G"", ""Karsh""])
my_Series.str.replace(""[^a-zA-Z]+"", "" "")
</code></pre>

<p>[OUT]</p>

<pre><code>0            
1         ASD
2     AUG M G
3     Air G G
4       Karsh
</code></pre>

<p>[IDEAL OUT]</p>

<pre><code>0            
1         ASD
2     AUG M4G
3     Air G2G
4       Karsh
</code></pre>

<p>My goal is to remove special characters and numbers but it there's a word that contains alphanumeric, it should stay. Can anyone help?</p>
","10193760","","8353711","","2019-07-12 11:09:55","2019-07-12 11:09:55","Removing special characters while retaining alpha numeric words","<regex><python-3.x><pandas>","1","2","","","","CC BY-SA 4.0","1"
"57100389","1","","","2019-07-18 18:09:02","","0","53","<p>I am trying to convert a column of Object dtype to float64</p>

<p>Pandas version - 2.21</p>

<p>I tried using convert_objects() to force rows that cannot convert to NaN and was successful at converting my column to a float64</p>

<p>I want to know what rows/data did not allow me to convert it into a float64.
Is there a function out their that can do that ?</p>

<p>For example :</p>

<pre><code>col1

2015
2016
NaN
NaN
3005
i_am_a_string
4006
another_string
5008
4005

df['col1'].astype(float64) 

FAILED!! because the column has string data and cannot convert them all to float64

</code></pre>

<p>My desired output I want to see those strings</p>

<pre><code>i_am_a_string
another_string
</code></pre>
","11331123","","11331123","","2019-07-18 19:09:17","2019-07-19 12:04:10","Find data that are not letting me convert string to float","<python-3.x><pandas><numpy>","2","1","","2019-07-18 18:44:01","","CC BY-SA 4.0","1"
"57196552","1","57201590","","2019-07-25 07:30:37","","0","53","<p>I am trying to use <code>python</code> to conduct a calculation which will <code>sum the values in a column</code> only for the time period that a certain condition is met. </p>

<p>However, the summation should begin when the conditions are met <code>(runstat == 0 and oil &gt;1)</code>. The summation should then stop at the point when <code>oil == 0</code>. </p>

<p>I am new to python so I am not sure how to do this.</p>

<p>I connected the code to a <code>spreadsheet</code> for testing purposes but the intent is to connect to live data. I figured a <code>while loop</code> in combination with an <code>if function</code> might work but I am not winning. </p>

<p>Basically I want to have the code start when <code>runstat</code> is zero and oil is higher than 0. It should stop summing the values of oil when the oil row becomes zero and then it should write the data to <code>a SQL database</code> (this I will figure out later - for now I just want to see if it can work). </p>

<p>This is what code I have tried so far.</p>

<pre><code>import numpy as np
import pandas as pd

data = pd.read_excel('TagValues.xlsx')
df = pd.DataFrame(data)
df['oiltag'] = df['oiltag'].astype(float)
df['runstattag'] = df['runstattag'].astype(float)
oil = df['oiltag']
runstat = df['runstattag']


def startup(oil,runstat):
    while oil.all() &gt; 0:
        if oil &gt; 0 and runstat == 0:       
            totaloil = sum(oil.all())
            print(totaloil)
        else:
            return None
    return 

print(startup(oil.all(), runstat.all()))
</code></pre>

<p>It should sum the values in the column but it is returning: None</p>
","11834689","","8250094","","2019-07-25 07:53:05","2019-07-25 12:24:05","Using while and if function together with a condition change","<python-3.x><pandas>","1","5","","","","CC BY-SA 4.0","1"
"48530658","1","","","2018-01-30 21:16:39","","0","53","<p>I have a dataframe like below:</p>

<pre><code>data = {'name': {0: 'mypath\\is\this', 1: 'mynewpath\\is\this'},
        'vals': {0: [{'name1': 'val1'}, {'name2': 'val2'}, {'name3': 'val3'}],
                 1: [{'name1': 'val1'}, {'name2': 'val2'}, {'name3': 'val3'}]}}
df = pd.DataFrame(data)

name                       vals
mypath\is\this             [{name1:val1},{name2:val2},{name3:val3}]
mynewpath\is\this          [{name1:val1},{name2:val2},{name3:val3}]
</code></pre>

<p>How can i expand the values column and map the key to the create a new name keeping the value associated to it appropriately. </p>

<p>Expected output like below:</p>

<pre><code>name                    vals
mypath\is\this\name1    val1
mypath\is\this\name2    val2
mypath\is\this\name3    val3
mynewpath\is\this\name1 val1
mynewpath\is\this\name2 val2
mynewpath\is\this\name3 val3
</code></pre>
","6916973","","837534","","2018-01-30 22:01:47","2018-01-30 22:12:01","List of dictionaires in series mapped to pandas dataframe","<python><python-3.x><pandas>","3","0","1","","","CC BY-SA 3.0","1"
"40706073","1","40706097","","2016-11-20 15:52:16","","1","53","<p>I'm trying to add a column at a specific index based on the content of a different column:</p>

<pre><code>df.insert(10, 'Escalation Status', np.where(df['Reasons for escalation'] == '', 'FALSE', 'TRUE'))
</code></pre>

<p>So, when Column ""Reasons for escalation"" is emtpy, write ""FALSE"", if not, write ""TRUE"" in the newly created ""Escalation Status"" column.</p>

<p>My problem: The content is always ""TRUE"", no matter if there is anything in the ""Reasons for escalation"" field.</p>

<p>example output:</p>

<pre><code>Country,Date,Vendor,Product Family,Case number,Reseller Name,OpeningDate,Closing Date,Case Status,Topic,Escalation Reason,Reasons for escalation
DEU,28.10.2016,VENDOR,,201610281078864,,28.10.2016,28.10.2016,closed,Software issue,TRUE,Software
DEU,28.10.2016,VENDOR,,201610281078862,,28.10.2016,28.10.2016,closed,Config help,TRUE,
</code></pre>

<p>The first row is correct (Reasons for escalation is not empty, it contains ""Software""), but the second row should be ""FALSE"", as the Reasons for escalation is empty</p>

<p>thanks</p>
","4370943","","624829","","2016-11-20 15:56:15","2016-11-20 15:56:59","python pandas add column content based on conditionals","<python><python-3.x><pandas><numpy>","1","0","","","","CC BY-SA 3.0","1"
"57699495","1","57699724","","2019-08-28 20:15:55","","0","53","<p>I am trying to find the corresponding date of the most recent ID which has corresponding True value </p>

<p>I've utilized df.id.rolling to locate my desired duplicates in my date range window. I just need to identify how far the duplicates are from the most recent occurrence of duplicates. </p>

<p>This is what my starting df looks like</p>

<pre><code>df_input:
date        id    duplicate   
1/10/18     1        true         
1/12/18     2        true         
1/20/18     1        false         
1/31/18     1        false         
</code></pre>

<p>This is what i'm trying to get to</p>

<pre><code>df_output:
date        id    duplicate   most_recent
1/10/18     1        true         Nan
1/12/18     2        true         Nan
1/20/18     1        false        1/10/18 
1/31/18     1        false        1/10/18 
</code></pre>

<p>Any tips are helpful!</p>

<p>Edited: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p>

<p>Thanks for the tip but this doesn't seem to find the most recent instances only the first instance in a series this returns the first event:</p>

<pre><code>         date  id  duplicate most_recent
   0  1/10/18   1       True         NaN
   1  1/12/18   2       True         NaN
   2  1/20/18   1      False     1/10/18
   3  1/31/18   1      False     1/10/18
   4  2/1/18    1      True          Nan
   5  2/8/18    1      False      1/10/18
</code></pre>

<p>I'm looking for:</p>

<pre><code>       date  id  duplicate most_recent
   0  1/10/18   1      True         NaN
   1  1/12/18   2      True         NaN
   2  1/20/18   1      False     1/10/18
   3  1/31/18   1      False     1/10/18
   4  2/1/18    1      True          Nan
   5  2/8/18    1      False     2/1/18
</code></pre>

<p>Thanks for the help, I don't think I full realized or explain my problem fully. 
Updated ~~~~~ </p>

<p>The coded provided works so maybe I should re-post but I need to be able to find the most recent and append a column then I need to be able to find it again based on conditions laid out in an If + For loop statement. See below for the code example</p>

<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>list2 = []

df.loc[~df.duplicates,'most_recent']=df['date'].where(df.duplicates).groupby(df['id']).ffill()
for index, row in df.iterrows():
 
  dup = row['duplicates']
  date = row['date']
  ndate = row['most_recent']
  d1 = date - ndate
  
  if d1 &gt; timedelta(days= 14):
      x= True
      
      if x == True:
          list2.append(x)     
  else:  
      list2.append(dup)
  df.loc[~df.duplicates,'most_recent']=df['date'].where(df.duplicates).groupby(df['id']).ffill()</code></pre>
</div>
</div>
</p>

<p>Example ouput:</p>

<pre><code>        date  id  duplicate most_recent
  0  1/10/18   1      True         NaN
  1  1/12/18   2      True         NaN
  2  1/20/18   1      False     1/10/18
  3  1/31/18   1      False     1/10/18
  4  2/1/18    1      True          Nan
  5  2/8/18    1      False     2/1/18
</code></pre>

<p>Some code</p>

<pre><code>        date  id  duplicate most_recent
  0  1/10/18   1      True         NaN
  1  1/12/18   2      True         NaN
  2  1/20/18   1      False     1/10/18
  3  1/31/18   1      False     1/10/18
  4  2/1/18    1      True          Nan
  5  2/8/18    1      True      2/1/18
</code></pre>
","11908237","","11908237","","2019-08-29 19:59:53","2019-08-29 19:59:53","Get date from most recent ID with corresponding Boolean-Updated2x","<python-3.x><pandas><datetime><merge>","2","0","1","","","CC BY-SA 4.0","1"
"57560303","1","57561418","","2019-08-19 16:02:08","","1","52","<pre><code>    Ode  Proceeds  Pos        Amount  Positions  Target  Weighting  Additions
0   676     30160  FPE   51741.25000          5    0.10   0.187636        NaN
1   676     30160  HFA   57299.63616          5    0.20   0.207794        NaN
2   676     30160  PFL   60437.40563          5    0.20   0.219173        NaN
3   676     30160  PSO   53053.57410          5    0.15   0.192396        NaN
4   676     30160  RNS   53220.36636          5    0.20   0.193001        NaN
5   953     34960  PFL    8506.19390          1    0.20   1.000000        NaN
6   637     14750  PFL    8341.21701          3    0.20   0.302517        NaN
7   637     14750  PSO   12669.65078          3    0.15   0.459499        NaN
8   637     14750  RNS    6561.85824          3    0.20   0.237984        NaN
9   673     12610  FPE   31220.47500          5    0.10   0.175041        NaN
10  673     12610  HFA   34020.29280          5    0.20   0.190738        NaN
11  673     12610  PFL   37754.00236          5    0.20   0.211672        NaN
12  673     12610  PSO   31492.56779          5    0.15   0.176566        NaN
13  673     12610  RNS   43873.58472          5    0.20   0.245982        NaN
14  318     93790  PFL   59859.39180          2    0.20   0.285266        NaN
15  318     93790  PSO  149977.71090          2    0.15   0.714734        NaN
16  222     75250  FPE   21000.00000          6    0.10   0.100000     7525.0
17  222     75250  HFA   42000.00000          6    0.20   0.200000    15050.0
18  222     75250  PFL   42000.00000          6    0.20   0.200000    15050.0
19  222     75250  PSO   31500.00000          6    0.15   0.150000    11287.5
20  222     75250  RNS   42000.00000          6    0.20   0.200000    15050.0
21  222     75250  CRD   31500.00000          6    0.15   0.150000    11287.5
</code></pre>

<p>Th information below is the desired output - simply a cut-out of the first 5 rows from above information that shows the new column <code>['Target Amount']</code>as well as the creation of the last row - when you compare Ode 676 it has 5 out of the 6 Pos that are in the below dictionary. Since Ode 676 is missing CRD, I need a way to create a row and fill in the information</p>

<p><code>target_dict = {""PFL"":.20,""RNS"":.20,""HFA"":.20,""PSO"":.15,""CRD"":.15,""FPE"":.10}</code></p>

<pre><code>        Ode  Proceeds  Pos        Amount  Positions  Target  Weighting   Target Amt     Additions
    0   676     30160  FPE   51741.25000          5    0.10   0.187636   30591.22       -21150.03
    1   676     30160  HFA   57299.63616          5    0.20   0.207794   61182.45       3882.81
    2   676     30160  PFL   60437.40563          5    0.20   0.219173   61182.45       745.04
    3   676     30160  PSO   53053.57410          5    0.15   0.192396   45886.83       -7166.74
    4   676     30160  RNS   53220.36636          5    0.20   0.193001   61182.45       7962.08
    5   676     30160  CRD   0                         0.15   0          45886.83       45886.83
</code></pre>

<p>CRD would be added to make the full 6 Positions then the <code>['Target Amt']</code> would be calculated based on the sum of all <code>['Amount']</code>plus the <code>['Proceeds']</code> to get a total for Ode 676. I can figure out the calculations but I can't figure out how to add the row for Ode where <code>['Positions'] &lt; 6</code> based on the differences between<code>'target_dict'</code>and the current <code>['Pos']</code>for Ode 676.</p>
","9310528","","3750257","","2019-08-19 16:03:05","2019-10-02 05:24:46","Creating row based on differences between dictionary and current Groupby","<python><python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"49282879","1","49283076","","2018-03-14 16:21:31","","1","52","<pre><code>I have a the following dataframe using pandas. 

Names

Jason
Jason M
John
John S
Nate
Dave
</code></pre>

<p>I want to get the unique names. In this case, the output I am looking for is</p>

<pre><code>Nate
Dave
</code></pre>

<p>I have the following codes but it does not print what I am looking for.</p>

<pre><code>df = pd.DataFrame(df.Names.unique())
print(df)
</code></pre>

<p>Where should I fix? Thanks.</p>
","9305276","","9209546","","2018-03-14 16:35:52","2018-03-14 16:35:52","Get Unique First Names in Pandas series","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"57700498","1","57716284","","2019-08-28 21:54:11","","-1","52","<p>I am using pandas to compare time durations to mark attendance.
I have used DateTime and timedelta to compare the durations. I want to save the output in the adjacent column so that we know who was present and who was not.</p>

<p>I can compare the values and print if the person was present or absent, but i want to do it in the excel file in front of the name of the person</p>

<pre><code>df1 =df[['Date', 'Agent Name', 'Login Time']]

for x in df1['Login Time']:
s1 = str(x)
s2 = '12:00:00'
s3 = '3:10:00'
s4 = '01:00:00'
FMT = '%H:%M:%S'
tdelta = datetime.strptime(s2, FMT) - datetime.strptime(s1, FMT)
if x == s4:
    print('Aussi')
elif (str(tdelta)) &lt; s3:
    print('Present')
else: 
    print('Check')
</code></pre>

<p>what i am thinking is, i will store the value of the output in a list and then add the list to a new column in the df.</p>
","11730554","","","","","2019-08-29 18:53:12","I need to save the output of Print command into a list","<python><python-3.x><pandas><loops><if-statement>","1","4","","","","CC BY-SA 4.0","1"
"57561426","1","57561464","","2019-08-19 17:28:10","","0","52","<p>I want to create a new columns for a big table using several criteria and columsn and was not sure the best way to approach it. </p>

<pre class=""lang-py prettyprint-override""><code>    df = pd.DataFrame({'a': ['A', ""B"", ""B"", ""C"", ""D""],
'b':['y','n','y','n', np.nan], 'c':[10,20,10,40,30], 'd':[.3,.1,.4,.2, .1]})
    df.head()

    def fun(df=df):
        df=df.copy()
        if df.a=='A' &amp; df.b =='n': 
            df['new_Col'] = df.c+df.d
        if df.a=='A' &amp; df.b =='y': 
            df['new_Col'] = df.d *2
        else:
            df['new_Col'] = 0
        return df
    fun()
</code></pre>

<p>OR </p>

<pre class=""lang-py prettyprint-override""><code>
    def fun(df=df):
            df=df.copy()
            if df.a=='A' &amp; df.b =='n': 
                return = df.c+df.d
            if df.a=='A' &amp; df.b =='y': 
                return  df.d *2
            else:
                return 0
    df['new_Col""] df.apply(fun)
</code></pre>

<p>OR using <code>np.where</code>:</p>

<pre class=""lang-py prettyprint-override""><code>    df['new_Col'] = np.where(df.a=='A' &amp; df.b =='n', df.c+df.d,0 )
    df['new_Col'] = np.where(df.a=='A' &amp; df.b =='y', df.d *2,0 )
</code></pre>
","7517610","","","","","2019-08-19 18:35:43","PANDAS NEW COLUMN BASED ON MULTIPLE CRITERIA AND COLUMNS","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"57073296","1","","","2019-07-17 10:01:28","","0","52","<h1>First Pandas Project</h1>

<p>Starting to learn pandas and wanted to test with a dataset of my weightlifting which I exported as a CSV format. The purpose of this was to analyze my progression, but I have unfortunately run into an issue where my data rows are all stored in the same column and not splitting the data into the different columns which looks correct based on the imported header.</p>

<p>I've tried to add the seperator function while importing the csv, but looking at the data it needs to be "","" that seperates the values (I guesss CSV always takes comma as default).</p>

<h2>I am using the following code:</h2>

<pre><code>import pandas as pd

data = pd.read_csv(""strong.csv"")
</code></pre>

<h2>Data from CSV looks like this:</h2>

<pre><code>Date,Workout Name,Exercise Name,Set Order,Weight,Reps,Distance,Seconds,Notes,Workout Notes


2018-05-08 19:27:54,""1: Back, Biceps &amp; Abs"",""Deadlift (Barbell)"",1,50,12,0,0,"""",""""


2018-05-08 19:27:54,""1: Back, Biceps &amp; Abs"",""Deadlift (Barbell)"",2,50,10,0,0,"""",""""

2018-05-08 19:27:54,""1: Back, Biceps &amp; Abs"",""Deadlift (Barbell)"",3,110,1,0,0,"""",""""
</code></pre>

<h2>See image to see data.head() result:</h2>

<p>( <a href=""https://i.imgur.com/qQtw66S.png"" rel=""nofollow noreferrer"">https://i.imgur.com/qQtw66S.png</a> )</p>

<h3>EDIT: See link to CSV file with first columns.</h3>

<p><a href=""https://github.com/Trools/StrongProject"" rel=""nofollow noreferrer"">https://github.com/Trools/StrongProject</a></p>
","9934534","","9934534","","2019-07-17 10:59:09","2019-07-17 11:18:17","Pandas: Not seperating rows into colums","<python-3.x><pandas><csv>","1","3","","","","CC BY-SA 4.0","1"
"49410188","1","49410758","","2018-03-21 15:19:24","","0","52","<p>I am new to python and pandas and I am struggling to figure out how to pull out the 10 counties with the most water used for irrigation in 2014. </p>

<pre><code>%matplotlib inline
import csv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv('info.csv') #reads csv

data['Year'] = pd.to_datetime(['Year'], format='%Y') #converts string to 
                                                     datetime
data.index = data['Year'] #makes year the index
del data['Year'] #delete the duplicate year column
</code></pre>

<p>This is what the data looks like (this is only partial of the data):</p>

<pre><code>County  WUCode  RegNo   Year    SourceCode  SourceID    Annual    CountyName 
1       IR      311     2014    WELL        1           946       Adams  
1       IN      311     2014    INTAKE      1           268056    Adams
1       IN      312     2014    WELL        1           48        Adams
1       IN      312     2014    WELL        2           96        Adams
1       IR      312     2014    INTAKE      1           337968    Adams
3       IR      315     2014    WELL        5           81900     Putnam
3       PS      315     2014    WELL        6           104400    Putnam
</code></pre>

<p>I have a couple questions:</p>

<p>I am not sure how to pull out only the ""IR"" in the <code>WUCode</code> Column with pandas and I am not sure how to print out a table with the 10 counties with the highest water usage for irrigation in 2014. </p>

<p>I have been able to use the <code>.loc</code> function to pull out the information I need, with something like this:</p>

<pre><code>data.loc['2014', ['CountyName', 'Annual', 'WUCode']]
</code></pre>

<p>From here I am kind of lost. Help would be appreciated!</p>
","9396878","","6049688","","2018-03-21 15:23:17","2018-03-21 15:45:13","How to use pandas to pull out the counties with the largest amount of water used in a given year?","<python><python-3.x><pandas>","2","2","","","","CC BY-SA 3.0","1"
"56627212","1","56627365","","2019-06-17 08:08:11","","-2","52","<p>I have a data-frame with 55 columns and 2 million rows having mix of categorical and numeric fileds. There are null/na values in the data-set. I want to fill Null values with Column names. </p>

<p>The data-set I have is:</p>

<pre><code>  A     B    C   D  .....
  1     na   na  3  .....
  na    3    4   na .....
  ........................
</code></pre>

<p>The output the I am trying to get is:</p>

<pre><code>  A     B    C   D  .....

  1     B    C   3  .....
  A    3    4    D .....
  ........................
</code></pre>

<p>I am trying to use :</p>

<p><code>df.fillna(method='ffill')</code></p>

<p>Is there another way?</p>

<p>Python:3.6.5</p>
","10477258","","10477258","","2019-06-17 08:44:00","2019-06-17 08:49:44","Fill Null values in Data-Frame with Column names","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"49621727","1","49622097","","2018-04-03 03:23:35","","2","52","<p>Environment - PY3.6, Jupyter Notebook:</p>

<pre><code>import pandas as pd
import os
from IPython.display import display, HTML
</code></pre>

<p>I have an excel file that I've read into a dataframe. I've recreated the data programmatically here:</p>

<pre><code>xl = [['group1'],
     ['john'],
     ['mike'],
     ['marry'],
     ['group2'],
     ['dan'],
     ['ann'],
     ['jacob'],
     ['susan']]
</code></pre>

<p>create a <code>pd.DataFrame()</code> and then <code>display</code> it.</p>

<pre><code>s = pd.DataFrame(xl)
display(s)

    0
0   group1
1   john
2   mike
3   marry
4   group2
5   dan
6   ann
7   jacob
8   susan
</code></pre>

<p>The desired output is:</p>

<pre><code>    0       1
0           group1
1   group1  john
2   group1  mike
3   group1  marry
4           group2
5   group2  dan
6   group2  ann
7   group2  jacob
8   group2  susan
</code></pre>

<p>I've tried several variations of <code>.melt()</code> but can only get that to work if the ""group1"" or ""group2"" is a column index.  I've also tried various forms of <code>.pivot()</code> and <code>.stack()</code>.  The <code>.stack</code> function seems to be the most promising, but it requires the DataFrame to be multi-indexed to work as I would hope.</p>

<p>My specific question is whether pandas has a function to accomplish this? </p>

<p>OR </p>

<p>Do i need to break the <code>df</code> down into component <strong>groups</strong> using the group names as column headers - then perform a <code>stack()</code> or <code>melt()</code> type function?</p>

<p>EDIT:  I suppose that I could also iterate over the rows - but this would be tricky as the ""groups"" are not always known.</p>
","3180386","","3180386","","2018-04-03 03:37:19","2018-04-03 04:06:54","Pandas: Moving group titles to new columns","<python><python-3.x><pandas>","3","0","","","","CC BY-SA 3.0","1"
"57698970","1","57699163","","2019-08-28 19:32:19","","1","52","<p>I am trying to replace the values of 3 columns within multiple observations based on two conditionals ( e.g., specific ID after a particular date).</p>

<p>I have seen similar questions.</p>

<ol>
<li><p><a href=""https://stackoverflow.com/questions/28910954/pandas-multiple-conditions-function-based-on-column"">Pandas Multiple Conditions Function based on Column</a></p></li>
<li><p><a href=""http://Pandas%20replace,%20multi%20column%20criteria"" rel=""nofollow noreferrer"">Pandas replace, multi column criteria</a></p></li>
<li><p><a href=""https://stackoverflow.com/questions/30631841/pandas-how-do-i-assign-values-based-on-multiple-conditions-for-existing-columns"">Pandas: How do I assign values based on multiple conditions for existing columns?</a></p></li>
<li><p><a href=""https://stackoverflow.com/questions/55081194/replacing-values-in-a-pandas-dataframe-based-on-multiple-conditions"">Replacing values in a pandas dataframe based on multiple conditions</a></p></li>
</ol>

<p>However, they did not quite address my problem or I can't quite manipulate them to solve my problem.</p>

<p><strong><em>This code will generate a dataframe similar to mine:</em></strong></p>

<pre><code>df = pd.DataFrame({'SUR_ID': {0:'SUR1', 1:'SUR1', 2:'SUR1', 3:'SUR1', 4:'SUR2', 5:'SUR2'}, 'DATE': {0:'05-01-2019', 1:'05-11-2019', 2:'06-15-2019', 3:'06-20-2019', 4: '05-15-2019', 5:'06-20-2019'}, 'ACTIVE_DATE': {0:'05-01-2019', 1:'05-01-2019', 2:'05-01-2019', 3:'05-01-2019', 4: '05-01-2019', 5:'05-01-2019'}, 'UTM_X': {0:'444895', 1:'444895', 2:'444895', 3:'444895', 4: '445050', 5:'445050'}, 'UTM_Y': {0:'4077528', 1:'4077528', 2:'4077528', 3:'4077528', 4: '4077762', 5:'4077762'}})
</code></pre>

<p><strong>Output Dataframe:</strong></p>

<p><a href=""https://i.stack.imgur.com/NbXVw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NbXVw.png"" alt=""enter image description here""></a></p>

<p><strong><em>What I am trying to do:</em></strong></p>

<p>I am trying to replace <em>UTM_X</em>,<em>UTM_Y</em>, AND <em>ACTIVE_DATE</em>  with</p>

<blockquote>
  <p>[444917, 4077830, '06-04-2019']</p>
</blockquote>

<p>when </p>

<blockquote>
  <p><em>SUR_ID</em> is ""SUR1"" and <em>DATE</em> >= ""2019-06-04 12:00:00""</p>
</blockquote>

<p><strong><em>This is a poorly adapted version of the solution for question 1 in attempts to fix my problem- throws error:</em></strong></p>

<pre><code>df.loc[[df['SUR_ID'] == 'SUR1' and df['DATE'] &gt;='2019-06-04 12:00:00'], ['UTM_X', 'UTM_Y', 'Active_Date']] = [444917, 4077830, '06-04-2019']
</code></pre>
","5950072","","5950072","","2019-08-29 00:26:45","2019-08-29 00:26:45","Replace values in observations (i.e., multiple columns within multiple rows) based on multiple conditionals","<python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"40909922","1","40910005","","2016-12-01 11:48:41","","1","52","<p>I have a pandas dataframe <code>test_df</code> that looks like this:</p>

<pre><code>                            reading                               
01-Jan-2016 00:00:00.000  20.464020
02-Jan-2016 00:00:00.000  22.440950
03-Jan-2016 00:00:00.000  27.181500
04-Jan-2016 00:00:00.000  25.318260
05-Jan-2016 00:00:00.000  25.376050
06-Jan-2016 00:00:00.000   0.067112
07-Jan-2016 00:00:00.000  19.313950
08-Jan-2016 00:00:00.000  26.677340
09-Jan-2016 00:00:00.000  26.801620
10-Jan-2016 00:00:00.000  22.583950
11-Jan-2016 00:00:00.000   0.002765
12-Jan-2016 00:00:00.000  26.496440
13-Jan-2016 00:00:00.000  23.233720
14-Jan-2016 00:00:00.000  23.956080
15-Jan-2016 00:00:00.000  26.958120
16-Jan-2016 00:00:00.000  27.351270
17-Jan-2016 00:00:00.000  28.348710
18-Jan-2016 00:00:00.000  25.494090
19-Jan-2016 00:00:00.000  26.342880
20-Jan-2016 00:00:00.000  24.645530
</code></pre>

<p>The problem: given a string like '2016-01' a.k.a 'yyyy-mm', I want to find out if <code>any</code> entry from the specified month is present in the index of the  pandas dataframe <code>test_df</code>.</p>

<p>What I'm expecting is <code>True</code> for '2016-01' and <code>False</code> for any other string. Looking for the most concise method to do that.</p>

<h2>Problem setup:</h2>

<p>To make things easy, this is the code to obtain test dataframe:</p>

<pre><code>import pandas as pd
temp_df = pd.read_json('{""reading"":{""01-Jan-2016 00:00:00.000"":20.46402,""02-Jan-2016 00:00:00.000"":22.44095,""03-Jan-2016 00:00:00.000"":27.1815,""04-Jan-2016 00:00:00.000"":25.31826,""05-Jan-2016 00:00:00.000"":25.37605,""06-Jan-2016 00:00:00.000"":0.06711243,""07-Jan-2016 00:00:00.000"":19.31395,""08-Jan-2016 00:00:00.000"":26.67734,""09-Jan-2016 00:00:00.000"":26.80162,""10-Jan-2016 00:00:00.000"":22.58395,""11-Jan-2016 00:00:00.000"":0.002765084,""12-Jan-2016 00:00:00.000"":26.49644,""13-Jan-2016 00:00:00.000"":23.23372,""14-Jan-2016 00:00:00.000"":23.95608,""15-Jan-2016 00:00:00.000"":26.95812,""16-Jan-2016 00:00:00.000"":27.35127,""17-Jan-2016 00:00:00.000"":28.34871,""18-Jan-2016 00:00:00.000"":25.49409,""19-Jan-2016 00:00:00.000"":26.34288,""20-Jan-2016 00:00:00.000"":24.64553}}')
</code></pre>

<p>I've tried:</p>

<pre><code>&gt;&gt;'2016-01' in test_df.index
False
</code></pre>
","5315126","","","","","2016-12-01 12:20:52","Finding if atleast one entry from a specified month string exists in Pandas datetime index","<python><python-3.x><datetime><pandas><datetimeindex>","1","0","1","","","CC BY-SA 3.0","1"
"56692421","1","56698280","","2019-06-20 19:06:58","","0","52","<p>I am a beginner in python. I have big data looks like this:</p>

<pre><code>df
Mean       id
0.089394    1
0.389394    2
0.047313    3
0.047313    4
0.767004    5
0.767004    6
0.363154    7
0.363154    8
0.098941    9
1.578785    10
0           11
.....
</code></pre>

<p>I want to eliminate or delete row mean column data with category below than 0 to 2 (example: >0, >0.1, >0.2, until >2). I used this code:</p>

<pre><code>df = df[df.Mean &gt; 0]
</code></pre>

<p>if I use this code, I have to put many threshold categories every single code. Is there an elegant way to calculate and save to multiple CSV automatically based on each threshold?</p>

<p>for example my desire output for >0</p>

<pre><code>df&gt;0
Mean       id
0.089394    1
0.389394    2
0.047313    3
0.047313    4
0.767004    5
0.767004    6
0.363154    7
0.363154    8
0.098941    9
1.578785    10
</code></pre>

<p>for >0.1</p>

<pre><code>df&gt;0.1
Mean       id
0.089394    1
0.389394    2
0.767004    5
0.767004    6
0.363154    7
0.363154    8
1.578785    10
</code></pre>

<p>and so on</p>
","8103545","","8572122","","2019-06-21 07:17:08","2019-06-21 07:17:08","delete row with threshold or category and save to multiple CSV in pandas","<python><python-3.x><pandas><csv><numpy>","1","6","","","","CC BY-SA 4.0","1"
"57506849","1","57507206","","2019-08-15 08:25:30","","1","52","<p>I have text file data.txt like this:</p>

<blockquote>
<pre><code>Name male  female
bayu 1 0
jonson 1 0
anna 0 1
</code></pre>
</blockquote>

<p>i have try to convert this file using pandas python</p>

<pre><code>import pandas as pd
df = pd.read_fwf('data.txt')
df.to_csv('data.csv')
</code></pre>

<p>I want to get results data.csv like this:</p>

<blockquote>
<pre><code>Gender name
bayu male
jonson male
Anna  Female
</code></pre>
</blockquote>

<p>what should i do?</p>
","7923590","","5178905","","2019-08-15 09:50:55","2019-08-15 10:24:20","how to manipulate data from text to CSV","<python><python-3.x><pandas><dataframe>","3","1","1","","","CC BY-SA 4.0","1"
"56871477","1","56871888","","2019-07-03 13:38:27","","2","52","<p>I have a pandas dataframe with Time and values columns. I am trying to create two new columns 'START_TIME"" and 'END_TIME'. It is medication related data and it is stored poorly in the database so I am trying to transform the table. 
In this case, the medication for a patient started at 2018-11-07 23:59:32 with a dose value of 80.o so, I want to capture that as the start time of the medication and end time is the first zero after the last value. That would be one round of medication. Whenever a new value starts it is considered as the second round of medication and I'd like to capture the start time and end time in the following way as explained earlier. </p>

<pre><code>Time                          Values
2018-11-07 23:59:32            80.0
2018-11-08 04:35:09            80.0
2018-11-08 05:31:24            40.0
2018-11-24 18:29:30             0.0
2018-11-24 18:33:14             0.0
2018-11-26 17:39:31            20.0
2018-11-26 18:51:07            20.0
2018-11-26 21:04:35             0.0
2018-11-26 21:05:20             0.0
2018-11-26 21:13:44             0.0
2018-11-26 21:25:57             0.0
2018-11-29 02:19:57             7.0
2018-12-09 16:02:06             5.0
2018-12-09 16:33:03             2.5
2018-12-09 21:02:10             0.0
</code></pre>

<p>I believe it cannot be done with a simple for and if loop as I started with a simple step and it failed</p>

<pre><code>for i in df['Values']:
    if i+1 != 0:
        df['START_TIME'] = df['TIME'].copy()
</code></pre>

<p>Expected DataFrame:</p>

<pre><code>Time                     Values   START_TIME              END_TIME
2018-11-07 23:59:32       80.0    2018-11-07 23:59:32 
2018-11-08 04:35:09       80.0
2018-11-08 05:31:24       40.0
2018-11-24 18:29:30        0.0                            2018-11-24 18:29:30
2018-11-24 18:33:14        0.0
2018-11-26 17:39:31       20.0    2018-11-26 17:39:31
2018-11-26 18:51:07       20.0
2018-11-26 21:04:35        0.0                            2018-11-26 21:04:35
2018-11-26 21:05:20        0.0
2018-11-26 21:13:44        0.0
2018-11-26 21:25:57        0.0
2018-11-29 02:19:57        7.0    2018-11-29 02:19:57
2018-12-09 16:02:06        5.0
2018-12-09 16:33:03        2.5
2018-12-09 21:02:10        0.0                            2018-12-09 21:02:10
</code></pre>

<p>I'd really appreciate if I can get some help.</p>
","10713420","","8852387","","2019-07-03 13:57:44","2019-07-03 14:11:07","Pandas creating and filling new columns based on other columns","<python-3.x><pandas><dataframe>","2","0","","","","CC BY-SA 4.0","1"
"57505365","1","","","2019-08-15 05:52:37","","0","52","<p>I did splitting among trained and testing data using the function train_test_split() and get the following.</p>

<p><code>print(X_train)</code>
<code>
+--------------------+
|     fre   loc      |
+--------------------+
| 1.208531  0.010000 |
| 0.169742  0.010000 |
| 0.119691  0.010000 |
| 0.151515  0.010000 |
| 0.632653  0.010000 |
| 0.104000  1.125000 |
| 3.313433  1.076923 |
| 0.323899  0.010000 |
| 3.513011  1.100000 |
| 0.184971  0.010000 |
| 0.158470  0.010000 |
| 0.175258  0.010000 |
| 0.149038  0.010000 |
| 0.158879  0.010000 |
+--------------------+
</code></p>

<p><code>print(X_test)</code>
<code>
+--------------------+
|     fre   loc      |
+--------------------+
| 1.208531  0.010000 |
| 0.169742  0.010000 |
| 0.119691  0.010000 |
| 0.151515  0.010000 |
| 0.632653  0.010000 |
| 0.104000  1.125000 |
| 3.313433  1.076923 |
+--------------------+
</code></p>

<p><code>print(y_train)</code>
<code>
+----------------+
| Critical Value |
+----------------+
|       1.208531 |
|       0.000000 |
|       0.000000 |
|       0.000000 |
|       0.632653 |
|       1.125000 |
|       4.390356 |
|       0.000000 |
|       4.613011 |
|       0.000000 |
|       0.000000 |
|       0.000000 |
|       0.000000 |
|       0.000000 |
+----------------+
</code></p>

<p><code>print(y_test)</code>
<code>
+----------------+
| Critical Value |
+----------------+
|       1.208531 |
|       0.000000 |
|       0.000000 |
|       0.000000 |
|       0.632653 |
|       1.125000 |
|       4.390356 |
+----------------+
</code></p>

<p>Then I performed Gradient Boosting Regressor in the following manner,</p>

<p><code>est_knc= GradientBoostingRegressor()
est_knc.fit(X_train, y_train)
pred = est_knc.score(X_test, y_test)
print(pred)
</code></p>

<p>and got the output,
<code>0.8879530974429752</code></p>

<p>it's ok till here. Now I want to plot this but its quite confusing for me to understand what and how parameters do I have to pass in order to plot a scatter plot using the above data. I'm new in visualisation. :(</p>
","10425067","","","","","2019-08-15 06:27:31","how to plot a scatter plot for the following data in python?","<python-3.x><pandas><dataframe><matplotlib><plot>","1","0","","","","CC BY-SA 4.0","1"
"56668824","1","56668865","","2019-06-19 13:28:07","","0","51","<p>I'm looking to add a %Y%m%d date column to my dataframe using a period column that has integers 1-32, which represent monthly data points starting at a defined environment variable ""odate"" (e.g. if odate=20190531 then period 1 should be 20190531, period 2 should be 20190630, etc.)</p>

<p>I tried defining a dictionary with the number of periods in the column as the keys and the value being odate + MonthEnd(period -1)</p>

<p>This works fine and well; however, I want to improve the code to be flexible given changes in the number of periods.</p>

<p>Is there a function that will allow me to fill the date columns with the odate in period 1 and then subsequent month ends for subsequent periods?</p>

<p>example dataset:</p>

<p>odate=20190531</p>

<pre><code>period value
1      5.5
2      5
4      6.2
3      5
5      40
11     5
</code></pre>

<p>desired dataset:</p>

<p>odate=20190531</p>

<pre><code>period value date
1      5.5   2019-05-31
2      5     2019-06-30
4      6.2   2019-08-31
3      5     2019-07-31
5      40    2019-09-30
11     5     2020-03-31
</code></pre>
","8570006","","8727339","","2019-06-19 13:58:18","2019-06-20 09:45:14","Add Months to Data Frame using a period column","<python><python-3.x><pandas><dataframe><datetime>","1","0","","","","CC BY-SA 4.0","1"
"57702888","1","","","2019-08-29 04:15:14","","0","51","<p>I am reading a data from table and want to converting to XML file.</p>

<p>I am successfully read data from table and store in dataframe but not able to convert to xml.</p>

<p><strong>Reading the data from table to dataframe</strong></p>

<pre><code>       import cx_Oracle
            import pandas as pd
            from datetime import date
            import time

             CONN_INFO = {
                    'host': 'xxxxxx',
                    'port': 1521,
                    'user': 'xxxxx',
                    'psw': 'xxxxx',
                    'service': 'xxxxx',
                }

                CONN_STR = '{user}/{psw}@{host}:{port}/{service}'.format(**CONN_INFO)

                connection=cx_Oracle.connect(CONN_STR)
                cursor=connection.cursor()

    cursor.execute(""""""with M0 as (select 
         os.order_id order_id
         , MIN(os.EFFECTIVE_TIMESTAMP) as M0_DT
    FROM
        V_F_OR_STATUS_HISTORY    os 
    WHERE
         os.status_type IN ('0170','0112','2364','2363')
         AND trunc(os.EFFECTIVE_TIMESTAMP)  &gt;= trunc(sysdate)-5
    GROUP BY
         os.order_id 
    """""")
 data=cursor.fetchall()
</code></pre>

<p><strong>Output</strong></p>

<pre><code> [('GLAXO.4501201394', datetime.datetime(2019, 8, 29, 0, 0)),
 ('GLAXO.SG70X4501162986-THNEO', datetime.datetime(2019, 9, 26, 0, 0)),
 ('GLAXO.8000334269', datetime.datetime(2019, 9, 5, 0, 0)),
 ('GLAXO.20190828168057', datetime.datetime(2019, 11, 6, 0, 0)),
 ('GLAXO.PH22X4501136042-THNEO', datetime.datetime(2019, 10, 13, 0, 0)),
 ('GLAXO.4501205247', datetime.datetime(2019, 11, 3, 0, 0)),
 ('GLAXO.256286313', datetime.datetime(2019, 10, 21, 0, 0)),
 ('GLAXO.4501205964', datetime.datetime(2019, 9, 7, 0, 0))]
</code></pre>

<p><strong>Coverting to XML</strong></p>

<p>I tried the below refernce avialable from stackoverlow its not converting to xml</p>

<pre><code> root = etree.Element('data');

    for i,row in data.iterrows():
        item = etree.SubElement(root, 'item', attrib=row.to_dict());

  etree.dump(root);

    xml.etree.ElementTree.parse('xml_file.xml');
</code></pre>

<p>How to fix this. Any help is appreciated</p>
","9161067","","9161067","","2019-08-29 04:37:26","2019-08-29 04:37:26","Error while reading data from table and converting to xml using pandas","<python><xml><python-3.x><pandas>","0","7","","","","CC BY-SA 4.0","1"
"57374343","1","57374442","","2019-08-06 10:49:40","","1","51","<p>I have two <code>df</code>s,</p>

<pre><code>df1

id    group
 1    0001
 2    0001
 3    0001
 4    0002
 5    0002
 6    0003

df2

group     name
0001      one
0002      one
0003      two
</code></pre>

<p>I tried to see if <code>name</code> contains <code>one</code> in <code>df2</code>, and finds the corresponding <code>group</code>s in <code>df1</code>; and then merge <code>df1</code> and <code>df2</code> in that regard;</p>

<pre><code>a = df2['name'].str.contains(pat=r'(?i)one', regex=True)
valid_groups = df2.loc[a]['group'].tolist()

c = df1['group'].isin(valid_groups)
df3 = df1.loc[c]

df4 = df3.merge(df2, how='left', on=['group'])

group    name    id
0001     one     1
0001     one     2
0001     one     3
0002     one     4
0002     one     5
</code></pre>

<p>I am wondering is there a better way to do this, more efficient way.</p>
","766708","","","","","2019-08-06 10:55:25","pandas merge two dataframes with one contains column values in another","<python><python-3.x><pandas><dataframe>","2","1","","","","CC BY-SA 4.0","1"
"57666246","1","57666451","","2019-08-27 00:14:01","","0","51","<p>I am loading csv files to sql server table. EmpNo Ename ProdID Sales Value Int Varchar Int Int Float</p>

<p>But I am getting ProdID,Sales,Amount Values as 0 for blanks.But want to keep them as blank. I am using this code for keeping nulls.
Please help me. Thanks in advance.</p>

<pre><code>df = pd.read_csv(f, header=None,names=file_titles,low_memory=False)
df.columns = df.columns.str.strip()
df = df.fillna(value=' ')
</code></pre>

<p><strong>Source Data</strong>             </p>

<pre><code>EmpNo   Ename   ProdID  Sales   Amount
1   E1      10  120.00
2   E2  1   2   100.00
3   E3          
4   E4  3   3   353.00
5   E5      6   443.00
6   E6  4   8   533.00
</code></pre>

<p>Expected Output             </p>

<pre><code>EmpNo   Ename   ProdID  Sales   Amount
1   E1  0   10  120.00
2   E2  1   2   100.00
3   E3  0   0   0.00
4   E4  3   3   353.00
5   E5  0   6   443.00
6   E6  4   8   533.00
</code></pre>
","10808871","","10808871","","2019-08-27 00:22:20","2019-08-27 00:55:27","Keeping blank values for int columns in python pandas","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57941191","1","57941322","","2019-09-15 04:39:04","","1","51","<p>Hello I have multiple csvs of stocks I generated from pandas. My goal seems quite simple, I measure the percentage difference in volume day by day</p>

<pre><code>    for ticker in tickers:
        df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))
        df['Volume_Pct_Change'] = df['Volume'].pct_change().fillna(0)
        df.to_csv('stonks_dfs/{}.csv'.format(ticker))
</code></pre>

<p>This was pretty easy. Now I want to find all the times the Volume is above 150%. I did this in another python script different from pulling the original data.</p>

<pre><code>with open('sp500tickers.pickle', ""rb"") as f:
        tickers = pickle.load(f)

    for ticker in tickers:
        df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))
        df_vpc = df.loc[df['Volume_Pct_Change'] &gt;= 1.5]
        df_vpc.to_csv('{}.csv'.format(ticker))
</code></pre>

<p>This works pretty good for me. </p>

<p>My problem is I want to pull a X amount of rows before and after the selected row ie (7 rows before and 30 rows after the selected row that has the Volume_Pct_change >= 1.5) that way I can graph out and see changes over time before and after the volume spike. Bonus for me if there is a way to use Numpy with this.</p>

<p>Edit 1:So running </p>

<p><code>df = pd.read_csv('AAPL.csv')
c=df['Volume_Pct_Change'] &gt;= 1.2
for idx in c:
    df.iloc[(idx-7):(idx+30)]</code></p>

<p>gives c the correct output of true when the condition exists</p>

<p><code>114,False
115,False
116,False
117,False
118,True
119,False
120,False
121,False</code></p>

<p>But after running <code>df.iloc[(idx-7):(idx+30)]</code> I see no change in df when I export it to a csv.</p>

<p>Edit 2: ok getting somewhere running</p>

<p><code>df = pd.read_csv('AAPL.csv')
c = df.index[df['Volume_Pct_Change'] &gt;= 1.2]
for idx in c:
    df.iloc[(idx-7):(idx+30)]</code></p>

<p>I see no difference in df when exporting but putting <code>d=df.iloc[(idx-7):(idx+30)]</code> works but only has one set of them in the dataframe which is confusing to me. Its only saving the last one. It seems it loops through overwriting the dataframe with everyone ending with the last one?</p>

<p>Final Edit: Thanks everyone for your help</p>

<p><code>df = pd.read_csv('AAPL.csv')
c = df.index[df['Volume_Pct_Change'] &gt;= 1.2]
for idx in c:
    d = df.iloc[(idx-7):(idx+30)]
    d.to_csv('{}.csv'.format(idx))</code></p>

<p>has the desired outcome and is pretty fast.</p>
","10604136","","10604136","","2019-09-15 06:54:45","2019-09-15 06:54:45","Using pandas or Numpy select a range of rows based on column data","<python><python-3.x><pandas><numpy>","2","0","","","","CC BY-SA 4.0","1"
"57503482","1","57503546","","2019-08-15 00:13:35","","0","51","<p>The following is a <code>minute</code> based <code>df</code>:</p>

<pre><code>GMT_Time                Open
2017-01-03 07:00:00     5.2475
2017-01-03 07:01:00     5.2475
2017-01-03 07:02:00     5.2475
2017-01-03 07:03:00     5.2475
2017-01-03 07:04:00     5.2475
2017-01-03 07:05:00     5.2475
2017-01-03 07:06:00     5.2475
.....
</code></pre>

<p>I want to use thefirst <code>GMT_Time</code> <code>Open</code> <code>Value(07:00:00)</code> and get the <code>COUNT</code> of values that are <code>Low</code> and <code>High</code> compared to <code>07:00:00</code> <code>Open</code> <code>Value</code> for that particular day.</p>

<p>My new <code>df</code> should look like this:</p>

<ul>
<li>It is a <code>Day</code> based dataset</li>
<li><code>Open</code> value is the value we used to compare</li>
<li><code>High</code> is the count of values that is greater than the <code>Open</code> at <code>07:00:00</code></li>
<li><code>Low</code> is the count of values that is less than the <code>Open</code> at <code>07:00:00</code></li>
<li><p><code>Same</code> is the count of values that is equal to the <code>Open</code> at <code>07:00:00</code></p>

<pre><code>GMT_Time                Open     High     Low    Same   
2017-01-03 07:00:00     5.2475   234      346    32 
2017-01-04 07:00:00     6.2475   234      346    12 
2017-01-05 07:00:00     4.2475   234      346    14 
</code></pre></li>
</ul>

<p><strong>What did I do?</strong></p>

<p>I am able to find the <code>High</code> and <code>low</code> value for a given day by doing:</p>

<pre><code>df.groupby(df['GMT_Time'].dt.floor('D')).Open.agg(['min','max'])
</code></pre>

<p>but I am trying to find the total count of values that are <code>High</code> or <code>low</code> compared to the <code>07:00:00</code> <code>Open</code> value. </p>

<p>How do I work on this problem?</p>
","9161607","","","","","2019-08-15 01:14:48","How to compare and count the high/low value using Pandas","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"56931413","1","56932524","","2019-07-08 08:53:35","","2","51","<p>I have a dataframe, a simplified example is below:</p>

<pre><code>   cycle  sensor  value
0      0       1   0.34
1      0       1   0.80
2      0       2   0.12
3      0       2   0.62
4      1       1   0.01
5      1       1   0.75
6      1       2   0.06
7      1       2   0.02
</code></pre>

<p>I'd like to plot the difference of the ""value"" column for each sensor, for each cycle: have x-axis as cycle number, y-axis as the difference in value, each series would be a given sensor number. e.g. the line for sensor one would be 0.46 and 0.74 over cycles 0 and 1.</p>

<p>In reality, I have many more columns (not used for this part of my code) and there are 144 cycles and 37 cycles. There are a few thousand values per sensor per cycle.</p>

<p>This is the code I have written, I don't get an error: a figure is created but no data shows.</p>

<pre><code>groups = unstacked_data.groupby([""cycle"", ""pressure""])

fig,ax = plt.subplots()
ax.set_xlabel(""Cycle Number"")
ax.set_ylabel(""Change in Normalised Pressure"")


for cycle, group in groups:
        ax.plot(cycle[0],group.value.max()-group.value.min(), label=group.pressure)
</code></pre>

<p>I'm not sure what I'm doing wrong, any advice would be appreciated! :)</p>
","11196704","","11196704","","2019-07-08 09:04:15","2019-07-08 11:13:01","Plot graph of difference in one column for grouped columns of pandas dataframe","<python><python-3.x><pandas><pandas-groupby>","1","0","","","","CC BY-SA 4.0","1"
"57701893","1","","","2019-08-29 01:36:04","","0","51","<p>I'm trying to read some text file (coordination file) using pandas. The contents are <a href=""https://i.stack.imgur.com/MPMTz.jpg"" rel=""nofollow noreferrer"">enter image description here</a>separated by space, and I used 'delim_whitespace=True', 'sep='\s+', but it doesn't work. I could just see some error message (parsererror: Expected 1 fields in line 5....).</p>

<p>My code for reading csv is as belows.</p>

<pre><code> df = pd.read_csv(filename, delimiter='\s+', header=None, engine='python')
</code></pre>

<p>I used delimiter, sep='  ', sep = '\s+', delim_whitespace=True.</p>

<p>Data reading of some coordination file</p>

<pre><code> df = pd.read_csv(filename, delimiter='\s+', header=None, engine='python')
</code></pre>

<p>parsererror: Expected 1 fields in line 5....</p>

<p>My file is as belows and I think the sixth line (coordination starting point) makes the problem. If I erase the initial five line, the separtor works. </p>

<p>If I use delimiter = '  ', then it works, but the separator does not work as shown belows.</p>

<p><a href=""https://i.stack.imgur.com/t7MrZ.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>
","11991843","","2535611","","2019-08-29 01:37:16","2019-08-29 01:37:16","Pandas Dataframe-space separator","<python-3.x><pandas>","0","2","","","","CC BY-SA 4.0","1"
"57704245","1","","","2019-08-29 06:30:17","","0","51","<p>I have a df that looks like </p>

<pre><code>         Date               Time 
2019-07-23 21:17:47.599  22:00:00.000
2019-07-23 21:11:46.973  21:50:00.000
</code></pre>

<p>I am using the following code to create the time difference value column </p>

<pre><code> df3['NEW'] = ((pd.to_timedelta(df3.Time.astype(str))- 
  pd.to_timedelta(df3.order_created_time_local.dt.strftime('%H:%M:%S'))) /
                 np.timedelta64(1, 'm'))
</code></pre>

<p>What I am getting is below and the needed column is what I want the values to be:</p>

<pre><code>Date                               Time      Difference      Needed
2019-08-21 21:57:28.721      21:30:00.000   -19.883333     -19.883333
2019-08-21 22:16:57.795      04:00:00.000   -1096.950000.    420.000
2019-08-21 21:00:00.00       21:30:00.000      30.000        30.000
</code></pre>

<p>because in the 24 hour clock 04:00:00.000 would be 4:00am and 22:16 would be 10:16PM, is there a way I can calculate the correct difference in minutes based on this? </p>

<p><strong>Detail</strong> : Time represents just a time for any day. Like the closing time for a shop. No date needed just a time and difference should be just the difference in actual minutes from Time - Date ( the time in date not needing the actual date) </p>

<p>Thanks!</p>
","8797830","","8797830","","2019-08-29 08:37:02","2019-08-29 08:37:02","Subtracting 2 timestamps based on a 24 hour time clock?","<python><python-3.x><pandas><datetime><data-science>","0","6","","","","CC BY-SA 4.0","1"
"57103316","1","","","2019-07-18 22:22:05","","1","51","<p>After hours of banging my head with my professor, I was finally able to read in a data file, navigate to the data I am concerned with, and build a pandas dataframe with some of that data.  My problem now, is that I need to do this 30 times, but cant figure out how to automate this while not overriding my data and while creating new columns. </p>

<pre><code>Filename = 'Al_v11.o' #Variable assigned to the file name
File_open = open(Filename,'r') #the r is for read access #opens my MCNP output in memory

Line = File_open.readline()  #Line will now call the command readline, which reads a single line of the MCNP file per time called

counts=0 #1tally appears mutiple times, this allows us to navigate to the second occurance. 
while (Line[0:15] != '1tally        1' or counts &lt; 2): #[0:15] is necessary to constain the while loop, otherwise it will compare 1tally to a whole line and not find a match
    Line = File_open.readline() #this reads a bunch of lines as the while loop iterates
    if Line[0:15] == '1tally        1':
        counts +=1  #we want the second iteration of 1tally
#print (counts)
print (File_open.readline())
print (File_open.readline())
print (File_open.readline())
print (File_open.readline())
print (File_open.readline())
print (File_open.readline()) #the data starts 6 lines below 1tally, thus we need these

myarray= []
while (Line[0:11] != '      total'): #total is the last line, we do not care about it
    Line = File_open.readline()
    if Line[0:11] != '      total': 
        myarray = np.append(myarray, np.fromstring(Line, dtype = float, sep='   ')) #writing data to myarray throughout whileloop

data = np.zeros((int(myarray.size/3),2)) #reshapes the array into a X,2       
data[:,0] = myarray[0:-1:3] #populates the first column
data[:,1] = myarray[1:-1:3] #populates the second column

Foil_1_Front = pd.DataFrame(data,columns=['Energy', 'Counts']) #creates pandas dataframe
</code></pre>

<p>I do not know how to iterate this throughout all 60 of my Energy and Counts columns (30 each), without overwriting myarray each time.</p>
","11805447","","6169688","","2019-07-19 02:27:26","2019-07-19 02:27:26","How to create multiple data frames with one loop?","<python><python-3.x><pandas>","0","6","","","","CC BY-SA 4.0","1"
"57748856","1","57749024","","2019-09-01 19:25:18","","-1","51","<p>I have two panda DataFrames:</p>

<p>Dataframe Yahoo:</p>

<pre><code>date        ticker  return
2017-01-03  CRM     0.018040121229614625
2017-01-03  MSFT    -0.0033444816053511683
2017-01-04  CRM     0.024198086662915008
2017-01-04  MSFT    -0.0028809218950064386
2017-01-05  CRM     -0.0002746875429199269
2017-01-05  MSFT    0.0017687731146487362
</code></pre>

<p>Dataframe Quandl:</p>

<pre><code>date        ticker  return
2017-01-03  CRM     0.018040120991250852
2017-01-03  MSFT    -0.003344466975803595
2017-01-04  CRM     0.024198103213211475
2017-01-04  MSFT    -0.0028809268004892363
2017-01-05  CRM     -0.00027464144673694513
2017-01-05  MSFT    0.0017687829680113065
</code></pre>

<p>I would like to get the standard deviation for the difference of Yahoo’s and Quandl’s 'return' data calculated across all ticker symbols for each day and data field. </p>

<p>How can I get that?</p>
","2132478","","13302","","2019-09-01 20:17:39","2019-09-01 20:17:39","Standard deviation for the difference of two dataframes with group by","<python><python-3.x><pandas><grouping><standard-deviation>","1","2","","2019-09-02 05:25:07","","CC BY-SA 4.0","1"
"56854798","1","56856030","","2019-07-02 14:32:59","","-1","50","<p>I have ndarray with shape <code>(2,1)</code>.
Every element is a ndarray with shape <code>(4)</code>
I want to make a dataframe with shape <code>(2,4)</code></p>

<p>currect shapes:</p>

<pre><code>df.shape = (2,1)
df[0].shape = (1,)
df[0][0].shape = (4,)
</code></pre>

<p>for example:</p>

<pre><code>df[0][0] = [1 2 2 4]
df[1][0] = [1 1 1 1]
</code></pre>

<p>I want its will look like this:</p>

<pre><code>df[0] = [1 2 2 4]
df[1] = [1 1 1 1]
</code></pre>
","2372856","","2372856","","2019-07-02 14:45:55","2019-07-03 08:01:03","Replace and flat Numpy array","<python><python-3.x><pandas><numpy><numpy-ndarray>","3","5","","","","CC BY-SA 4.0","1"
"56802740","1","56803015","","2019-06-28 07:34:38","","-1","50","<p>I have a dataframe:</p>

<pre><code>import numpy as np
import pandas as pd

arr = np.array([['a', 0, 1.2,12.5,3], ['a',1, 4,5.,6.885],
                ['a', 2, 2.3,3.133,4.3], ['a', 3, 5.678,6.,7.34556],
                ['a', 4, 6.5,7,8.1344], ['b',0, 10.7,11.4,12.1332],
                ['b',1, 14.,15,16.0155], ['b',2, 17.3,18.,9.11],
                ['b', 3, 22.2, 33.233, 1.2323], 
                ['c', 0, 1.1, 2.2, 3.3], 
                ['c', 1, 2.2, 3.43, 54.5],
                ['d', 0 , 2.2, 2.2, 3.],
                ['d',1, 3.4, 4., 5.6],
                ['d', 2, 3.3, 4, 5.]])

df = pd.DataFrame(arr, columns=['name', 'id', 'x', 'y', 'z'])

df['id'] = pd.to_numeric(df['id'])
df['x'] = pd.to_numeric(df['x'])
df['y'] = pd.to_numeric(df['y'])
df['z'] = pd.to_numeric(df['z'])

df
    name    id  x       y       z
0   a       0   1.2     12.5    3
1   a       1   4       5.0     6.885
2   a       2   2.3     3.133   4.3
3   a       3   5.678   6.0     7.34556
4   a       4   6.5     7       8.1344
5   b       0   10.7    11.4    12.1332
6   b       1   14.0    15      16.0155
7   b       2   17.3    18.0    9.11
8   b       3   22.2    33.233  1.2323
9   c       0   1.1     2.2     3.3
10  c       1   2.2     3.43    54.5
11  d       0   2.2     2.2     3.0
12  d       1   3.4     4.0     5.6
13  d       2   3.3     4       5.0
</code></pre>

<p>And I have an array with the same size:</p>

<pre><code>the_array = np.array([['a', 82.365],
                      ['a', 82.365],
                      ['a', 82.365],
                      ['a', 82.365],
                      ['b', 136.879],
                      ['b', 136.879],
                      ['b', 136.879],
                      ['b', 136.879],
                      [None, None],
                      [None, None],
                      [None, None],
                      [None, None],
                      [None, None],
                      [None, None]], dtype=object)
</code></pre>

<p>Now, I want to create a new column in df, in which I will fill the values of <code>thearray</code> according to column <code>name</code>.</p>

<p>I want at every row in df where the name is the same as the name in <code>thearray</code> to have the same value (as in <code>thearray</code>).</p>

<p><strong>Result I want:</strong></p>

<pre><code>    name    id  x         y       z         new_col
0   a       0   1.200   12.500  3.00000     82.365
1   a       1   4.000   5.000   6.88500     82.365
2   a       2   2.300   3.133   4.30000     82.365
3   a       3   5.678   6.000   7.34556     82.365
4   a       4   6.500   7.000   8.13440     82.365
5   b       0   10.700  11.400  12.13320    136.879
6   b       1   14.000  15.000  16.01550    136.879
7   b       2   17.300  18.000  9.11000     136.879
8   b       3   22.200  33.233  1.23230     136.879
9   c       0   1.100   2.200   3.30000     None
10  c       1   2.200   3.430   54.50000    None
11  d       0   2.200   2.200   3.00000     None
12  d       1   3.400   4.000   5.60000     None
13  d       2   3.300   4.000   5.00000     None
</code></pre>

<p>I tried:</p>

<pre><code>df['new_col'] = np.where(df['name'] == the_array[:, 0], the_array[:, 1], the_array[:, 1])
</code></pre>

<p>but I received:</p>

<pre><code>    name    id  x   y   z   new_col
0   a       0   1.200   12.500  3.00000     82.365
1   a       1   4.000   5.000   6.88500     82.365
2   a       2   2.300   3.133   4.30000     82.365
3   a       3   5.678   6.000   7.34556     82.365
4   a       4   6.500   7.000   8.13440     136.879
5   b       0   10.700  11.400  12.13320    136.879
6   b       1   14.000  15.000  16.01550    136.879
7   b       2   17.300  18.000  9.11000     136.879
8   b       3   22.200  33.233  1.23230     None
9   c       0   1.100   2.200   3.30000     None
10  c       1   2.200   3.430   54.50000    None
11  d       0   2.200   2.200   3.00000     None
12  d       1   3.400   4.000   5.60000     None
13  d       2   3.300   4.000   5.00000     None
</code></pre>
","583464","","1000551","","2019-06-28 07:54:30","2019-06-28 08:09:38","fill in new dataframe column according to array condition","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57104411","1","57104449","","2019-07-19 01:21:01","","0","50","<p>I've been working with large lists of data, where each element is a single string, ex: </p>

<pre><code>[""apple"", ""egg"", ""carrot"", ""milk"", ""apple"", ""apple""]
</code></pre>

<p>I've been turning these into pandas Series and then using value_counts() to count the occurrences of each unique item in the list. However, now I'm facing lists that contain lists: </p>

<pre><code>[""apple"", [""apple"", ""egg""], ""egg"", ""carrot"", [""milk"", ""egg""], 
 [""milk"", ""apple"", ""carrot""], ""apple""]
</code></pre>

<p>I'm looking for a way to count the occurrences of each unique list in addition to each unique string. </p>

<p>I've tried turning these lists of lists into series and using value_counts() on them, but it throws an <code>unhashable type: list</code> error. I get the same error when I change these sublists to sets or ndarrays.</p>

<p>I could always throw these lists into a bunch of for loops but I don't know if my computer has the computational power to do that in any reasonable amount of time. I'd love something that works with similar speed to value_counts(), but I can't for the life of me figure out how to implement it. Any insight would be appreciated.</p>
","8854642","","","","","2019-07-19 01:26:43","Counting occurrences of elements in a pandas series containing lists","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57604151","1","57604490","","2019-08-22 07:30:11","","0","50","<p>I have two dataframes <code>test1</code> and <code>test2</code>. My program logic is like below</p>

<pre><code>def write_file():
   test1.to_csv(('test1.csv'),index=None)

def process_file():
    test2= pd.read_csv('test1.csv',low_memory=False)

def write_processed_file():
   test2.to_csv(('test2.csv'),index=None)
</code></pre>

<p>I invoke all the above functions like below</p>

<pre><code>write_file()
process_file()
write_processed_file()
</code></pre>

<p>As you can see, I have two <code>write</code> functions just to write the dataframe because I want the <code>.csv</code> file names to be different for both the dataframes. If I follow the below <code>input argument</code> approach to have just one write function then I can have only one common file name. How do we get the datframe name?</p>

<pre><code>def write_file(df_name):
   df_name.to_csv(('common_file_name.csv'),index=None)
</code></pre>

<p>I expect my output <strong>to have two csv files with the name <code>test1.csv</code> and <code>test2.csv</code> without having two write functions</strong> </p>

<p>Basically I have 400-500 lines of code where has 15-18 lines if code to write dataframe to csv files. I would like to have one write function which accepts dataframe as input and provides the name of the dataframe as csv file name. </p>

<p>Is there anyway to get the dataframe name and save the file with the same name in a elegant and efficient manner?</p>
","10829044","","10829044","","2019-08-22 07:44:47","2019-08-22 07:50:48","Elegant way to fetch the dataframe name and save the file with same name","<python><python-3.x><pandas><dataframe><file-writing>","1","0","0","","","CC BY-SA 4.0","1"
"48835899","1","48836651","","2018-02-16 22:44:32","","3","50","<p>I have some data that I'm trying to group together:</p>

<pre><code>Serial_Num     Latitude Longitude
1950004S11059   -11.1   59.1
1950004S11059   -11.6   57.8
1950004S11059   -12.4   56
1950004S11059   -13.2   54.6
1950004S11059   -13.8   53.8
1950004S11059   -14.8   52.7
1950004S11059   -15.9   52
1950004S11059   -18.3   52.4
1950004S11059   -20     54
1950004S11059   -22.1   55.9
1950004S11059   -26.2   59.8
1950012S14150   -14     146.9
1950012S14150   -14.4   145.8
1950012S14150   -14.9   145.4
1950012S14150   -15.8   145.6
1950012S14150   -18.9   149.1
1950012S14150   -22.3   152.5
1950013S14139   -16     139
1950013S14139   -16.3   139
</code></pre>

<p>So simply, for each unique <code>Serial_Num</code>, I want the coordinates. I'm expecting something similar to:</p>

<pre><code>1950004S11059: {""GPS"": (-11.1 , 59.1) , (-11.6, 57.8) , (-12.4, 56), ..., (-26.2, 59.8)}
</code></pre>

<p>And then I can loop through the <code>GPS</code> coordinates for each <code>Serial_Num</code> and plot.</p>

<p>I have some scripts that I have used elsewhere, but mainly relies on the .csv data being used to set up a dictionary, with the <code>Serial_Num</code> as the key.</p>

<p>However, the data in the csv is sequential, and the sequence is important.  </p>

<p>What's a way to output, for each <code>Serial_Num</code>, the list of coordinates, in order as they are in the CSV?</p>

<p>Edit: I'm looking at Pandas right now, as it has a <code>groupBy</code> method that may help.</p>
","4650297","","9209546","","2018-06-21 22:48:54","2018-06-21 22:48:54","From CSV, group similar data together?","<python><python-3.x><pandas><csv><dataframe>","2","0","","","","CC BY-SA 3.0","1"
"49039289","1","49039609","","2018-02-28 21:58:30","","1","50","<p>I am using Pandas and Matplotlib in to make visualizations. I am having trouble creating a graph even though I feel as though I have all the information I should need. </p>

<pre><code>from matplotlib import rcParams
%matplotlib inline
rcParams['figure.figsize'] = 5, 4
sb.set_style('ticks')
B = df.groupby(['DATE']).BOROUGH.value_counts()
B

DATE        BOROUGH      
2018-02-14  QUEENS           205
            BROOKLYN         160
            MANHATTAN        123
            BRONX             85
            STATEN ISLAND     30
2018-02-15  QUEENS           177
            BROOKLYN         160
            MANHATTAN        130
            BRONX             84
            STATEN ISLAND     24
2018-02-16  QUEENS           152
            BROOKLYN         125
            MANHATTAN        118
            BRONX             90
            STATEN ISLAND     24
2018-02-17  QUEENS           163
            MANHATTAN        138
            BROOKLYN         130
            BRONX             76
            STATEN ISLAND     16
2018-02-18  QUEENS           138
            BROOKLYN         104
            MANHATTAN         96
            BRONX             54
            STATEN ISLAND     22
2018-02-19  QUEENS           132
            BROOKLYN         127
            MANHATTAN         60
            BRONX             53
            STATEN ISLAND     11
2018-02-20  QUEENS           141
            BROOKLYN         134
            MANHATTAN         73
            BRONX             57
            STATEN ISLAND      8
</code></pre>

<p>How can I plot this subset where x is the date and each Borough forms a seperate line on the same graph?</p>
","9425952","","5741205","","2018-02-28 22:29:56","2018-02-28 22:29:56","Making Multiline graphs with Groupby sub sets","<python-3.x><pandas><matplotlib><pandas-groupby>","1","2","","2018-02-28 23:29:38","","CC BY-SA 3.0","1"
"57417226","1","57417263","","2019-08-08 16:42:06","","1","50","<p>I have column in a <code>Pandas</code> dataframe (final_combine_df) that is called <code>GEOID</code>. I will have a 15 character string number like this : '371899201001045'. I want to create a new column in my data frame called <code>'CB_GrpID'</code> that is equal to just the first 12 characters of the <code>GEOID</code> values (ex: '371899201001'). I tried this, but it just returned the same <code>GEOID</code> value (non-truncated) in the new <code>'CB_GrpID'</code>:</p>

<pre><code>final_combine_df['CB_GrpID'] = final_combine_df['GEOID'][:12]
</code></pre>

<p>What am I doing wrong here?</p>

<pre><code>final_combine_df.iloc[0]['CB_GrpID']
&gt;&gt;371899201001045
</code></pre>
","5527646","","","","","2019-08-08 16:50:05","How to populating one column in a dataframe from the truncated value of another column","<python-3.x><pandas><dataframe><truncate>","2","1","","","","CC BY-SA 4.0","1"
"49461528","1","49461552","","2018-03-24 05:23:22","","2","50","<p>I am learning Pandas DataFrame and came across this code:</p>

<pre><code>df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6]]))
</code></pre>

<p>Now when I use <code>print(list(df.columns.values))</code> as suggested on <a href=""https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python"" rel=""nofollow noreferrer"">this page</a>, the output is:</p>

<pre><code>[0, 1, 2]
</code></pre>

<p>I am unable to understand the output. What are the values 0,1,2 signifying. Since the height of DataFrame is 2, I suppose the last value 2 is signifying the height. What about 0 and 1?</p>

<p>I apologize if this question is a duplicate. I couldn't find any relevant explanation. If there is any similar question, please mention the link. </p>

<p>Many thanks.</p>
","9361311","","9361311","","2018-03-24 05:34:18","2018-03-24 06:10:51","What does the output of this line in pandas dataframe signify?","<python-3.x><pandas><dataframe>","2","0","","","","CC BY-SA 3.0","1"
"56657420","1","56657584","","2019-06-18 21:40:32","","0","50","<p>I have a data frame where each of the rows (there are 2 rows total) are currently filled with 0's. Row 1 has three 0's and Row 2 has three 0's. I also have values that I have calculated using a loop and stored in a list. List 1 has three values and List 2 has three values. I need to replace the 0's in row 1 of my data frame with the three different values in List 1. I need to do the same thing with row 2 and List two. </p>

<p>I have really tried to read through the forums and find some code that applies but due to my beginner's knowledge, I am having difficulty applying any kind of code. I would greatly appreciate being pointed in the right direction. </p>

<p>For example, say Row 1 is 0,0,0 in my data frame and List 1 is 1,2,3, After running the necessary code, I would expect Row 1 to be updated to 1,2,3.</p>
","11257708","","","","","2019-06-18 21:59:47","Replacing values in a dataframe from a list","<python-3.x><pandas><dataframe>","1","1","","","","CC BY-SA 4.0","1"
"57263262","1","","","2019-07-30 01:38:09","","0","49","<p>I have a string which resulted from a datetime difference operation:</p>

<pre><code>a=pd.to_datetime(df['timestamp1'], format='%Y-%m-%dT%H:%M:%SZ')
b=pd.to_datetime(df['timestamp2'], format='%Y-%m-%dT%H:%M:%SZ')

df['timestamp3'] = b-a

OUT 
19 days 06:45:43 
</code></pre>

<p>Now I need to convert it into the integer 19. How can this be done?
Thanks!</p>
","11846355","","6622587","","2019-07-30 01:47:54","2019-07-30 01:47:54","How to convert datetime value into integer?","<python><python-3.x><pandas>","0","7","","","","CC BY-SA 4.0","1"
"56907065","1","56907185","","2019-07-05 17:18:03","","2","49","<p>I have a data frame that looks like this.</p>

<pre><code>0                                             1.144921                     
1                                             1.000000                     
2                                             1.119507                     
3                                                  inf                     
4                                             0.000000                     
5                                                  inf                     
6                                             0.000000                     
7                                             0.000000                     
8                                             1.000000                     
9                                             0.000000                     
10                                            0.000000                     
11                                            0.000000                     
12                                            1.793687                     
13                                                 inf    
</code></pre>

<p>I am trying to get rid of the '<code>inf</code>' string.  Basically, I just want to strip out all strings and keep only the numbers in the dataframe.  </p>

<p>I tried the following code below.</p>

<pre><code>kepler = re.sub(""\D"", """", kepler)
kepler = re.sub('[^0-9]','0', kepler)
</code></pre>

<p>When I run either of these lines of code I get the following error.</p>

<pre><code>TypeError: expected string or bytes-like object
</code></pre>

<p>If I have a very simple string, it actually does work.  So, this will work.</p>

<pre><code>s = '83jjdmi239450  19dkd'
s = re.sub(""\D"", """", s)
</code></pre>

<p>Unfortunately, the code doesn't work on my dataframe.  Any thoughts?  Thanks.</p>
","5212614","","2597213","","2019-07-05 19:28:04","2019-07-05 19:28:04","Can't seem to strip numbers from a string","<python><python-3.x><pandas><dataframe>","3","5","","2020-02-08 03:13:13","","CC BY-SA 4.0","1"
"56742470","1","","","2019-06-24 19:00:59","","0","49","<p>I have a pandas dataframe like this:</p>

<pre><code>index  country   value1   value2   value3
  0      USA      6         5        4
  1      USA      4         7        3
  2      USA      3         2        1
  3      CAN      4         5        7
  4      CAN      10        8        6
</code></pre>

<p>I want to convert this to a column-oriented dataframe like below for each value (i.e. value1, value2 or value3)</p>

<p>The first dataframe for value1 should look like this:</p>

<pre><code>index   USA     CAN
  0      6       4 
  1      4       10
  2      3       NaN
</code></pre>

<p>I have tried this code</p>

<pre><code>column = ['USA', 'CAN']
df_value1 = pd.DataFrame(columns=column)
temp = df['value1'].where(df['country']=='USA').dropna()
df_value1['USA'] = temp
temp = df['value1'].where(df['country']=='CAN').dropna()
pd.concat([df_value1, temp], axis=1, ignore_index=True)
</code></pre>

<p>When I concatenate along the column (axis=1) since CAN values start from row 3 I get the resulting df_vaue1 dataframe like this:</p>

<pre><code>index   USA     CAN
  0      6       NaN
  1      4       NaN
  2      3       NaN
  3     NaN       4
  4     NaN       10
</code></pre>

<p>But I want the df_value1 dataframe like this:</p>

<pre><code>index   USA     CAN
  0      6       4 
  1      4       10
  2      3       NaN
</code></pre>

<p>How can I achieve this?</p>
","10909507","","10909507","","2019-06-24 19:05:33","2019-06-24 19:37:00","Concatenating columns in pandas","<python-3.x><pandas><sklearn-pandas>","2","0","","","","CC BY-SA 4.0","1"
"57419728","1","57419847","","2019-08-08 19:53:31","","0","49","<p>I need help generating <code>Minute</code> based <code>time-range</code> for a pre-defined <code>Date</code>.</p>

<p>The <code>Date</code> range values will change so I should be able to update it.</p>

<p>I also want to exclude <code>Friday</code> and <code>Saturday</code> from the generated data.</p>

<p><strong>What did I do?</strong></p>

<p>I successfully generated the <code>date-range</code> by doing this:</p>

<pre><code>pd.date_range(start='1/1/2017', end='8/06/2019', freq='T')
</code></pre>

<p>But how do I add <code>Minute</code> data and exclude <code>Friday</code> and <code>Saturday</code>?</p>

<p>Once this is done, I want to create a column name called <code>'TIME_MIN'</code> and assign it to a <code>df</code></p>

<p>Could you please help?</p>
","9161607","","","","","2019-08-08 20:10:53","How to generate Fixed Minute based DateTime using Pandas","<python-3.x><pandas><dataframe><time-series>","1","2","","","","CC BY-SA 4.0","1"
"49621634","1","","","2018-04-03 03:11:35","","0","49","<p>What is the fastest way to split a very large file and write it to disk.</p>

<p>For e.g if I have data like</p>

<pre><code>chr    a_val    b_val   a_idx
2      1355     25d     abd
2      1785     25d     abd
2      1825     36g     ahj
3      1125     25d     abd
3      1568     25d     aky
3      2398     g67     abd
3      1125     25d     afd
3      1525     25d     abd
3 ....................
4 ..........
4 ........
</code></pre>

<p>Where I want to split by ""chr"" values.</p>

<p>I was thinking of applying pandas method in the following way:</p>

<pre><code>my_df = pd.read_csv(""my_file.txt"", sep='\t')
my_df = my_df.groupby('chr')

# split the file
for chr_, data in my_df:
    pd.Dataframe.to_csv(data, 'data_' + chr_ + '.txt', sep = '\t', header = True, index=False)
</code></pre>

<p>Pandas is very fast. But, would there be any other unix, linux or python based process method to do so in the fastest way possible.</p>

<p>Thanks,</p>
","6346698","","6346698","","2018-04-03 15:06:09","2018-04-03 15:06:09","fastest way to split files by unique group","<python><python-3.x><pandas><unix><split>","3","2","","","","CC BY-SA 3.0","1"
"56832320","1","","","2019-07-01 08:28:45","","0","49","<p>I have a data set include with temperature, humidity and wind. Here I want to predict future temperature value in next hour. </p>

<p>I used LSTM to predict future temperature value.
But when I run the model it showed up this error <code>Error when checking input: expected lstm_132_input to have 3 dimensions, but got array with shape (23, 1, 3, 1)</code></p>

<p>Can anyone help me to solve this problem?</p>

<p>Here is my code:</p>

<pre><code>    import datetime
    import time
    from sklearn.metrics import mean_squared_error
    import matplotlib.pyplot as plt 
    from matplotlib.dates import DateFormatter
    import numpy as np
    import pandas as pd 
    from sklearn.preprocessing import MinMaxScaler

    from sklearn import preprocessing
    from keras.layers.core import Dense, Dropout, Activation
    from keras.activations import linear
    from keras.layers.recurrent import LSTM
    from keras.models import Sequential
    from sklearn.preprocessing import MinMaxScaler


    data = pd.read_csv('data6.csv' , sep=',')
    data['date'] = pd.to_datetime(data['date'] + "" "" + data['time'], format='%m/%d/%Y %H:%M:%S')
    data.set_index('time', inplace=True)
    data = data.values
    data = data.astype('float32')
    # normalize the dataset
    def create_data(train,X,n_out=1):
    #data = np.reshape(train, (train.shape[0], train_shape[1], train_shape[2]))
    x,y=list(),list()
    start =0
    for _ in range(len(data)):
        in_end = start+X
        out_end= in_end + n_out
        if out_end &lt; len(data):
            x_input = data[start:in_end]
            x.append(x_input)
            y.append(data[in_end:out_end,0])
        start +=1
    return np.array(x),np.array(y)
    scaler = MinMaxScaler()
    data = scaler.fit_transform(data)
    # split into train and test sets
    train = int(len(data) * 0.6)
    test = len(data) - train
    train, test = data[0:train,:], data[train:len(data),:]
    X=1
    x_train, y_train = create_data(train,X)
    x_test, y_test = create_data(test,X)
    x_train=x_train.reshape(x_train.shape +(1,))
    x_test=x_test.reshape(x_test.shape + (1,))


    n_timesteps, n_features, n_outputs = x_train.shape[1], x_train.shape[2], x_train.shape[1]


    model = Sequential()
    model.add(LSTM(8, activation='relu', input_shape=(n_timesteps, n_features)))
    model.add(Dense(8,activation='relu'))
    model.add(Dense(n_outputs))
    model.compile(loss='mse', optimizer='adam')
    # fit network
    model.fit(x_train,y_train, epochs=10,batch_size=1, verbose=0)
</code></pre>

<p>My csv file:</p>

<p><a href=""https://docs.google.com/spreadsheets/d/1KsAtm0vSsBfOL8z42UAPAwljaO1-DSKfdD9lSu3lv80/edit#gid=35707506"" rel=""nofollow noreferrer"">My csv file.</a></p>

<p>My error:</p>

<p><a href=""https://i.stack.imgur.com/q1sLe.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q1sLe.png"" alt=""enter image description here""></a></p>

<p><a href=""https://i.stack.imgur.com/XdRMZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XdRMZ.png"" alt=""enter image description here""></a></p>

<p>model summary :</p>

<p><a href=""https://i.stack.imgur.com/WkdoE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/WkdoE.png"" alt=""enter image description here""></a></p>
","","user10270654","","user10270654","2019-07-01 17:52:04","2019-07-01 17:52:04","Error when checking input: expected lstm_132_input to have 3 dimensions, but got array with shape (23, 1, 3, 1)","<python-3.x><pandas><time><deep-learning><lstm>","1","0","","","","CC BY-SA 4.0","1"
"56855143","1","56855185","","2019-07-02 14:53:48","","4","49","<p>I have 2 col as</p>

<pre><code>  Latitude       Longitude    
  35.827085869   -95.67496156
</code></pre>

<p>Both are in float and I want it to convert into</p>

<pre><code> Latitude       Longitude       final
 35.827085869   -95.67496156    [35.827085869,-95.67496156]
</code></pre>

<p>How can I achieve that?</p>
","6759267","","","","","2019-07-03 17:09:52","concatenating two columns and get the new column","<python><python-3.x><pandas><python-2.7>","4","2","","","","CC BY-SA 4.0","1"
"49169705","1","","","2018-03-08 09:33:07","","0","48","<p>I have a pandas series with this type of format: </p>

<pre><code>    date
2017-03-15     1236.43
2017-03-16     1118.96
2017-03-17     1063.48
2017-03-18      940.18
2017-03-19      967.31
2017-03-20     1005.05
2017-03-21     1043.87
2017-03-22      997.78
2017-03-23     1022.02
2017-03-24      927.35
2017-03-25      890.43
2017-03-26      946.65
2017-03-27      961.80
2017-03-28     1015.45
               ...  
2018-03-06    10589.28
2018-03-07     9470.73
2018-03-08     9534.02
Name: BTC, Length: 1382, dtype: float64
</code></pre>

<p>I can't find a good way to split this by month, I have already tried with groupby and it gave me a pretty good output but it also assembles datas from different years and that's a problem </p>

<pre><code>IN[]: dflist_BTC = []
for group in data.BTC.groupby(df.index.month):
    dflist_BTC.append(group[1])

print(dflist_BTC)

OUT[]: [date
2018-01-01    12877.67
2018-01-02    12934.16
2018-01-03    14579.71
2018-01-04    14244.67
              ...
2018-01-28    11407.94
2018-01-29    11089.52
2018-01-30     9871.21
2018-01-31     9698.13
Name: BTC, dtype: float64, date
2018-02-01     8726.95
2018-02-02     7786.20
2018-02-03     8194.68
               ...
2018-02-27    10154.24
2018-02-28    10303.14
Name: BTC, dtype: float64, date
2017-03-15     1236.43
2017-03-16     1118.96
2017-03-17     1063.48
2017-03-18      940.18
2017-03-19      967.31
2017-03-20     1005.05
2017-03-21     1043.87
2017-03-22      997.78
2017-03-23     1022.02
2017-03-24      927.35
2017-03-25      890.43
2017-03-26      946.65
2017-03-27      961.80
2017-03-28     1015.45
2017-03-29     1008.34
2017-03-30     1020.93
2017-03-31     1035.18
#Here there is the problem, it combines 2017 and 2018
2018-03-01    10247.56
2018-03-02    10801.45
2018-03-03    11043.12
2018-03-04    11084.01
2018-03-05    11431.55
2018-03-06    10589.28
2018-03-07     9470.73
2018-03-08     9534.02
Name: BTC, dtype: float64, date
2017-04-01    1067.47
2017-04-02    1074.21
              ...
2017-12-30    11962.09
2017-12-31    12359.43
Name: BTC, dtype: float64]
</code></pre>

<p>I'm new in Stackoverflow and in coding in general, so I'm sorry if I didn't explain myself in a better way. I'll be grateful if you can help me </p>
","6765826","","","","","2018-03-08 09:40:05","Python Pandas splitting float series by date","<python><python-3.x><pandas><series><pandas-groupby>","1","2","","2018-03-08 09:38:01","","CC BY-SA 3.0","1"
"57555146","1","57555309","","2019-08-19 10:42:55","","1","48","<p>I have a pandas series with the following string values (For simplicity i have chosen to display it as a list):</p>

<pre><code>['quiero ir desde Calle Diagonal, 100, Barcelona, hasta Diagonal, 200, Barcelona',
 'quiero ir desde Plaza Catalunya, Barcelona, Barcelona, hasta Torrent Olla 218, Barcelona',
 'quiero ir desde Calle de Alcalá, 37, Madrid, hasta Puerta del Sol, 7, Madrid',
 'quiero ir desde Gran vía Corts Catalanes 290, Barcelona, hasta Plaza universitat, Barcelona',
 'quiero ir desde Carrer Rocafort, 160, Barcelona, hasta Plaça universitat, Barcelona',
 'quiero ir desde Calle paris, 62, Barcelona, hasta Entença metro, Barcelona',
 'quiero ir desde Urgell 1, Barna, hasta Paral.lel 190, Barna',]
</code></pre>

<p>I want to remove the duplicated words here, but to do that, i need to remove the punctuation first:</p>

<pre><code># Remove punctuation

import string

l = []

for i in df[""origin_destination""]:
    l.append(i.translate(str.maketrans('', '', string.punctuation)))
</code></pre>

<p>Now i can remove duplicated words:</p>

<pre><code># Remove Duplicates

def unique_string(l):
    ulist = []
    [ulist.append(x) for x in l if x not in ulist]
    return ulist

no_dup = []

for i in l:
    no_dup.append(' '.join(unique_string(i.split())))
</code></pre>

<p>This is the result i have now:</p>

<pre><code>['quiero ir desde Calle Diagonal 100 Barcelona hasta 200',
 'quiero ir desde Plaza Catalunya Barcelona hasta Torrent Olla 218',
 'quiero ir desde Calle de Alcalá 37 Madrid hasta Puerta del Sol 7',
 'quiero ir desde Gran vía Corts Catalanes 290 Barcelona hasta Plaza universitat',
 'quiero ir desde Carrer Rocafort 160 Barcelona hasta Plaça universitat',
 'quiero ir desde Calle paris 62 Barcelona hasta Entença metro',
 'quiero ir desde Urgell 1 Barna hasta Parallel 190',]
</code></pre>

<p>This is ok, my problem now is that i need to keep the punctuation but i haven't seen any way of removing duplicated words without stripping out the punctuation characters in the strings. So my questions are:</p>

<ol>
<li><p>Is there any way of removing duplicated words from a string without removing the punctuation?? How to do that if so in this case?</p></li>
<li><p>If the answer for the first question was negative, is there any way then to reinsert back the punctuation characters where they were?? How could i do that if so?</p></li>
</ol>

<p>Thank you very much in advance</p>
","9542954","","9542954","","2019-08-19 10:48:05","2019-08-19 10:53:46","How can i remove the punctuation in a string and reinsert it back after performing transformations","<python><python-3.x><string><pandas><punctuation>","1","3","","","","CC BY-SA 4.0","1"
"56647245","1","56647263","","2019-06-18 10:38:35","","1","48","<p>I have a <code>df</code>,</p>

<pre><code>doc_date    date_string
2019-06-03  WW0306
2019-06-07  EH0706
2019-08-08  19685
2019-08-09  258
2019-08-10  441573556
</code></pre>

<p><code>doc_date</code> is of <code>dateimte64</code> dtype, <code>date_string</code> is <code>string</code>, removing non-digit characters,</p>

<pre><code>s = df['date_string'].str.replace(r'\D+', '')

doc_date    date_string
2019-06-03  0306
2019-06-07  0706
2019-08-08  19685
2019-08-09  258
2019-08-10  441573556

s1 = to_datetime(s, errors='ignore', format='%d%m')

doc_date    date_string
2019-06-03  1900-06-03
2019-06-07  1900-06-07
2019-08-08  19685
2019-08-09  258
2019-08-10  441573556
</code></pre>

<p>Here I am wondering how to ignore those rows whose <code>date_string</code> cannot be converted to datetime; so I can create a boolean mask as,</p>

<pre><code> c1 = (df.doc_date.dt.dayofyear - s1.dt.dayofyear).abs().le(180)
</code></pre>

<p>another thing is how to get <code>c1</code> the same length as of <code>s</code> that any <code>date_string</code> that cannot be converted to <code>datetime</code> will get <code>False</code> in <code>c1</code>;</p>
","766708","","","","","2019-06-18 10:55:36","pandas how to ignore column cells that cannot be converted to datetime for calculating time delta","<python><python-3.x><pandas><datetime><series>","1","0","","","","CC BY-SA 4.0","1"
"57102349","1","57102975","","2019-07-18 20:39:59","","0","48","<p>I would like to parse as a table the data in the list of lists Wikipedia page at <a href=""https://en.wikipedia.org/wiki/List_of_trees_and_shrubs_by_taxonomic_family"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/List_of_trees_and_shrubs_by_taxonomic_family</a></p>

<p>I want to create a table linking the first word of each subheading containing 'family' to the first word each of its main bullet points, so I am not interested in much part of the text.</p>

<p>For an example, I would like an output like this:</p>

<pre><code>    Araucariaceae   Agathis
    Araucariaceae   Araucaria
    Cupressaceae    Athrotaxis
</code></pre>

<p>for a page like this: [sorry, I have enough reputation to post pictures in the Maths StackExchange, but not here]</p>

<pre><code>    Gymnosperms
    Conifers
    Araucariaceae: monkey-puzzle family
    Agathis – kauri conifers        
    Agathis australis – kauri pine; dammar
    Agathis lanceolata – red kauri
    Agathis robusta – Dundathu pine; Queensland kauri; smooth bark kauri
    Araucaria – monkey puzzle trees
    Araucaria angustifolia – Paraná pine
    Araucaria araucana – monkey-puzzle tree
    Araucaria bidwillii – bunya-bunya
    Araucaria columnaris – Cook pine
    Araucaria cunninghamii – Moreton Bay pine; hoop pine
    Araucaria heterophylla – Norfolk Island pine
    Araucaria hunsteinii – klinki
    Cupressaceae: cypress family
    Athrotaxis – Tasmanian cedars
    Athrotaxis cupressoides – pencil pine
    Athrotaxis selaginoides – King Billy pine
</code></pre>

<p>I looked up the wikipediaAPI package, but didn't find anything specific. Is there a function which does something similar?
Moreover, I don't think I can handle so much raw text in a neat way with the str package.</p>

<p>Any advice?</p>
","11805119","","","","","2019-07-18 21:43:12","Organising a 'list of lists' Wiki page into a table","<python><python-3.x><pandas><wikipedia><wikipedia-api>","2","2","","","","CC BY-SA 4.0","1"
"56657699","1","56691382","","2019-06-18 22:12:49","","0","47","<p>I have not worked with Pandas before and I am seeking guidance on the best course of action.</p>

<p>Currently, I have an excel(.xlsx)spreadsheet that I am reading into a data Pandas DataFrame. Within that excel spread sheet, it contains account data, document control number, contract id, manufacturer contract id, series number, include exclude, start date, end date and vendors customer id.</p>

<p>From that data, all of the account numbers need to be copied back to every row of data from document key co, document control number, contract id, manufacturer contract id, series number, include exclude, start date, end date and vendors customer id.</p>

<p>Here is a sample of the data:
<img src=""https://i.imgur.com/2CmjIYQ.jpg"" alt=""spreadsheet""></p>

<p>I've read in the DataFrame and iterated over the DataFrame with the following code:</p>

<pre><code>#reads in template data. Keeps leading zeros in column B and prevents ""NaN"" from appearing in blank cells
df = pd.read_excel('Contracts.xlsx', converters = {'document_key_co' : lambda x: str(x)}, na_filter = False)


#iterates over rows
for row in df.itertuples():
    print(row)
</code></pre>

<p>After doing those things, that is where I am stuck. The desired outcome is this:</p>

<p><img src=""https://i.imgur.com/sQE2tE9.jpg"" alt=""spreadsheet""></p>

<p>As you can see there are three accounts copied to the each of the contract id's.</p>

<p>Reading through the Pandas documentation, I considered separating each account into a separate DataFrame and using concat/merging it into another DataFrame that included document key co - vendors customer id, but felt like that was a lot of extra code when there's a likely a better solution.</p>
","11632865","","11632865","","2019-06-19 16:09:51","2019-06-20 17:47:27","Copying data from a DataFrame and writing back to excel?","<python-3.x><pandas><dataframe>","1","2","","","","CC BY-SA 4.0","1"
"57705851","1","57706083","","2019-08-29 08:11:17","","1","47","<p>I have a dataframe looks like below:</p>

<pre><code>test = pd.DataFrame({""location"": [""a"", ""b"", ""c""], ""store"": [1,2,3], ""barcode1"" : [1, 0 ,25], ""barcode2"" : [4,0,11], ""barcode3"" : [5,5,0]})   
</code></pre>

<p><a href=""https://i.stack.imgur.com/QebXs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/QebXs.png"" alt=""test""></a></p>

<p>I'd like to replace values of bar codes with ""zero"", when they are less than zero,""low"", when they are less than a threshold (for example ""5"") and ""ok"" when they are above that threshold.however I do not want to write it in a loop, since my real dataframe is the size of (1415,402) and that would be much time consuming.</p>

<p>I have tried the below code:</p>

<pre><code>test.replace(test.iloc[:,2:] &lt;= 0 , ""zero"", inplace = True)
</code></pre>

<p>looks fine for replacing zeros. but when I want to go to next replacement like below:</p>

<pre><code>test.replace(test.iloc[:,2:] &lt;= 5 , ""low"", inplace = True)
</code></pre>

<p>I get this error ""'&lt;=' not supported between instances of 'str' and 'int'"" and I think thats because the 0 values are now replaced with ""zero"".
therefore I'd like to do the replacements once at a time and without for loop.
Any help would be appreciated and sorry for long explanation.</p>
","8794465","","1830895","","2019-08-29 08:17:38","2019-09-22 06:59:03","replacing values based on different conditions","<python-3.x><pandas>","2","2","","","","CC BY-SA 4.0","1"
"57703100","1","57703510","","2019-08-29 04:44:31","","1","47","<p>I'm trying to create a pandas dataframe using parts of a nested dictionary containing keys, sub-keys, and lists. I can create a dataframe with the keys and sub-keys I need, but I can't figure how to pull in the lists that are associated with the sub-keys.</p>

<p>My dictionary looks like this:</p>

<pre><code>medical_code_dict = {'Cardiac': {'snomed': ['123456789'],
                                 'icd10': [],
                                 'icd9': ['V12.3'],
                                 'loinc': {'125-6':['234','567','890'],
                                           '542-0':['098','765','432']}},
                      'Stroke': {'snomed': [],
                                 'icd10': ['Z12.3'],
                                 'icd9': [],
                                 'loinc': {}},
                      'Blindness': {'snomed': [],
                                    'icd10': [],
                                    'icd9': [],
                                    'loinc': {'345-7':['345','780']}}}
</code></pre>

<p>I would like to create a dataframe of the loinc code info, including the lists. I can get part of the way there by doing this:</p>

<pre><code>loinc_test_1 = {}

for key in medical_code_dict.keys():
    loinc_test_1[key] = (medical_code_dict[key]['loinc'])

loinc_test_2 = {}

for k,v in loinc_test_1.items():
    for x in v:
        loinc_test_2.setdefault(x,[]).append(k)

loinctable = pd.DataFrame(loinc_test_2.items(), columns=['loinc', 'SDOH'])

loinctable

     loinc   SDOH
0    125-6   [Cardiac]
1    542-0   [Cardiac]
2    345-7   [Blindness]
</code></pre>

<p>But I can't figure out how to pull in the lists into the dataframe. I would like the output to look like this:</p>

<pre><code>    loinc   SDOH         response
0   125-6   [Cardiac]    234
1   125-6   [Cardiac]    567
2   125-6   [Cardiac]    890
3   542-0   [Cardiac]    098
4   542-0   [Cardiac]    765
5   542-0   [Cardiac]    432
6   345-7   [Blindness]  345
7   345-7   [Blindness]  780
</code></pre>
","9447301","","9447301","","2019-08-29 04:53:00","2019-08-29 05:26:57","Creating dataframe from a nested python dictionary of keys, sub-keys, and lists","<python-3.x><pandas><dataframe><dictionary>","1","0","1","","","CC BY-SA 4.0","1"
"56772444","1","56772724","","2019-06-26 12:07:14","","1","47","<p>I have an excel file with product names. First row is the category (A1: Water, A2: Sparkling, A3:Still, B1: Soft Drinks, B2: Coca Cola, B3: Orange Juice, B4:Lemonade etc.), each cell below is a different product. I want to keep this list in a viewable format (not comma separated etc.) as this is very easy for anybody to update the product names (I have a second person running the script without understanding the script)</p>

<p><a href=""https://i.stack.imgur.com/LWhgW.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LWhgW.png"" alt=""A1: Water, A2: Sparkling, A3:Still, B1: Soft Drinks, B2: Coca Cola, B3: Orange Juice, B4:Lemonade etc.""></a></p>

<p>If it helps I can also have the excel file in a CSV format and I can also move the categories from the top row to the first column</p>

<p><a href=""https://i.stack.imgur.com/IkErI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/IkErI.png"" alt=""Alternative format of Excel file""></a></p>

<p>I would like to replace the cells of a dataframe (df) with the product categories. For example, Coca Cola would become Soft Drinks. If the product is not in the excel it would not be replaced (ex. Cookie).</p>

<pre><code>print(df)

       Product  Quantity
0      Coca Cola  1234
1      Cookie     4
2      Still      333
3      Chips      88
</code></pre>

<p>Expected Outcome:</p>

<pre><code>print (df1)

       Product      Quantity
0      Soft Drinks   1234
1      Cookie        4
2      Water         333
3      Snacks        88
</code></pre>
","10748447","","","","","2019-06-26 12:33:01","Use Excel sheet to create dictionary in order to replace values","<python><python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"48725214","1","48725547","","2018-02-10 20:23:37","","1","47","<p>I have a dataframe that looks like:</p>

<pre><code>import pandas as pd
import datetime as dt

df= pd.DataFrame({'date':['2017-12-31','2017-12-31'],'type':['Asset','Liab'],'Amount':[100,-100],'Maturity Date':['2019-01-02','2018-01-01']})

df
</code></pre>

<p>I am trying to build a roll-off profile by checking if the 'Maturity Date' is greater than a 'date' in the future. I am trying to achieve something like:</p>

<pre><code>#First Month
df1=df[df['Maturity Date']&gt;'2018-01-31']
df1['date']='2018-01-31'

#Second Month
df2=df[df['Maturity Date']&gt;'2018-02-28']
df2['date']='2018-02-28'

#third Month
df3=df[df['Maturity Date']&gt;'2018-03-31']
df3['date']='2018-02-31'

#first quarter
qf1=df[df['Maturity Date']&gt;'2018-06-30']
qf1['date']='2018-06-30'


#concatenate
df=pd.concat([df,df1,df2,df3,qf1])


df
</code></pre>

<p>I was wondering if there is a way to :</p>

<p>Allow an arbitrary long number of dates without repeating code</p>
","8300917","","8300917","","2018-02-10 21:48:14","2018-02-13 07:12:44","roll off profile stacking data frames","<python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"48521986","1","48522190","","2018-01-30 12:54:43","","-1","47","<p>i have a string like below.</p>

<p>stg = ""Abel read (reading)|book(peripheral)~Q27.8#basillary NEC~Q28.1|| ""</p>

<h2>Requirement:</h2>

<p>Need to delete the character between two keywords ~ and # and then print the remaining.</p>

<h2>Output:</h2>

<p>""Abel read (reading)|book(peripheral)basillary NEC~Q28.1|| ""</p>
","7993248","","","","","2018-01-30 13:08:44","deleting characters between two different keywords","<python-3.x><pandas><pandas-groupby>","1","3","","2018-01-31 15:47:34","","CC BY-SA 3.0","1"
"58731756","1","58731867","","2019-11-06 13:53:56","","1","47","<p>I would like to count the frequency of words in a data frame. Here is an example of what i'm trying to achieve.</p>

<pre><code>words = ['Dungeon',
'Crawling',
'Puzzle',
'RPG',]

desc = 
0       [Dungeon, count, game, kid, draw, toddler, Unique]
1       [Beautiful, simple, music, application, toddle]
2       [Fun, intuitive, number, game, baby, toddler]
</code></pre>

<p>Note that desc is a 1690 rows pandas data frame.</p>

<p>Now I would like to check <code>words[i] in desc</code>
I do not want to have nested for loop, so  made a function to just check if the word is in the desc and then use <code>apply()</code> to each row and then use <code>sum</code>.</p>

<p>The function I got is:</p>

<pre><code>def tmp(word, desc):
    return (word in desc)
</code></pre>

<p>However, when I use the following code: <code>desc.apply(tmp, args = words[0])</code> I get the error that states: <code>tmp() takes 2 positional arguments but 8 were given</code>. However, when I manually use it with values <code>tmp(words[0], desc[0])</code> it works just fine....</p>
","9144522","","","","","2019-11-06 14:00:22","Counting in how many rows a value exists","<python-3.x><pandas>","1","2","","","","CC BY-SA 4.0","1"
"48857403","1","48857794","","2018-02-18 22:42:36","","1","47","<p>I am dealing a rare error while making some machine learning with a dataset loaded using pandas.
This is the error I am getting: 
<a href=""https://i.stack.imgur.com/V9zxF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/V9zxF.png"" alt=""enter image description here""></a></p>

<p>I have been reading something releated to it and it seems to be due to the columns and how pandas interpret them but I have no clue what can be wrong.
This is the code I am using for this:</p>

<pre><code>import pandas as pd
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/pima-
indians-diabetes/pima-indians-diabetes.data'
col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 
'pedigree', 'age', 'label']
pima = pd.read_csv(url, header=None, names=col_names)
# define X and y
feature_cols = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 
'pedigree', 'age']
X = pima[feature_cols]
y = pima.label
#k fold cv
from sklearn.model_selection import KFold, cross_val_score
kf = KFold(n_splits=10) #define number of splits
kf.get_n_splits(X) #to check how many splits will be done.
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
clf = LinearDiscriminantAnalysis() #select the model for train, test in kf.split(X, y):
for train, test in kf.split(X, y):
    y_pred_prob = clf.fit(X[train], y[train]).predict_proba(X[test])
    y_pred_class = clf.predict(X[test])
</code></pre>

<p>Thanks in advance</p>
","7540620","","","","","2018-02-19 00:06:56","Rare error while computing dataframe by using pandas","<python-3.x><pandas><scikit-learn>","1","0","","","","CC BY-SA 3.0","1"
"56757624","1","56757919","","2019-06-25 15:39:02","","1","47","<p>I have a dataframe with 4 columns and I want to do the following steps (ideally in one code):
- Filter rows where the sum of the 4 columns is lower than 0.9
- Multiply each cell in each row so that the sum of the row is 0.9
- In case there is a 0 in any cell, this cell stays unchanged (as multiplying 0 with anything remains 0)
- At the end display all rows, also the ones that were not changed</p>

<p>Here is an example dataframe:</p>

<pre><code>df = pd.DataFrame({'A':[0.03, 0.0, 0.7],
           'B': [0.1234, 0.4, 0.333],
           'C': [0.5, 0.4, 0.0333]})


print (df)
  Name    A    B    C   
0 Bread  0.03 0.1234 0.5000 
1 Butter 0.00 0.4000 0.4000
2 Cheese 0.70 0.3330 0.0333 

Sum = df[""A""]+df[""B""]+df[""C""]
print (Sum)

0    0.6534
1    0.8000
2    1.0663
</code></pre>

<p>Now only rows 0 and 1 should be affected by the algorithm</p>

<p>I had used this one which worked partly here: </p>

<pre><code>df = df4.mul(0.9/df4.sum(axis=1),axis=0)
</code></pre>

<p>But I do now know how to work only with the columns A to C and how I can first filter by the rows where the sum is below 0.9 and then how to show all rows again.</p>

<p>So my desired outcome is something like this:</p>

<pre><code>print (df)
   Name     A         B         C
0  Bread    0.0414  0.170292  0.690000
1  Butter   0.0000  0.452000  0.452000
2  Cheese   0.70    0.3330   0.0333
</code></pre>

<p>Important, all columns (including product column) and rows should still be there and the format be a dataframe with all of the rows. I only added the sum function below to see that they add up to 0.9 or more. </p>

<pre><code>Sum = df[""A""]+df[""B""]+df[""C""]
    print (Sum)

0    0.9
1    0.9
2    1.0663
</code></pre>
","10748447","","10748447","","2019-06-26 09:50:50","2019-06-26 11:14:44","Increase specific rows by multiplication until sum of columns fulfils criteria","<python><python-3.x><pandas><dataframe>","2","1","","","","CC BY-SA 4.0","1"
"42434944","1","","","2017-02-24 09:23:12","","1","47","<p>I have a csv file.
One column is the list of the mean altitude of some town.
One of his element, for exemple, can be [571.0, 428.0, 600.0, 410.0, 588.0, 520.0, 649.0, 374.0]</p>

<p>When I do:</p>

<pre><code>data=pd.read_csv('test.csv', sep="";"", encoding=""utf-8-sig"",dtype={'Mean_Altitude':list})

a=data['Mean_Altitude'][0]
print a
print type(a)
</code></pre>

<p>I get:</p>

<pre><code>[571.0, 428.0, 600.0, 410.0, 588.0, 520.0, 649.0, 374.0]
&lt;type 'unicode'&gt;
</code></pre>

<p>I would like to have a list of float.... Not a unicode or a string....</p>

<p>Do you know how I can do that?</p>
","7615984","","2867928","","2017-02-24 09:25:19","2017-02-24 09:35:17","How to import a csv column with a dtype liste rather than str","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"57862525","1","57862625","","2019-09-10 00:10:36","","0","47","<p>I am manipulating strings sequentially. However, it looks bulky and may also not be efficient in performance in code execution. Is there a better way to run this maybe in a function?</p>

<pre><code>df=['Apple sauce','Banana &amp; peach','c(&amp;)a']

df.columns = df.columns.str.lower()
df.columns = df.columns.str.replace(' ', '')
df.columns = df.columns.str.replace('&amp;','') 
df.columns = df.columns.str.replace('(','')
df.columns = df.columns.str.replace(')','')

Desired Out: df=['applesauce','bananapeach','ca']
</code></pre>
","9727789","","5014455","","2019-09-10 00:22:49","2019-09-10 02:22:02","A more efficient way to format strings sequentially?","<python><python-3.x><pandas>","2","3","","","","CC BY-SA 4.0","1"
"57300260","1","57301282","","2019-08-01 00:03:13","","1","46","<p>I have a .csv file that I import into a Python Pandas dataframe. It starts off looking like this:</p>

<p><a href=""https://i.stack.imgur.com/eJI4Z.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eJI4Z.png"" alt=""First""></a></p>

<p>...then after I run some one-hot encoding, I employ a ""df.sample(frac=1)"" function to randomize all the rows of the dataframe, which gets me a result that looks like this:</p>

<p><a href=""https://i.stack.imgur.com/FBNC8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/FBNC8.png"" alt=""Second""></a></p>

<p>...but how do I now get rid of that added first row? I don't want that row to be included in my next regression step. I've tried:</p>

<pre><code>df.sample(frac=1).reset_index(drop=True)
</code></pre>

<p>...but all that does is order that first column into ascending order (1 - X).</p>

<p>Note how the column doesn't have a name, so using "".drop"" with a column name won't work? Ideas? Thanks!!!</p>
","10018602","","","","","2019-08-01 03:01:38","How to Drop Added Column After Using ""df.sample(frac=1)""?","<python><python-3.x><pandas>","1","2","","","","CC BY-SA 4.0","1"
"49656332","1","49656553","","2018-04-04 16:47:47","","1","46","<p>I'm trying to pick a particular column from a csv file using Python's Pandas module, where I would like to fetch the <code>Hostname</code> if the column <code>Group</code> is <code>SJ</code> or <code>DC</code>.</p>

<p>Below is what I'm trying but it's not printing anything:</p>

<pre><code>import csv
import pandas as pd
pd.set_option('display.height', 500)
pd.set_option('display.max_rows', 5000)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 500)

low_memory=False
data = pd.read_csv('splnk.csv', usecols=['Hostname', 'Group'])
for line in data:
    if 'DC' and 'SJ' in line:
        print(line)
</code></pre>

<p>The <code>data</code> variable contains the values for <code>Hostname</code> &amp; <code>Group</code> columns as follows:</p>

<pre><code>11960      NaN          DB-Server
11961       DC          Sap-Server
11962       SJ          comput-server
</code></pre>

<p>Note: while printing the data it stripped the data and does not print complete data.</p>

<p>PS: I have used the <code>pandas.set_option</code> to get the complete data on the terminal!</p>
","","user9265709","","user9265709","2018-04-04 17:48:11","2018-04-04 17:48:11","Use Pandas to extract the values from a column based on some condition","<python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"50486933","1","50487023","","2018-05-23 11:12:06","","0","46","<p>I am wondering how to use regex remove any non-numeric chars while only selecting non-empty and spaces (a single value may contain one or multiple spaces) values for a series in a more efficient way,</p>

<pre><code>df['numeric_no'] = df['id'].apply(lambda x: re.sub(""[^0-9]"", """", x))
df = df[(df['numeric_no'] != '') &amp; (df['numeric_no'] != ' ')]
</code></pre>

<p>some sample data for the <code>df</code></p>

<pre><code>numeric_no
B-27000
44-11-E
LAND-11-4
17772A
88LL9A
321LP-3
UNIT 9 CAM -00-12
WWcard_055_34QE
EE119.45
aaa
b  b
</code></pre>

<p>the result will look like</p>

<pre><code>numeric_no
27000
4411
114
17772
889
3213
90012
05534
119.45
</code></pre>
","766708","","1020526","","2018-05-23 19:52:11","2018-05-23 19:52:11","how to replace non-numeric chars using regex","<regex><python-3.x><pandas><dataframe>","3","6","","","","CC BY-SA 4.0","1"
"57024082","1","","","2019-07-14 02:14:08","","1","46","<p>Trying the following code to get all information like price, location, area, etc. of the listed real estate. I have the following code which is returning <code>No tables found</code>:</p>

<pre><code>import pandas as pd
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager

driver = webdriver.Chrome(ChromeDriverManager().install())
driver.get(""https://uybor.uz/en/prodazha-kvartir/kvartiry-v-tashkente?page=2"")

html = driver.page_source

tables = pd.read_html(html)
data = tables[1]

driver.close()
</code></pre>

<p>Any help would be appreciated!</p>
","7256873","","","","","2019-07-14 10:37:21","Cannot get the tables from website using pandas and selenium","<python-3.x><pandas><selenium>","1","0","","","","CC BY-SA 4.0","1"
"57859603","1","57896385","","2019-09-09 18:42:43","","0","46","<p>I have 2 dataframes, df1 and df2 as shown below:</p>

<p><strong>df1:</strong></p>

<pre><code>     Name          Code          Title_num
 0  Title_1        0             TN_1234_4687
 1  Title_2        0             TN_1234_7053
 2  off_1          18301         TN_1234_1915
 3  off_2          18302         TN_1234_7068
 4  off_3          18303         TN_1234_1828
</code></pre>

<p><strong>df2:</strong></p>

<pre><code>     A_Code     T_Code
 0  000000086   18301   
 1  000000126   18302   
 2  000001236   18303   
 3  000012346   18938   
 4  000123456   18910   
 5  000123457   18301
</code></pre>

<p>Where T_code in df2 is the same as Code in df1. I want to join column Title_num in df1 to df2. </p>

<p>For example, if 'T_Code' in df2 matches 'code' in df1, i want the value in column df1['Title_num'] to be joined to df2. If the value does not exist, NaN should be populated.</p>

<p><strong>Expected output (df2 after join):</strong></p>

<pre><code>    A_Code      T_Code   Title_num
 0  000000086   18301    TN_1234_1915
 1  000000126   18302    TN_1234_7068
 2  000001236   18303    TN_1234_1828
 3  000012346   18938    NaN
 4  000123456   18910    NaN
 5  000123457   18301    TN_1234_1915
</code></pre>

<p>For this, I renamed column code in df1 to 'T_code' so as to match the name on df2. Then I ran the following code:</p>

<pre><code> df2.merge(df1,on='T-Code',how='left')
</code></pre>

<p>This gave the following error: 'T_code' # Check for duplicates</p>

<p>Now, one thing to note is in df2, duplicate T_codes will exist while in df1, Code is unique. I want the Title_num values in df2 to be always appear based on the T_code value [Check row 5 of expected output. T_code value is same as row 1]. </p>

<p>Do let me know of a method to perform this. Any help is much appreciated!</p>
","10552640","","10552640","","2019-09-11 20:29:18","2019-09-11 20:29:18","Join one column of a dataframe with another dataframe based on a condition","<python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"57264698","1","57264961","","2019-07-30 05:12:49","","0","46","<p>I'd like to know if someone can help me with separating two different strings properly.</p>

<p>Dataset:</p>

<pre><code>my_series = pd.Series([[""asd"", ""#ghj""],[""qwe""],[""dsa"", ""#asd""],[""poi""]])
</code></pre>

<p>My Code:</p>

<pre><code>category1 = []
category2 = []

for word_list in my_series:
    for v in word_list:
        if ""#"" not in v:
            category1.append({v : 1})
        else:
            category2.append({v : 1})
</code></pre>

<p>Category1 [Out]:</p>

<pre><code>[{'asd': 1}, {'qwe': 1}, {'dsa': 1}, {'poi': 1}]
</code></pre>

<p>Category2 [Out]:</p>

<pre><code>[{'#ghj': 1}, {'#asd': 1}]
</code></pre>

<p>Expecting</p>

<p>Category2 [Out]:</p>

<pre><code>[{'#ghj': 1}, {""No category 2"" : 1}, {'#asd': 1}, {""No category 2"" : 1}]
</code></pre>
","10193760","","10193760","","2019-07-30 05:22:15","2019-07-30 07:02:29","Separating two different string format in a list","<python-3.x><pandas>","2","6","","","","CC BY-SA 4.0","1"
"57748771","1","57748882","","2019-09-01 19:15:19","","0","45","<p>Suppose my data looks like:</p>

<pre class=""lang-py prettyprint-override""><code>data = {'Date':['2019-07-06', '2019-08-04', '2019-07-05', '2019-08-06'], 'Attending Cost': [1, 1, 1, 1]}
data_2 = pd.DataFrame.from_dict(data)
</code></pre>

<p>I want to select from it all the data that is between 2019-08-04 and 2019-08-06 inclusive. More generally, I have data arranged by month, and I want to select all data from one particular month. However, I have some outliers in my data which is not from that month but it is sitting in between them. I want to include these outliers in my selections as well. Note also within one month the date is not ordered. How should I achieve this?</p>
","11970440","","6925185","","2019-09-01 19:20:18","2019-09-01 19:50:56","How to select the data between some conditions","<python><python-3.x><pandas>","3","1","","","","CC BY-SA 4.0","1"
"57339315","1","","","2019-08-03 14:39:21","","0","45","<p>I have data of concentrations for every day of year 2005 until 2018. I want to read three columns of three different files and combine them into one, so I can plot them. </p>

<p>Data:file 1</p>

<pre><code>time, mean_OMNO2d_003_ColumnAmountNO2CloudScreened
2005-01-01,-1.267651e+30
2005-01-02,4.90778397e+15
...
2018-12-31,-1.267651e+30
</code></pre>

<p>Data:file 2</p>

<pre><code>time, OMNO2d_003_ColumnAmountNO2TropCloudScreened
2005-01-01,-1.267651e+30
2005-01-02,3.07444147e+15
...
</code></pre>

<p>Data:file 3</p>

<pre><code>time, OMSO2e_003_ColumnAmountSO2_PBL
2005-01-01,-1.267651e+30
2005-01-02,-0.0144000314
...
</code></pre>

<p>I want to plot <code>time</code> and <code>mean_OMNO2d_003_ColumnAmountNO2CloudScreened</code>, <code>OMNO2d_003_ColumnAmountNO2TropCloudScreened</code>, <code>OMSO2e_003_ColumnAmountSO2_PBL</code> into one graph.</p>

<pre><code>import glob
import pandas as pd

file_list = glob.glob('*.csv')

no= []
no2=[]
so2=[]


for f in file_list:
    df= pd.read_csv(f, skiprows=8, parse_dates =['time'], index_col ='time')
    df.columns=['no','no2','so2']
    no.append([df[""no""]])
    no2.append([df[""no2""]])
    so2.append([df[""so2""]])
</code></pre>

<p>How do I solve the problem?</p>
","11584006","","6553328","","2019-08-04 00:30:55","2019-08-04 00:30:55","How to read columns from different files and plot?","<python><python-3.x><pandas><plot>","1","1","","","","CC BY-SA 4.0","1"
"57259392","1","","","2019-07-29 18:17:32","","0","45","<p>I am trying to do some calculation across rows and columns in python. It is taking painfully longer time to execute for large dataset. </p>

<p>I am trying to do some calculation as follows:</p>

<pre><code>Df =pd.DataFrame({'A': [1,1,1,2,2,2,2],
                   'unit': [1,2,1,1,1,1,2],
                   'D1':[100,100,100,200,300,400,3509],
                   'D2':[200,200,200,300,300,400,2500],
                   'D3':[50,50,50,60,50,67,98],
                   'Level1':[1,4,0,4,4,4,5],
                   'Level2':[45,3,0,6,7,8,9],
                   'Level3':[0,0,34,8,7,0,5]
                 })
</code></pre>

<p>For each value of A (in above example A=1 and 2) I am running a function sequentially (i.e., I can not run the same function for A=1 and A=2 at the same time since outcome of A=1 changes some other values for A=2). I am calculating a Score as:</p>

<pre><code>def score(data):
    data['score_Level1']=np.where(data['Level1']&gt;=data['unit'], data['unit'], 0)*(((np.where(data['Level1']&gt;=data['unit'], data['unit'], 0)).sum()*100) +(10/data['D1']))
    data['score_Level2']=np.where(data['Level2']&gt;=data['unit'], data['unit'], 0)*(((np.where(data['Level2']&gt;=data['unit'], data['unit'], 0)).sum()*100) +(10/data['D2']))
    data['score_Level3']=np.where(data['Level3']&gt;=data['unit'], data['unit'], 0)*(((np.where(data['Level3']&gt;=data['unit'], data['unit'], 0)).sum()*100) +(10/data['D3']))

    return(data)
</code></pre>

<p>What above code does is it goes row by row and gives score for Leveli (i=1,2,3) as follows:</p>

<pre><code>Step1:
compare Value of ""Leveli' with corresponding ""unit"" column, if Leveli &gt;=unit then unit else 0. 

Step2:
Then it (sums up result for above operation across all rows for Leveli)*100+ (1/Di) = Lets say ""S""

Step3:
It goes row by row again and assign a score for Leveli as:

Step1*Step2 (for each row)

Above code should yield results for A=1 as:

score(Df[Df['A']==1])

I am listing only scoring for Level1, same thing happends for Level2 and Level3
Step1:
Compare 1&gt;=1 = True Yields 1, 4&gt;=2 = true Yields 2, 0&gt;=1 =False Yields 0

Step2:
(1+2+0)*100+1/100=300.1

Step3:
Compare 1&gt;=1 = True Yields 1 *300.1=300.1
Compare 4&gt;=2 = True Yields 2 *300.1=600.2
Compare 0&gt;=1 = False Yields 0 *300.1=0
</code></pre>

<p>I am doing this activity for 200 million values of A. Since it has to be done sequentially (A=n depends on outcome of A=n-1), it is taking a long time to compute.</p>

<p>Any suggestion of making it faster is much appreciated.</p>
","5596610","","","","","2019-07-29 19:13:28","Calculating across rows and columns at same time","<python-3.x><pandas><numpy-ndarray>","1","0","0","","","CC BY-SA 4.0","1"
"56729886","1","","","2019-06-24 04:33:45","","1","45","<p>Given that i have a df like this:</p>

<pre><code>    ID                Date Amount
0    a 2014-06-13 12:03:56     13
1    b 2014-06-15 08:11:10     14
2    a 2014-07-02 13:00:01     15
3    b 2014-07-19 16:18:41     22
4    b 2014-08-06 09:39:14     17
5    c 2014-08-22 11:20:56     55
              ...
129  a 2016-11-06 09:39:14     12
130  c 2016-11-22 11:20:56     35
131  b 2016-11-27 09:39:14     42
132  a 2016-12-11 11:20:56     18
</code></pre>

<p>I need to create a column df['Checking'] to show that ID will appear in next month or not and i tried the code as below:</p>

<pre><code>df['Checking']= df.apply(lambda x: check_nextmonth (x.Date, 
                     x.ID), axis=1)
</code></pre>

<p>where</p>

<pre><code>def check_nextmonth(date, id)=
  x= id in df['user_id'][df['Date'].dt.to_period('M')== ((date+ 
              relativedelta(months=1))).to_period('M')].values
  return x
</code></pre>

<p>but it take too long to process a single row.
How can i improve this code or another way to achieve what i want?</p>
","11252815","","11252815","","2019-06-26 01:27:24","2019-06-26 01:27:24","Search value in Next Month Record Pandas","<python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"57702380","1","","","2019-08-29 02:54:11","","0","45","<p>I have a df such as below,</p>

<pre><code>       Date                Time 
2019-07-23 21:17:47.599  22:00:00.000
2019-07-23 21:11:46.973  21:50:00.000
</code></pre>

<p>I am trying something like:</p>

<pre><code>pd.to_timedelta(pd.to_datetime(df3.Time)-pd.to_timedelta(df3.Date).dt.strftime('%H:%M:%S')) / np.timedelta64(1, 'm')
</code></pre>

<p>But I am getting an error of </p>

<pre><code>ValueError: only leading negative signs are allowed
</code></pre>

<p>Not sure what I am doing wrong? I just want the output to be the value in minutes whether positive or negative</p>

<p>Thanks!</p>
","8797830","","8797830","","2019-08-29 03:38:06","2019-08-29 03:38:06","Separating column value into a new field in a dataframe","<python><python-3.x><pandas><datetime><data-science>","1","0","","","","CC BY-SA 4.0","1"
"57504342","1","","","2019-08-15 02:52:58","","1","45","<p>After applying groupby on my data I want to save some values. I have two columns in dataframe, x1 and x2. apply groupby function by x2 column and get Value from x1 before the first value of group.</p>

<pre><code>df=pd.DataFrame({'x1':[3,4,5,6,7,8,9,2,7],x2:[1,3,3,3,2,2,2,2]})
   x1  x2
0   3   1
1   4   3
2   5   3
3   6   3
4   7   2
5   8   2
6   9   2
7   2   2
8   7   2

desired output:

df_out=pd.DataFrame({'x1_value':[3,6]})
      x1_value
0         3
1         6
</code></pre>
","11327242","","6361531","","2019-08-15 21:27:20","2019-08-15 21:27:20","How to take value before first value of groupy group","<python-3.x><pandas><numpy><pandas-groupby>","3","3","","","","CC BY-SA 4.0","1"
"49514860","1","","","2018-03-27 13:55:26","","2","45","<p>I have the following classes:</p>

<pre><code>class Result(UserDict):
    """"""Implements a especial version of dictionary, that will return the keys 
ordered in the initialized way""""""
    def __init__(self, keys_order, items):
        super().__init__(self)
        self.__keys_order = keys_order
        self.data = items

    def __repr__(self):
        attributes = [""{}:{}"".format(_stringify(k), _stringify(self.data[k])) for k in self.keys()]
        return ""{}"".format("", "".join(attributes))

    def keys(self):
        return [key for key in self.__keys_order]


class Results(UserList):
    """"""Implements a especial kind of list, that has a method to_df""""""
    def to_df(self):
        return pd.DataFrame(self.data, columns=self.data[0].keys())
</code></pre>

<p>In this way, when I print an instance of <code>Result</code>, it will show it with the keys in the desired way (determined by <code>keys_order</code>). Also, the class <code>Results</code> implements the method <code>to_df</code>, which returns a pandas <code>DataFrame</code> with the columns ordered by the keys.</p>

<p>I know for example that if I want the <code>len(results)</code> function to behave in a especial way, I have to implement the <code>__len__</code> method in it, in a similar way, is it possible to implement a special method so when <code>pd.DataFrame(results)</code> is called upon a <code>results</code> instance it will call <code>to_df</code> method instead? so I have the columns ordered by the keys.</p>
","1411197","","","user9558706","2018-03-27 14:25:01","2018-03-27 19:08:02","call special function when DataFrame is called upon custom class","<python-3.x><pandas><oop><metaprogramming>","1","1","","","","CC BY-SA 3.0","1"
"56582430","1","","","2019-06-13 14:10:41","","1","45","<p>As a Newbie in python, i had a basic question and it looks not easy for me.</p>

<p>I had More than 10 Netcdf files in one folder and all of them had dimensions (Latitude, Longnitude)
    Latitude=1800
    Longitude=3600</p>

<p>I opened all the files using xrray mfdataset but don't know wheather it is correct or wrong.</p>

<pre><code>import xarray as xr
aa=xr.open_mfdataset('Data/*.nc', concat_dim=None)
</code></pre>

<p>It shows all the variables from all Netcdf files and Dimension of each files correctly.
How to update the pandas data frame with Netcdf variable values using respective Latitude and Longnitude columns (another pandas dataframe)?</p>

<p>As a beginner i felt it hard and posting here. Any sample code would be really helpful. </p>

<p>Thanks in advance</p>
","9937482","","9937482","","2019-06-17 13:06:39","2019-06-17 13:06:39","How to update pandas dataframe from the Netcdf file based on latitude , longitude","<python-3.x><pandas><dataframe><python-xarray><netcdf4>","0","0","1","","","CC BY-SA 4.0","1"
"49650751","1","","","2018-04-04 12:13:56","","1","45","<p>I'm working on a data that contains duplicates. If ""similarity_index"" of the row is equal to another row, that means they are duplicates. I'm trying to merge this duplicates.</p>

<p>Here is my DataFrame:</p>

<pre><code>           ad    soyad similarity_index
0       hakan  özdemir                0
1       hasan    yaman                1
2        naci    şenli                2
3      naciye      şen                2
4       osman    uygur                3
5        elif    sözen                4
6        irem   derici                5
</code></pre>

<p>Here is what I tried to do:</p>

<pre><code>test_df.set_index(""similarity_index"").sort_index()
</code></pre>

<p>Here is the output:</p>

<pre><code>                          ad    soyad
similarity_index                     
0                      hakan  özdemir
0                 hakan utku  özdemir
1                      hasan    yaman
2                       naci    şenli
2                     naciye      şen
3                      osman    uygur
4                       elif    sözen
5                       irem   derici
5                       irem   delici
6                       hako  özdemir
</code></pre>

<p>Here is what I want:</p>

<pre><code>                          ad    soyad
similarity_index                     
0                      hakan  özdemir
                  hakan utku  özdemir
1                      hasan    yaman
2                       naci    şenli
                      naciye      şen
3                      osman    uygur
4                       elif    sözen
5                       irem   derici
                        irem   delici
6                       hako  özdemir
</code></pre>

<p>With this I'm trying to accomplish selecting duplicate rows with the same index. I tried <code>groupby()</code> and <code>pivot_table()</code>. But I couldn't find a proper way to do it.</p>
","9321414","","","","","2018-08-22 09:56:36","Pandas groupby result into a dataframe","<python-3.x><pandas-groupby>","1","0","","","","CC BY-SA 3.0","1"
"56616019","1","56616102","","2019-06-16 04:12:52","","0","45","<p>I have two Dataframes as:</p>

<p>Master_DF:</p>

<pre><code>Symbol,Strike_Price,C_BidPrice,Pecentage,Margin_Req,Underlay,C_LTP,LotSize
JETAIRWAYS,110.0,1.25,26.0,105308.9,81.05,1.2,2200
JETAIRWAYS,120.0,1.0,32.0,96156.9,81.05,1.15,2200
PCJEWELLER,77.5,0.95,27.0,171217.0,56.95,1.3,6500
PCJEWELLER,80.0,0.8,29.0,161207.0,56.95,0.95,6500
PCJEWELLER,82.5,0.55,31.0,154772.0,56.95,0.95,6500
PCJEWELLER,85.0,0.6,33.0,147882.0,56.95,0.7,6500
PCJEWELLER,90.0,0.5,37.0,138977.0,56.95,0.55,6500
</code></pre>

<p>and Child_DF:</p>

<pre><code>Symbol,Strike_Price,C_BidPrice,Pecentage,Margin_Req,Underlay,C_LTP,LotSize
JETAIRWAYS,110.0,1.25,26.0,105308.9,81.05,1.2,2200
JETAIRWAYS,150.0,1.3,22.0,44156.9,81.05,1.05,2200
PCJEWELLER,77.5,0.95,27.0,171217.0,56.95,1.3,6500
PCJEWELLER,100.0,1.8,29.0,441207.0,46.95,4.95,6500
</code></pre>

<p>I want compare child_DF with master_DF base on Column (Symbol,Strike_Price) i.e. <strong>if the Symbol &amp; Strike_Price are already available in master_DF then it will not be consider as new data.</strong> </p>

<p>New Rows are:</p>

<pre><code>Symbol,Strike_Price,C_BidPrice,Pecentage,Margin_Req,Underlay,C_LTP,LotSize
JETAIRWAYS,150.0,1.3,22.0,44156.9,81.05,1.05,2200
PCJEWELLER,100.0,1.8,29.0,441207.0,46.95,4.95,6500
</code></pre>
","11653862","","","","","2019-06-16 05:04:37","Compare master and child dataframe and extract new rows base on two column values only","<python><python-3.x><pandas><dataframe>","2","0","","","","CC BY-SA 4.0","1"
"56910436","1","56911094","","2019-07-06 00:45:05","","0","45","<pre><code>def conv_name(x):
    try:
        #library to convert strings to name dict
        return pp.tag(str(x))[0]
    except:
        return np.nan

dfn = df.name.to_frame()
dfn['conv'] = dfn.name.apply(lambda x: conv_name(x))
dfn['given_name'] = dfn.conv.apply(pd.Series).GivenName
dfn['sunname'] = dfn.conv.apply(pd.Series).Surname
</code></pre>

<p>Result</p>

<p><a href=""https://i.stack.imgur.com/q3Btk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/q3Btk.png"" alt=""enter image description here""></a></p>

<ul>
<li>I have a Pandas Series (df.name) of names.  </li>
<li>I want to parse them using an external library (probablepeople). </li>
<li>The library returns an exception on some rows so I have put in into the function ""conv_name""</li>
<li>I run the function using df.apply() to create a new column with the results</li>
<li>I then parse the results into two further column for first and last names</li>
</ul>

<p>I feel like there must be a way to create the firstname and surname columns without creating dfn.conv but if I do something like ...</p>

<pre><code>dfn['given_name'] = dfn.name.apply(lambda x: conv_name(x)['GivenName'])
</code></pre>

<p>I get a KeyError.</p>

<p>So my specific questions is, how can I create the first and surname columns without creating the conv column?</p>
","2175913","","","","","2019-07-06 04:02:30","Pandas: Run external library function to create new column efficiently","<python-3.x><pandas><parsing><processing-efficiency>","1","1","","","","CC BY-SA 4.0","1"
"57297877","1","","","2019-07-31 19:42:41","","0","45","<p>I have a pandas table called <code>df</code> contains 100+ columns with column names:
<code>C0</code>,<code>C1</code>,<code>C2</code>,...,<code>C100</code> </p>

<p>Now I want to remove all the rows where at least one value in column<code>C0 ~ C99</code>has a value of <code>-9999</code></p>

<p>How to write a general query to do this? </p>

<p>I do not want a answer like<code>df = df.loc[~((df['C1']==-9999) | (df['C2']==-9999)|......)]</code>
 since that is a lot of work to write a query like this with so many columns. </p>

<p>Are there any more efficient way? Thank you.</p>
","11766953","","11766953","","2019-07-31 20:01:31","2019-07-31 20:14:02","How to remove rows with unwanted value in python?","<python><python-3.x><pandas>","0","5","","2019-07-31 19:47:48","","CC BY-SA 4.0","1"
"57688676","1","","","2019-08-28 09:02:03","","1","45","<p>I have a .csv file that has the list of all subjects and what group they belong to (in addition to other information, but only these two columns are important for this situation).</p>

<pre><code>Subject    Group    Age
Subject1   A        ...
Subject2   B        ...
Subject3   C        ...
Subject4   D        ...
Subject5   A        ...
...
</code></pre>

<p>And my directory (let's call it Database) currently looks like this (A~D are folder names):</p>

<pre><code>-Database
    - A
    - B
    - C
    - D
    - Subject1_MRI_3T_Ph.nii
    - Subject2_MRI_3T_Ph.nii
    - Subject3_MRI_3T_Ph.nii
    - Subject4_MRI_3T_Ph.nii
    - Subject5_fMRI_Axial_Se.nii

..
</code></pre>

<p>My goal is to read the .csv file to get two strings (Subject and Group) and move the files to their respective groups like this:</p>

<pre><code>-Database
    - A
        - Subject1_MRI_3T_Ph.nii
        - Subject5_fMRI_Axial_Ph.nii
    - B
        - Subject2_MRI_3T_Ph.nii
    - C
        - Subject3_MRI_3T_Ph.nii
    - D
        - Subject4_MRI_3T_Ph.nii
</code></pre>

<p>I've tried using pandas library to sort the data, but I got stuck on trying to use that information to tell the os to move the files.</p>

<pre class=""lang-py prettyprint-override""><code>import glob
import os
import shutil
import numpy as np
import pandas as pd

filename = '/home/Desktop/.../csvfile.csv'
df = pd.read_csv(filename)

df = df.sort_values('Subject', ascending=True)
df = df[['Subject' , 'Group']]
print(df)


</code></pre>

<p>I know that the following two lines output the subject name and the group name respectively...</p>

<p>print(df.iloc[0,0])</p>

<p><code>= Subject1</code></p>

<p>print(df.iloc[0,1])</p>

<p><code>= A</code></p>

<p>but it doesn't seem like you can specifically refer to ""df.iloc[0,0]"", and do the following</p>

<pre><code>for x in df.iloc[] #or
#for x in df[['Subject']]
</code></pre>

<p>Is there a simpler way of doing this? Maybe pandas isn't what I am looking for?</p>
","11757791","","","","","2019-08-28 09:02:03","Use csv file to sort files into folders","<python><python-3.x><pandas><csv>","0","0","","","","CC BY-SA 4.0","1"
"57502937","1","57510413","","2019-08-14 22:42:34","","0","44","<p>C</p>

<pre><code>Ode  Proceeds   Pos   Amount       Positions   Target   Weighting   Addition
676  30160      FPE   51741.25     5           0.1      0.187636
676  30160      HFA   57299.63616  5           0.2      0.2077939
676  30160      PFL   60437.40563  5           0.2      0.2191728
676  30160      PSO   53053.5741   5           0.15     0.1923958
676  30160      RNS   53220.36636  5           0.2      0.1930006
953  34960      PFL   8506.1939    1           0.2      1
637  14750      PFL   8341.21701   3           0.2      0.3025169
637  14750      PSO   12669.65078  3           0.15     0.4594993
637  14750      RNS   6561.85824   3           0.2      0.2379836
673  12610      FPE   31220.475    5           0.1      0.1750410
673  12610      HFA   34020.2928   5           0.2      0.1907384
673  12610      PFL   37754.00236  5           0.2      0.2116719
673  12610      PSO   31492.56779  5           0.15     0.1765665
673  12610      RNS   43873.58472  5           0.2      0.2459820
318  93790      PFL   59859.3918   2           0.2      0.2852660
318  93790      PSO   149977.7109  2           0.15     0.7147339
222  75250      FPE   21000        6           0.1      0.1         7525
222  75250      HFA   42000        6           0.2      0.2         15050
222  75250      PFL   42000        6           0.2      0.2         15050
222  75250      PSO   31500        6           0.15     0.15        11287.5
222  75250      RNS   42000        6           0.2      0.2         15050
222  75250      CRD   31500        6           0.15     0.15        11287.5
</code></pre>

<p>The last 6 rows consist of a complete grouping. For all of the groupings that do not have the position row == 6 I want to take the amount that is in the <code>[""Proceeds""]</code>column in addition to the total value of the grouping under <code>[""Ode""]</code> and create a ""delta"" column that shows what the change should be in order to get the <code>[""Weighting""]</code> column as close to target as possible.</p>
","9310528","","9310528","","2019-08-15 12:52:35","2019-08-19 15:37:01","Optimize by Grouping using Pandas","<python-3.x><pandas><optimization>","1","4","","","","CC BY-SA 4.0","1"
"56769296","1","","","2019-06-26 09:17:46","","0","44","<p>I don't no how to use matplotlib in pyqt5 GUI.
I am trying to embed matplotlib in my GUI application and I don't no where I need to define the animation inside the GUI.</p>

<p>please don't consider any other parameters except sensor-1 QCheckBox and button-1(plot_button) </p>

<p>Did i defined it  properly? or what else changes I should  make? </p>

<pre><code>flag = False
x1 = []
y1 = []
class Ui_MainWindow(QtWidgets.QMainWindow):
    def __init__(self):
        super().__init__()
        self.numberOfPressed = 0

    def Plot_Button(self,i):
        self.numberOfPressed = self.numberOfPressed + 1
        print(""button Pressed. Number : = "" + str(self.numberOfPressed))
        lineEdit_11 = self.lineEdit_1.text
        print('Sampling Frequency: ' + self.lineEdit_1.text())
        print('Offset: ' + self.lineEdit_2.text())
        if self.Sensor1.isChecked():
           flag =True                 

    def plot_data(self):
     if(flag == True):
        optimal_frequency = 100
        for i in range(optimal_frequency):
            x = np.arange((2*i),(2+(2*i)))
            y = np.random.rand(2)
            print(x,y)
            self.graphicsView.plot(x,y) 
        else:
            print(""nothing"")  

    def animate(i):
        self.anim = animation.FuncAnimation(fig, self.plot_data, interval=optimal_frequency)    

    def Pause_Button(self):
        QCoreApplication.instance().quit()
        print(""fail"")

    def setupUi(self, MainWindow):
        MainWindow.setObjectName(""MainWindow"")
        MainWindow.resize(795, 359)     
        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName(""centralwidget"")        
        self.Plot = QtWidgets.QPushButton(self.centralwidget)
        self.Plot.setGeometry(QtCore.QRect(530, 260, 80, 23))
        self.Plot.setObjectName(""Plot"")
        self.Plot.clicked.connect(self.Plot_Button)
        self.Pause = QtWidgets.QPushButton(self.centralwidget)
        self.Pause.setGeometry(QtCore.QRect(620, 260, 80, 23))
        self.Pause.setObjectName(""Pause"")
        self.Pause.clicked.connect(self.Pause_Button)
        self.frame = QtWidgets.QFrame(self.centralwidget)
        self.frame.setGeometry(QtCore.QRect(520, 10, 251, 291))
        self.frame.setFrameShape(QtWidgets.QFrame.StyledPanel)
        self.frame.setFrameShadow(QtWidgets.QFrame.Raised)
        self.frame.setObjectName(""frame"")        
        self.label = QtWidgets.QLabel(self.frame)
        self.label.setGeometry(QtCore.QRect(140, 6, 101, 20))
##      self.label.setObjectName(""Sampling Frequency"")
        self.label.setObjectName(""label"")              
        self.label_2 = QtWidgets.QLabel(self.frame)
        self.label_2.setGeometry(QtCore.QRect(150, 40, 51, 20))
##        self.label_2.setObjectName(""Offset"")
        self.label_2.setObjectName(""label_2"")
##        self.label.setText('Offset:')      
        self.Sensor1 = QtWidgets.QCheckBox(self.frame)
        self.Sensor1.setGeometry(QtCore.QRect(30, 70, 74, 21))
        self.Sensor1.setObjectName(""Sensor1"")       
        self.Sensor2 = QtWidgets.QCheckBox(self.frame)
        self.Sensor2.setGeometry(QtCore.QRect(30, 90, 74, 21))
        self.Sensor2.setObjectName(""Sensor2"")      
        self.Sensor3 = QtWidgets.QCheckBox(self.frame)
        self.Sensor3.setGeometry(QtCore.QRect(30, 110, 74, 21))
        self.Sensor3.setObjectName(""Sensor3"")      
        self.Sensor4 = QtWidgets.QCheckBox(self.frame)
        self.Sensor4.setGeometry(QtCore.QRect(30, 130, 71, 21))
        self.Sensor4.setObjectName(""Sensor4"")      
        self.Sensor5 = QtWidgets.QCheckBox(self.frame)
        self.Sensor5.setGeometry(QtCore.QRect(30, 150, 74, 21))
        self.Sensor5.setObjectName(""Sensor5"")      
        self.Sensor6 = QtWidgets.QCheckBox(self.frame)
        self.Sensor6.setGeometry(QtCore.QRect(30, 170, 74, 21))
        self.Sensor6.setObjectName(""Sensor6"")     
        self.Sensor7 = QtWidgets.QCheckBox(self.frame)
        self.Sensor7.setGeometry(QtCore.QRect(30, 190, 74, 21))
        self.Sensor7.setObjectName(""Sensor7"")       
        self.Sensor8 = QtWidgets.QCheckBox(self.frame)
        self.Sensor8.setGeometry(QtCore.QRect(30, 210, 71, 21))
        self.Sensor8.setObjectName(""Sensor8"")
        self.lineEdit_1 = QtWidgets.QLineEdit(self.frame)
        self.lineEdit_1.setGeometry(QtCore.QRect(20, 10, 113, 21))
        self.lineEdit_1.setObjectName(""lineEdit_1"")        
        self.lineEdit_2 = QtWidgets.QLineEdit(self.frame)
        self.lineEdit_2.setGeometry(QtCore.QRect(20, 40, 113, 20))
        self.lineEdit_2.setObjectName(""lineEdit_2"")        
##        self.graphicsView = QtWidgets.QGraphicsView(self.centralwidget)
        self.graphicsView = pg.PlotWidget(self.centralwidget)
        self.graphicsView.setGeometry(QtCore.QRect(5, 11, 501, 291))
        self.graphicsView.setObjectName(""graphicsView"")       
        self.frame.raise_()
        self.Plot.raise_()
        self.Pause.raise_()
        self.graphicsView.raise_()
        MainWindow.setCentralWidget(self.centralwidget)
        self.menubar = QtWidgets.QMenuBar(MainWindow)
        self.menubar.setGeometry(QtCore.QRect(0, 0, 795, 21))       
        self.menubar.setObjectName(""menubar"")
        self.menuFile = QtWidgets.QMenu(self.menubar)        
        self.menuFile.setObjectName(""menuFile"")
        self.menuAbout = QtWidgets.QMenu(self.menubar)        
        self.menuAbout.setObjectName(""menuAbout"")
        MainWindow.setMenuBar(self.menubar)       
        self.statusbar = QtWidgets.QStatusBar(MainWindow)
        self.statusbar.setObjectName(""statusbar"")
        MainWindow.setStatusBar(self.statusbar)
        self.actionOpen = QtWidgets.QAction(MainWindow)
        self.actionOpen.setObjectName(""actionOpen"")        
        self.actionSave_As = QtWidgets.QAction(MainWindow)
        self.actionSave_As.setObjectName(""actionSave_As"")        
        self.actionScreenshot = QtWidgets.QAction(MainWindow)
        self.actionScreenshot.setObjectName(""actionScreenshot"")      
        self.actionExit = QtWidgets.QAction(MainWindow)
        self.actionExit.setObjectName(""actionExit"")        
        self.menuFile.addAction(self.actionOpen)
        self.menuFile.addAction(self.actionSave_As)
        self.menuFile.addAction(self.actionScreenshot)
        self.menuFile.addAction(self.actionExit)
        self.menubar.addAction(self.menuFile.menuAction())
        self.menubar.addAction(self.menuAbout.menuAction())
        self.retranslateUi(MainWindow)
        QtCore.QMetaObject.connectSlotsByName(MainWindow)

    def retranslateUi(self, MainWindow):
        _translate = QtCore.QCoreApplication.translate
        MainWindow.setWindowTitle(_translate(""MainWindow"", ""MainWindow""))
        self.Plot.setText(_translate(""MainWindow"", ""Plot""))
        self.Pause.setText(_translate(""MainWindow"", ""Pause""))
        self.label.setText(_translate(""MainWindow"", ""Sampling Frequency""))
        self.label_2.setText(_translate(""MainWindow"", ""Offset""))
        self.Sensor1.setText(_translate(""MainWindow"", ""Sensor1""))
        self.Sensor2.setText(_translate(""MainWindow"", ""Sensor2""))
        self.Sensor3.setText(_translate(""MainWindow"", ""Sensor3""))
        self.Sensor4.setText(_translate(""MainWindow"", ""Sensor4""))
        self.Sensor5.setText(_translate(""MainWindow"", ""Sensor5""))
        self.Sensor6.setText(_translate(""MainWindow"", ""Sensor6""))
        self.Sensor7.setText(_translate(""MainWindow"", ""Sensor7""))
        self.Sensor8.setText(_translate(""MainWindow"", ""Sensor8""))
        self.menuFile.setTitle(_translate(""MainWindow"", ""File""))
        self.menuAbout.setTitle(_translate(""MainWindow"", ""About""))
        self.actionOpen.setText(_translate(""MainWindow"", ""Open""))
        self.actionSave_As.setText(_translate(""MainWindow"", ""SavAs""))
        self.actionScreenshot.setText(_translate(""MainWindow"",""Screenshot""))
        self.actionExit.setText(_translate(""MainWindow"", ""Exit""))
        MainWindow.show()

if __name__ == ""__main__"":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = Ui_MainWindow()
    ui.setupUi(MainWindow)
    MainWindow.show()
    qapp = QtWidgets.QApplication(sys.argv)
    sys.exit(app.exec_())
</code></pre>
","10478787","","6622587","","2019-06-26 10:22:04","2019-06-26 10:26:36","Matplotlib animation function inside GUI","<python><python-3.x><pandas><matplotlib><pyqt5>","0","5","","2019-06-26 10:21:52","","CC BY-SA 4.0","1"
"56850575","1","56854761","","2019-07-02 10:36:48","","0","44","<p>I have excel spreadsheets of results that a new column is added to each year. I want to use Pandas to select key columns from the spreadsheets and create a table that shows the last X years of results. My code runs, and the columns with text in appear to swap round as expected, but the numeric data is lost and replaced with NaNs.  </p>

<p>From solutions to related issues on stackoverflow etc. it looks as though I should send a list of the required columns to the reindex method for dataframes. </p>

<pre class=""lang-py prettyprint-override""><code>def yearTable2(filename='5years.xlsx',SheetName='PartA',interactive=True,A_year=2018,nyears=3,debug=False):
    """"""Outputs latex code of table of nyears years of results for a given part's
    module results
    Input:filename is the excel file with the data in, 
    sheetname contains the data for the part to be tablularised
    A_year is the current academic year""""""
    xl=pd.ExcelFile(filename)
    df=xl.parse(SheetName)
    df2=df.round(1) # rounds numeric data to 1 decimal place
    if debug: print(df.head())
    #Have data in df2, it probably has more years of data than really needed 
    # extract just the needed ones
    # Build up list of column names in required order
    column_list=[""Module Name"",""Module Code""] # these are standard
    # now generate the years required
    for year in list(range(A_year,A_year-nyears,-1)):
        list_item=str(year*1)
        column_list.append(list_item)
    print(column_list)
    df3=df2.reindex(columns=column_list)
    return (df3) # outputs pretty Jupyter table
</code></pre>

<p>I call this with:<code>yearTable2(filename='Test.xlsx',SheetName='PartC',debug=True)</code></p>

<p>where Test.xlsx is an example file that has the following content:</p>

<pre><code>|Module Code|Module Name|2013|2014|2015|2016|2017|2018|
______________________________________________________
|abc        |Harry      | 23 | 45 | 32 | 54 | 56 | 12 |
|fgr        |Jannice    | 28 | 65 | 21 | 34 | 21 | 54 |
</code></pre>

<p>I am expecting to get the following columns: Module Name, Module Code, 2018, 2017, 2016</p>

<p>The first two columns are fine, but the numeric (year) columns just contain NaNs</p>

<pre><code>    Module Name Module Code 2018    2017    2016
0   Harry        abc         NaN    NaN     NaN
1   Jannice      fgr         NaN    NaN     NaN
</code></pre>
","9636786","","11607986","","2019-07-02 11:18:22","2019-07-02 15:26:41","How to re-order columns in data frame without losing values?","<python-3.x><pandas>","1","2","","","","CC BY-SA 4.0","1"
"57659720","1","","","2019-08-26 14:18:23","","0","44","<p>I have a dataframe like</p>

<pre><code>In [1]: df
Out[1]:
   a  b  c
   d  e  f
   g  h  i
0  1  2  3
1  4  5  6
2  7  8  9
</code></pre>

<p>where first 3 rows are columns</p>

<pre><code>In [2]: df.columns.values
Out[2]: array([('a', 'd', 'g'), ('b', 'e', 'h'), ('c', 'f', 'i')], dtype=object)
</code></pre>

<p>I want to convert this dataframe to </p>

<pre><code>   a  b  c
0  d  e  f
1  g  h  i
2  1  2  3
3  4  5  6
4  7  8  9
</code></pre>
","3845604","","","","","2019-08-26 14:20:21","Flatten multilevel column in pandas","<python><python-3.x><pandas><dataframe>","1","0","","2019-08-26 14:24:49","","CC BY-SA 4.0","1"
"56851945","1","","","2019-07-02 11:56:58","","0","44","<p>I am building an NLP pipeline and I am trying to get my head around in regards to the optimal structure. My understanding at the moment is the following: </p>

<ul>
<li>Step1 - Text Pre-processing [a. Lowercasing, b. Stopwords removal, c. stemming, d. lemmatisation,] </li>
<li>Step 2 - Feature extraction </li>
<li>Step 3 - Classification - using the different types of classifier(linearSvC etc) </li>
</ul>

<p>From what I read online there are several approaches in regard to feature extraction but there isn't a solid example/answer. </p>

<ul>
<li>a. Is there a solid strategy for feature extraction ? 
I read online that you can do [a. Vectorising usin ScikitLearn b. TF-IDF] 
but also I read that you can use Part of Speech or word2Vec or other embedding and Name entity recognition. </li>
<li>b. What is the optimal process/structure of using these? </li>
<li>c. On the text pre-processing I am ding the processing on a text column on a df and the last modified version of it is what I use as an input in my classifier. If you do feature extraction do you do that in the same column or you create a new one and you only send to the classifier the features from that column? </li>
</ul>

<p>Thanks so much in advance</p>
","11729065","","10337630","","2019-07-02 12:17:42","2019-07-02 12:17:59","NLP Structure Question (best way for doing feature extraction)","<python-3.x><pandas><nlp><jupyter-notebook><spacy>","1","0","","","","CC BY-SA 4.0","1"
"56770196","1","56770274","","2019-06-26 10:07:05","","1","44","<p>I have a data frame,</p>

<pre><code>    Software Product    Case Number Num of days
    MDM9607.LE.1.0          2774904     -19.13888889
    MDM9607.LE.1.0          2774203     -19.60069444
    MDM9607.LE.1.0          2768088       -24.81597222
    MDM9607.LE.1.0          2767500       -25.0125
    MDM9607.LE.1.0          2764617        -26.67916667
    MDM9607.LE.1.0          2766991      -25.17430556
    MDM9607.LE.1.0          2765696
    MDM9607.LE.1.0          2764204
    MDM9607.LE.1.0         2764199
    MDM9607.LE.1.0         2774434           365
    MDM9607.LE.1.0         2769029           377
    MDM9607.LE.1.0         2764195           380
    MDM9607.LE.1.0        2763721             25
    MDM9607.LE.1.0        2770456             380
    MDM9607.LE.1.0       2768423


</code></pre>

<p>Required output conditions:</p>

<pre><code>    If:
        f9['Num of days'] &gt; 365 than print L
        f9['Num of days'] &lt; 365 than print N
        f9['Num of days'] == NaN than print U

</code></pre>

<p>Code:</p>

<pre><code>    import pandas as pd
    import numpy as np

    df1 = pd.read_excel(r""Rawreport_2017.xlsx"")
    df2 = pd.read_excel(r""Sampleswpl.xlsx"")
    f9 = pd.merge(df1, df2, on=['Software Product'], how='outer')
    f9.to_excel(r""merge_new_1.xlsx"")
    f9['Num of days'] = f9['Date/Time Opened'] - f9['CSDate']
    f9['Num of days_u']=f9['Num of days'].fillna('u')
    f9['status'] = np.where(f9['Num of days'] &gt; 365, 'L', 'NL','u')
    f9.to_excel(r""merge_status_5.xlsx"")
</code></pre>

<p>I am using Dataframe which contains some missing values, that missing values should be printed as Unknown, but I am some logic if the column is greater than 365 then it should be printed as ""L"" &lt;365 should printed as ""N"", but this missing value also considered as 0(zero) and printing as ""N"".</p>

<p>The expected output should be</p>

<pre><code>     Software Product   Case Number     Num of days    Status
    MDM9607.LE.1.0          2774904     -19.13888889        N
    MDM9607.LE.1.0          2774203     -19.60069444        N
    MDM9607.LE.1.0          2768088       -24.81597222      N
    MDM9607.LE.1.0          2767500       -25.0125          N
    MDM9607.LE.1.0          2764617        -26.67916667     N
    MDM9607.LE.1.0          2766991      -25.17430556       N
    MDM9607.LE.1.0          2765696                         U
    MDM9607.LE.1.0          2764204                         U
    MDM9607.LE.1.0         2764199                          U
    MDM9607.LE.1.0         2774434           365            L
    MDM9607.LE.1.0         2769029           377            L
    MDM9607.LE.1.0         2764195           380            L
    MDM9607.LE.1.0        2763721             25            N
    MDM9607.LE.1.0        2770456             380           L
</code></pre>

<p>I used the above, but I got:</p>

<p><code>TypeError: where() takes at most 3 arguments (4 given)</code></p>
","10064897","","63550","","2019-07-12 00:44:54","2019-07-12 00:44:54","How add a value to missing value i","<python-3.x><pandas><numpy>","1","3","","","","CC BY-SA 4.0","1"
"57860142","1","","","2019-09-09 19:26:35","","-1","44","<p>I want to split the text and form a new row whenever a DateTime occurred. Raw dataframe looks like</p>

<pre><code>Patient_id   |        Issue  
 -----------------------------------------------------------------------
 1           |12-02-2018 12:15:52 -abc-Patient have headache 20-02-2018 2:15:52 -abc- Previous medication 
             |had some side effects 20-03-2018 5:30:52 -abc- Patient got cured xyz worked well.
  -----------------------------------------------------------------------              
 2           | 19-02-2018 2:50:52 -cbf- cbf is allergic def medicine and 
             |  have a fever with a body ache 25-02-2018 2:50:52 -cbf-
             |  Patient got cured by def medicine.
</code></pre>

<p>I tried the following but not getting expected results</p>

<pre><code>df = pd.DataFrame(re.split('(\d{2}-\d{2}-\d{4} \d{2}:\d{2}:\d{2} - .*\s+)',df['Issue'], re.DOTALL),columns = ['Issue'])
</code></pre>

<p>Expected result</p>

<pre><code>Patient_id   |        Issue  
-----------------------------------------------------------------------
 1           |12-02-2018 12:15:52 -abc-Patient have headache 
             | 20-02-2018 2:15:52 -abc- Previous medication had some side effects.
             |20-03-2018 5:30:52 -abc-Patient got cured xyz worked well.
-----------------------------------------------------------------------              
 2            | 19-02-2018 2:50:52 -cbf- cbf is allergic def medicine and have fever with bodyache
              |25-02-2018 2:50:52 -cbf-Patient got cured by def medicine.
</code></pre>

<p>Then I tried to split the information into different columns like Number, date, name, and issues respectively.</p>

<pre><code>df = df.Issue.str.split('-',n=3)

df = pd.DataFrame(df.values.tolist(),columns=['Number', 'Date','Name', 'Issue'])
</code></pre>

<p>Expected final output</p>

<pre><code>Patient_id|Name|Date of admission  |Issue  
------------------------------------------------------------------------------------------
1         |abc |12-02-2018 12:15:52|Patient have headache 
------------------------------------------------------------------------------------------
1         |abc |20-02-2018 2:15:52 |Previous medication had some side effects.
-------------------------------------------------------------------------------------------
1         |abc |20-03-2018 5:30:52 |Patient got cured xyz worked well.
-------------------------------------------------------------------------------------------                
2         |cbf |19-02-2018 2:50:52 |cbf is allergic def medicine and have to fever with a body ache
 --------------------------------------------------------------------------------------------
2         |cbf |25-02-2018 2:50:52 |Patient got cured by def medicine.
</code></pre>
","10899894","","","","","2019-09-09 20:01:53","How to split a row into new rows for every new entry of datetime?","<regex><python-3.x><pandas><nlp>","2","0","","","","CC BY-SA 4.0","1"
"57652508","1","57652556","","2019-08-26 06:01:08","","0","44","<p>I have an array in json text file which has a list of dicts. I need to extract all of it into a dataframe. Array is something on these lines:-</p>

<pre><code>[{ ""_id"" : ""abc"" , ""players"" : [ ""1"" , ""2""] , ""tId"" : ""1"" , ""ef"" : 200 , ""pr"" : 360 , ""mode"" : 1.0 , ""1"" : { ""before"" : { ""rm"" : { ""$numberLong"" : ""1070""} , ""cap"" : 450.0 , ""nrrm"" : 20.0} , ""after"" : { ""rm"" : { ""$numberLong"" : ""970""} , ""cap"" : 250.0 , ""nrrm"" : 120.0}} , ""2"" : { ""before"" : { ""rm"" : { ""$numberLong"" : ""470""} , ""cap"" : 0.0 , ""nrrm"" : 310.0} , ""after"" : { ""rm"" : { ""$numberLong"" : ""730""} , ""cap"" : 0.0 , ""nrrm"" : 410.0}} , ""ts"" : { ""$date"" : { ""$numberLong"" : ""1565548200670""}} , ""shots"" : [ { ""iBS"" : 1 , ""bSTOP"" : 1 , ""aSTOP"" : 1 , ""bSPB"" : ""NOT DECIDED"" , ""aSPB"" : ""NOT DECIDED"" , ""lBP"" : [ 1] , ""iBP"" : [ ] , ""bP"" : [ 1] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( -0.6728522,0.04,-0.5520813 )"" , ""iF"" : ""( 0.480835,-2.9104E-16,0.1699932 )"" , ""iP"" : ""( -0.7105647,0.04,-0.5654141 )"" , ""cA"" : ""( 6.539359E-14,70.5296,2.670466E-14 )"" , ""iTBA"" : 1 , ""tBID"" : 2 , ""tBP"" : ""( 0.724014,0.0335,-0.041 )"" , ""tT"" : 12.66367 , ""iSU"" : """" , ""uSV"" : """" , ""iPB"" : 0 , ""bHBC"" : 7 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 0 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""2cfdd0"" , ""toT"" : 12.63438} , { ""iBS"" : 0 , ""bSTOP"" : 1 , ""aSTOP"" : 0 , ""bSPB"" : ""NOT DECIDED"" , ""aSPB"" : ""SOLID BALLS"" , ""lBP"" : [ 3] , ""iBP"" : [ ] , ""bP"" : [ 3] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( 1.07788,0.04,-0.5111128 )"" , ""iF"" : ""( -0.2340834,1.694598E-16,0.4531059 )"" , ""iP"" : ""( 1.096239,0.04,-0.5466505 )"" , ""cA"" : ""( -3.80758E-14,332.6783,-1.613636E-14 )"" , ""iTBA"" : 1 , ""tBID"" : 3 , ""tBP"" : ""( 0.4538818,0.04000001,0.54636 )"" , ""tT"" : 15.64122 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 0 , ""bHBC"" : 4 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 0 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""2cfdd0"" , ""toT"" : 15.62162} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""SOLID BALLS"" , ""aSPB"" : ""SOLID BALLS"" , ""lBP"" : [ ] , ""iBP"" : [ ] , ""bP"" : [ ] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( 0.1115417,0.04,-0.1958274 )"" , ""iF"" : ""( 0.002243765,-1.426218E-18,0.0006675497 )"" , ""iP"" : ""( 0.07320246,0.04,-0.2072338 )"" , ""cA"" : ""( 6.981425E-14,73.43156,3.15853E-14 )"" , ""iTBA"" : 1 , ""tBID"" : 5 , ""tBP"" : ""( 0.7939866,0.04000001,-0.07077976 )"" , ""tT"" : 24.50556 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 0 , ""bHBC"" : 0 , ""cHRB"" : 0 , ""cHSM"" : 0 , ""hRBIP"" : 0 , ""cIH"" : 1 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 0 , ""iGO"" : 0 , ""sSC"" : 0 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""2cfdd0"" , ""toT"" : 24.4843} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""STRIPE BALLS"" , ""aSPB"" : ""STRIPE BALLS"" , ""lBP"" : [ 14] , ""iBP"" : [ ] , ""bP"" : [ 14] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( -0.1471594,0.04,0.2157262 )"" , ""iF"" : ""( 0.09524104,0,0.1471072 )"" , ""iP"" : ""( -0.1688982,0.04,0.182149 )"" , ""cA"" : ""( 0,32.92009,0 )"" , ""iTBA"" : 1 , ""tBID"" : 14 , ""tBP"" : ""( -0.06695016,0.04000001,0.3848823 )"" , ""tT"" : 9.197196 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 0 , ""bHBC"" : 0 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 1 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""1bdf72"" , ""toT"" : 9.181252} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""STRIPE BALLS"" , ""aSPB"" : ""STRIPE BALLS"" , ""lBP"" : [ 12] , ""iBP"" : [ ] , ""bP"" : [ 12] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( 0.3852863,0.04,0.4167935 )"" , ""iF"" : ""( 0.282476,0,0.2797301 )"" , ""iP"" : ""( 0.3568642,0.04,0.3886477 )"" , ""cA"" : ""( 0,45.27985,0 )"" , ""iTBA"" : 1 , ""tBID"" : 12 , ""tBP"" : ""( 0.6180266,0.04000001,0.5658391 )"" , ""tT"" : 6.139053 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 0 , ""bHBC"" : 7 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 0 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""1bdf72"" , ""toT"" : 6.123772} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""STRIPE BALLS"" , ""aSPB"" : ""STRIPE BALLS"" , ""lBP"" : [ 11] , ""iBP"" : [ ] , ""bP"" : [ 11] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( 0.7701012,0.04,-0.2259813 )"" , ""iF"" : ""( 0.200658,0,-0.4688671 )"" , ""iP"" : ""( 0.7543634,0.04,-0.1892074 )"" , ""cA"" : ""( 0,156.8309,0 )"" , ""iTBA"" : 1 , ""tBID"" : 11 , ""tBP"" : ""( 0.941402,0.04000002,-0.4693463 )"" , ""tT"" : 5.77317 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 0 , ""bHBC"" : 6 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 0 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""1bdf72"" , ""toT"" : 5.756089} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""STRIPE BALLS"" , ""aSPB"" : ""STRIPE BALLS"" , ""lBP"" : [ 9] , ""iBP"" : [ ] , ""bP"" : [ 9] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( 0.0779118,0.04,-0.53505 )"" , ""iF"" : ""( -0.2458796,0,0.2141081 )"" , ""iP"" : ""( 0.1080778,0.04,-0.5613181 )"" , ""cA"" : ""( 0,311.0488,0 )"" , ""iTBA"" : 1 , ""tBID"" : 9 , ""tBP"" : ""( -0.4028816,0.04000001,-0.1177362 )"" , ""tT"" : 25.78172 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 0 , ""bHBC"" : 0 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 1 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""1bdf72"" , ""toT"" : 25.76452} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""STRIPE BALLS"" , ""aSPB"" : ""STRIPE BALLS"" , ""lBP"" : [ 15] , ""iBP"" : [ 0] , ""bP"" : [ 15 , 0] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( -0.6202361,0.04,0.07899453 )"" , ""iF"" : ""( 0.5099897,0,-0.003231468 )"" , ""iP"" : ""( -0.6602353,0.04,0.07924797 )"" , ""cA"" : ""( 0,90.36304,0 )"" , ""iTBA"" : 1 , ""tBID"" : 15 , ""tBP"" : ""( 0.4645979,0.04000001,0.1173781 )"" , ""tT"" : 17.43617 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 0 , ""bHBC"" : 5 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 1 , ""cIP"" : 1 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 0 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""1bdf72"" , ""toT"" : 17.41947} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""SOLID BALLS"" , ""aSPB"" : ""SOLID BALLS"" , ""lBP"" : [ 5] , ""iBP"" : [ ] , ""bP"" : [ 5] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( 0.4321823,0.03999999,-0.146901 )"" , ""iF"" : ""( 0.1381158,0,0.1290963 )"" , ""iP"" : ""( 0.40296,0.03999999,-0.174215 )"" , ""cA"" : ""( 0,46.93323,0 )"" , ""iTBA"" : 1 , ""tBID"" : 5 , ""tBP"" : ""( 0.5929074,0.04,0.002491749 )"" , ""tT"" : 25.0796 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 0 , ""bHBC"" : 0 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 1 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""2cfdd0"" , ""toT"" : 26.25124} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""SOLID BALLS"" , ""aSPB"" : ""SOLID BALLS"" , ""lBP"" : [ 7] , ""iBP"" : [ ] , ""bP"" : [ 7] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( 0.6570191,0.04,0.06490942 )"" , ""iF"" : ""( 0.1059603,0,-0.2250292 )"" , ""iP"" : ""( 0.6399788,0.04,0.1010982 )"" , ""cA"" : ""( 0,154.7855,0 )"" , ""iTBA"" : 1 , ""tBID"" : 7 , ""tBP"" : ""( 0.7849708,0.04000001,-0.1436278 )"" , ""tT"" : 27.20016 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 0 , ""bHBC"" : 1 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 1 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""2cfdd0"" , ""toT"" : 27.18202} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""SOLID BALLS"" , ""aSPB"" : ""SOLID BALLS"" , ""lBP"" : [ 2] , ""iBP"" : [ ] , ""bP"" : [ 2] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( 0.4290362,0.04,-0.4845348 )"" , ""iF"" : ""( -0.1070574,0,0.3149122 )"" , ""iP"" : ""( 0.4419109,0.04,-0.5224062 )"" , ""cA"" : ""( 0,341.2241,0 )"" , ""iTBA"" : 1 , ""tBID"" : 2 , ""tBP"" : ""( 0.35649,0.04000001,-0.2899367 )"" , ""tT"" : 27.1818 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 0 , ""bHBC"" : 2 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 0 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""2cfdd0"" , ""toT"" : 27.15446} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""SOLID BALLS"" , ""aSPB"" : ""SOLID BALLS"" , ""lBP"" : [ 4] , ""iBP"" : [ ] , ""bP"" : [ 4] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( 0.5885352,0.04,-0.1053962 )"" , ""iF"" : ""( -0.2677357,0,0.2490637 )"" , ""iP"" : ""( 0.6178222,0.04,-0.1326408 )"" , ""cA"" : ""( 0,312.9308,0 )"" , ""iTBA"" : 1 , ""tBID"" : 4 , ""tBP"" : ""( -0.2299831,0.04000001,0.5813306 )"" , ""tT"" : 16.6083 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 0 , ""bHBC"" : 2 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 1 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""2cfdd0"" , ""toT"" : 16.57993} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""SOLID BALLS"" , ""aSPB"" : ""SOLID BALLS"" , ""lBP"" : [ ] , ""iBP"" : [ ] , ""bP"" : [ ] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( -0.1893151,0.04,-0.4421305 )"" , ""iF"" : ""( 0.5092477,0,-0.02769054 )"" , ""iP"" : ""( -0.2292561,0.04,-0.4399587 )"" , ""cA"" : ""( 0,93.11242,0 )"" , ""iTBA"" : 1 , ""tBID"" : 6 , ""tBP"" : ""( 0.6927831,0.04,-0.5543976 )"" , ""tT"" : 17.58389 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 0 , ""bHBC"" : 5 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 0 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 0 , ""iGO"" : 0 , ""sSC"" : 0 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""2cfdd0"" , ""toT"" : 25.99698} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""STRIPE BALLS"" , ""aSPB"" : ""STRIPE BALLS"" , ""lBP"" : [ 13] , ""iBP"" : [ ] , ""bP"" : [ 13] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( 0.6336635,0.04,0.07858364 )"" , ""iF"" : ""( -0.1611403,0,0.06044259 )"" , ""iP"" : ""( 0.6711156,0.04,0.06453566 )"" , ""cA"" : ""( 0,290.5607,0 )"" , ""iTBA"" : 1 , ""tBID"" : 13 , ""tBP"" : ""( -0.5944556,0.04000001,0.5183017 )"" , ""tT"" : 14.77339 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 0 , ""bHBC"" : 2 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 1 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""1bdf72"" , ""toT"" : 14.75704} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""STRIPE BALLS"" , ""aSPB"" : ""STRIPE BALLS"" , ""lBP"" : [ 10] , ""iBP"" : [ ] , ""bP"" : [ 10] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( -0.6597628,0.04,0.539317 )"" , ""iF"" : ""( 0.3268366,0,-0.3805755 )"" , ""iP"" : ""( -0.6858233,0.04,0.5696625 )"" , ""cA"" : ""( 0,139.3442,0 )"" , ""iTBA"" : 1 , ""tBID"" : 10 , ""tBP"" : ""( 0.3564146,0.04000001,-0.5567175 )"" , ""tT"" : 10.13039 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 1 , ""bHBC"" : 4 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 0 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""1bdf72"" , ""toT"" : 10.11172} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""STRIPE BALLS"" , ""aSPB"" : ""STRIPE BALLS"" , ""lBP"" : [ ] , ""iBP"" : [ ] , ""bP"" : [ ] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( 0.2228186,0.04,0.1685899 )"" , ""iF"" : ""( -0.246861,0,-0.3906604 )"" , ""iP"" : ""( 0.2441863,0.04,0.2024045 )"" , ""cA"" : ""( 0,212.2891,0 )"" , ""iTBA"" : 1 , ""tBID"" : 8 , ""tBP"" : ""( -0.2992525,0.04000001,-0.5290359 )"" , ""tT"" : 12.02916 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 1 , ""bHBC"" : 3 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 0 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 0 , ""iGO"" : 0 , ""sSC"" : 0 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""1bdf72"" , ""toT"" : 12.01164} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""SOLID BALLS"" , ""aSPB"" : ""SOLID BALLS"" , ""lBP"" : [ 6] , ""iBP"" : [ ] , ""bP"" : [ 6] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( -0.3779429,0.03999999,0.07964184 )"" , ""iF"" : ""( 0.2466588,0,0.09699224 )"" , ""iP"" : ""( -0.4151683,0.03999999,0.0650039 )"" , ""cA"" : ""( 0,68.53404,0 )"" , ""iTBA"" : 1 , ""tBID"" : 6 , ""tBP"" : ""( 0.7465774,0.04000001,0.5018011 )"" , ""tT"" : 21.34981 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 1 , ""bHBC"" : 2 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 1 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 0 , ""aBP"" : 1 , ""iGO"" : 0 , ""sSC"" : 1 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""2cfdd0"" , ""toT"" : 21.3332} , { ""iBS"" : 0 , ""bSTOP"" : 0 , ""aSTOP"" : 0 , ""bSPB"" : ""SOLID BALLS"" , ""aSPB"" : ""SOLID BALLS"" , ""lBP"" : [ 8] , ""iBP"" : [ ] , ""bP"" : [ 8] , ""iTO"" : 0 , ""iCOPG"" : 0 , ""nTCCOP"" : 0 , ""iCOPS"" : 0 , ""cBP"" : ""( 0.9329018,0.04,0.4941435 )"" , ""iF"" : ""( -0.230935,0,-0.1550016 )"" , ""iP"" : ""( 0.9661143,0.04,0.5164354 )"" , ""cA"" : ""( 0,236.1308,0 )"" , ""iTBA"" : 1 , ""tBID"" : 8 , ""tBP"" : ""( -0.6080351,0.04000002,-0.4977565 )"" , ""tT"" : 22.92525 , ""iSU"" : """" , ""uSV"" : ""( 0,0,0 )"" , ""iPB"" : 1 , ""bHBC"" : 2 , ""cHRB"" : 1 , ""cHSM"" : 1 , ""hRBIP"" : 0 , ""cIH"" : 0 , ""cIP"" : 0 , ""bIP"" : 1 , ""aBP"" : 1 , ""iGO"" : 1 , ""sSC"" : 0 , ""sBC"" : 0 , ""dBC"" : 0 , ""cSC"" : 0 , ""iSC"" : 0 , ""pSId"" : ""2"" , ""toT"" : 22.90602}] , ""winner"" : ""2"" , ""reason"" : 12}]
</code></pre>

<p>I have tried reading the json file and separately open that 'shots' column from json file but it fails.</p>

<pre><code>#reading data
with open(""z.txt"", 'r', encoding = 'utf-8') as datafile:
    data = json.load(datafile)

df1 = pd.DataFrame(data)
</code></pre>

<pre><code>shots = pd.DataFrame([d[""shots""] for d in data])
</code></pre>

<p>I want to extract the ""_id"" along with all the columns that would be inside ""shots"" in a single dataframe.</p>
","10800593","","","","","2019-08-26 06:05:42","How to convert a dict which is inside a list inside an array of json object into a dataframe?","<python><json><python-3.x><pandas><dictionary>","1","2","","","","CC BY-SA 4.0","1"
"49412161","1","49413714","","2018-03-21 16:48:20","","0","44","<p>I have the following JSON</p>

<pre><code>ds = [{
    ""name"": ""groupA"",
    ""subGroups"": [{
        ""subGroup"": 1,
        ""categories"": [{
                ""category1"": {
                    ""value"": 10
                }
            },
            {
                ""category2"": {}
            },
            {
                ""category3"": {}
            }
        ]
    }]
},
{
    ""name"": ""groupB"",
    ""subGroups"": [{
        ""subGroup"": 1,
        ""categories"": [{
                ""category1"": {
                    ""value"": 500
                }
            },
            {
                ""category2"": {}
            },
            {
                ""category3"": {}
            }
        ]
    }]
}]
</code></pre>

<p>I can get a dataframe for all the categories by doing:</p>

<pre><code>json_normalize(ds, record_path=[""subGroups"", ""categories""], meta=['name', ['subGroups', 'subGroup']], record_prefix='cat.')
</code></pre>

<p>This will give me: </p>

<pre><code>  cat.category1 cat.category2   cat.category3 subGroups.subGroup    name
    0   {'value': 10}   NaN             NaN           1    groupA
    1   NaN             {}              NaN           1    groupA
    2   NaN             NaN             {}            1    groupA
    3   {'value': 500}  NaN             NaN           1    groupB
    4   NaN             {}              NaN           1    groupB
    5   NaN             NaN             {}            1    groupB
</code></pre>

<p>But, I don't care about category 2 and category 3 at all.  I only care about the category 1. 
So'd I prefer something like:
        cat.category1   subGroups.subGroup  name
    0   {'value': 10}   1   groupA
    1   {'value': 500}  1   groupB</p>

<p>Any ideas how I get to this?</p>

<p>And even better, I really want the value of value in category1. So something like: </p>

<pre><code>    cat.category1.value subGroups.subGroup  name
0   10                  1                   groupA
1   500                 1                   groupB
</code></pre>

<p>Any ideas?</p>
","1114773","","1114773","","2018-03-21 17:04:10","2018-03-21 22:32:09","Flatting Pandas JSON Dataframe for a specific path","<python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"56910664","1","56910700","","2019-07-06 01:51:21","","1","43","<p>i have two dataframes
df1 and df2 have same values on columns, i would like to have a function to fill df1 exchange rates column with exchange rates from a date in df2</p>

<p>also if possible interpolate nans from df2</p>

<pre><code>df1
                                                                                                                                                                                                                                                                                                                                                                                                                               ""date""        ""value""     ""Exchange_Rate""
2019-Jan    35691
2019-Jan    17076
2019-Dec    988
2019-Dec    1996


df2

""exchange_rate"" ""date""     ""real_or_forecast""

19.126386   2019-Jan  real
19.197585   2019-Feb  real
19.269133   2019-Mar  real
19.089059   2019-Apr  real
19.042815   2019-May  real
19.142962   2019-Jun  real
NaN         2019-Jul  forecast
NaN         2019-Aug  forecast
19.237154   2019-Sep  forecast
NaN         2019-Oct  forecast
19.559262   2019-Nov  forecast
NaN         2019-Dec  forecast
19.559262       2020-Jan  forecast

    def get_fill_currency(df):   
    for value in df['date']
        if df1['date'] == date :
            return value['exchange_rate']

df1
                                                                                                                                                                                                                                                                                                                                                                                                                           ""date""        ""value""     ""Exchange_Rate""
2019-Jan    35691        19.126386       
2019-Jan    17076        19.126386
2019-Dec    988          19.559262
2019-Dec    1996         19.559262
</code></pre>
","11708916","","6361531","","2019-07-06 01:54:17","2019-07-06 02:01:27","how to fill a column value from another dataframe if dates are the same","<python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"49374698","1","49374736","","2018-03-20 00:46:51","","0","43","<p>I have a 1d numpy array of 530 number, which I created like so, <code>np.array([i for i in range(530)])</code>. So the shape of this 1d array is <code>(530,)</code>. I also have a 2d array which is an array of 530 lists where each list contains 100 elements. To be clear the shape of this 2d array is <code>(530, 100)</code>. </p>

<p><code>&gt;&gt;&gt;indices = np.array([i for i in range(530)])
</code>
<code>&gt;&gt;&gt;print(test_data.shape)
 (530,100)
</code></p>

<p>Using these two arrays, <code>indices</code> and <code>test_data</code>, what I want to do is create a pandas dataframe with only 2 columns where the first column is indices(1 integer per row), and the second column is a single list(length 100) from <code>test_data</code>. The sequential nature of each arrays should be maintained, so the first int in <code>indices</code> corresponds to the first 100 length array in <code>test_data</code>.</p>

<p>I tried using zip with these two arrays, and then creating a dataframe but it doesn't work. </p>
","4616611","","","","","2018-03-20 00:51:55","Create dataframe with 2 columns where a one column is a sequence of 100 elements","<python><arrays><python-3.x><pandas><numpy>","1","1","","","","CC BY-SA 3.0","1"
"56910039","1","56910109","","2019-07-05 23:11:00","","0","43","<p><code>(Not a duplicate question)</code></p>

<p>I have the following dataset with <code>Gmt time</code> and <code>Open</code> value</p>

<pre><code>Gmt time, Open
2018-01-01 00:00:00,2.7321
2018-01-01 00:01:00,2.7323
2018-01-01 00:02:00,2.7322
2018-01-01 00:03:00,2.7321
2018-01-01 00:04:00,2.7323
2018-01-01 00:05:00,2.7325
2018-01-01 00:06:00,2.7322
...., ....
2018-12-31 23:59:00,3.1463
</code></pre>

<p>The <code>Gmt time</code> is a <code>DateTime</code> with <code>yyyy-mm-dd hh:mm:ss</code> given above.
You can see that each data point is <code>1 minute</code> apart.</p>

<p>I want to find the highest <code>Open</code> value and the lowest <code>Open</code> value for every single day(24hours time frame). Eg: <code>2018-01-01 00:00:00</code> to <code>2018-01-01 23:59:00</code></p>

<p>My new <code>DataFrame</code> should look like this:</p>

<pre><code>Gmt time, Open-high, Open-Low
2018-01-01 ,2.7321, ,2.7321
2018-01-02 ,2.7321, ,2.7321
2018-01-03 ,2.7321, ,2.7321
...., ...., ...., ....
2018-12-31 ,2.7321, ,2.7321
</code></pre>

<p>Could you please let me know how can I do this in pandas?</p>
","9161607","","6622587","","2019-07-05 23:25:59","2019-07-05 23:26:40","Find the highest and lowest value for a time frame","<python><python-3.x><pandas><dataframe>","1","0","1","","","CC BY-SA 4.0","1"
"57705786","1","57705818","","2019-08-29 08:07:34","","0","43","<p>I am trying to find the average specific columns of my csv file which has been read into a Dataframe by pandas. I would like to find the mean for 2018 Jul to 2018 Sep and then display them.</p>

<pre><code>Variable | 2018 Jul | 2018 Aug | 2018 Sep | 2018 Oct | 2018 Nov | 2018 Dec |  ....
GDP      | 100      | 200      | 300      | 400      | 500      | 600      | ....

</code></pre>

<p>I have tried to use this code but end up with 'Nan'</p>

<pre><code>vam2['2018 Jul-Sep'] = vam2.iloc[0:1, :2].mean()

vam2
</code></pre>

<p>I believe that '2018 Jul-Sep' should be 200 after finding the mean.</p>

<pre><code>Variable | 2018 Jul | 2018 Aug | 2018 Sep | 2018 Oct | 2018 Nov | 2018 Dec | 2018 Jul-Sep |   ....
GDP      | 100      | 200      | 300      | 400      | 500      | 600      | 200          | ....
</code></pre>
","7245579","","7245579","","2019-08-29 08:28:14","2019-08-29 08:32:39","How to sum specific columns in pandas","<python-3.x><pandas>","2","4","","","","CC BY-SA 4.0","1"
"57163147","1","57163580","","2019-07-23 11:27:09","","1","43","<p>I have this data:</p>

<pre><code>                                 count
Year         Month  Code    
2016         1      ENE001SOLC   121
                    SAL016DECL   92
                    TRN002SIGN   54
                    HAB002SOLC   38
                    TRE001SIGN   37
                    ESP003SOLC   36
                    TRN002SOLC   32
             2      ENE001SOLC   151
                    CAT001SOLC   143
                    VIT001SOLC   90
                    TRE001SIGN   80
                    TRN002SOLC   74
                    BOM001SOLC   72
             3      ENE001SOLC   114
                    ENT002JUST   96
                    TRE001SIGN   94
                    TRN002SIGN   89
                    ENT002APOR   76
                    TRN002SOLC   56
</code></pre>

<p>This data samole is grouped by year and month. It displays the frequencies of a given code as you can see.</p>

<p>What i want to do, is to take all the code values that appears in the same month, and put them into a python a dictionary where the key is the code, and the count is the value so i can have in the same row, all the keys and their respective counts. The desired ouput should look like this:</p>

<pre><code>Year         Month  Code_count    
2016         1      {ENE001SOLC: 121, SAL016DECL: 92, TRN002SIGN: 54, HAB002SOLC: 38, HAB002SOLC: 38, TRE001SIGN: 37, ESP003SOLC: 36, TRN002SOLC: 32}

             2      {ENE001SOLC: 151, CAT001SOLC: 143, VIT001SOLC: 90, TRE001SIGN: 80, TRN002SOLC: 74, BOM001SOLC: 72}
</code></pre>

<p>You can see what my idea is. I have tried to use the <code>pd.to_dict()</code> method but with this i get all the code values with their code but the grouping is not respected. How could i do this ?? Any help will be much appreciated.</p>

<p>Thank you very much in advance</p>
","9542954","","","","","2019-07-23 11:49:08","How to insert a dictionary into dataframe's row based on grouped values?","<python><python-3.x><pandas><dataframe><dictionary>","1","0","","","","CC BY-SA 4.0","1"
"58377302","1","","","2019-10-14 12:53:33","","0","43","<p>I want to merge content for respective rows' data only where some specific conditions are met.
Here is the <code>test</code> dataframe I am working on</p>

<pre><code>    Date        Desc    Debit   Credit  Bal
0   04-08-2019  abcdef  45654   NaN     345.0
1   NaN         jklmn   NaN     NaN     6
2   04-08-2019  pqr     NaN     23      368.06
3   05-08-2019  abd     23      NaN     345.06
4   06-08-2019  xyz     NaN     350.0   695.06
</code></pre>

<p>in which, I want to join the rows where there is <code>nan</code> into <code>Date</code> to the previous row.
Output required:</p>

<pre><code>    Date        Desc        Debit   Credit  Bal
0   04-08-2019  abcdefjklmn 45654   NaN     345.06
1   NaN         jklmn       NaN     NaN     6
2   04-08-2019  pqr         NaN     23      368.06
3   05-08-2019  abd         23      NaN     345.0
4   06-08-2019  xyz         NaN     350.0   695.06
</code></pre>

<p>If anybody help me out with this? I have tried the following:</p>

<pre><code>for j in [x for x in range(lst[0], lst[-1]+1) if x not in lst]:
    print (test.loc[j-1:j, ].apply(lambda x: ''.join(str(x)), axis=1))
</code></pre>

<p>But could not get the expected result.</p>
","3578273","","3578273","","2019-10-15 04:36:13","2019-10-15 04:36:13","Combine text from multiple rows in pandas","<python-3.x><pandas><merge><concatenation><rows>","2","0","","","","CC BY-SA 4.0","1"
"49621811","1","49621945","","2018-04-03 03:33:37","","2","43","<p>I have a pandas <code>DataFrame</code>. I am trying to manipulate a column to show month count. If a record is <code>01m</code> then make it 1. Otherwise if it is a <code>01y</code>, times 1 x 12 to make 12. However sometimes I do have a field called <code>_variable_value</code> which I would like to leave as is. (ignore)</p>

<p>The current <code>dataframe</code> looks like this:</p>

<pre><code>      institution_short_name            product_name             Term  term
0                        One                Standard       _01y_value  4.85
1                        One                Standard       _02y_value  5.15
2                        One                Standard       _03y_value  5.49
3                        One                Standard       _04y_value  5.89
4                        One                Standard       _05y_value  6.09
5                        One                Standard       _06m_value  4.99
6                        One                Standard       _18m_value  5.15
7                        One                Standard  _variable_value  5.79
</code></pre>

<p>I currently get an error as it is trying to convert <code>'va'</code> to int which is not possible.</p>

<pre><code>df['Time'] = np.where(df['Time'].str.contains(""y""), df['Time'].map(lambda x: str(x)[1:3]).astype(int).apply(lambda x: x*12), df['Time'].map(lambda x: str(x)[1:3]).astype(int))
</code></pre>

<p>This is my expected output:</p>

<pre><code>      institution_short_name            product_name             Term  term
0                        One                Standard               12  4.85
1                        One                Standard               24  5.15
2                        One                Standard               36  5.49
3                        One                Standard               48  5.89
4                        One                Standard               60  6.09
5                        One                Standard                6  4.99
6                        One                Standard               18  5.15
7                        One                Standard  _variable_value  5.79
</code></pre>
","8624402","","8624402","","2018-04-03 03:54:28","2018-04-03 04:00:51","only convert certain rows with Pandas","<python><python-3.x><python-2.7><pandas><dataframe>","1","3","","","","CC BY-SA 3.0","1"
"49868377","1","49868407","","2018-04-17 01:14:28","","3","43","<p>I have the following df:</p>

<pre><code>dfdict = {'letter': ['a', 'a', 'a', 'b', 'b'], 'category': ['foo', 'foo', 'bar', 'bar', 'spam']}
df1 = pd.DataFrame(dfdict)

  category  letter
0   foo      a
1   foo      a
2   bar      a
3   bar      b
4   spam     b
</code></pre>

<p>I want it to output me an aggregated count df like this:</p>

<pre><code>     a    b
foo  2    0
bar  1    1
spam 0    1
</code></pre>

<p>This seems like it should be an easy operation. I have figured out how to use
 <code>df1 = df1.groupby(['category','letter']).size()</code> to get:</p>

<pre><code>category  letter
bar       a         1
          b         1
foo       a         2
spam      b         1
</code></pre>

<p>This is closer, except now I need the letters <code>a, b</code> along the top and the counts coming down.</p>
","7748514","","","","","2018-04-17 01:18:36","Python: dataframe manipulation and aggregation in pandas","<python><python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 3.0","1"
"33074978","1","33075203","","2015-10-12 06:58:30","","1","43","<p>I have a df like this:</p>

<pre><code>  ID     Number   
  1        0  
  1        0
  1        1
  2        0
  2        0
  3        1
  3        1
  3        0
</code></pre>

<p>I want to apply a 5 to any ids that have a 1 anywhere in the number column and a zero to those that don't.  For example, if the number ""1"" appears anywhere in the Number column for ID 1, I want to place a 5 in the total column for every instance of that ID.  </p>

<p>My desired output would look as such</p>

<pre><code>  ID     Number    Total 
  1        0        5
  1        0        5
  1        1        5
  2        0        0
  2        0        0
  3        1        5
  3        1        5
  3        0        5
</code></pre>

<p>Trying to think of a way leverage applymap for this issue but not sure how to implement.</p>
","3682157","","3682157","","2015-10-12 07:14:29","2015-10-12 08:32:03","Apply a value to all instances of a number based on conditions","<python><python-3.x><pandas>","2","4","","","","CC BY-SA 3.0","1"
"57000857","1","","","2019-07-12 05:50:18","","2","42","<p>I have two data frames and want to combine them into a single data frame. I used a common key to merge two frames. The final result was a data frame that some of rows have nearly identical fields except a few columns have different values. I want to combine these nearly identical rows into a single row considering adding appropriate columns. 
Here are the data frames:</p>

<p>stores:</p>

<pre><code>Banner - Region - Store ID 

Walmart - NC - 66999 

TJ - NY - 4698
</code></pre>

<p>prices: </p>

<pre><code>Price - Store ID - UPC 

3.6 - 66999 - 234565 

4.5 - 4698 - 334526 
</code></pre>

<p>I already merged tow frames and played a little bit to converge to the desired frame.</p>

<pre><code>store_cross = pd.crosstab(stores['Store ID'],stores['Region'],margins=True)
merged_df2 = pd.merge(store_cross,prices,left_on='Store ID', right_on='Store ID')
merged_df2 = pd.merge(merged_df2,stores,left_on='Store ID', right_on='Store ID')
</code></pre>

<p>This is the result so far: </p>

<pre><code>NY - NC - Price - UPC - Banner 

1 - 0 - 3.6 - 234565 - Walmart 

0 - 1 - 4.5 - 334526 - TJ 
</code></pre>

<p>It is possible to have a UPC at different stores. It means that there are other rows in the frame that have the same UPC and Banner but at different locations. </p>

<p>What I am looking to have is something like this: </p>

<pre><code>Banner - UPC - NC - NY 

Walmart - 234565 - 3.9 - 3.6 

TJ - 334526 - 4.5 - 4.3 
</code></pre>
","5311025","","8727339","","2019-07-12 05:52:55","2019-07-12 06:06:21","Combining rows with nearly same fields","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"56932258","1","","","2019-07-08 09:45:10","","0","42","<p>I have an xlsx file on my computer with initial data (value) - two sheets were created. I decide to calculate some value in sheet1 and in sheet2. After this I try to save the result, but when I download a file from GoogleColab and get my start file with initial values. <br>
And I don't know how to creat a new Sheet3 and make this calcuation in it: copy one column from Sheet1 and another column Sheet2 and make <code>P_roz = P_nav + P-osvit</code> the result in a new column in Sheet3.</p>

<p>How to download the xlsx file with the result in picture 2 and picture 4?</p>

 <pre class=""lang-py prettyprint-override""><code>    from google.colab import drive
    drive.mount('/content/gdrive')
    !pip install -q xlrd

    import pandas as pd 
    import numpy as np

    # Works with Sheet1

    df_1 = pd.read_excel('my_path', name_sheet = 'Sheet1')

    df_1['T_potyz'] = round((((1 / df_1['K_potyz']**2) - 1))**(1/2.0),3)

    df_1['P_nav, кВт'] = df_1['P_ust, кВт']\
    * df_1['K_poputy1']

    df_1['Q_nav, кВАр'] = round(df_1['P_ust, кВт']\
    * df_1['T_potyz'],3)

    df_1['S_nav, кВА'] = round((df_1['P_nav, кВт']**2\
                                            + df_1['Q_nav, кВАр']**2)\
    **(1/2.0),3)

    df_1['P_nav, кВт'].sum(), df_1['Q_nav, кВАр'].sum(), df_1['S_nav, кВА'].sum()
    sum_row1 = df_1[['P_nav, кВт', 'Q_nav, кВАр', 'S_nav, кВА']].sum()
    sum_row1

    # transpose the data and convert the Series to a DataFrame so that it is easier to concat onto our existing data. 
    #The T function allows us to switch the data from being row-based to column-based.

    df_sum1 = pd.DataFrame(data = sum_row1).T
    df_sum1

    # to add the missing columns.

    df_sum1 = df_sum1.reindex(columns = df_1.columns)
    df_sum1

    # add it to our existing one using append.

    df_final_1 = df_1.append(df_sum1, ignore_index = True)
    df_final_1.tail()

    # Works with Sheet2

    df_2 = pd.read_excel('my_path', sheet_name='Sheet2') 

    K_po = float(input(""Коефіцієнт попиту загально освітлення: ""))

    df_2['P_ust'] = (round(df_2['k'] * df_2['P_put'] * 10**-3 * 
                           df_2['A, м'] * df_2['B, м'],3))

    df_2['P_osvit'] = round(K_po * df_2['P_ust'],3)

    df_2['T_potyz2'] = round((((1 / df_2['K_poputy2']**2) - 1))**(1/2.0),3)

    df_2['Q_osvit'] = round(df_2['P_osvit'] * df_2['T_potyz2'],3)

    df_2['S_osvit'] = round((df_2['P_osvit']**2\
                                            + df_2['Q_osvit']**2)\
    **(1/2.0),3)

    df_2['P_osvit'].sum(), df_2['Q_osvit'].sum(), df_2['S_osvit'].sum()

    sum_row2 = df_2[['P_osvit', 'Q_osvit', 'S_osvit']].sum()
    sum_row2

    df_sum2 = pd.DataFrame(data = sum_row2).T
    df_sum2

    df_sum2 = df_sum2.reindex(columns = df_2.columns)
    df_sum2

    df_final2 = df_2.append(df_sum2, ignore_index = True)
    df_final2.tail()

    # Here I want create a new Sheet3 and make some calculation

    P_roz = df_1['P_nav, кВт'] + df_2['P_osvit']
    Q_roz = df_1['Q_nav, кВАр'] + df_2['Q_osvit']
    S_roz = (P_roz**2 + Q_roz**2)**(1 / 2.0)

    frame_data3 = {'P_roz' : [P_roz], 'Q_roz' : [Q_roz], 'S_roz' : [S_roz]}
    df_4 = pd.DataFrame(frame_data3)
    df_4
</code></pre>

<p><a href=""https://i.stack.imgur.com/oahsz.jpg"" rel=""nofollow noreferrer"">Sheet 1</a>  <a href=""https://i.stack.imgur.com/NSJT9.jpg"" rel=""nofollow noreferrer"">Sheet 1 + sum</a> <a href=""https://i.stack.imgur.com/zfync.jpg"" rel=""nofollow noreferrer"">Sheet 2</a> <a href=""https://i.stack.imgur.com/oahsz.jpg"" rel=""nofollow noreferrer"">Sheet 2 + sum</a> </p>
","11753499","","11753499","","2019-07-08 11:16:12","2019-07-08 11:16:12","Can't create a new sheet in GoogleColab and make calculcation","<python><python-3.x><pandas><xlsx><google-colaboratory>","0","3","","","","CC BY-SA 4.0","1"
"56598766","1","56598819","","2019-06-14 13:01:34","","2","42","<p>I have this:</p>

<pre><code>dfData['dt']
Out[53]: 
0   2013-01-02
1   2016-10-20
Name: dt, dtype: datetime64[ns]
</code></pre>

<p>I try this:</p>

<pre><code>dfData['dt'].dtype==np.datetime64
Out[56]: False
</code></pre>

<p>I also try this:</p>

<pre><code>isinstance(dfData['dt'], pd.DatetimeIndex)
Out[62]: False
</code></pre>

<p>What am I doing wrong? How can I identify general date types?</p>
","2065691","","","","","2019-06-14 13:19:14","Check date column in pandas","<python-3.x><pandas><date>","1","0","","","","CC BY-SA 4.0","1"
"57660489","1","","","2019-08-26 15:04:02","","0","42","<p>I am calling an API that returns me a dataframe. When i do print(df) it prints something like this,</p>

<pre><code>             val1    val2    val3     val4    val5      
Date                                                         
2019-07-29   415.75  433.00  435.35   433.00  435.35   
2019-07-30   429.35  433.00  438.70   433.00  435.35   
2019-07-31   425.35  423.90  430.20   433.00  435.35   
2019-08-01   424.60  423.25  426.35   433.00  435.35   
2019-08-02   417.05  413.90  417.85   433.00  435.35   
2019-08-05   410.60  403.65  406.65   433.00  435.35   
2019-08-06   402.65  401.00  411.50   433.00  435.35 
</code></pre>

<p>Now I have a new entry like 2019-08-07, val1=500, val2=501, val3=502 that i need to add to the end of the dataframe so that it should look like below when i do print(df). For columns which are not populated it should be nan. Please let me know how to do that.</p>

<pre><code>             val1    val2    val3     val4    val5      
Date                                                         
2019-07-29   415.75  433.00  435.35   433.00  435.35   
2019-07-30   429.35  433.00  438.70   433.00  435.35   
2019-07-31   425.35  423.90  430.20   433.00  435.35   
2019-08-01   424.60  423.25  426.35   433.00  435.35   
2019-08-02   417.05  413.90  417.85   433.00  435.35   
2019-08-05   410.60  403.65  406.65   433.00  435.35   
2019-08-06   402.65  401.00  411.50   433.00  435.35 
2019-08-07   500.00  501.00  502.00   nan     nan  
</code></pre>
","2162521","","2162521","","2019-08-26 15:31:36","2019-08-26 15:31:36","Adding a row to existing dataframe","<python-3.x><pandas><numpy><dataframe>","0","2","","2019-08-26 15:23:33","","CC BY-SA 4.0","1"
"57511350","1","57511373","","2019-08-15 14:37:46","","-1","42","<p>I have a dataframe that I'm trying to tidy up and clean out.</p>

<p>I'm basically testing for any cell in column 'Header 3' that is empty, and if it is, then I delete the row.</p>

<p>Unfortunately everything I've tried to highlight if the cell is empty doesn't appear to be working.</p>

<p>I've checked the cell I'm validating the method against, and it comes up as None when I check it as a string and also as a NoneType when I check the type.</p>

<p>I've tried checking using <code>if cell == 'nan'</code> and also if it equals None but neither appear to catch the cell.</p>

<pre><code>if str(import_sheet.loc[23,'Header 3']) == 'nan':
    print('Yep')
else:
    print('Nope')

if str(import_sheet.loc[23,'Header 3']) == None:
    print('Yep')
else:
    print('Nope')

Nope
Nope

print(str(import_sheet.loc[23,'Header 3']))
print(type(import_sheet.loc[23,'Header 3']))


None
&lt;class 'NoneType'&gt;
</code></pre>
","2493856","","1723857","","2019-08-15 16:38:13","2019-08-15 16:38:13","Trying to Test for NoneType or Empty Cell in DataFrame but Method is not working","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"59261364","1","","","2019-12-10 06:23:36","","1","42","<p>I have to dfs that look like: </p>

<p>dfa:</p>

<pre><code>ID   |          Time        |   Mode
B121   2017-10-19 11:00:00     Processed
C232   2017-10-19 12:00:00     Cancelled
B121   2017-10-19 15:00:00     Cancelled
C455   2017-10-19 18:00:00     Processed
N776   2017-10-19 11:00:00     Processed
</code></pre>

<p>dfb:</p>

<pre><code>ID   |          Time        
B121   2017-10-19 11:43:32    
C232   2017-10-19 12:00:00    
B121   2017-10-19 15:33:55    
C455   2017-10-19 18:31:33     
N776   2017-10-19 11:08:00     
</code></pre>

<p>I want to add the Time, and Mode columns from table A to table B based on joining them on
ID and Time column. </p>

<p>But the times in table A are based on intervals for each hour and the times in table B are exact times. </p>

<p>How can I join or merge the two dfs based on if the timestamp from df B is in the hour time interval in df A?</p>

<p>Thanks</p>
","8797830","","","","","2019-12-10 06:23:36","Joining two dfs based on timestamp column","<python><python-3.x><pandas>","0","3","","2019-12-10 06:29:59","","CC BY-SA 4.0","1"
"57706566","1","","","2019-08-29 08:56:48","","1","42","<p>I want to use the color coding from a pandas style object and apply the exact same coloring to the corresponding cells in another dataframe with the same size. The data is different in the two dataframes which means i can not use the same styling function.</p>

<p>I have tried using style.render() from the style object with the desired color and then apply it to the new dataframe. This did not work for me though.</p>

<p>EDIT:
I have posted a picture below of what i have tried to do. In the picture i use the .render() method to get the HTML of the dataframe with the color coding and apply it to a new dataframe, but without success.</p>

<p><a href=""https://i.stack.imgur.com/bHIBz.png"" rel=""nofollow noreferrer"">styling</a></p>
","11993306","","11993306","","2019-08-29 11:29:39","2019-08-29 11:29:39","Use color coding from pandas style object to another pandas DataFrame","<python-3.x><pandas><pandas-styles>","0","4","","","","CC BY-SA 4.0","1"
"56757408","1","56758214","","2019-06-25 15:24:47","","0","42","<p>My data frame look like this</p>

<pre><code> TeamA   TeamB 
 [a,b]   [[b,c],[d,c],[d,f] .....upto 15  such arrays]
 [b,c]   [[v,c],[e,c],[g,f] .....upto 15  such arrays]
</code></pre>

<p>TeamB has 15 sets of records in each row.</p>

<p>I want it to look like </p>

<pre><code> TeamA   Team0 Team1  Team2 ...... 
 [a,b]   [b,c] [d,c]  [d,f]] ......
 [b,c]   [v,c] [e,c]  [g,f] .....
</code></pre>

<p>For 1D I can use </p>

<pre><code>newcol = df['TeamB '].apply(pd.Series)
newcol = newcol.rename(columns = lambda x : 'Team_' + str(x))
pd.concat([df[:], newcol [:]], axis=1)
</code></pre>

<p>How can i do that with 2D arrays.</p>

<p>Here is my df.head(5).to_dict(). In this case i have taken only 2 arrays per row in TeamB  </p>

<pre><code>{'TeamA   ': {0: array([ 35.82, -95.67]),
  1: array([ 36.27, -95.91]),
  2: array([ 35.99 , -95.88]),
  3: array([ 36.18, -96.40]),
  4: array([ 35.99 , -95.90])},
 'TeamB': {0: array([[ 35.74  , -95.36],
         [ 36.19 , -95.92]]),
  1: array([[ 35.82, -95.67],
         [ 35.98 , -95.81 ]]),
  2: array([[ 36.27, -95.91],
         [ 36.08 , -95.96 ]]),
  3: array([[ 35.99 , -95.88],
         [ 35.75 , -95.34]]),
  4: array([[ 36.18, -96.40],
         [ 36.07 , -95.89 ]])}} 
</code></pre>
","6759267","","6759267","","2019-06-25 15:46:37","2019-06-25 16:15:23","Expand Cells Containing 2D arrays Into Their Own Variables In Pandas","<python><python-3.x><pandas><numpy>","1","5","1","","","CC BY-SA 4.0","1"
"57507776","1","57508150","","2019-08-15 09:43:47","","0","42","<p>Lets say I take the <code>iris</code> data set for example. I sample the data randomly to get a subset of the data. Next I want to find the count of the number of classes so I group the data by Species and use the <code>.count()</code> function to get the count of the number of instances in each class. So far so good</p>

<p>Here is the code to do that:</p>

<pre><code>import numpy as np
import pandas as pd
iris_df = pd.read_csv('./data/iris.csv') # this file has 150 rows
subset_df = iris_df.iloc[np.random.randint(1, 150, 60), ]
subset_df.groupby('Species', as_index = False).count()

## Output
      Species  Sepal.Length  Sepal.Width  Petal.Length  Petal.Width
0      setosa            19           19            19           19
1   virginica            20           20            20           20
2  versicolor            21           21            21           21
</code></pre>

<p>Now this is my question: Is there a way to get the group label of the instance with most samples. So in the above output: <code>versicolor</code> has the most number of samples so I want to get that group label.</p>

<p>I tried taking the max of the above line but that will sort the species column by character and return <code>virginica</code> which is definitely not correct, but the output makes sense.</p>

<p>There is one other way I can think of for getting the group name is by using <code>.groups</code> on the grouped by data frame by running the following code</p>

<pre><code>species_dict = subset_df.groupby('Species', as_index = False).groups
max_ind = np.argmax([len(species_dict[k]) for k in species_dict.keys()])
print(list(species_dict.keys())[max_ind])
</code></pre>

<p>Is there a better way, more efficient way, using some Pandas functionality that I've missed. Please let me know</p>
","11323300","","","","","2019-08-15 10:58:51","Efficient way to get groupby label for the group with max count","<python-3.x><pandas><pandas-groupby>","2","0","","","","CC BY-SA 4.0","1"
"57702957","1","57703014","","2019-08-29 04:24:40","","1","42","<p>I cant seem to figure out what I am doing wrong,
I have 2 columns in my df that look like below,</p>

<pre><code>    Date                Time 
2019-07-23 21:17:47.599  22:00:00.000
2019-07-23 21:11:46.973  21:50:00.000
</code></pre>

<p>I want to create a new column in the df that calculates the difference in minutes between these two columns. I am trying 2 methods that aren't working. </p>

<p>First method is where I use <code>pd.to_timedelta(pd.to_datetime(df3.Time)-pd.to_timedelta(df3.Date).dt.strftime('%H:%M:%S')) / np.timedelta64(1, 'm')
</code></p>

<p>If I try this I get an error of <code>ValueError: only leading negative signs are allowed
</code></p>

<p>Second method I am trying is where I convert the Date column by doing</p>

<pre><code>df3['new_time'] = [d.time() for d in df3['Date']]
</code></pre>

<p>that gives me a timestamp such as <code>22:25:03.895000</code></p>

<p>I then use <code>(pd.to_datetime(df3.Time) - pd.to_datetime(df3.new_time))</code></p>

<p>The error I am getting using that is <code>TypeError: &lt;class 'datetime.time'&gt; is not convertible to datetime
</code></p>

<p>I am not sure what I am missing or not doing correctly, but is there a way where I can correctly have a new column that gives me the value in minutes of Time - Date? </p>

<p>Thanks so much</p>
","8797830","","","","","2019-08-29 04:37:37","Calculating time difference in minutes between a date and a timestamp columnn","<python><python-3.x><pandas><datetime><data-science>","1","0","","","","CC BY-SA 4.0","1"
"41049567","1","","","2016-12-08 21:58:11","","0","42","<p>I have a series object of strings where there is a specific characters i can go along with. For instance, the one with the end character of <code>[]</code> will be corresponded to those with end character of <code>()</code></p>

<pre><code>s = pd.Series(['September[jk]', 'firember hfh(start)','secmber(end)','Last day(hjh)',
              'October[jk]','firober fhfh (start)','thber(marg)','lasber(sth)',
              'December[jk]','anober(start)','secber(start)','Another(hkjl)'])
</code></pre>

<p>I can simply clean the data but these characters at the end should help me build the resulting data frame like this</p>

<pre><code>0   September   firember hfh
1   September   secmber
2   September  Last day
3    October   firober fhfh
4    October     thber
5    October    lasber
6   December    anober
7   December    secber
8   December   Another
</code></pre>
","3917772","","3917772","","2016-12-08 23:52:59","2016-12-08 23:56:19","How to convert a series object in to data frame using string cleaning","<python-3.x><pandas>","1","3","","","","CC BY-SA 3.0","1"
"57605141","1","57605499","","2019-08-22 08:30:21","","0","42","<p>I have a <code>df</code> as follows:</p>

<pre><code>  Date                values
20190101000000  1384.4801224435887
20190101000001  1384.5053056232982
20190101000002  1384.5304889818935
20190101000003  1384.5556725193492
20190101000004  1384.5808562356392
20190101000005  1384.606040130739
20190101000006  1384.631224204622
20190101000007  1384.6564084572635
20190101000008  1384.6815928886372
20190101000009  1384.7067774987179
20190101000010  1384.7319622874802
20190101000011  1384.757147254898
20190101000012  1384.7823324009464
20190101000013  1384.8075177255998
20190101000014  1384.8327032288325
20190101000015  1384.8578889106184
20190101000016  1384.8830747709321
20190101000017  1384.9082608097488
20190101000018  1384.9334470270423
20190101000019  1384.958633422787
20190101000020  1384.9838199969574
20190101000021  1385.0090067495285
20190101000022  1385.034193680474
20190101000023  1385.0593807897685
20190101000024  1385.0845680773864
20190101000025  1385.1097555433028
20190101000026  1385.134943187491
20190101000027  1385.160131009926
20190101000028  1385.1853190105826
20190101000029  1385.2105071894343
20190101000030  1385.2356955464566
</code></pre>

<p>where the <code>Date</code> column is of the format <code>%Y%m%d%H%M%S</code>. I take start date and end date as the user inputs and split it in a frequency of 1 second.</p>

<p>Now, I would like to take a second value of frequency from the user and obtain the value from the <code>values</code> column at that instant.</p>

<p><strong>Example:</strong></p>

<p>If the second resolution is 10secs, then the output must be as follows:</p>

<pre><code>start              end                 value
20190101000000  20190101000010  1384.7319622874802
20190101000011  20190101000020  1384.9838199969574
20190101000021  20190101000030  1385.2356955464566
</code></pre>

<p>from the above <code>df</code>, we can see that if the resolution is 10sec, then the value at every 10th second must be obtained.</p>

<p>If the second resolution is 15mins, then the output must be as follows:</p>

<pre><code> start                 end             values
20190101000000  20190101001500  1407.2142300429964
20190101001501  20190101003000  1416.6996533329484
20190101003001  20190101004500  1424.2467631293005
</code></pre>

<p>How can this be done?</p>

<p><strong>My code till now:</strong></p>

<pre><code>import datetime
import pandas as pd

START_DATE = str(input('Enter start date in %Y-%m-%d %H:%M:%S format: '))
END_DATE = str(input('Enter end date in %Y-%m-%d %H:%M:%S format: '))
RESOLUTION = 'S'

dates = pd.date_range(START_DATE, END_DATE, freq = RESOLUTION)
dates = pd.DataFrame(pd.Series(dates).dt.strftime('%Y%m%d%H%M%S'), columns = ['Date'])
</code></pre>
","11135962","","11135962","","2019-08-22 08:57:35","2019-08-22 09:21:04","How to obtain values from a date column at particular time instance?","<python><python-3.x><pandas>","2","2","","","","CC BY-SA 4.0","1"
"49561868","1","49562628","","2018-03-29 17:20:52","","0","42","<p>The error from the title crops up in a very strange case for me.</p>

<p>I have a dataframe <code>copy_data</code> and I am trying to remove a set of columns from it.</p>

<p>I generate the set of column names to remove:</p>

<pre><code>set_to_remove = set(list(copy_data)) - set([self.farmConfig['mapping'][column]['column'].split('.')[0] for column in self.farmConfig['mapping']])
</code></pre>

<p>where <code>self.farmConfig</code> has a structure like this:</p>

<pre><code>farmConfig = {
    'mapping': {
        'A': {
            'column': 'a'
        },
        'B': {
            'column': 'b'
        },
        ...
    }
}
</code></pre>

<p>and I can confirm that the result of <code>set_to_remove</code> is a valid subset of the set of columns. Then I call <code>copy_data.drop(list(set_to_remove), inplace=True)</code>, where I run into this error.</p>

<p>Note that for some reason, this works when copy_data is smaller, but fails when I run it on the full size.</p>

<p>So my questions are - why am I getting this error, and how can I fix it?</p>
","1190376","","1325646","","2018-03-29 18:36:48","2018-03-29 18:36:48","pd.drop() throws TypeError: '>' not supported between instances of 'str' and 'int'","<python><python-3.x><pandas><dataframe><typeerror>","1","0","","","","CC BY-SA 3.0","1"
"58373383","1","","","2019-10-14 09:03:14","","1","42","<p>We have a multi-index DataFrame <code>df</code></p>

<pre><code>            0  1  2
Name Stock         
Tom  AAPL   0  0  0
     GOOG   0  0  0
     NFLX   0  0  0
John AAPL   0  0  0
     GOOG   0  0  0
     NFLX   0  0  0
</code></pre>

<p>and a Series <code>s</code></p>

<pre><code>AAPL    99
NFLX    11
dtype: int64 
</code></pre>

<p><strong>Question:</strong> How can we set the values in column <code>2</code> of the dataframe <code>df</code> using values from the series <code>s</code>?</p>

<p>In other words, only the values for index <code>('Tom', 'AAPL')</code> and <code>('Tom', 'NFLX)</code> in dataframe <code>df</code> should be set to <code>99</code> and <code>11</code>, respectively. <code>('Tom', 'GOOG')</code> should remain unchanged.</p>

<p><strong>Failed Attempt</strong></p>

<pre><code>idx = pd.IndexSlice
df.loc[idx['Tom', :], 2] = s
print(df)
</code></pre>

<pre><code>            0  1    2
Name Stock           
Tom  AAPL   0  0  NaN
     GOOG   0  0  NaN
     NFLX   0  0  NaN
John AAPL   0  0  0.0
     GOOG   0  0  0.0
     NFLX   0  0  0.0
</code></pre>

<p><strong>Code to Reproduce Problem</strong></p>

<pre><code>stocks = ['AAPL', 'GOOG', 'NFLX']
names = ['Tom', 'John']
midx = pd.MultiIndex.from_product([names, stocks], names=['Name','Stock'])
df = pd.DataFrame(index=midx)
for i in range(3):
    df[i] = [0,0,0,0,0,0]
print(df)

s = pd.Series([99, 11], index=['AAPL','NFLX'])
print('\n', s, '\n')

idx = pd.IndexSlice
df.loc[idx['Tom', :], 2] = s
print(df)
</code></pre>
","741099","","","","","2019-10-14 09:25:40","Modify Pandas Multi-Index DataFrame Values using Pandas Series","<python><python-3.x><pandas><dataframe><multi-index>","3","2","","","","CC BY-SA 4.0","1"
"57457379","1","","","2019-08-12 07:49:19","","0","41","<p>I am getting error in below line </p>

<pre><code>for i in split_json:
        read_values = json.loads(i + ""]"",strict = False)
</code></pre>

<p>split_json has the data in format </p>

<pre><code>[[{""Key1"":""Value1""},
[{""Key2"":""Value2""}]
</code></pre>

<p>The value starts with a <code>'['</code> but  doesn't end with <code>']'</code>. So I am padding <code>']'</code> while calling <code>json.loads()</code>.</p>

<p>I am getting this error in the code mentioned above. My dataframe is correct.</p>

<pre><code>raise JSONDecodeError(""Expecting value"", s, err.value) from None

JSONDecodeError: Expecting value
</code></pre>
","5886855","","10035985","","2019-08-12 07:54:56","2019-08-12 07:55:12","I am loading JSON data from a file. then trying to decode one of the value corresponding to a Key","<json><python-3.x><pandas>","1","3","","","","CC BY-SA 4.0","1"
"56754950","1","","","2019-06-25 13:14:02","","0","41","<p>I do not understand the below segment of code of use of <code>replace()</code> function in the python csv library.</p>

<p>Can someone please explain me the use of <code>**c</code> in the second line? Why is it used? Also if possible please explain the whole line of code.</p>

<pre><code># replace''with 0
cast0 = [{**c,'n':c['n'].replace('','0')} for c in casts]

cast0[3:5]
</code></pre>
","11697732","","5276797","","2019-06-25 13:19:44","2019-06-25 13:19:44","pandas in python; not able to understand the use of replace() in python-csv library","<python><python-3.x><pandas><csv><sklearn-pandas>","0","4","","","","CC BY-SA 4.0","1"
"57652309","1","57652334","","2019-08-26 05:40:06","","1","41","<p>I have a dataframe that has domain names. But the problem is every character of the domain name is in single cell of a dataframe. Below is how it looks. the 'Column' is just column name for the first column. </p>

<pre><code>testing = pd.DataFrame({'col':['h','h'],
                        'Unnamed :1':['t','t'],
                        'Unnamed :2':['t','t'],
                        'Unnamed :3':['p','p'],
                        'Unnamed :4':['s',':']})


print (testing)
  col Unnamed :1 Unnamed :2 Unnamed :3 Unnamed :4
0   h          t          t          p          s
1   h          t          t          p          :
</code></pre>

<p>I wish to concatenate every column and the resultant should look like</p>

<pre><code>https
http:
</code></pre>

<p>My code :
I read the excel sheet which has data, convert to dataframe and see if the first column of every row has one character or a string. If it is a character, I have to concatenate all the characters present in that entire row. </p>

<pre><code>testing = pd.read_excel(""path to .xlsx file"")  
for i in range(len(testing)):      
    if len(testing.iloc[i,0]) == 1:
        testing.iloc[i,0] = testing.astype(str).values.sum(axis=1)
</code></pre>

<p>But this gives:</p>

<pre><code>['https' 'http:' 'http:' 'http:' 'http:']

['https' 'http:' 'http:' 'http:' 'http:']
</code></pre>
","10866704","","2901002","","2019-08-26 05:51:00","2019-08-26 06:01:03","How to concatenate characters of certain columns of a dataframe?","<python-3.x><pandas><dataframe>","2","0","","","","CC BY-SA 4.0","1"
"57414781","1","57414886","","2019-08-08 14:20:00","","0","41","<p>I'm trying to select 10 cases into one new data frame with pandas, but I getting one problem. I'm using this code:</p>

<pre><code>    import pandas as pd
    import csv
    import geopy
    import numpy as np
    import geopandas as gpd

    new_df = df.loc(axis=0)[df['cod'] == 569852, 478521, 
    159632, 458216, 521562, 258632, 584526, 596325, 596325, 512584]
</code></pre>

<p>I'm getting the following error</p>

<blockquote>
  <p>TypeError: 'Series' objects are mutable, thus they cannot be hashed</p>
</blockquote>

<p>I already tried with</p>

<pre><code>new_df = df.loc(axis=0)[df['cod'] == '569852', '478521', 
        '159632', '458216', '521562', '258632', '584526', '596325', '596325', '512584']
</code></pre>

<p>But the error is the same. What I'm doing wrong? </p>
","11545193","","","","","2019-08-08 14:25:41","Select 10 cases from one column with Pandas (python)","<python><python-3.x><pandas><jupyter-notebook>","1","0","","2019-08-08 14:38:11","","CC BY-SA 4.0","1"
"48931944","1","48933573","","2018-02-22 16:03:33","","1","41","<p>I am trying to achieve the following output as shown below.</p>

<pre><code>immatrix = array(array(Image.open(path2 + '\\' + file)).flatten())
# print(immatrix)
image_dict[file] = immatrix
# print(image_dict)
image_df = pd.DataFrame.from_dict(image_dict)#, orient='index')#.reset_index()
print(image_df.transpose())
image_df = image_df.rename(index=str, columns={'index':'filename', 0:'image_object'})

 def getPID(row):
     return int(row.split('-')[0])
  image_df['Image_Id'] = image_df.filename.apply(getPID)
</code></pre>

<p>Am getting an output like.</p>

<pre><code>filename            image_object  1   2   3   4   5 ...................... 39998    39999     40000   image_id
8003233-214x261.jpg 214 214 214 214 214 214 214 214 214 ... 216 217 217 217 217 217 217 217 217         8003233 
8003234-214x261.jpg 210 210 210 210 210 210 210 210 211 ... 230 230 230 230 230 230 230 230 230         8003234 
8003235-214x261.jpg 214 215 215 216 216 215 215 214 215 ... 230 230 230 230 230 230 230 230 230         8003235 
8003236-214x261.jpg 215 215 215 215 215 215 215 215 216 ... 225 225 225 225 225 225 225 225 225         8003236 
</code></pre>

<p>Am  trying to achieve output as shown below.</p>

<pre><code>filename            image_object                                                                        image_id
8003233-214x261.jpg [214,214,214,214,214,214,214,214,214 ... 216,217,217,217,217,217,217,217,217]       8003233 
8003234-214x261.jpg [210,210,210,210,210,210,210,210,211 ... 230,230,230,230,230,230,230,230,230]       8003234 
8003235-214x261.jpg [214,215,215,216,216,215,215,214,215 ... 230,230,230,230,230,230,230,230,230]       8003235 
8003236-214x261.jpg [215,215,215,215,215,215,215,215,216 ... 225,225,225,225,225,225,225,225,225]       8003236 
</code></pre>
","4789626","","4789626","","2018-02-22 16:19:14","2018-02-22 17:19:39","want to convert a dictionary to a dataframe","<python><python-3.x><image><pandas><image-processing>","1","4","1","","","CC BY-SA 3.0","1"
"57074015","1","57074059","","2019-07-17 10:42:27","","0","41","<p>I try to fix an embarrassing problem in Python Pandas. I want to add a new column, and depending on another column get the new values.</p>

<pre><code>signal  nom
0       value is 0
1       value is outcome of some calculations
-1      value is outcome of some other calculations
</code></pre>

<p>I thought of using np.where - but I believe this has only the possibility to use 2 different outcomes (and I have 3).</p>

<p>I tried using a sequential approach, first check the col1 for the existence of a ""1"" and set the newcol based on this. Followed by a check if col1 is ""-1"", and input values based on this. But this overwrites.</p>

<p>I tried setting the new col with one line, but this gives True/False output.</p>

<pre class=""lang-py prettyprint-override""><code>df['nom'] = (df.signal[df.signal ==1] * -10000) | (df.signal[df.signal ==-1] *30)
</code></pre>
","3460063","","","","","2019-07-17 10:45:19","Setting a new column based on another column, 3 options","<python-3.x><pandas><numpy>","1","0","","","","CC BY-SA 4.0","1"
"57655292","1","","","2019-08-26 09:34:52","","0","41","<p>I have a excel file with some 2018-19 data, dates in d/m/y format are present for each data but I want to generate a crosstab between one type and year(year only not date) how can I do that?</p>

<p>I used this:</p>

<pre><code># generate crosstab of TYp and QN Created date
pd.crosstab(data_BM[""Typ""],data_BM[""QN Crtd"",strftime('%d-%b-%y')])
</code></pre>

<p>Result I got:</p>

<pre><code>---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-40-01be21abf4c1&gt; in &lt;module&gt;
      1 # generate crosstab of TYp and QN Created date
----&gt; 2 pd.crosstab(data_BM[""Typ""],data_BM[""QN Crtd"",strftime('%d-%b-%y')])

NameError: name 'strftime' is not defined
</code></pre>
","11977677","","1000551","","2019-08-26 09:45:07","2019-08-26 10:23:17","Pandas fpr python","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57865870","1","57866386","","2019-09-10 07:22:14","","1","41","<p>Hi Hoping to get some help, I have a Dataframe df like this:</p>

<pre><code>label   cell_name hour  kpi1    kpi2
train   c1  1   10  20
train   c1  2   10  44
train   c1  3   11  33
train   c1  4   5   1
train   c1  5   2   6
test    c1  1   78  66
test    c1  2   45  2
test    c1  3   23  12
test    c1  4   65  45
test    c1  5   86  76
</code></pre>

<p>My intention is to conditionally subtract some value lets say(50) from kpi1,kpi2 columns of the test set and divide the same columns with a train set(groupby cell &amp; hour) and attach this to the original data frame so the new column would look like;</p>

<pre><code>label   cell_name hour  kpi1    kpi2    kpi1_index  kpi2_index
train   c1  1   10  20      
train   c1  2   10  44      
train   c1  3   11  33      
train   c1  4   5   1       
train   c1  5   2   6       
test    c1  1   78  66   2.8         0.8
test    c1  2   45  2    -0.5       -1.09
test    c1  3   23  12  -2.45       -1.15
test    c1  4   65  45    3          -5
test    c1  5   86  76    18        4.33
</code></pre>

<p>I tried the following code:</p>

<pre><code>import pandas as pd
import os
rr=os.getcwd()
df=pd.read_excel(rr+'\\KPI_test_train.xlsx')
print(df.columns)


def f(x,y):
    return ((x-50)/y)     
df_grouped = df.groupby(['label'])
[dtest,dtrain]=[y for x,y in df_grouped]
dtest=dtest.groupby(['label','cell_name','hour']).sum()
dtrain=dtrain.groupby(['label','cell_name','hour']).sum()

for i in dtest.columns:
    dtest[i+'_index']=f(dtest[i],dtrain[i])
</code></pre>

<p>the function f returns NaN values for all rows.
but this is kind of nasty, considering how nice pandas usually is at these things. What's the built-in way of doing this?</p>
","8783169","","8783169","","2019-09-10 07:30:56","2019-09-10 07:56:36","conditional diff,divide on one same column of a data-frame pandas","<python><python-3.x><pandas><pandas-groupby>","1","0","","","","CC BY-SA 4.0","1"
"57558158","1","57558897","","2019-08-19 13:51:36","","2","41","<p>Lets suppose I have two excel sheets. named <em>fruits</em> and <em>fruitsDetail</em>.</p>

<pre><code> fruits.xlsx                                          fruitsDetail.xlsx
**Name**                                        **Name  height  weight** circumference
apple                                       apple    25cm  50g      10cm
banana                                      apple    35cm  60g      10cm
orange                                      banana    15cm  20g     7cm
                                            banana    24cm   66g    6cm
....                                        orange    45cm  60g     20cm
                                            orange    36cm  76g     20cm
                                            orange    88cm  100g    30cm
                                                      .......
</code></pre>

<p>I have been using <em>jupyter notebook and using pandas</em></p>

<pre><code>import pandas as pd
df = pd.read_excel(open(r'C:\Users\fruits.xlsx','rb'))
mf = pd.read_excel(open(r'C:\Users\fruitsDetail.xlsx','rb'))
</code></pre>

<p>Now i want to save the data by circumference...
 output follows as below in separate excel sheet </p>

<pre><code>name    circumference     weight
apple   10cm              50g,60g
banana  7cm                20g
banana  6cm                66g
orange  20cm               60g,76g
orange   30cm              100g
</code></pre>
","11946630","","7841468","","2019-08-19 14:22:09","2019-08-19 14:34:53","matching data in two excel sheets, and storing the data got matched","<python><python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"57163934","1","57165050","","2019-07-23 12:10:30","","0","41","<p>After making a connection with mysql library, i'd like to dowload all the database from the connection in my local space (tranforming them into pandas dataframe).</p>

<p>Here's my code:</p>

<pre><code>    import MySQLdb
    import pandas as pd

    conn = MySQLdb.connect(host='host' , user='datbase', passwd='password', db='databases' )
cursor = conn.cursor()

query = cursor.execute('  SELECT *  FROM pro ')

df = pd.read_sql(query , conn)

row = cursor.fetchone()

conn.close()
</code></pre>

<p>I finnaly got the connection, so i can make some query. But i'd like to use these sql database as a pandas dataframe, '''how can i do it'''?</p>
","10061482","","9272910","","2019-08-15 20:31:25","2019-08-15 20:31:25","How to download data from mysql connection","<mysql><python-3.x><pandas><database-connection>","1","0","","","","CC BY-SA 4.0","1"
"57668474","1","57668851","","2019-08-27 06:02:17","","2","41","<p>So I counted the frequency of a column 'address' from the dataframe 'df_two' and saved the data as dict. used that dict to create a series 'new_series'. so now I want to join this series into a dataframe making 'df_three' so that I can do some maths with the column 'new_count' and the column 'number' from 'new_series' and 'df_two' respectively.</p>

<p>I have tried to use merge / concat the items of 'new_count' were changed to NaN</p>

<p><strong>Image for what i got(NaN)</strong></p>

<p><img src=""https://i.stack.imgur.com/EAbiF.png"" alt=""Screen Shot""></p>

<p>df_three</p>

<p>number  address name  new_Count
14  12 ab   pra   NaN
49  03 cd   ken   NaN
97      07 ef   dhi   NaN
91  10 fg   rav   NaN</p>

<p><strong>Image for input</strong></p>

<p><img src=""https://i.stack.imgur.com/kblT6.png"" alt=""Screen Shot""></p>

<p><strong>Input</strong></p>

<p>new_series
        new_count
12 ab   8778
03 cd   6499
07 ef   5923
10 fg   5631</p>

<p>df_two
number  address name
14  12 ab   pra
49  03 cd   ken
97      07 ef   dhi
91  10 fg   rav</p>

<p><img src=""https://i.stack.imgur.com/IrTrI.png"" alt=""Screen Shot""></p>

<p><strong>output</strong></p>

<p>df_three
number  address name  new_Count
14  12 ab   pra   8778
49  03 cd   ken   6499
97      07 ef   dhi   5923
91  10 fg   rav   5631</p>
","11977590","","10716450","","2019-08-27 09:16:05","2019-08-27 09:16:05","How to join a series into a dataframe","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"57023182","1","57023539","","2019-07-13 22:13:00","","0","41","<p>My dataset looks like this (simplied):</p>

<pre><code>+----+------+-------------------------------+
| ID | Name |            Options            |
+----+------+-------------------------------+
|  1 | John | {Sofa,Fridge,Pets,TV}         |
|  2 | Mary | {TV,Sofa,Fridge,Parking}      |
|  3 | Bob  | {TV,Sofa,Parking,Pets,Fridge} |
|  4 | Todd | {TV,Sofa,Fridge,Pets,AC}      |
+----+------+-------------------------------+
</code></pre>

<p>My expected output</p>

<pre><code>+----+------+----+------+--------+---------+------+----+
| ID | Name | TV | Sofa | Fridge | Parking | Pets | AC |
+----+------+----+------+--------+---------+------+----+
|  1 | John |  1 |    1 |      1 |       0 |    1 |  0 |
|  2 | Mary |  1 |    1 |      1 |       1 |    0 |  0 |
|  3 | Bob  |  1 |    1 |      1 |       1 |    1 |  0 |
|  4 | Todd |  1 |    1 |      1 |       0 |    1 |  1 |
+----+------+----+------+--------+---------+------+----+
</code></pre>

<p>My Code</p>

<pre><code>import numpy as np
import pandas as pd
pd.set_option(""max_columns"", None)
listings = pd.read_csv(""../listings.csv"")
final_list = list(map(lambda val:val.replace(""{"","""").replace(""}"","""") , listings['amenities'])) 
final_list_1 = """"

for values in final_list:
    final_list_1 += "","" + values

final_list_2 = final_list_1.split(',')
print(list(set(final_list_2))[1:])
</code></pre>

<p>With above output I am able to get each unique value in that column like</p>

<pre><code>['TV','Sofa','Fridge','Pets','AC','Parking']
</code></pre>

<p>From here on my attempt is to run a <code>for</code> loop and check if the values are available in the row or not and subsequently put true(1) or false(0).
 I have about 50 such options, so 50 new columns. This does look like pivot but there's no aggregation. </p>

<p>However, I am not sure how to <em>convert these list values inside a row value to their respective new column as boolean</em> in pandas dataframe.</p>
","4046274","","6622587","","2019-07-13 23:55:01","2019-07-13 23:55:01","Adding new columns from row list as boolean","<python><python-3.x><pandas><pivot>","1","0","","","","CC BY-SA 4.0","1"
"57656404","1","57656698","","2019-08-26 10:45:54","","2","40","<p>there are two dataframes <code>df_one</code> and <code>df_two</code> I want to create a new data frame by with selective column from each of the dataframes </p>

<pre><code>df_one
e b c d 
1 2 3 4 
5 6 7 8 
6 2 4 8 
9 2 5 6
</code></pre>

<p>and </p>

<pre><code>df_two

e f g h
1 8 7 6 
5 6 6 4 
6 6 2 4 
9 5 3 2 
</code></pre>

<p>I want to create a new dataframe new_df</p>

<pre><code>e b g h d
1 6 7 6 4
5 2 6 4 8
6 2 2 4 8
9 2 3 2 6
</code></pre>

<p><a href=""https://i.stack.imgur.com/MnZY1.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","11977590","","9698684","","2019-08-26 10:57:15","2019-08-26 11:06:20","how to join two dataframe by picking couple of column from each if one of the column has same data","<python><python-3.x><pandas>","2","4","","","","CC BY-SA 4.0","1"
"57694918","1","","","2019-08-28 14:39:14","","-1","40","<p>I have a csv file with 1500 columns where only some of the columns will have a value like below:</p>

<p>Emp|C1|C2|C3|C4|C5|C6|C7</p>

<p>ABC||Y|||X||A</p>

<p>DEF|A||Y||B||T</p>

<p>Output should be:</p>

<p>Emp Col_Name    Col_Value</p>

<p>ABC C2  Y</p>

<p>ABC C5  X</p>

<p>ABC C7  A</p>

<p>DEF C1  A</p>

<p>DEF C3  Y</p>

<p>DEF C5  B</p>

<p>DEF C6  T</p>
","11989821","","11989821","","2019-08-28 14:46:09","2019-08-29 14:59:43","How to print column names along with not null values in python?","<python-3.x><pandas>","1","4","","","","CC BY-SA 4.0","1"
"56712799","1","","","2019-06-22 05:18:11","","0","40","<p>I have written a script that depends on importing pandas and random libraries as a first step. However, when I try to run this script in python3 on a Mac computer, </p>

<p>I think I could just get away with using pandas and without importing numpy at all, but when I do, the same problem occurs, just with pandas instead of numpy. I get the Same ModuleNotFoundError. So there's something wrong with the way that I'm trying to import the libraries.</p>

<p>I'm confused because everything works fine on multiple windows machines. I can't imagine that numpy or pandas are restricted to Windows only.</p>

<p>This is how I'm importing the libraries.</p>

<pre><code>import numpy as np
import pandas as pd
import random

</code></pre>

<p>I expect this to import numpy, pandas, and random without comment, but I actually get this error:</p>

<pre><code>Traceback (most recent call last):
    File ""pandasplay.py,"" line 1, in &lt;module&gt; 
    import numpy as np 
ModuleNotFoundError: No module named 'numpy'

</code></pre>

<p>Did something go wrong with installation of python three on the mac? I'm using Python 3.7.3.</p>
","11523818","","","","","2019-06-22 05:18:11","Windows written python script can't import pandas or numpy when run on mac ; everything works fine on windows","<python><python-3.x><pandas><numpy><incompatibility>","0","6","1","","","CC BY-SA 4.0","1"
"57667455","1","57674396","","2019-08-27 03:51:22","","0","40","<p>I am currently scraping through an XML API response. I am looking to gather a piece of information for each request and create a dictionary each time I find this piece of data. Each request can have several IDs. So one response can have 2 IDs while the next response might have 3 IDs. For example, let's say the first response has 2 IDs. I am storing this data in a list at the moment when the second request is done the additional 3 IDs are being stored under this same list as well. </p>

<pre><code>import requests
import pandas as pd
from pandas import DataFrame
from bs4 import BeautifulSoup
import datetime as datetime
import json
import time


trackingDomain = ''
domain = ''
aIDs = []
cIDs = []
url = ""https://"" + domain + """"


print(url)

df = pd.read_csv('campids.csv')
for index, row in df.iterrows():

    payload = {'api_key':'',
                'campaign_id':'0',
                'site_offer_id':row['IDs'],
                'source_affiliate_id':'0',
                'channel_id':'0',
                'account_status_id':'0',
                'media_type_id':'0',
                'start_at_row':'0',
                'row_limit':'0',
                'sort_field':'campaign_id',
                'sort_descending':'TRUE'
            }
    print('Campaign Payload', payload)
    r = requests.get(url, params=payload)
    print(r.status_code)
    soup = BeautifulSoup(r.text, 'lxml')
    success = soup.find('success').string
    for affIDs in soup.select('campaign'):
        affID = affIDs.find('source_affiliate_id').string
        aIDs.append(affID)
        dataDict = dict()
        dataDict['offers'] = []
        affDict = {'affliate_id':aIDs}
        dataDict['offers'].append(dict(affDict))
</code></pre>

<p>The result ends up being as follows:</p>

<pre><code>dictData = {'offers': [{'affliate_id': ['9','2','45','47','14','8','30','30','2','2','9','2']}]}
</code></pre>

<p>What I am looking to do is this:</p>

<pre><code>dictData = {'offers':[{'affiliate_id'['9','2','45','47','14','8','30','30','2','2']},{'affiliate_id':['9','2']}]}
</code></pre>

<p>On the first request, I obtain the following:</p>

<pre><code>IDs['9','2','45','47','14','8','30','30','2','2']
</code></pre>

<p>On the second request these IDs are returned:</p>

<pre><code>['9','2']
</code></pre>

<p>I am new to Python so please bear with me as far etiquette goes and I am missing something. I'll be happy to provide any additional information.</p>
","11315276","","4684861","","2019-08-27 04:00:38","2019-08-27 12:10:20","appending Dict to nested list per request made","<python-3.x><pandas><beautifulsoup><python-requests>","1","1","","","","CC BY-SA 4.0","1"
"57698669","1","","","2019-08-28 19:05:39","","0","40","<p>I am reading in multiple excel files (file1_.xlsx, file2_.xlsx, etc) that contain multiple sheets of data, combining everything and then printing it to a new excel file called output. Also in output are sheets containing multiple tables of summary data for each file. So, for example, the output file should have sheets: final, file1sum, file2sum, etc. And file1sum will contain tables sumtable1,sumtable2,etc, file2sum will also contain sumtable1, sumtable2, etc but for file 2. Hope that makes sense </p>

<pre><code> for f in files:
     name=f.split(""_"")[0]
     xls=pd.ExcelFile(f)
     sheets=xls.sheet_names

     for i in range(len(sheets))
         create final table (dataframe)
         create sumtable1 (dataframe)
         create sumtable2 (dataframe)
         create sumtable3 (dataframe)

     writer=pd.ExcelWriter(""output.xlsx"")
     if not os.path.exist(""output.xlsx""):
         print final table to sheet final in output
     else: 
         df=pd.read_excel(""output.xlsx"")
         merge df and final table 
         print df to output

     listOfSumTables=[sumtable1,sumtable2,sumtable3]
     row=0
     for dataframe in listOfSumTables:
         if not dataframe.empty:
              writeDF(dataframe,name,row,writer)
              row=row+len(dataframe)+2
     writer.save()
</code></pre>

<p>where writeDF is the following function</p>

<pre><code>def writeDF(df,name,row,writer):
     if name=='file1':
       df.to_excel(writer,sheet_name='file1sum', index=False, 
       startrow=row, startcol=0)
     elif name=='file2':
       df.to_excel(writer,sheet_name='file2sum', index=False, 
       startrow=row, startcol=0)
</code></pre>

<p>The problem is in the output file I am getting sheets final and file2sum only when I should be getting final, file1sum and file2sum. final and file2sum look how they are supposed to, it is just not making all the sheets for some reason. It is creating the tables for each file correctly, triggering the writeDF function correctly. I think it has to do with where I am opening and saving the writer but I tried moving it around and it doesn't make all three sheets.</p>
","11956484","","","","","2019-08-28 19:05:39","Code not creating new excel sheet in output excel file","<python><excel><python-3.x><pandas>","0","3","","","","CC BY-SA 4.0","1"
"57663781","1","","","2019-08-26 19:23:39","","0","40","<p>I have two dataframes whose values are correlated. The first one, let's call DF, has some values plues each recording number, like this:</p>

<pre><code>                   ts  pou     X     Y  value   recording_number
0  1/1/12 12:15:00 AM  NaN     1     3  -0.37       4089
1  1/1/12 12:30:00 AM  NaN     1     3  -0.42       4089
2  1/1/12 12:45:00 AM  NaN     1     3  -0.32       4089
3   1/1/12 1:00:00 AM  NaN     1     3  -0.36       4089
4   1/1/12 1:15:00 AM  NaN     1     3  -0.33       4089
5   1/1/12 1:30:00 AM  NaN     1     3  -0.40       4089
6   1/1/12 1:45:00 AM  NaN     1     3  -0.38       4089
7   1/1/12 2:00:00 AM  NaN     1     3  -0.43       4089
8   1/1/12 2:15:00 AM  NaN     1     3  -0.35       4089
9   1/1/12 2:30:00 AM  NaN     1     3  -0.38       4089
</code></pre>

<p>DF is pretty long so that the recording_number is gonna change many times.
The second dataframe, let's call it 'Table', has the recording numbers, as well, plus an id_number.  </p>

<pre><code> id_number  recording_number
0         167      9206
1         167      9824
2         167     10890
3         167     10895
4         167     10942
5         167     10944
6         167     11244
7         167     11249
8         167     11567
9         167     11568
</code></pre>

<p>Many recording numbers belong to the same id_number. </p>

<p>I would like to groupby all the data in DF belonging to the same id_number in Table. i.e. to groupby all the data with a specific recording_number in DF which have the same id_number in Table.  </p>

<p>Thank you very much!</p>
","11980301","","11980301","","2019-08-26 20:15:19","2019-08-26 20:19:23","How to groupby the values in one dataframe based on a condition in another dataframe?","<python-3.x><pandas><dataframe><group-by>","1","2","","","","CC BY-SA 4.0","1"
"56740761","1","","","2019-06-24 16:38:36","","0","40","<p>very confused here and just not knowledgable enough to get to the bottom of the problem. I am charting data on the value of construction equipment in jupyter notebook, and everything is going great. Except, on one particular data set I keep getting an error. After googling and trying to find a solution myself I must come to stackoverflow. It's weird, because I am not doing anything different with this data set. I even tried recreating it. This is all I am doing:</p>

<pre><code>dataretry.plot(kind=""scatter"", x=""Price"", y=""Hours"", alpha=0.3)
</code></pre>

<p>I expected a scatterplot to show. However, I get this massive error message:</p>

<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   3077             try:
-&gt; 3078                 return self._engine.get_loc(key)
   3079             except KeyError:

pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'Price'

During handling of the above exception, another exception occurred:

KeyError                                  Traceback (most recent call last)
&lt;ipython-input-35-81e70cca10a1&gt; in &lt;module&gt;
----&gt; 1 dataretry.plot(kind=""scatter"", x=""Price"", y=""Hours"", alpha=0.3)

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\plotting\_core.py in __call__(self, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)
   2939                           fontsize=fontsize, colormap=colormap, table=table,
   2940                           yerr=yerr, xerr=xerr, secondary_y=secondary_y,
-&gt; 2941                           sort_columns=sort_columns, **kwds)
   2942     __call__.__doc__ = plot_frame.__doc__
   2943 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\plotting\_core.py in plot_frame(data, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)
   1975                  yerr=yerr, xerr=xerr,
   1976                  secondary_y=secondary_y, sort_columns=sort_columns,
-&gt; 1977                  **kwds)
   1978 
   1979 

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\plotting\_core.py in _plot(data, x, y, subplots, ax, kind, **kwds)
   1741         if isinstance(data, ABCDataFrame):
   1742             plot_obj = klass(data, x=x, y=y, subplots=subplots, ax=ax,
-&gt; 1743                              kind=kind, **kwds)
   1744         else:
   1745             raise ValueError(""plot kind %r can only be used for data frames""

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\plotting\_core.py in __init__(self, data, x, y, s, c, **kwargs)
    843             # the handling of this argument later
    844             s = 20
--&gt; 845         super(ScatterPlot, self).__init__(data, x, y, s=s, **kwargs)
    846         if is_integer(c) and not self.data.columns.holds_integer():
    847             c = self.data.columns[c]

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\plotting\_core.py in __init__(self, data, x, y, **kwargs)
    817         if is_integer(y) and not self.data.columns.holds_integer():
    818             y = self.data.columns[y]
--&gt; 819         if len(self.data[x]._get_numeric_data()) == 0:
    820             raise ValueError(self._kind + ' requires x column to be numeric')
    821         if len(self.data[y]._get_numeric_data()) == 0:

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in __getitem__(self, key)
   2686             return self._getitem_multilevel(key)
   2687         else:
-&gt; 2688             return self._getitem_column(key)
   2689 
   2690     def _getitem_column(self, key):

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\frame.py in _getitem_column(self, key)
   2693         # get column
   2694         if self.columns.is_unique:
-&gt; 2695             return self._get_item_cache(key)
   2696 
   2697         # duplicate columns &amp; possible reduce dimensionality

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\generic.py in _get_item_cache(self, item)
   2487         res = cache.get(item)
   2488         if res is None:
-&gt; 2489             values = self._data.get(item)
   2490             res = self._box_item_values(item, values)
   2491             cache[item] = res

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\internals.py in get(self, item, fastpath)
   4113 
   4114             if not isna(item):
-&gt; 4115                 loc = self.items.get_loc(item)
   4116             else:
   4117                 indexer = np.arange(len(self.items))[isna(self.items)]

~\AppData\Local\Continuum\anaconda3\lib\site-packages\pandas\core\indexes\base.py in get_loc(self, key, method, tolerance)
   3078                 return self._engine.get_loc(key)
   3079             except KeyError:
-&gt; 3080                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   3081 
   3082         indexer = self.get_indexer([key], method=method, tolerance=tolerance)

pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas\_libs\index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas\_libs\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'Price'
</code></pre>

<p>Any help is appreciated!</p>
","11693440","","","","","2019-06-24 16:38:36","Weird error trying to chart data in matplotlib","<python-3.x><pandas><matplotlib><jupyter-notebook>","0","3","","","","CC BY-SA 4.0","1"
"49012051","1","","","2018-02-27 15:15:30","","-1","40","<p>I have a given data shown below</p>

<p><a href=""https://i.stack.imgur.com/3c5GL.png"" rel=""nofollow noreferrer"">Given data frame (input)</a></p>

<p>I want to change this given data to the following data frame </p>

<p><a href=""https://i.stack.imgur.com/caaS0.png"" rel=""nofollow noreferrer"">modifide dataframe (output)</a></p>

<p>I am using panda from Python library for my work. I am new to panda and python can anyone please help me how to solve this problem using any panda function like pivot <code>table = pd.pivot_table(table, ......)</code>or any other python libraries.   </p>

<p>Edit : </p>

<p>Sample Data </p>

<pre><code>df=pd.DataFrame({'Acc':[1,2,4,2,1,3],'Event':list('ABCACA'),'exit':[0,1,1,1,0,0]})
</code></pre>

<p>EDIT:</p>

<p>I am` sorry @jpp here is an example for my question:</p>

<p>Let's say this is given input like this</p>

<pre><code>`DataFrame({'Acc':[1,2,4,2,1,1,3],'Event':list('ABCACBA'),'exit':[1,1,1,1,0,0,0]})`
</code></pre>

<p>I am expecting this kind of output</p>

<pre><code># Acc  A  B  C  exit
# 1    1  1  1  1   
# 2    1  1  0  1  
# 4    0  0  1  1  
# 3    1  0  0  0 
</code></pre>
","9054122","","9054122","","2018-02-27 21:57:36","2018-02-27 21:57:36","How to transform a data frame with duplicated key for different value to a single key with values as column","<python><python-3.x><pandas><machine-learning><anaconda>","2","1","","","","CC BY-SA 3.0","1"
"40569393","1","","","2016-11-13 00:54:56","","0","40","<p>For example:</p>

<pre><code>        a  b  c
coffee  1  2  3
sprite  2  3  1
coffee  1  3  2
coke    2  4  5
sprite  2  3  1
coke    3  4  5
</code></pre>

<p>if I wanna to use <code>groupby</code> method to combine these rows (or columns) by using their mean, how can I do that?</p>

<pre><code>        a  b  c
coffee  x  x  x
sprite  x  x  x
coke    x  x  x
</code></pre>

<p>x is the mean</p>
","6928513","","624829","","2016-11-13 01:32:55","2016-11-13 01:41:43","How to use groupby method to combine rows or columns by using their mean?","<python><python-3.x><pandas><dataframe><group-by>","1","0","","","","CC BY-SA 3.0","1"
"57606536","1","57606607","","2019-08-22 09:47:03","","1","40","<p>I have one dataframe, I need to filter the dates on the basis of start and end date of the other dataframe</p>

<p>example set is given below. What is the best way in pandas to achieve that?</p>

<p>Considering sample dataframes as below, I have included the expected result set</p>

<pre><code>df1 

ID all_date   clicks
1  2019-08-21   5
1  2019-08-22   4
2  2019-07-18   5
2  2019-07-21   5
2  2019-07-23   6

df2

ID start_date  end_date
1  2019-08-21 2019-08-21
2  2019-07-18 2019-08-21
</code></pre>

<p>expected output:</p>

<pre><code>df1

ID all_date   clicks
1  2019-08-21   5
2  2019-07-18   5
2  2019-07-21   5
</code></pre>
","11753555","","","","","2019-08-22 10:06:10","How to filter one pandas dataframe dates on the basis of other dataframe","<python><python-3.x><pandas><pandasql>","1","0","","","","CC BY-SA 4.0","1"
"56927851","1","56961775","","2019-07-08 02:52:55","","0","40","<p>I have a Matplotlib stacked bar plot - </p>

<pre><code>df.groupby(['date', 'flavor'])['minutes'].sum().unstack().plot(kind='bar', stacked=True)
</code></pre>

<p>I would like to convert this to an interactive plot using <code>interact</code> from <code>ipywidgets</code>. But, when I try this - </p>

<pre><code>@interact
df.groupby(['date', 'flavor'])['minutes'].sum().unstack().iplot(kind='bar', stacked=True)
</code></pre>

<p>It gives me an <code>Exception: Invalid keyword : 'stacked'</code>. Can someone tell me the right syntax for this kind of interactive chart with <code>ipywidgets</code>?</p>

<p>Thanks! </p>
","2552610","","","","","2019-07-09 23:52:47","Converting matplotlib chart to interactive chart in Jupyter","<python-3.x><pandas><matplotlib><ipywidgets>","1","1","","","","CC BY-SA 4.0","1"
"50419870","1","","","2018-05-18 22:14:17","","0","40","<p>When I run </p>

<pre><code>the_id in attrs['ID']
2941264 in pd.Series([2941264,2941273,2941273,2941282]) # or this
</code></pre>

<p>I get <code>False</code>. But when I run</p>

<pre><code>any(attrs['ID'].isin([the_id]))
</code></pre>

<p>I get <code>True</code>.</p>

<hr>

<p><strong>How does a Pandas Series implement <code>in</code>?</strong></p>

<p>That is, <strong>why</strong> does Series have this unintuitive behaviour?</p>

<p>And how could different results be produced from it?</p>
","2741091","","2741091","","2018-05-18 22:34:17","2018-05-18 22:34:17","Pandas: Why is 'in' broken Series?","<python-3.x><pandas>","0","9","","","","CC BY-SA 4.0","1"
"57410493","1","","","2019-08-08 10:27:26","","0","39","<p>In <a href=""https://viblo.asia/p/predict-independent-values-with-text-data-using-linear-regression-aWj5314eZ6m"" rel=""nofollow noreferrer"">this article on predicting values with linear regression</a> there's a cleaning step</p>

<pre><code># For beginning, transform train['FullDescription'] to lowercase using text.lower()
train['FullDescription'].str.lower()

# Then replace everything except the letters and numbers in the spaces.
# it will facilitate the further division of the text into words.
train['FullDescription'].replace('[^a-zA-Z0-9]', ' ', regex = True)
</code></pre>

<p>This isn't actually assigning the changes to the dataframe, is it? But if I try something like this...</p>

<pre><code>train['FullDescription'] = train['FullDescription'].str.lower()
train['FullDescription'] = train['FullDescription'].replace('[^a-zA-Z0-9]', ' ', regex = True)
</code></pre>

<p>Then I get a warning...</p>

<blockquote>
  <p>SettingWithCopyWarning:  A value is trying to be set on a copy of a
  slice from a DataFrame</p>
  
  <p>See the caveats in the documentation:
  <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy"" rel=""nofollow noreferrer"">http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy</a></p>
</blockquote>

<p>What's the right way to apply these transformations? Are they in fact already being applied? A <code>print(train['FullDescription'])</code> seems to show me they're not.</p>

<p><strong>EDIT</strong>: @EdChum and @jezrael are very much onto something about missing code. When I'm actually trying to run this, my data needs to be split into test and train sets.</p>

<pre><code>from sklearn.model_selection import train_test_split
all_data = pandas.read_csv('salary.csv')
train, test = train_test_split(all_data, test_size=0.1)
</code></pre>

<p>That's what seems to be causing this error. If I make the next line</p>

<pre><code>train = train.copy()
test = test.copy()
</code></pre>

<p>then everything is happy.</p>

<p>You may be wondering if I shouldn't then just apply this step to <code>all_data</code>, which works, but then lower down in the code <code>train['Body'].fillna('nan', inplace=True)</code> still causes an error. So it seems indeed the problem is with <code>train_test_split</code> not creating copies.</p>
","1487413","","1487413","","2019-08-08 11:06:56","2019-08-08 11:06:56","Is this example data cleaning code updating the pandas dataframe?","<python-3.x><pandas><data-cleaning>","1","11","","2019-08-08 10:35:25","","CC BY-SA 4.0","1"
"50053611","1","50068306","","2018-04-27 00:26:15","","0","39","<p>If using the Pandas built in data visualisation capabilities to plot charts, is it possible to explicitly set the colour parameters for those charts?</p>

<p>If plotting charts via the object oriented method, then it seems quite clear how to proceed, but how would I set the various colours explicitly via the method in question?</p>

<p>For example, is it possible to set the background colour of the axes on the following histrogram to another colour?  If relevant, I think on this particular example, the axes background colour is actually transparent!</p>

<p><a href=""https://i.stack.imgur.com/UQfhZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UQfhZ.png"" alt=""enter image description here""></a></p>

<p>Note: I do realise that it is possible to use matplotlib style sheets, but that is not the specific answer that I am looking for.</p>

<p>Thank you!</p>
","9230013","","","","","2018-04-27 18:28:49","Is it possible to explicitly set chart colour parameters when plotting charts via the pandas built in data visualisation method?","<python-3.x><pandas><matplotlib><visualization>","1","6","0","","","CC BY-SA 3.0","1"
"57512334","1","","","2019-08-15 15:43:04","","0","39","<p>When mapping a dictionary of tuples to a DataFrame the mapping results in NaN value</p>

<pre><code>dfTemp = pd.DataFrame(columns=['Lokaal','LocalX','LocalY', 'test'])
dfTemp['LocalX'] = pd.Series([22, 32])
dfTemp['LocalY'] = pd.Series([40, 50])
dfTemp['Lokaal'] = pd.Series([lab1, lab2])


dict = {""lab1"":(337, 50, 230, 275),
           ""lab2"":(337, 315, 230, 320)
          }

dfTemp[""test""] = dfTemp[""Lokaal""].map(dict)




expected result:

LocalX LocalY Lokaal test
22     40     lab1   (337,50,230,275)
32     50     lab2   (337, 315, 230, 320)

actual result:

LocalX LocalY Lokaal test
22     40     lab1   NaN
32     50     lab2   NaN
</code></pre>

<p>No error message is given.</p>
","9133728","","","","","2019-08-15 15:43:04","Mapping dictionary of tuples to DataFrame","<python><python-3.x><pandas><dictionary><tuples>","0","5","","","","CC BY-SA 4.0","1"
"57692491","1","57692653","","2019-08-28 12:30:35","","0","39","<p>I have a column of strings that contains ID numbers but some of the rows contain different names which I don't need</p>

<p>Ex: </p>

<pre><code>12-1
name
12-2
name
12-3
</code></pre>

<p>sometimes the ID numbers are just numbers (contain no dashes)</p>

<pre><code>12
name
13
name
14
</code></pre>

<p>I'd like the output to be </p>

<pre><code>12-1
12-2
12-3
</code></pre>

<p>or </p>

<pre><code>12
13
14
</code></pre>

<p>is there a more elegant way of doing this besides removing the dashes when they are present, converting to numeric, coerce errors and then removing rows with nan in them?</p>
","11956484","","","","","2019-08-28 12:57:19","Remove names from a column of numbers (which are strings)","<python><python-3.x><pandas>","2","2","","","","CC BY-SA 4.0","1"
"49866551","1","49866660","","2018-04-16 21:31:20","","-1","39","<p>I have a reference dataframe:</p>

<p>ex:</p>

<pre><code>  time latitude longtitude pm2.5
  0 .  0        0          0
  1 .  0        5          1

  ......
</code></pre>

<p>And I have a query with</p>

<p>ex:</p>

<pre><code>  time latitude longtitude
  0 .  1        3
  1 .  0        5

  .......
</code></pre>

<p>I want to get the pm2.5 which matches the rows in query.</p>

<p>I have used the iteration of rows but it seems very slow.</p>

<pre><code>predications_phy = []
for index, row in X_test.iterrows():    
    Y = phyDf[(phyDf[""time""] == row[""time""]) &amp; (phyDf[""latitude""] == row[""latitude""]) &amp; (phyDf[""longtitude""] == row[""longtitude""])]
    predications_phy.append(Y)
</code></pre>

<p>What is the efficient and correct way to get the rows?</p>
","876748","","9209546","","2018-04-16 21:41:49","2018-04-16 21:41:49","Pandas get rows by its values from dataframes","<python><python-3.x><pandas><scikit-learn>","1","0","","","","CC BY-SA 3.0","1"
"57609236","1","57609277","","2019-08-22 12:20:48","","1","39","<p>I have a pivot table which has incorrect column order and i need the columns to be in order as i wish</p>

<p>The below code is for pivot table:</p>

<p>data_frame1 = pd.pivot_table(data_frame, index=['PC', 'Geo', 'Comp'], values=['Bill', 'Bill2'], columns=['Month'], fill_value=0)</p>

<p><strong>Code output</strong></p>

<pre><code>               Bill1              Bill2
 Month       Jan    Feb        Jan    Feb     
PC Geo Comp
A  Ind  OP    1     1.28        1    1.28
B  US   OS    1     1.28        1    1.28
C  Can  OV    1     1.28        1    1.28
</code></pre>

<p><strong>Expected output</strong></p>

<pre><code>                Bill2              Bill1
 Month       Jan    Feb        Jan    Feb     
PC Geo Comp
A  Ind  OP    1     1.28        1    1.28
B  US   OS    1     1.28        1    1.28
C  Can  OV    1     1.28        1    1.28
</code></pre>
","11906428","","11906428","","2019-08-22 12:23:22","2019-08-22 12:23:22","ordering the multiindex level manually","<python-3.x><pandas><pivot-table>","1","0","","","","CC BY-SA 4.0","1"
"40779184","1","40779224","","2016-11-24 06:13:43","","1","39","<p>i have a table in pandasas df:</p>

<pre><code>p_id_x    p_id_y    count
  a         b         2
  b         c         4
  a         c         8
  d         a         1 
  x         a         6
  m         b         3
  c         z         7
</code></pre>

<p>i wam tring to write a function</p>

<pre><code>def function_combination(p_id):
    df[['p_id_x', 'p_id_y']] = df[['p_id_x', 'p_id_y']].apply(sorted, axis=1)
    df.groupby(['p_id_x', 'p_id_y'], as_index=False)['count'].sum()
</code></pre>

<p>(the function is not completed and has errors.)
and i got the result by seperately running the code inside the function.</p>

<pre><code>df['p_id_x','p_id_y']

p_id_x  p_id_y
  a         b
  b         c
  a         c
  a         d
  a         x
  b         m
  c         z
</code></pre>

<p>but what i want my output to look like is:</p>

<pre><code>p_id_x    p_id_y
  a         b
  a         c
  a         d
  a         x         
  b         c
  b         m
  c         z
</code></pre>

<p>i'e all the combinations for a first, followed by b, and followed by c.</p>

<p>this is only a part of my rows. i've 20+ such rows.
is there a way to do this, so that i can have both my codes inside the function? </p>
","6803114","","","","","2016-11-24 06:23:49","arrange values in a order in a pandas df","<python><python-2.7><python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"56953724","1","","","2019-07-09 13:28:20","","0","39","<p>We have a large DataFrame with 30 ""Process-Starts"" and ""Process-Ends"". We want to calculate the ""Process Time"" (Process-End minus Process-Start). Picture <img src=""https://i.stack.imgur.com/KYGsF.jpg"" alt=""img""></p>

<p>Is there a function to calculate all these Process Times and get them in a new DataFrame?</p>

<p>We tried it with this code: Picture <img src=""https://i.stack.imgur.com/lmpiO.jpg"" alt=""img""></p>

<p>Unfortunately it doesn't work. </p>

<p>Is there anyone who can help us?</p>

<p><a href=""https://i.stack.imgur.com/KYGsF.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>

<p><a href=""https://i.stack.imgur.com/lmpiO.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>
","11739995","","4238408","","2019-07-09 13:31:00","2019-07-09 13:31:00","How can I calcualte a difference between 2 columns?","<python-3.x><pandas><dataframe><difference>","0","3","","","","CC BY-SA 4.0","1"
"55480504","1","55480880","","2019-04-02 17:29:31","","0","39","<p>I am looking for a an efficient and elegant way in Pandas to remove ""duplicate"" rows in a DataFrame that have exactly the same value set but in different columns.</p>

<p>I am ideally looking for a vectorized way to do this as I can already identify very inefficient ways using the Pandas <code>pandas.DataFrame.iterrows()</code> method.</p>

<p>Say my DataFrame is:</p>

<pre><code>source|target|
----------------
| 1   |  2   |
| 2   |  1   |
| 4   |  3   |
| 2   |  7   |
| 3   |  4   |
</code></pre>

<p>I want it to become:</p>

<pre><code>source|target|
----------------
| 1   |  2   |
| 4   |  3   |
| 2   |  7   |
</code></pre>
","6744260","","2535611","","2019-04-02 17:30:27","2019-04-03 03:17:31","Efficient way in Pandas for removing columns with duplicate values in different columns","<python><python-3.x><pandas>","1","4","","","","CC BY-SA 4.0","1"
"57261893","1","57261941","","2019-07-29 21:59:34","","1","39","<p><strong>Background</strong></p>

<p>I have the following sample <code>df</code> which is an alternation of <a href=""https://stackoverflow.com/questions/57031837/alter-number-string-in-pandas-column"">Alter number string in pandas column</a></p>

<pre><code>import pandas as pd
df = pd.DataFrame({'Text' : ['Jon J Smith  Record #:  0000004 is this ', 
                                   'Record #:  0000003 Mary Lisa Hider found here', 
                                   'Jane A Doe is also here Record #:  0000002',
                                'Record #:  0000001'], 

                      'P_ID': [1,2,3,4],
                      'N_ID' : ['A1', 'A2', 'A3', 'A4']

                     })

#rearrange columns
df = df[['Text','N_ID', 'P_ID']]
df

                                    Text             N_ID   P_ID
0   Jon J Smith Record #: 0000004 is this       A1  1
1   Record #: 0000003 Mary Lisa Hider fou...    A2  2
2   Jane A Doe is also here Record #: 000...    A3  3
3   Record #: 0000001                           A4  4
</code></pre>

<p><strong>Goal</strong></p>

<p>1) replace number after <code>Record #:</code> with <code>**BLOCK**</code></p>

<p><code>Jon J Smith  Record #:  0000004 is this</code><br>
<code>Jon J Smith  Record #:  **BLOCK** is this</code> </p>

<p>2) create new column </p>

<p><strong>Desired Output</strong></p>

<pre><code>    Text    N_ID    P_ID    New_Text              
0                          Jon J Smith Record #: **BLOCK** is this      
1                          Record #: **BLOCK**  Mary Lisa Hider fou...  
2                          Jane A Doe is also here Record #: **BLOCK**  
3                          Record #: **BLOCK**                          
</code></pre>

<p><strong>Tried</strong></p>

<p>I have tried the following but this is not quite right</p>

<pre><code>df['New_Text']= df['Text'].replace(r'(?i)record\s+#: \d+', r""Date of Birth: **BLOCK**"", regex=True)
</code></pre>

<p><strong>Question</strong></p>

<p>How do I alter my code to get my desired output?</p>
","6598999","","6598999","","2019-10-21 22:47:55","2019-10-21 22:47:55","modification of alter number string pandas","<regex><python-3.x><string><pandas><replace>","1","0","","","","CC BY-SA 4.0","1"
"57331172","1","","","2019-08-02 17:35:45","","0","39","<p>I have two column in my dataframe. I want to combine/overwrite the columns.</p>

<p>Two Columns:</p>

<pre><code>Column1 Column2
0   NaT NaT
1   2019-07-17 11:33:22 NaT
2   NaT NaT
3   NaT 2018-05-24 10:00:48
</code></pre>

<p>Desired Result:</p>

<pre><code>NewColumn
0   NaT
1   2019-07-17 11:33:22
2   NaT
3   2018-05-24 10:00:48
</code></pre>

<p>NaT type:</p>

<pre><code>type(df['Column1'][0])
</code></pre>

<p>Output:</p>

<pre><code>pandas._libs.tslibs.nattype.NaTType
</code></pre>

<p>Date type:</p>

<pre><code>type(df['Column1'][1])
</code></pre>

<p>Output:</p>

<pre><code>pandas._libs.tslibs.timestamps.Timestamp
</code></pre>
","4808519","","","","","2019-08-02 17:50:33","How to combine/overwrite 2 columns in Pandas DataFrame","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"56725494","1","","","2019-06-23 15:49:32","","0","38","<p>My code:  </p>

<pre><code>from pandas import read_csv

ds = read_csv('hpc.txt', sep=';', header=0, low_memory=False, infer_datetime_format=True, parse_dates={'datetime':[0,1]}, index_col=['datetime'])
print(ds.shape)
print(ds.head())
</code></pre>

<p>Snapshot of <em>hpc.txt</em>:</p>

<p><a href=""https://i.stack.imgur.com/UHKy9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UHKy9.png"" alt=""enter image description here""></a></p>

<p>the code is meant to output the shape of the file but it isn't.</p>
","11519635","","8306666","","2019-06-23 18:31:36","2019-06-23 18:31:36","I am trying to load the txt file but it's giving following error:Out of Memory","<python><python-3.x><pandas>","0","8","","","","CC BY-SA 4.0","1"
"57131853","1","","","2019-07-21 09:20:37","","0","38","<p>As an exercise, I was trying to analyse and visualize the following data.
<a href=""http://www.calvin.edu/~stob/data/Berkeley.csv"" rel=""nofollow noreferrer"">http://www.calvin.edu/~stob/data/Berkeley.csv</a>
I managed to achieve what I had in mind but my code seems very repetitive. 
Is there any way to achieve the following in fewer lines of code? </p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_csv(""""ucb.csv"")

df_male = df[df['Gender'] == 'Male']
df_female = df[df['Gender'] == 'Female']

dmd = pd.pivot_table(df_male, values=""Freq"", index = 'Dept', 
columns='Admit')
dfd = pd.pivot_table(df_female, values=""Freq"", index = 'Dept', 
columns='Admit')
ddm = pd.pivot_table(df_male, values=""Freq"", index = 'Admit', 
columns='Dept')
ddf = pd.pivot_table(df_female, values=""Freq"", index = 'Admit', 
columns='Dept')

ma, mb, mc, md, me, mf = list(ddm['A']), list(ddm['B']), list(ddm['C']), 
list(ddm['D']), list(ddm['E']), list(ddm['F'])
fa,fb,fc,fd,fe,ff=list(ddf['A']),list(ddf['B']),list(ddf['C']),
list(ddf['D']),list(ddf['E']),list(ddf['F'])

a,b,c,d = dmd['Admitted'].sum(), dmd['Rejected'].sum(), 
dfd['Admitted'].sum(), dfd['Rejected'].sum()

males = ""Accepted Vs Rejected ratio for "" + str(a+b) + "" Men""
females = ""Accepted Vs Rejected ratio for "" + str(c+d) + "" Women""
male_ratio = [a,b]
female_ratio = [c,d]
labels = ""Accepted"", ""Rejected""

fig = plt.figure(figsize=(18,18))
axm = fig.add_subplot(3,6,1)  
axm.title.set_text(males)
axf = fig.add_subplot(3,6,4)
axf.title.set_text(females)
axma = fig.add_subplot(3,6,7)
axma.title.set_text(""Dept A Men"")
axmb = fig.add_subplot(3,6,8)
axmb.title.set_text(""Dept B Men"")
axmc = fig.add_subplot(3,6,9)
axmc.title.set_text(""Dept C Men"")
axmd = fig.add_subplot(3,6,10)
axmd.title.set_text(""Dept D Men"")
axme = fig.add_subplot(3,6,11)
axme.title.set_text(""Dept E Men"")
axmf = fig.add_subplot(3,6,12)
axmf.title.set_text(""Dept F Men"")
axfa = fig.add_subplot(3,6,13)
axfa.title.set_text(""Dept A Women"")
axfb = fig.add_subplot(3,6,14)
axfb.title.set_text(""Dept B Women"")
axfc = fig.add_subplot(3,6,15)
axfc.title.set_text(""Dept C Women"")
axfd = fig.add_subplot(3,6,16)
axfd.title.set_text(""Dept D Women"")
axfe = fig.add_subplot(3,6,17)
axfe.title.set_text(""Dept E Women"")
axff = fig.add_subplot(3,6,18)
axff.title.set_text(""Dept F Women"")
axm.pie(male_ratio, labels=labels, autopct='%1.1f%%')
axf.pie(female_ratio, labels=labels, autopct='%1.1f%%')
axma.pie(ma, labels=labels, autopct='%1.1f%%')
axmb.pie(mb, labels=labels, autopct='%1.1f%%')
axmc.pie(mc, labels=labels, autopct='%1.1f%%')
axmd.pie(md, labels=labels, autopct='%1.1f%%')
axme.pie(me, labels=labels, autopct='%1.1f%%')
axmf.pie(mf, labels=labels, autopct='%1.1f%%')
axfa.pie(fa, labels=labels, autopct='%1.1f%%')
axfb.pie(fb, labels=labels, autopct='%1.1f%%')
axfc.pie(fc, labels=labels, autopct='%1.1f%%')
axfd.pie(fd, labels=labels, autopct='%1.1f%%')
axfe.pie(fe, labels=labels, autopct='%1.1f%%')
axff.pie(ff, labels=labels, autopct='%1.1f%%')
</code></pre>

<p>Here is the output that I got: <a href=""https://imgur.com/pwAUhjR"" rel=""nofollow noreferrer"">https://imgur.com/pwAUhjR</a></p>
","11752759","","","","","2019-07-21 11:49:20","How to use a loop while defining subplots in matplotlib?","<python-3.x><pandas><matplotlib>","1","0","","","","CC BY-SA 4.0","1"
"57129086","1","","","2019-07-20 22:34:19","","1","38","<p>Working with search console api,
made it through the basics.</p>

<p>Now i'm stuck on splitting and arranging the data:</p>

<p>When trying to split, i'm getting a NaN, nothing i try works.</p>

<pre><code>46    ((174.0, 3753.0, 0.04636290967226219, 7.816147...
47    ((93.0, 2155.0, 0.0431554524361949, 6.59025522...
48    ((176.0, 4657.0, 0.037792570324243074, 6.90251...
49    ((20.0, 1102.0, 0.018148820326678767, 7.435571...
50    ((31.0, 1133.0, 0.02736098852603707, 8.0935569...
Name: test, dtype: object
</code></pre>

<p>When trying to manipulate the data like this (and similar interactions):</p>

<pre><code>data=source['test'].tolist() 
data
</code></pre>

<p>Its clear that the data is not really available... </p>

<pre><code>[&lt;searchconsole.query.Report(rows=1)&gt;,
 &lt;searchconsole.query.Report(rows=1)&gt;,
 &lt;searchconsole.query.Report(rows=1)&gt;,
 &lt;searchconsole.query.Report(rows=1)&gt;,
 &lt;searchconsole.query.Report(rows=1)&gt;]
</code></pre>

<p>Anyone have an idea how can i interact with my data ?</p>

<p>Thanks.</p>

<p>for reference, this is the code and the program i work with:</p>

<pre><code>account = searchconsole.authenticate(client_config='client_secrets.json', credentials='credentials.json')
webproperty = account['https://www.example.com/']

def APIsc(date,keyword):
    results=webproperty.query.range(date, days=-30).filter('query', keyword, 'contains').get()
    return results

source['test']=source.apply(lambda x: APIsc(x.date, x.keyword), axis=1)                                          

source
</code></pre>

<p>made by: <a href=""https://github.com/joshcarty/google-searchconsole"" rel=""nofollow noreferrer"">https://github.com/joshcarty/google-searchconsole</a></p>
","11557499","","","","","2019-07-20 22:34:19","Why am i getting <searchconsole.query.Report(rows=1)> instead of numbers/strs","<python-3.x><pandas><dataframe><google-search-console><google-api-console>","0","1","","","","CC BY-SA 4.0","1"
"58323827","1","58323874","","2019-10-10 13:15:17","","1","38","<p>I have a question.</p>

<p>I have a table like this</p>

<pre><code>TAC | Latitude | Longitude
1 | 50.4 | -1.5 
</code></pre>

<p>In Pandas, I wanted to say:</p>

<p>For each TAC, give me a zipped list of latitude and longitude (each TAC can have many rows).</p>

<p>I've tried things like the below, but I am doing something wrong! Can you help?</p>

<pre><code>df1['coordinates'] = list(zip(df1.Lat, df1.Long))
new_df = df1.iloc[ : , : ].groupby('TAC').agg(df1['coordinates'])
</code></pre>

<p>For reference, DF1 is created as below</p>

<pre><code>df = pd.read_csv('tacs.csv')
df1 = df[['magnet.tac','magnet.latitude', 'magnet.longitude']]
df1.columns = ['TAC','Lat','Long']
</code></pre>
","9629771","","9209546","","2019-10-10 13:34:29","2019-10-10 15:39:04","For each loop on Pandas, each category","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"56888824","1","56888886","","2019-07-04 13:16:51","","2","38","<p>Topic_Details</p>

<pre><code>                   Topic Source_Code      Anchor            Sub_Topic_Dataset            Dataset_Id
42  Macroeconomic Accounting Systems      GESAMT              Financial Accounts ESA 1995    DBB_GESAMTFAE12019
43  Macroeconomic Accounting Systems      GESAMT  GESAMTFINZ  Financial Accounts ESA 2010  DBB_GESAMTFINANZ2019
44  Macroeconomic Accounting Systems      GESAMT  GESAMTVOLK            National Accounts    DBB_GESAMTVOLK2019
</code></pre>

<p>Dataset_List:</p>

<pre><code>       anchor                     text_eng            Dataset_Id
0  GESAMTVOLK            National             DBB_GESAMTVOLK2019
0  GESAMTFINZ  Financial accounts           DBB_GESAMTFINANZ2019
0              Financial accounts ESA 1995                   NaN
</code></pre>

<p>Map Script:</p>

<pre><code>MapDF = dict(zip(Topic_Details['Sub_Topic_Dataset'].str.upper(), Topic_Details['Dataset_Id']))
Dataset_List['Dataset_Id'] = Dataset_List['text_eng'].apply(str.upper).map(MapDF)
</code></pre>

<p>I just want to run below script for the Dataset Dataset_List where Dataset_Id == NaN, not for other rows.</p>

<pre><code>Dataset_List['Dataset_Id'] = Dataset_List['text_eng'].apply(str.upper).map(MapDF)
</code></pre>
","2717063","","","","","2019-07-04 13:22:27","Apply Map for NaN rows in DataFrame, Python 3.6","<python><python-3.x><pandas><dataframe><map-function>","2","0","","","","CC BY-SA 4.0","1"
"56635186","1","56635272","","2019-06-17 16:13:33","","1","38","<p>I have the following <code>df</code>,</p>

<pre><code>doc_date    date_string
2019-06-03  WW0306
2019-06-07  EH0706
</code></pre>

<p><code>doc_date</code> is of <code>datetime64</code> with <code>year-month-day</code> format; <code>date_string</code> is of string <code>dtype</code> with <code>day/month</code> or <code>month/day</code> format if non-digit characters removed;</p>

<pre><code>df['date_string'].str.replace(r'\D+', '')
</code></pre>

<p>How to convert <code>date_string</code> into <code>datetime64</code>, and set a flag <code>within_180</code> to <code>true</code> if <code>date_string</code> is within +/- 180 days of <code>doc_date</code> not considering that it does not have the year and any date format that it converted to;</p>

<pre><code> df['within_180'] = df.apply(lambda x: x.between(x.doc_date -
                                          Timedelta(180, unit='d'),
                                          x.doc_date +
                                          Timedelta(180, unit='d')))
</code></pre>

<p>the result should looks like,</p>

<pre><code>doc_date    date_string    within_180
2019-06-03  WW0306         true
2019-06-07  EH0706         true
</code></pre>
","766708","","","","","2019-06-17 16:41:04","pandas how to calculate delta only given month and day","<python><python-3.x><pandas><dataframe><datetime>","1","0","","","","CC BY-SA 4.0","1"
"57079360","1","57079525","","2019-07-17 15:25:16","","1","38","<p>I am trying to join two dataframes by a common column:</p>

<pre><code>import pandas as pd
df1 = pd.DataFrame({'Well_n': ['A', 'B', 'C', 'D'],
                'Qo_rate': [200, 150, 170, 0],
                'year': [2001, 2002, 2003, 2004]})
df2 = pd.DataFrame({'Well_n': ['A', 'C', 'B', 'D'],
                'Well_ECL': ['P1', 'P3', 'P2', 'P4']})
df1.set_index('Well_n',inplace=True)
df2.set_index('Well_n',inplace=True)
joined = df1.join(df2,on='Well_n')
print(joined)
</code></pre>

<p>the output I get is :</p>

<pre><code>         Qo_rate  year Well_ECL
Well_n                        
A           200  2001       P1
B           150  2002       P2
C           170  2003       P3
D             0  2004       P4
</code></pre>

<p>However, when I try to index this new dataframe by year I get this:</p>

<pre><code>joined.set_index('year')

      Qo_rate Well_ECL
year
2001      200       P1
2002      150       P2
2003      170       P3
2004        0       P4
</code></pre>

<p>The column <code>'Well_n'</code> dissappears. I tried to fine something related with the <code>set_index</code> method, but even after using <code>drop=False</code>, I get exactly the same result.
the Output I would like is :</p>

<pre><code>      Qo_rate Well_ECL  Well_n
year
2001      200       P1   A
2002      150       P2
2003      170       P3
2004        0       P4
</code></pre>
","7745799","","1011724","","2019-07-17 15:29:45","2019-07-17 15:32:35","recover a column used as an index in pandas df","<python-3.x><pandas><dataframe>","1","6","","","","CC BY-SA 4.0","1"
"57229479","1","","","2019-07-27 05:43:03","","-1","38","<p>I have a data frame which includes 3 columns - <code>Test</code>, <code>X</code> and <code>Y</code>. I want to add new columns <code>Xmean</code> which include the mean value of <code>X</code> with a condition on <code>Y</code> for each <code>Test</code>.</p>

<p>For example <code>Xmean</code> include the mean value on <code>X</code> while <code>Y &gt;= 5</code> for each <code>Test</code>.</p>

<p><a href=""https://i.stack.imgur.com/Pcr5i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pcr5i.png"" alt=""enter image description here""></a></p>
","10302530","","1158599","","2019-07-29 06:11:25","2019-07-29 15:07:32","DataFrame: how can I groupby Z and calculate the mean X in Y range","<python-3.x><dataframe><pandas-groupby>","1","0","","","","CC BY-SA 4.0","1"
"57511783","1","57512478","","2019-08-15 15:07:34","","1","37","<p>I had a DataFrame like below:</p>

<pre><code>  Item  Date      Count
    a   6/1/2018    1
    b   6/1/2018    2
    c   6/1/2018    3
    a   12/1/2018   3
    b   12/1/2018   4
    c   12/1/2018   1
    a   1/1/2019    2
    b   1/1/2019    3
    c   1/1/2019    2
</code></pre>

<p>I would like to get the sum of Count per Item with the specified duration from 7/1/2018 to 6/1/2019. For this case, the expected output will be:</p>

<pre><code>  Item    TotalCount
    a       5
    b       7
    c       3
</code></pre>
","11729738","","","","","2019-08-15 15:53:15","How to group by value for certain time period","<python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"57748368","1","","","2019-09-01 18:06:38","","0","37","<p>Here is a small set of the dataset that I am currently working on.</p>

<pre><code>FirstName  LastName   cities       occupation         time
---------------------------------------------------------------
---------------------------------------------------------------
Alice      Oumi       Queens       software engineer  1/1/2019
Alice      Oumi       New York     software engineer  12/3/2018
Sam        Charles    Santa Clara  Engineer           2/5/2017
Sam        Charles    Santa Monica Engineer           8/9/2018
Sam        Charles    Santa Clara  Engineer           12/12/2019
Alice      Oumi       New York     software engineer  1/2/2017
</code></pre>

<p>As you see above, the same person could be living in a same place but for a different duration of a time. I want to make clean this dataset that should what places did Alice and Sam live. For example, instead of having 2 rows of Alice living in New York, I only need to have one. Something similar to the following table</p>

<pre><code>FirstName  LastName   cities         FirstTime    SecondTime
---------------------------------------------------------------
---------------------------------------------------------------
Alice      Oumi       Queens         1/1/2019     NA
Alice      Oumi       New York       1/2/2017     12/3/2018
Sam        Charles    Santa Clara    2/5/2017     12/12/2019
Sam        Charles    Santa Monica   8/9/2018     NA 
</code></pre>

<p>I am kinda new to python and trying to learn. but i have tried to use for loops using iterrows() but didn't work. 
What can use to achieve this table? </p>

<p>Thank you so much in advance</p>
","8622554","","8622554","","2019-09-01 18:12:34","2019-09-01 18:44:02","How to identify where each person have lived in different cities in each time?","<python-3.x><database><pandas><dataframe>","1","0","1","","","CC BY-SA 4.0","1"
"57102141","1","","","2019-07-18 20:24:11","","0","37","<p>I have a table of 3 categorical variables (salary, face_amount, and area_code) that I was looking to decorrelate from one another. In other words, I'm trying to find how much of some output can be attributed solely to each one of these variables. So I would want to see how much of this output is due to the salary variable and not the correlated amount of salary with face_amount for example if that makes sense.</p>

<p>I noticed that there is Multiple Correspondence Analysis for this type of problem that will decorrelate the variables, however, the issue I'm having is that I need the original variables and not the ones that are produced from multiple correspondence analysis. I'm very confused as to how to go about analyzing this type of problem and would appreciated any help with this kind of problem.</p>

<p>Sample of data:</p>

<pre><code>salary      face_amount     area_code
'1-50'      1000            67
'1-50'      500             600
'1-50'      500             600
'51-200'    2000            623
'51-200'    1000            623
'201-500'   500             700
</code></pre>

<p>I'm not exactly sure how to go about this kind of problem</p>
","9837743","","","","","2019-07-18 20:24:11","Decorrelating 3 categorical variables","<python-3.x><pandas><machine-learning><statistics>","0","2","","","","CC BY-SA 4.0","1"
"57606968","1","57607079","","2019-08-22 10:11:57","","1","37","<p>I have 2 <code>df</code>s:</p>

<pre><code>df2
dec_pl    cur_key
0         JPY
1         HKD

df1
cur    amount
JPY    80
HKD    20
USD    70
</code></pre>

<p>I like to reference <code>del_pl</code> in <code>df2</code> for 'cur' in <code>df1</code>, and calculate <code>df1.converted_amount = df1.amount * 10 ** (2 - df2.dec_pl)</code> for <code>df1</code>; i.e. <code>df1.amount</code> times the 10 to the power of <code>(2 - df2.dec_pl)</code> and if there cannot find a corresponding <code>df2.cur_key</code> from <code>df1.cur</code>, e.g. <code>USD</code>, then just use its amount;</p>

<pre><code>df1 = df1.set_index('cur')
df2 = df2.set_index('cur_key')
df1['converted_amount'] = (df1.amount*10**(2 - df2.dec_pl)).fillna(df1['amount'], downcast='infer')
</code></pre>

<p>but i got </p>

<pre><code>ValueError: cannot reindex from a duplicate axis 
</code></pre>

<p>I am wondering whats the best way to do this, so the results should look like,</p>

<pre><code>df1
cur    amount    converted_amount
JPY    80        8000
HKD    20        200
USD    70        70
</code></pre>
","766708","","","","","2019-08-22 10:17:54","pandas ValueError: cannot reindex from a duplicate axis when trying to do calculation based on values from another df","<python><python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"56752832","1","","","2019-06-25 11:15:00","","1","37","<p>I have large Excel files, which contain observations of objects. I read the files via pandas, group them and then iterate over each group. For each group I calculate specific and quite complex results - let's say result1,  result2 and optional result3. 
I define an empty df with predefined columns, in which I insert my calculated values. In the end I combine all dfs to one final df.
Maybe better explained by code:</p>

<pre class=""lang-py prettyprint-override""><code>data = pd.read_excel()
grouped = data.groupby('obj_id')

columns = ['result1','result2','result3']

combined_results = pd.DataFrame(columns=columns)

for obj_id, obj_df in grouped:
    obj_results = pd.DataFrame(columns=columns,index=[0])
    # creates empty df with one all NaN row

    obj_results['result1'] = fooCalculation(obj_df)
    obj_results['result2'] = fooCalculation(obj_df)

    combined_results = combined_results.append(obj_results, sort=False)


</code></pre>

<p>I like my current method, because if optional columns end up having no value, the col still exists, because it was set (to NaN) initially. This way I can one by one calculate my result values per object and update my df row as soon as I have updates.</p>

<p>I can't help but think, that this is not the cleanest way. Especially because obj_results['result1'] = fooCalculation() sets an entire column and I falsely use it to set one value.</p>

<p>What is the clean / best-pratice way here.
Should I instead ""cache"" the results in a dict and insert it into the combine_results?</p>
","5845824","","5845824","","2019-06-28 13:09:41","2019-06-28 13:09:41","Cleaner way to one-by-one construct a row and add it to a final dataframe?","<python-3.x><pandas>","0","0","1","","","CC BY-SA 4.0","1"
"57337839","1","","","2019-08-03 11:20:14","","0","37","<p>I'm having a python dictionary with many nested values which some of them are for example: <code>int64</code>, <code>timestamp</code> or other <code>pandas</code> types which are not json serializable.  (the format of the dictionary is not fixed.)</p>

<p>Do anybody aware of some function for converting python dict with pandas types to python types recursively?</p>

<p>Thanks</p>
","965778","","965778","","2019-08-03 12:29:33","2019-08-03 12:29:33","How to convert dictionary with pandas types to json serialzable","<python><python-3.x><pandas>","0","2","","","","CC BY-SA 4.0","1"
"56892862","1","56893203","","2019-07-04 18:27:58","","1","37","<p>I am trying to merge 2 sheets from excel.xlsx using python script. I want when sheet1('CLASS') matches to sheet2('C_MAP') then merge DSC and ASC after CLASS in sheet1 or in a new sheet.</p>

<p>To clarify it i am attaching my excel sheets.</p>

<p>this is my Sheet1:</p>

<pre><code>  P_MAP  Q_GROUP    CLASS
0   ram        2     pink
1              4   silver
2  sham        5    green
3              0  default
4   nil        2     pink
</code></pre>

<p>it contains <code>P_MAP,Q_GROUP,CLASS</code></p>

<p>this is my Sheet2:</p>

<pre><code>    C_MAP DSC    ASC
0    pink  h1  match
1   green  h2  match
2  silver  h3  match
</code></pre>

<p>it contains <code>C_MAP,ASC,DSC</code></p>

<p>So, I want when the CLASS matches to C_MAP it should add ASC and DSC and if it doesnt match add NA.</p>

<p>The output i want will be like this:</p>

<pre><code>  P_MAP  Q_GROUP    CLASS DSC    ASC
0   ram        2     pink  h1  match
1              4   silver  h3  match
2  sham        5    green  h2  match
3              0  default   0     NA
4   nil        2     pink  h1  match
</code></pre>
","11699581","","6509519","","2019-07-04 19:04:53","2019-07-04 19:04:53","Merging two sheets of one excel into single sheet","<excel><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57559800","1","57583338","","2019-08-19 15:30:41","","0","37","<p>So, I have successfully got a function built where I can gather some data, convert it to a dataframe, and then convert it into an excel document.  The problem I am having is, I don't know how to create a responsive title name or storage location.  I have the path using pandas hard coded like so:</p>

<pre><code>developmentdata = pd.DataFrame(dev_names).to_excel(r'C:\Desktop\Dev\devproj\test.xlsx', header=False, index=False)
</code></pre>

<p>and the excel is titled 'test.xlsx'.  What can I do to fix this?</p>
","5212436","","","","","2019-08-21 01:49:22","How do I create a responsive excel document title using pandas?","<python><excel><python-3.x><pandas><dataframe>","1","2","","","","CC BY-SA 4.0","1"
"58376329","1","58376366","","2019-10-14 11:59:42","","1","37","<p>I have a pandas dataframe with 2 columns , say A and B.<br>
All the elements of the columns A and B are of type string.<br>
eg </p>

<pre><code>        A      B  
0      str1   str2  
1      str3   str4  
2      str5   str6  
3      str7   str8  
</code></pre>

<p>So, I have a function f which takes as input 2 strings, does some non trivial stuff and returns an output.<br>
eg  <code>def f(x, y):
         ""do something to x and y to make z""
         return z</code><br>
What I want the output to look like is</p>

<pre><code>        A      B      C
0      str1   str2  f(str1, str2)
1      str3   str4  f(str3, str4)
2      str5   str6  f(str5, str6)
3      str7   str8  f(str7, str8)
</code></pre>

<p>I don't want to  use loops as it is a very big dataframe.<br>
How to apply the function f in a vectorized way to the columns A and B?</p>
","3643544","","","","","2019-10-14 16:13:06","How to apply custom function to 2 columns of a pandas dataframe?","<python-3.x><pandas><dataframe>","2","0","","","","CC BY-SA 4.0","1"
"57608517","1","","","2019-08-22 11:39:57","","0","37","<p>I have one dataframe, I need to filter the dates on the basis of start and end date of the other dataframe. </p>

<p>df1 should have all_dates that is in the range of start_date and end_date of df2</p>

<p>example set is given below. What is the best way in pandas to achieve that?</p>

<p>Considering sample dataframes as below, I have included the expected result set</p>

<pre><code>df1 

ID all_date   clicks
1  2019-08-21   5
1  2019-08-22   4
1  2019-08-25   2
1  2019-08-27   2
2  2019-07-18   5
2  2019-07-21   5
2  2019-07-23   6
2  2019-07-25   6
2  2019-07-27   6

df2

ID start_date  end_date
1  2019-08-21 2019-08-23
2  2019-07-18 2019-07-24
</code></pre>

<pre><code>expected output:

df1

ID all_date   clicks
1  2019-08-21   5
1  2019-08-22   4
2  2019-07-18   5
2  2019-07-21   5
2  2019-07-23   6
</code></pre>

<p>Output should contain range of date i.e start_date and end_date of df2</p>
","11753555","","","","","2019-08-22 11:46:11","How to filter one dataframe on the basis of other","<python><python-3.x><pandas><pandasql>","1","0","","","","CC BY-SA 4.0","1"
"50065684","1","50065954","","2018-04-27 15:28:51","","0","37","<pre><code>             Open     High     Low   Close  Shifted_Close  Movements  Sign
Date                                                                   
2018-04-21  8875.1  9075.0  8629.3  8944.6         8875.0       69.6   Positive
2018-04-22  8939.7  9072.0  8760.5  8811.8         8944.6     -132.8   Negative
2018-04-23  8796.3  9032.1  8784.9  8954.1         8811.8      142.3   Positive
2018-04-24  8959.0  9749.0  8947.0  9661.7         8954.1      707.6   Positive
2018-04-25  9661.7  9750.0  8767.0  8974.5         9661.7     -687.2   Negative
</code></pre>

<p>This is the table I have got using panda. I am wondering how to do a simple if function for sign on the latest entry ""2018-04-25 "".</p>

<p>It would be something like:</p>

<pre><code>if btc_usd_price_kraken['Sign'] == 'Negative':
      print(""Buy coins now"")
</code></pre>

<p>But I only want just one entry.</p>

<p>Thanks</p>
","9711169","","9711169","","2018-04-27 15:38:15","2018-04-27 16:10:49","How to print one entity from a table using Pandas","<python-3.x><pandas>","3","1","","","","CC BY-SA 3.0","1"
"56818822","1","56818911","","2019-06-29 15:41:28","","-2","36","<p>I have a data frame with 5 columns, I only want to add the second and the third, but each point in the third column has to be <strong>multiplied by 3</strong>,<br><br> so I need to add a new column called <br>
<code>""Total score"" which is df['Second'] + 3* df['Third']</code></p>

<p>I have tried with sum but I don't know how to indicate that I want weigh and select only two columns</p>
","11216960","","11607986","","2019-06-29 19:50:21","2019-06-29 19:50:21","How can I weigh columns in data frame and add them up","<python><python-3.x><pandas><dataframe>","1","1","","","","CC BY-SA 4.0","1"
"58374012","1","","","2019-10-14 09:37:57","","0","36","<p>I'm trying to select the first 2 words after the string 'POS PURCHASE' in my data set.</p>

<p>This is my data set.</p>

<pre><code>df:   
    ID        transaction_description
     1         POS PURCHASE MR PRICE WHK FAC
     2         WITHDRAWAL FEE
     3         POS PURCHASE KFC WERNHIL STATE
     4         REJECTED ATM TRANSACTION
     5         ATM CASH WITHDRAWAL
     6         POS PURCHASE EDGARS GROVE
</code></pre>

<p>This is how i want my output to look like:</p>

<pre><code>dfnew:
    ID       transaction_description                 TRANX
     1       POS PURCHASE MR PRICE WHK FAC          MR PRICE
     2       WITHDRAWAL FEE                         WITHDRAWAL FEE
     3       POS PURCHASE KFC WERNHIL STATE         KFC WERNHIL
     4    REJECTED ATM TRANSACTION               REJECTED ATM TRANSACTION
     5         ATM CASH WITHDRAWAL                   ATM CASH WITHDRAWAL  
     6         POS PURCHASE EDGARS GROVE MALL        EDGARS GROVE
</code></pre>

<p>I tried using this code but I'm not able to create a new column that contains the output I want.</p>

<pre><code>code:

   for value in df['transaction_description'].values:
       non_data = re.split('POS PURCHASE |POS PURCHASE ',value)
       terms_list = [term for term in non_data if len(term) &gt; 0] 
       substrs = [term.split()[0:1] for term in terms_list] 
       result = [' '.join(term) for term in substrs] 
   print (result)
</code></pre>
","11841670","","11841670","","2019-10-14 09:44:28","2019-10-14 10:06:00","Is there a function for selecting the first 2 words after a specified string?","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"56801575","1","","","2019-06-28 05:54:31","","0","36","<p>I have data frame </p>

<pre><code>Software Product      Case Number    Created date   End date    CS date 
MDM9607.LE.1.0      2774904     2/3/2017                   3/4/2019
MDM9607.LE.1.0      2774203     8/7/2017       3/9/2018    7/8/2016
MDM9607.LE.1.0      2768088     9/3/2018                   1/2/2019 
MDM9607.LE.1.0      2767500     10/10/2016      3/4/2017   3/2/2015
MDM9607.LE.1.0      2764617     11/12/2017      8/5/2018   4/5/2016
</code></pre>

<p>here I am using two conditions</p>

<pre><code>if f9 f9['Created date'] &lt; f9['End date']
    f9['Status'] = np.select([ m2], ['EOL']) 
</code></pre>

<p>and</p>

<pre><code>if f9['Created date'] &gt; f9['End date']
    f9['Num of days'] = (f9['Created date'] - f9['CS date']).dt.days
    m1 = f9['Num of days'].isna()
    m2 = f9['Num of days'] &gt; 365
    m3 = f9['Num of days'] &lt; 365
    f9['Status'] = np.select([m1, m2, m3], ['U','L', 'N']
</code></pre>

<p>  </p>

<pre><code>Software Product  Case Number    Created date   End date    CS date   Sta
MDM9607.LE.1.0      2774904     2/3/2017                   3/4/2019   L
MDM9607.LE.1.0      2774203     8/7/2017       3/9/2017    7/8/2016   Y
MDM9607.LE.1.0      2768088     9/3/2018                   1/2/2019   NMDM9607.LE.1.0      2767500     10/10/2016     3/4/2017   3/2/2015   L
MDM9607.LE.1.0      2764617     11/12/2017      8/5/2018   4/5/2016   N
</code></pre>

<p>I used above code but I got </p>

<pre><code>File ""&lt;ipython-input-134-3943efb08731&gt;"", line 1
                    if SyntaxError: invalid syntax
</code></pre>
","10064897","","4909087","","2019-06-28 05:56:47","2019-06-28 06:04:51","How to use if condition in pandas using logical operators","<python-3.x><pandas><numpy>","1","0","","","","CC BY-SA 4.0","1"
"57688874","1","57689201","","2019-08-28 09:12:11","","1","36","<p>Here is the DataFrame column and its datatype</p>

<pre><code>df['Hours'].head()
OutPut: 

0   00:00:00
1   00:00:00
2   11:38:00
3   08:40:00
Name: Hours, dtype: timedelta64[ns]
</code></pre>

<p>I want to conditionally form anaother column from it, such that it will look like.</p>

<pre><code>Hours        Test
00:00:00     N/A
00:00:00     N/A
11:38:00     02:38:00
08:40:00     Under Worked
</code></pre>

<p>Where , </p>

<pre><code>if df['Hours'] == '00:00:00':
  df[Test] = 'N/A'
elif (df['Hours'].dt.total_seconds()//3600) &lt; 9:
  df['Test'] = 'Under Worked' 
else:
  df['Test'] = (df['Hours'].dt.total_seconds()//3600)-9
</code></pre>

<p>But it gives me error </p>

<pre><code>    ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
</code></pre>

<p>Also I tried with using <code>np.select</code> </p>

<pre><code>conditions = [
(str(df['Hours']) == '0 days 00:00:00'),
(df['Hours'].dt.total_seconds()//3600) &lt; 9]
choices = ['NA', 'Worked Under']
df['Test'] = np.select(conditions, choices, default=(df['Hours'].dt.total_seconds()//3600)-9)
</code></pre>

<p>This is the error I get</p>

<pre><code>ValueError: list of cases must be same length as list of conditions
</code></pre>

<p>How can it be solved?</p>
","6123057","","6123057","","2019-08-28 09:34:05","2019-08-28 10:09:53","Form a column out of a Datetime field python","<python-3.x><pandas><datetime><if-statement>","1","5","","","","CC BY-SA 4.0","1"
"57655171","1","57655222","","2019-08-26 09:26:24","","1","36","<p>I have a dataframe <code>district_df</code> which contains series Borough such as Bronx, Brooklyn, Manhatten etc. and another series Borough number such as 2, 4, 8 etc.
i want to create another series Board in that dataframe combining Borough and borough number such as 02 Bronx, 04 Brooklyn, 08 manhatten etc,</p>

<pre><code>Borough CD Number
Bronx    2
Brooklyn 4 
Manhatten 8  
</code></pre>

<p>into </p>

<pre><code>Borough  CD Number    Board
Bronx     2          02 Bronx
Brooklyn  4          04 Brooklyn
Manhatten 8          08 Brooklyn
</code></pre>
","11977590","","9698684","","2019-08-26 09:32:37","2019-08-26 10:28:41","How to add a new series by combining two series from a dataframe?","<python><python-3.x><pandas>","1","2","","","","CC BY-SA 4.0","1"
"56955962","1","","","2019-07-09 15:28:01","","0","36","<p>I have three dataframes that are summaries of various statistics about countries.  I've created a join of the three dataframes on the 'Country Name' column.  But I want to know how many entries exist in the three original dataframes that were excluded from the join. Whats the best way code wise to count this?</p>
","11693582","","","","","2019-07-11 11:29:04","What is the best way to count the number of entries across 3 dataframes that aren't shared?","<python><python-3.x><pandas><dataframe>","1","3","","","","CC BY-SA 4.0","1"
"57379276","1","","","2019-08-06 15:21:29","","0","36","<p>I am getting error of duplicate entries. I have a dataframe <em>df</em>. I am converting it into another dataframe by following code. However, I am getting error when I unstacking and then reindexing the dataframe as <em>df1</em> in the loop. </p>

<p><strong>How I can avoid duplicate entries problem?</strong>   </p>

<pre><code>import pandas as pd
import numpy as np
data = [{'Petal_width': 0.2, 'Petal_length': 1.4, 'Sepal_width': 3.5, 'Sepal_length': 5.1, 'Colour': 'a', 'Species_name': ' Setosa'}, {'Petal_width': 0.3, 'Petal_length': 1.4, 'Sepal_width': 3.0, 'Sepal_length': 4.6, 'Colour': 'b', 'Species_name': ' Setosa'}, {'Petal_width': 0.2, 'Petal_length': 1.3, 'Sepal_width': 3.6, 'Sepal_length': 4.7, 'Colour': 'a', 'Species_name': ' Setosa'}, {'Petal_width': 0.2, 'Petal_length': 1.5, 'Sepal_width': 3.1, 'Sepal_length': 4.6, 'Colour': 'c', 'Species_name': ' Setosa'}, {'Petal_width': 0.2, 'Petal_length': 1.4, 'Sepal_width': 3.6, 'Sepal_length': 5.0, 'Colour': 'b', 'Species_name': ' Setosa'}, {'Petal_width': 0.4, 'Petal_length': 1.7, 'Sepal_width': 3.9, 'Sepal_length': 5.4, 'Colour': 'b', 'Species_name': ' Setosa'}, {'Petal_width': 0.3, 'Petal_length': 1.4, 'Sepal_width': 3.4, 'Sepal_length': 4.6, 'Colour': 'b', 'Species_name': ' Setosa'}]
df = pd.DataFrame(data)
df = df.select_dtypes(exclude=['object']) #numerical
print(df)

values = [[0.1, 0.2, 1.3, 1.8, 2.5], [1.0, 1.4, 4.5, 5.1, 6.9], [2.0, 3.0, 3.4, 4.4], [4.3, 5.0, 5.5, 6.3, 7.9]]


for i in range(len(df.columns)):
    s=pd.cut(df.iloc[:,i],values[i]).dropna()
    x=s.map(lambda x : x.left).astype(int).to_frame('V')
    y=s.map(lambda x : x.right).astype(int).to_frame('V')
    y['r']=abs((x.V-df.iloc[:,i])/(y.V-x.V))
    x['r']=abs(1-y['r'])

    df1=pd.concat([x,y]).set_index('V',append=True).\
           r.unstack(fill_value=0).reset_index(drop=True).\
            reindex(columns=values[i],index=df.index,fill_value=0)
    print(df1)
</code></pre>
","9942021","","9942021","","2019-08-06 16:15:52","2019-08-06 16:15:52","Error: Index contains duplicate entries, cannot reshape in pandas","<python-3.x><pandas>","0","3","","","","CC BY-SA 4.0","1"
"56752873","1","","","2019-06-25 11:17:54","","2","36","<p>I have a simple excel file with product names. First row is the category (A1: Water, A2: Sparkling, A3:Still, B1: Soft Drinks, B2: Coca Cola, B3: Orange Juice, B4:Lemonade etc.), each cell below is a different product. I would like to replace the cells of another dataframe (df1) with the product categories. For example, Coca Cola would become Soft Drinks. If the product is not in the excel it would not be replaced (ex. Cookie).</p>

<p><a href=""https://i.stack.imgur.com/ucPsR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ucPsR.png"" alt=""A1: Water, A2: Sparkling, A3:Still, B1: Soft Drinks, B2: Coca Cola, B3: Orange Juice, B4:Lemonade etc.""></a></p>

<p>print(df1)</p>

<pre><code>         Product  Quantity
0      Coca Cola  1234
1      Cookie     4
2      Still      333
3      Chips     88
</code></pre>

<p>Outcome:</p>

<pre><code>print (df1)

       Product      Quantity
0      Soft Drinks   1234
1      Cookie        4
2      Water         333
3      Snacks        88
</code></pre>
","11330653","","","","","2019-06-25 12:04:36","Use data from excel sheet to replace values in dataframe","<python><excel><python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"49661316","1","49661365","","2018-04-04 22:21:43","","1","36","<p>Using the Kiva Loan_Data from Kaggle I aggregated the Loan Amounts by country. Pandas allows them to be easily turned into a DataFrame, but indexes on the country data. The reset_index can be used to create a numerical/sequential index, but I'm guessing I am adding an unnecessary step. Is there a way to create an automatic default index when creating a DataFrame like this?</p>

<p><a href=""https://i.stack.imgur.com/ZHf77.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZHf77.png"" alt=""DataFrame From Groupby Pandas Index Reset""></a></p>
","6352735","","","","","2018-04-04 22:25:45","How to automatically index DataFrame created from groupby in Pandas","<python-3.x><pandas><jupyter-notebook>","1","0","","","","CC BY-SA 3.0","1"
"56771384","1","","","2019-06-26 11:05:22","","0","36","<p>I have a list of <code>csv</code> files which are in the same directory and trying to combine these 2 files and make one new <code>csv</code> file which has the contents of both input files. here is an example of 2 input files:</p>

<p>small_example1.csv</p>

<pre><code>    CodeClass,Name,Accession,Count
    Endogenous,CCNO,NM_021147.4,18
    Endogenous,MYC,NM_002467.3,1114
    Endogenous,CD79A,NM_001783.3,178
    Endogenous,FSTL3,NM_005860.2,529
</code></pre>

<p>small_example2.csv</p>

<pre><code>    CodeClass,Name,Accession,Count
    Endogenous,CCNO,NM_021147.4,196
    Endogenous,MYC,NM_002467.3,962
    Endogenous,CD79A,NM_001783.3,390
    Endogenous,FSTL3,NM_005860.2,67
</code></pre>

<p>and here is the expected output file (<code>result.csv</code>): </p>

<pre><code>    Probe_Name,Accession,Class_Name,small_example1,small_example2
    CCNO,NM_021147.4,Endogenous,18,196
    MYC,NM_002467.3,Endogenous,1114,962
    CD79A,NM_001783.3,Endogenous,178,390
    FSTL3,NM_005860.2,Endogenous,529,67
</code></pre>

<p>to do so, I made this function in python3: </p>

<pre><code>    import pandas as pd
    filenames = ['small_example1.csv', 'small_example2.csv']
    path = '/home/Joy'
    def convert(filenames):
        for file in filenames:
            df1 = pd.read_csv(file, skiprows=26, skipfooter=5, sep=',')
            df = df1.merge(df2, on=['CodeClass', 'Name', 'Accession'])
            df = df.rename(columns={'Name': 'Probe_Name',
                            'CodeClass': 'Class_Name',
                             file: file})
            df.to_csv('result.csv')
</code></pre>

<p>the results look like this and the last 2 columns are not like expected (both <code>headers</code> and <code>numbers</code>).</p>

<pre><code>        Class_Name  Probe_Name  Accession   Count_x Count_y
    0   Endogenous  CCNO    NM_021147.4 18  18
    1   Endogenous  MYC NM_002467.3 1114    1114
    2   Endogenous  CD79A   NM_001783.3 178 178
    3   Endogenous  FSTL3   NM_005860.2 529 529

</code></pre>

<p>do you know how to fix the problem?</p>
","3925736","","11607986","","2019-06-26 12:04:31","2019-06-26 12:53:00","parsing and combining csv files into another csv file in python3","<python-3.x><pandas><csv>","2","0","","","","CC BY-SA 4.0","1"
"57667106","1","57667596","","2019-08-27 02:49:36","","2","36","<p>I am new to pandas .I want the a table in a format which I need to export csv.</p>

<p>What I have tried is:</p>

<p>o_rg,o_gg,a_rg,a_gg are arrays</p>

<pre><code>  df1=pd.DataFrame({'RED':o_rg,'GREEN':o_gg})
  df2=pd.DataFrame({'RED':a_rg,'RED':a_gg})
  df=df1-(df2)
  pop_complete = pd.concat([df, df1, df2], keys=[""O-A"", ""O"", ""A""], axis=1)
  pop_complete.index = ['A1','A3','A8']
  df1 = pop_complete.stack(0)[['RED','GREEN']].reindex([""O"", ""A"", ""O-A""], axis=0, level=1)
  df1.to_csv(""OUT.CSV"")
</code></pre>

<p>What I get the output as:</p>

<pre><code>       RED     GREEN       

    A1 O        14.0     14.0
       A        14.0     12.0
       O-A      0.0      2.0

    A3 O        12.0     9.0
       A        12.0     10.0
       O-A      0.0      -1.0

    A8 O        15.0     12.0
       A        15.0     12.0
       O-A      0.0      0.0
</code></pre>

<p>What I actually want is:</p>

<pre><code>                RED     GREEN       
       A1
       O        14.0     14.0
       A        14.0     12.0
       O-A      0.0      2.0
       A3
       O        12.0     9.0
       A        12.0     10.0
       O-A      0.0      -1.0
       A8
       O        15.0     12.0
       A        15.0     12.0
       O-A      0.0      0.0
</code></pre>

<p>where 'A1','A3','A8' ... can be stored in array cases=[]
How to get the actual output?</p>
","11591582","","","","","2019-08-27 04:13:03","How to get a table in a format using pandas and export to csv?","<python-3.x><pandas><python-2.7><csv><export-to-csv>","1","1","","","","CC BY-SA 4.0","1"
"49138016","1","","","2018-03-06 18:51:31","","0","36","<p>I have an issue with Pivot table in Python. Let's say that I have below values in list:</p>

<p>team_A_id = [1,5,10]</p>

<p>team_A_result = 0</p>

<p>and below data frame:</p>

<pre><code>id             points
3                36
4                0
5                11
7                6
10               23
</code></pre>

<p>How could I using (perhaps) ""for loop"" find by team A id in list points and count them. Output should be:</p>

<p>result_team_A = 34</p>

<p>Thanks for any help </p>
","9284651","","","","","2018-03-06 18:58:08","Python Pandas Pivot Table - counting points","<python-3.x><list><pandas><dataframe><iterator>","2","0","","","","CC BY-SA 3.0","1"
"49463123","1","49463162","","2018-03-24 09:23:16","","1","36","<p>User defined function=> my_fun(x): returns a list</p>

<p>XYZ = file with <strong>LOTS</strong> of lines</p>

<pre><code>pandas_frame = pd.DataFrame() # Created empty data frame
for index in range(0,len(XYZ)):
    pandas_frame = pandas_frame.append(pd.DataFrame(my_fun(XYZ[i])).transpose(), ignore_index=True)
</code></pre>

<p>This code is taking very long time to run like in days. How do I speed up?</p>
","6403976","","","","","2018-03-24 09:28:07","Convert huge number of lists to pandas dataframe","<python-3.x><pandas><numpy>","1","0","","","","CC BY-SA 3.0","1"
"56784198","1","56784327","","2019-06-27 05:02:43","","2","35","<p>I have have dataframe with column A , i want to divide column in bins and count of each bin as column of dataframe , for example bin from 0 to how many points and add this in in dataframe. </p>

<p>i used this code for binning but i am not sure how to insert count column in df.</p>

<pre><code>df=pd.DataFrame({'max':[0.2,0.3,1,1.5,2.5,0.2]})
print(df)
   max
0  0.2
1  0.3
2  1.0
3  1.5
4  2.5
5  0.2

    bins = [0, 0.5, 1, 1.5, 2, 2.5]

    x=pd.cut(df['max'], bins)
</code></pre>

<p>desired output</p>

<pre><code>print(df)
   0_0.5_count  0.5_1_count
0            3            1
</code></pre>
","10813833","","10813833","","2019-06-27 05:10:58","2019-06-27 05:20:02","How to make each bin of data as column of dataframe","<python-3.x><pandas><numpy>","1","2","","","","CC BY-SA 4.0","1"
"49414693","1","49414987","","2018-03-21 19:05:38","","1","35","<p>I have done code which save df to different spreadsheets ( in the single workbook), BUT is any source or sample where df can be saved (to_excel) to different excel workbooks?</p>

<p>Currently I use</p>

<pre><code>df.to_excel(writer, sheet_name = name)
</code></pre>

<p>Is it possible to do something like this?</p>

<pre><code>df.to_excel(writer, workbook_name = name)
</code></pre>
","","user9530648","","user9530648","2018-03-21 19:21:46","2018-03-21 19:24:27","Python Save to different workbook","<python><excel><python-3.x><pandas>","1","3","","2018-03-21 22:04:21","","CC BY-SA 3.0","1"
"57865968","1","57866008","","2019-09-10 07:28:34","","2","35","<p>I have a <code>dirty</code> dataset where a column contains a legitimate date with year first or year last or no date at all.</p>

<p>I am trying to convert this to a legitimate date, creating blanks for cells where it is not a real date</p>

<p>I have tried using <code>to_datetime</code>, but the <code>format='%Y%m%d'</code> will only work for the full frame. Adding <code>errors = ""ignore""</code>, change none of the data.</p>

<p>Added the above to a ""Try"" statement does not work either.</p>

<pre><code>newdf = pd.DataFrame()
newdf['name'] = ('leon','eurika','monica','wian')
newdf['surname'] = ('swart','swart','swart','swart')
newdf['birthdate'] = ('14051981','19800911','1012','20100621')

newdf['birthdate'] = pd.to_datetime(newdf['birthdate'], format='%Y%m%d')


     name surname  birthdate
0    leon   swart 1981-05-14
1    eurika   swart 1980-09-11
2    monica   swart  
3    wian   swart 2010-06-21
</code></pre>
","12024038","","4407564","","2019-09-10 08:47:38","2019-09-10 08:47:38","Converting number to date with the year sometime first and sometimes last","<python><python-3.x><pandas>","1","0","1","","","CC BY-SA 4.0","1"
"57340167","1","","","2019-08-03 16:26:26","","0","35","<p>I want the input str to match with str in file that have fix row and then I will minus the score column of that row  </p>

<p>1!! == i think this is for loop to find match str line by line from first to last</p>

<p>2!! == this is for when input str have matched it will minus score of matched row by 1.</p>

<p>CSV file:</p>

<p><a href=""https://i.stack.imgur.com/LNLaI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LNLaI.png"" alt=""enter image description here""></a></p>

<pre><code>article = pd.read_csv('Customer_List.txt', delimiter = ',',names = ['ID','NAME','LASTNAME','SCORE','TEL','PASS'])

y = len(article.ID)

line=article.readlines()

for x in range (0,y):  # 1!!

    if word in line :
       newarticle = int(article.SCORE[x]) - 1 #2!!       
       print(newarticle) 
    else: 
       x = x + 1
</code></pre>

<p>P.S. I have just study python for 5 days, please give me a suggestion.Thank you.</p>
","11878337","","1592364","","2019-08-03 17:26:50","2019-08-03 17:26:50","How to match input data and a data in df, for loop minus","<python><python-3.x><pandas>","1","2","","","","CC BY-SA 4.0","1"
"57688814","1","57689760","","2019-08-28 09:09:40","","0","35","<p>I have a pandas dataframe which looks like this</p>

<pre><code>timestamp               phase
2019-07-01 07:10:00     a 
2019-07-01 07:11:00     a
2019-07-01 07:12:00     b
2019-07-01 07:13:00     b
2019-07-01 07:17:00     a
2019-07-01 07:19:00     a
2019-07-01 07:20:00     c
</code></pre>

<p>I am working on a function that creates a dataframe with an duration for every phase, till it hits the next phase.
I already have a solution but I have no clue how to write this in an <strong>user-defined-function</strong>, as I am new to python.</p>

<p>This is my ""static"" solution:</p>

<pre><code>df['prev_phase'] = df[""phase""].shift(1)
df['next_phase'] = df[""phase""].shift(-1)

dfshift = df[df.next_phase != df.prev_phase]

dfshift[""delta""] = (dfshift[""timestamp""]-dfshift[""timestamp""].shift()).fillna(0)

dfshift[""helpcolumn""] = dfshift[""phase""].shift(1)

dfshift2 = dfshift[dfshift.helpcolumn == dfshift[""phase""]]

dfshift3 = dfshift2[[""timestamp"",""phase"",""delta""]]

dfshift3[""deltaminutes""] = dfshift3['delta'] / np.timedelta64(60, 's')
</code></pre>

<p>This gives me this as output (example):</p>

<pre><code>timestamp            phase    delta             deltam
2019-05-01 06:44:00     a     0 days 04:51:00   291.0
2019-05-01 07:25:00     b     0 days 00:40:00   40.0
2019-05-01 21:58:00     a     0 days 14:32:00   872.0
2019-05-01 22:07:00     c     0 days 00:08:00   8.0
</code></pre>

<p>I just need this in a function.
Thanks in advance </p>

<p>Edit for @Tom</p>

<pre><code>timestamp   phase
2019-05-05 08:58:00 a
2019-05-05 08:59:00 a
2019-05-05 09:00:00 b
2019-05-05 09:01:00 b
2019-05-05 09:02:00 b
2019-05-05 09:03:00 b
...
...
2019-05-05 09:38:00 b
2019-05-05 09:39:00 c
2019-05-05 09:40:00 c
2019-05-05 09:41:00 c
</code></pre>

<p>Those are the two colums + Index</p>
","11883717","","11883717","","2019-08-28 11:55:10","2019-08-28 11:55:10","Function to calculate timespan for a certain event","<python-3.x><pandas><function><timestamp>","1","0","","","","CC BY-SA 4.0","1"
"57699291","1","57702359","","2019-08-28 19:58:23","","0","35","<p>I have a CSV that I want to edit in Pandas. He has such columns:</p>

<pre><code>P-N,U-N,I,R,C,I-L,L-D,RS,em,D,l_1,l_2,l_3,l_4,l_5,l_6,l_7,l_8,l_9,l_10,l_11,l_12,l_13,l_14,l_15,l_16,l_17,l_18
</code></pre>

<p>Example:</p>

<pre><code>TEXT,TEXT,https://is5-ssl.mzstatic.com/image/thumb/Podcasts123/v4/11/46/8b/11468ba5-05a1-6b61-56e8-9389416c3fed/mza_8280916651621281968.jpg/268x0w.jpg,6,TEXT,https://podcasts.apple.com/us/podcast/alav-el-pinche-podcast/id1459010039,Aug 1 2019,https://feed.podbean.com/alav/feed.xml,quinonescristian@yahoo.com,TEXT,,,,,,,,,,,,,,,,,,
TEXT,TEXT,https://is2-ssl.mzstatic.com/image/thumb/Podcasts123/v4/9c/3c/5d/9c3c5dd7-049c-0a36-51a5-146a6b67c032/mza_2227232542399366991.png/268x0w.jpg,22,TEXT,https://podcasts.apple.com/us/podcast/couple-goals-with-s-m/id1354222194,18 Aug 2019,https://couplegoals.podbean.com/feed.xml,No email,TEXT,,,,,,,,,,,,,,,,,,
TEXT,TEXT,https://is4-ssl.mzstatic.com/image/thumb/Podcasts123/v4/19/99/2a/19992a8a-c302-6416-27d2-78d4a751ff2d/mza_8707044730009010204.jpg/268x0w.jpg,1053,TEXT,https://podcasts.apple.com/us/podcast/all-crime-no-cattle/id1327729036,Jul 26 2019,https://www.spreaker.com/show/3202978/episodes/feed,allcrimenocattle@gmail.com,TEXT"",https://podcasts.apple.com/us/podcast/id1207505792,https://podcasts.apple.com/us/podcast/id1298179135,https://podcasts.apple.com/us/podcast/id1255329049,https://podcasts.apple.com/us/podcast/id1256792057,https://podcasts.apple.com/us/podcast/id1250294822,https://podcasts.apple.com/us/podcast/id1317929691,https://podcasts.apple.com/us/podcast/id1328036628,https://podcasts.apple.com/us/podcast/id1244309070,https://podcasts.apple.com/us/podcast/id1377988275,https://podcasts.apple.com/us/podcast/id1214679007,https://podcasts.apple.com/us/podcast/id1166399817,https://podcasts.apple.com/us/podcast/id1242028887,https://podcasts.apple.com/us/podcast/id1184429621,https://podcasts.apple.com/us/podcast/id1289005078,https://podcasts.apple.com/us/podcast/id1396717244,https://podcasts.apple.com/us/podcast/id1259478351,https://podcasts.apple.com/us/podcast/id1295887970,https://podcasts.apple.com/us/podcast/id1278924392
TEXT,TEXT,https://is2-ssl.mzstatic.com/image/thumb/Podcasts123/v4/30/b6/46/30b6469c-0e15-e3d3-4063-6207d8020824/mza_1356338398462313394.jpg/268x0w.jpg,1186,TEXT,https://podcasts.apple.com/us/podcast/extraterrestrial/id1449220604,Aug 13 2019,https://feeds.megaphone.fm/extraterrestrial,support@parcast.com,TEXT,https://podcasts.apple.com/us/podcast/id1156724104,https://podcasts.apple.com/us/podcast/id1441968144,https://podcasts.apple.com/us/podcast/id1441824608,https://podcasts.apple.com/us/podcast/id1449997236,https://podcasts.apple.com/us/podcast/id1440306805,https://podcasts.apple.com/us/podcast/id1434465245,https://podcasts.apple.com/us/podcast/id1437682381,https://podcasts.applcom/us/podcast/id1441348407,https://podcasts.apple.com/us/podcast/id1449762156,https://podcasts.apple.com/us/podcast/id1438804143,https://podcasts.apple.com/us/podcast/id1440107092,https://podcasts.apple.com/us/podcast/id1454411295,https://podcasts.apple.com/us/podcast/id1449191242,https://podcasts.apple.com/us/podcast/id1094490100,https://podcasts.apple.com/us/podcast/id1450027383,https://podcasts.apple.com/us/podcast/id1294529191,https://podcasts.apple.com/us/podcast/id1197087242,https://podcasts.apple.com/us/podcast/id1468956772
TEXT,TEXT,https://is3-ssl.mzstatic.com/image/thumb/Podcasts123/v4/06/9e/c7/069ec760-12cb-fdef-4b28-ac96cc649f3f/mza_1930766855883297019.jpg/268x0w.jpg,No rating,TEXT,https://podcasts.apple.com/us/podcast/another-dead-man-walking/id1449476437,Nov 7 2018,https://www.spreaker.com/show/3287452/episodes/feed,radio@sky.uk,TEXT,https://podcasts.apple.com/us/podcast/id1448403218,https://podcasts.apple.com/us/podcast/id1453587264,https://podcasts.apple.com/us/podcast/id1453829416,https://podcasts.apple.com/us/podcast/id1435009373,https://podcasts.apple.com/us/podcast/id1462249765,https://podcasts.apple.com/us/podcast/id1448205703,https://podcasts.apple.com/us/podcast/id1458485904,https://podcasts.apple.com/us/podcast/id1451783176,https://podcasts.apple.com/us/podcast/id1436485458,https://podcasts.apple.com/us/podcast/id1462332471,https://podcasts.apple.com/us/podcast/id1447656582,https://podcasts.apple.com/us/podcast/id1447037392,https://podcasts.apple.com/us/podcast/id1459952501,https://podcasts.apple.com/us/podcast/id1458931325,https://podcasts.apple.com/us/podcast/id1448556563,https://podcasts.apple.com/us/podcast/id1409087641,https://podcasts.apple.com/us/podcast/id1400426755,https://podcasts.apple.com/us/podcast/id1462323616
</code></pre>

<p>Expected result:</p>

<pre><code>TEXT,TEXT,https://is5-ssl.mzstatic.com/image/thumb/Podcasts123/v4/11/46/8b/11468ba5-05a1-6b61-56e8-9389416c3fed/mza_8280916651621281968.jpg/268x0w.jpg,6,TEXT,id1459010039,Aug 1 2019,https://feed.podbean.com/alav/feed.xml,quinonescristian@yahoo.com,TEXT,,,,,,,,,,,,,,,,,,
TEXT,TEXT,https://is2-ssl.mzstatic.com/image/thumb/Podcasts123/v4/9c/3c/5d/9c3c5dd7-049c-0a36-51a5-146a6b67c032/mza_2227232542399366991.png/268x0w.jpg,22,TEXT,id1354222194,18 Aug 2019,https://couplegoals.podbean.com/feed.xml,No email,TEXT,,,,,,,,,,,,,,,,,,
TEXT,TEXT,https://is4-ssl.mzstatic.com/image/thumb/Podcasts123/v4/19/99/2a/19992a8a-c302-6416-27d2-78d4a751ff2d/mza_8707044730009010204.jpg/268x0w.jpg,1053,TEXT,id1327729036,Jul 26 2019,https://www.spreaker.com/show/3202978/episodes/feed,allcrimenocattle@gmail.com,TEXT"",id1207505792,id1298179135,id1255329049,id1256792057,id1250294822,id1317929691,id1328036628,id1244309070,id1377988275,id1214679007,id1166399817,id1242028887,id1184429621,id1289005078,id1396717244,id1259478351,id1295887970,id1278924392
TEXT,TEXT,https://is2-ssl.mzstatic.com/image/thumb/Podcasts123/v4/30/b6/46/30b6469c-0e15-e3d3-4063-6207d8020824/mza_1356338398462313394.jpg/268x0w.jpg,1186,TEXT,id1449220604,Aug 13 2019,https://feeds.megaphone.fm/extraterrestrial,support@parcast.com,TEXT,id1156724104,id1441968144,id1441824608,id1449997236,id1440306805,id1434465245,id1437682381,id1441348407,id1449762156,id1438804143,id1440107092,id1454411295,id1449191242,id1094490100,id1450027383,id1294529191,id1197087242,id1468956772
TEXT,TEXT,https://is3-ssl.mzstatic.com/image/thumb/Podcasts123/v4/06/9e/c7/069ec760-12cb-fdef-4b28-ac96cc649f3f/mza_1930766855883297019.jpg/268x0w.jpg,No rating,TEXT,id1449476437,Nov 7 2018,https://www.spreaker.com/show/3287452/episodes/feed,radio@sky.uk,TEXT,id1448403218,id1453587264,id1453829416,id1435009373,id1462249765,id1448205703,id1458485904,id1451783176,id1436485458,id1462332471,id1447656582,id1447037392,id1459952501,id1458931325,id1448556563,id1409087641,id1400426755,id1462323616
</code></pre>

<p>Now I am editing it like this:</p>

<pre class=""lang-py prettyprint-override""><code>for key, value in enumerate(df[""I-L""]):
    df[""I-L""][key] = value.split(""/"")[-1]

for ln in range(1, 19):
    for key, value in enumerate(df[f""l_{ln}""]):
        try:
            df[f""l_{ln}""][key] = value.split(""/"")[-1]
        except AttributeError:
            pass
</code></pre>

<p>But I have very large files with approximately 500,000 entries each and this method is sooooooo slow.</p>

<p>How can I do it faster?</p>
","11436357","","11436357","","2019-08-28 20:26:01","2019-08-29 02:51:11","How to change the value of cells in a pandas?","<python-3.x><pandas>","1","6","","","","CC BY-SA 4.0","1"
"57294477","1","57294673","","2019-07-31 15:39:42","","0","35","<p>I tried to identify NaNs in my dataframe and later removed them as they weren't necessary for this particular problem. Although now while performing another operation, I came across something called ""floating NaN"" and because of that I'm unable to do this operation.</p>

<pre><code>transaction id
54789
56334
56233
58789
C57832
C53347
C58963
C58797
</code></pre>

<p>The operation which I was performing involved identifying integers and strings in this column and add a new column which would have ""Ordered"" if integers, and ""Cancelled"" if strings. For this operation I used following code.</p>

<pre><code>data_clean['transaction status'] = data_clean['transaction id'].str.isnumeric().astype(int)
</code></pre>

<p>the above line of code would have given me the output as</p>

<pre><code>  transaction id  transaction status
0         654656                   1
1         546466                   1
2         654646                   1
3         844886                   1
4        C846464                   0
5        C384448                   0
6        C468788                   0
7        C873316                   0
</code></pre>

<p>for this line of code, I get the following error.</p>

<pre><code>cannot convert float NaN to integer
</code></pre>

<p>I didn't know until this point that floating NaNs existed until this point. How do i detect them? The first time i tried running </p>

<pre><code>data_clean['trasaction id'].isnull().any()
</code></pre>

<p>it came as </p>

<pre><code>False
</code></pre>

<p>How do i find such floating NaNs if the above line of code couldn't detect?</p>
","10632473","","","","","2019-07-31 15:51:09","Is there a way to identify floating NaN and then replacing them in pandas dataframe?","<python><python-3.x><pandas><dataframe>","1","6","1","","","CC BY-SA 4.0","1"
"57193910","1","","","2019-07-25 03:35:05","","0","35","<p>TABLE_1</p>

<pre><code>x   y   z   amount  absolute amount
121 abc def 500 700
131 fgh xyz -800    800
121 abc xyz 900 900
131 fgh ijk 800 800
141 obc pqr 500 500
151 mbr pqr -500    500
141 obc pqr -500    500
151 mbr pqr 900 900
</code></pre>

<p>applying the following code, i get the desired output-</p>

<pre><code>c=df.groupby(['x','y'])['amount'].transform('sum')
df[c.ne(0) &amp; c.abs().ne(df.absolute_amount)]
</code></pre>

<p>TABLE_2  </p>

<pre><code>x   y   z   amount  absolute amount
121 abc def 500 700
121 abc xyz 900 900
151 mbr pqr -500    500
151 mbr pqr 900 900
</code></pre>

<p>But when i further want a table in which Return rows in TABLE_1 which are not present in TABLE_2 and generate a new table (TABLE_3)</p>

<p>And apply some group by and filters to TABLE_3-</p>

<pre><code>lis = anti_join_all_cols(TABLE_1, TABLE_2) 

dd = lis.groupby(['x','y'])['amount'].transform('sum')
TABLE_3 = lis[c.ne(0) &amp; c.abs().ne(df.Absolute Amount)]
</code></pre>

<p>the following warning pops up-</p>

<pre><code>UserWarning: Boolean Series key will be reindexed to match DataFrame index.
</code></pre>
","9168092","","8285811","","2019-07-25 04:40:54","2019-07-25 05:41:27","I need help in figuring out the code error after applying anti_join","<python><python-3.x><pandas><pandas-groupby>","1","2","","","","CC BY-SA 4.0","1"
"50048194","1","","","2018-04-26 16:44:18","","0","35","<p>This may be a relatively amateur question, but how do I find the last row of a pandas dataframe containing data in python?</p>

<p>I have a poorly structured spreadsheet I am trying to read in and manipulate, but the doc has an excessive number of extra cells below the end of the actual data.</p>
","7162873","","","","","2018-04-26 17:40:24","Finding the last row of a Pandas data frame containing Data","<python-3.x><pandas>","0","3","","","","CC BY-SA 3.0","1"
"56770553","1","56771182","","2019-06-26 10:24:38","","0","35","<p>I have a grouped bar chart created from a pandas dataframe. I want to set an axhline across each set of bars, but have it stop just beyond the edges of the bar groups, so I need to find a way to get a list of the xtick locations, and the width of the bars that are plotted, so that I can put the xmin and xmax args into the axhline. If I can't do this, can I set the tick locations in the plot function and use those values?</p>

<p>The only thing I have tried is setting the line locations manually, but the plots I'm making come from dataframes with different numbers of columns, so I don't always have the same number of groups on the chart.</p>

<p>Using <code>get_xticks</code> doesn't help me much as it just gives me an array of <code>[1,2,3]</code>, which is no use for axhline.</p>

<p>The dataframe is set up like so (the dataframe will always have 3 rows, but has a variable number of columns):</p>

<pre><code>import matplotlib.pyplot as plt
import pandas as pd

fig, ax = plt.subplots()

df = pd.DataFrame(data=[[1,2,3], [4,5,6], [7,8,9]], index=['type1', 'type2', 'type3'], columns=['group1', 'group2', 'group3'])

df.plot(kind='bar', ax=ax)
</code></pre>

<p>Each group has a threshold value which I want to indicate on the graph as a horizontal line, but in some of my cases I have 10 or 11 groups, so I don't want to have lines across the entire plot as it would be unreadable.</p>

<p>Thanks in advance</p>
","11702424","","9537244","","2019-06-26 10:29:03","2019-06-26 10:54:52","How do I get the locations of the x-ticks on a bar plot created in pandas?","<python><python-3.x><pandas><matplotlib>","1","0","","","","CC BY-SA 4.0","1"
"57298934","1","","","2019-07-31 21:15:38","","1","35","<p>The problem is the dataset has variable data rates per ID, I would like to filter out the IDs that do not have at least one data point per day.</p>

<p>I have a dataframe with IDs, dates, and data, in which I counted the daily sampling rate for each ID. </p>

<pre><code>dfcounted = df.reset_index().groupby(['id', pd.Grouper(key='datetime', freq='D')]).count().reset_index()
</code></pre>

<p>Now, i have taken the first and last date of the dataframe, and created a dataframe of each day between the starting and ending dates:</p>

<pre><code># take dates
sdate = df['datetime'].min()   # start date
edate = df['datetime'].max()   # end date

# interval
delta = edate - sdate       # as timedelta

# empty list
dates = []

# store each date in list
for i in range(delta.days + 1):
    day = sdate + timedelta(days=i)
    dates.append(day)

# convert to dataframe 
dates = pd.DataFrame(data = dates, columns=[""date""])
</code></pre>

<p>From here, I am lost on how to proceed. I have created a sample dataframe</p>

<pre><code>import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random
import string

letters = string.ascii_lowercase
ids = random.choices(letters,k=100) 


date_today = datetime.now()
days = pd.date_range(date_today, date_today + timedelta(99), freq='D')

np.random.seed(seed=1111)
data = np.random.randint(1, high=100, size=len(days))

df = pd.DataFrame({'date': days,'ids': ids, 'data': data})
df = df.set_index('date')
</code></pre>

<p>With the sample df, i would expect to create a ""results"" df with only the ids that have data in each date.</p>
","7775026","","7775026","","2019-08-15 20:40:59","2019-08-15 20:40:59","How to verify if Ids are present in each day within a period of x days?","<python-3.x><pandas><list><loops>","0","3","","","","CC BY-SA 4.0","1"
"56951504","1","","","2019-07-09 11:25:50","","0","35","<p>I'm trying to cleanup a text file from a URL using pandas and my idea is to split it into separate columns, add 3 more columns and export to a csv.</p>

<p>I have tried cleaning up the file (I believe it is being separated by "" "") and so far no avail. </p>

<pre><code># script to check and clean text file for 'aberporth' station
import pandas as pd
import requests

# api-endpoint for current weather
URLH = ""https://www.metoffice.gov.uk/pub/data/weather/uk/climate/stationdata/aberporthdata.txt""

with requests.session() as s:
    # sending get for histroy text file
    r = s.get(URLH)
    df1 = pd.read_csv(io.StringIO(r.text), sep="" "", skiprows=5, error_bad_lines=False)
    df2 = pd.read_csv(io.StringIO(r.text), nrows=1)
    # df1['location'] = df2.columns.values[0]
    # _, lat, _, lon = df2.index[0][1].split()
    # df1['lat'], df1['lon'] = lat, lon
    df1.dropna(how='all')
    df1.to_csv('Aberporth.txt', sep='|', index=True)
</code></pre>

<p>What makes it worse is that the file itself has uneven columns, and somewhere down line 944, it adds one more column for which I to skip error on bad lines. At this point I'm a bit lost as for how I should proceed and if I should look at something else beyond Pandas.</p>
","11754953","","","","","2019-07-09 12:14:58","Cleaning a text file for csv export","<python-3.x><pandas-datareader>","1","0","","","","CC BY-SA 4.0","1"
"56999179","1","57015236","","2019-07-12 01:36:49","","0","35","<p>I have dataframe  x2 with two columns. i am trying to plot but didnt get xticks.
data:</p>

<pre><code>       bins       pp
0     (0, 1]  0.155463
1     (1, 2]  1.528947
2     (2, 3]  2.436064
3     (3, 4]  3.507811
4     (4, 5]  4.377849
5     (5, 6]  5.538044
6     (6, 7]  6.577340
7     (7, 8]  7.510983
8     (8, 9]  8.520378
9    (9, 10]  9.721899
</code></pre>

<p>i tried this code result is fine just cant find x-axis ticks just blank. i want bins column value should be on x-axis</p>

<pre><code>x2.plot(x='bins',y=['pp'])

x2.dtypes
Out[141]: 
bins          category
pp             float64
</code></pre>

<p><a href=""https://i.stack.imgur.com/ai0Oo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ai0Oo.png"" alt=""enter image description here""></a></p>
","11327242","","11327242","","2019-07-12 01:58:09","2019-07-13 00:19:35","how to line plot values column vs groups","<python-3.x><pandas><matplotlib><pandas-groupby>","1","5","","","","CC BY-SA 4.0","1"
"57807059","1","","","2019-09-05 13:51:44","","0","34","<p>I want to create a histogram serie. However, I also need to use weights (I cannot use seaborn).</p>

<p>I tried to use ""for"" to create this serie using:</p>

<pre><code>list=range(28,37)
for i in list:
    plt.hist(Base.iloc[:,i],weights=Base['weights']
</code></pre>

<p>But I got a Strange histogram:</p>

<p><img src=""https://i.stack.imgur.com/FiCO6.png"" alt=""Strange histogram""></p>

<p>I have 2 questions:</p>

<ol>
<li><p>how do I create this serie; and,</p></li>
<li><p>what is this strange histogram?</p></li>
</ol>
","11842662","","4124317","","2019-09-05 13:55:09","2019-09-05 15:39:29","Using ""for"" in MatPlotLib","<python-3.x><pandas><matplotlib>","1","0","","","","CC BY-SA 4.0","1"
"49766233","1","","","2018-04-11 03:53:35","","0","34","<p>Env:  Python 3.6, Pandas</p>

<p>I have a line of code that reads:</p>

<pre><code>df['column_1'][df['column_2'].str.contains('keyword')] = df['column_3']
</code></pre>

<p>The code is designed to look at <code>column_2</code> and if the <code>keyword</code> is present to set <code>column_1</code> string to what is in <code>column_3</code>.  It works just fine and does what I would expect.  But it throws <a href=""http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy"" rel=""nofollow noreferrer"">this</a> warning:   <code>A value is trying to be set on a copy of a slice from a DataFrame</code>.</p>

<p>Based on the docs and the examples I changed to:</p>

<pre><code>df.loc[:,('column_1', df['column_2'].str.contains('keyword'))] = df['column_3']
</code></pre>

<p>But, this code throws:  <code>ValueError: setting an array element with a sequence</code>.</p>

<p>I believe the error is a result of the second element of the tuple.  There is another example <a href=""https://stackoverflow.com/questions/48323370/pandas-indexing-view-versus-copy"">here</a> that works with the same construction (and I can run that without the err).  There are a several other similar questions but none of them deal with a search on a column in the tuple.  So that narrows it down to how do I write <code>df['column_2'].str.contains('keyword')</code> in a way that will work in a tuple?</p>

<p>Or is there something else that I'm missing?</p>

<p><strong>UPDATE:</strong></p>

<p>After more research, it maybe relevant that I setup <code>column_1</code> by</p>

<pre><code>df['column_1'] = np.nan    #numpy NaN
</code></pre>

<p>It seems that how the column is constructed can also cause this type of error.  The dtype as a NaN is <code>float64</code>. I've tried recasting <code>.astype(str)</code> without any change.</p>
","3180386","","3180386","","2018-04-11 22:48:22","2018-04-11 22:48:22","Pandas: Returns a view versus a copy, tuple construction","<python><python-3.x><pandas>","0","2","","","","CC BY-SA 3.0","1"
"56929866","1","","","2019-07-08 07:05:12","","0","34","<p>I have 20 CSV files having a maximum size of 1 GB. In all these files, there are only two common columns ""X"", ""Y"". I am trying to merge these files on [""X"", ""Y""] to get a single file with all the columns. But, while doing so, I am getting <strong>MemoryError</strong> after merging 10 files.<br>Please help me to find a solution.<br>
Please find the below specifications:<br><br></p>

<pre><code>RAM: 504 GB
CPU: 160 Core
Python Version: 3.7.0
Pandas Version: 0.23.4
</code></pre>

<p>Sample Code:</p>

<pre><code>final_df = pd.DataFrame()
for f in file_list:
    df = pd.read_csv(f)
    if final_df.empty:
        final_df = df
    else:
        final_df = final_df.merge(df, on = [""X"",""Y""], how = ""left"")
return final_df
</code></pre>
","6676953","","11607986","","2019-07-08 09:07:49","2019-07-08 10:10:43","Merge operation is occupying full RAM","<python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"57158491","1","57159132","","2019-07-23 07:06:46","","0","34","<pre><code>x   y   z   amount  absolute_amount
121 abc def  500       500
131 fgh xyz -800       800
121 abc xyz  900       900
131 fgh ijk  800       800
141 obc pqr  500       500
151 mbr pqr -500       500
141 obc pqr -500       500
151 mbr pqr  900       900
</code></pre>

<p>I need to find the duplicate rows in the dataset where the x and y are same, with conditions being-</p>

<blockquote>
  <ol>
  <li>sum(amount) !=0 </li>
  <li>abs(sum(amount)) != absolute_amount</li>
  </ol>
</blockquote>

<p>I tried grouping them and the code i used in R is working but i need it to work in python</p>

<pre><code>logic1 &lt;- tablename %&gt;%
group_by('x','y')%&gt;%
filter(n()&gt;1 &amp;&amp; sum(`amount`) != 0 &amp;&amp; abs(sum(`amount`)) != absolute_amount)
</code></pre>

<p>Expected output             </p>

<pre><code>x   y   z   amount  absolute_amount
121 abc def  500       500
121 abc xyz  900       900
151 mbr pqr -500       500
151 mbr pqr  900       900
</code></pre>
","9168092","","8123710","","2019-07-23 07:15:20","2019-07-23 07:45:18","Please suggest approaches and code to solve the defined problem statement","<python-3.x><pandas><group-by>","1","1","","","","CC BY-SA 4.0","1"
"56952499","1","56952549","","2019-07-09 12:21:57","","1","34","<p>I have got a dataframe of several hundred thousand rows. Which is of the following format:</p>

<pre><code>   time_elapsed  cycle
0          0.00      1
1          0.50      1
2          1.00      1
3          1.30      1
4          1.50      1
5          0.00      2
6          0.75      2
7          1.50      2
8          3.00      2
</code></pre>

<p>I want to create a third column that will give me the percentage of each time instance that the row is of the cycle (until the next time_elapsed = 0). To give something like:</p>

<pre><code>   time_elapsed  cycle  percentage
0          0.00      1           0
1          0.50      1          33
2          1.00      1          75
3          1.30      1          87
4          1.50      1         100
5          0.00      2           0
6          0.75      2          25
7          1.50      2          50
8          3.00      2         100
</code></pre>

<p>I'm not fussed about the number of decimal places, I've just excluded them for ease here.</p>

<p>I started going along this route, but I keep getting errors.</p>

<pre><code>data['percentage'] = data['time_elapsed'].sub(data.groupby(['cycle'])['time_elapsed'].transform(lambda x: x*100/data['time_elapsed'].max()))
</code></pre>

<p>I think it's the lambda function causing errors, but I'm not sure what I should do to change it. Any help is much appreciated :)</p>
","11196704","","","","","2019-07-09 12:25:29","Add a column of normalised values based on sections of a dataframe column","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57507244","1","57507393","","2019-08-15 08:59:03","","1","34","<p>Given a df</p>

<pre><code>session article_id  article_type    primary_section
1        nan        nan             nan
1        123        magazine        sport
1        125        tech            laptops
2        126        food            asian_food
2        127        food            euro_food
</code></pre>

<p>I want to groupby by session_id and get the article_type, primary_section and page in session based on the result of ""first"" function on article_id
Output:</p>

<pre><code>session first_article_id    firt_article_type   primary_section page_in_sess
1        123                magazine            sport           2
2        126                food                asian_food      1
</code></pre>
","10377244","","","","","2019-08-15 09:12:10","Get values from different column in a groupby based on result","<python-3.x><pandas><pandas-groupby>","1","0","","","","CC BY-SA 4.0","1"
"56850398","1","56852369","","2019-07-02 10:27:02","","0","34","<p>I wish to use this sql query using pure pandas:</p>

<pre><code>select *
from A
where Id is not in (select id from B)
</code></pre>

<p>Is there something using pandas?
Any alternative suggestion?</p>
","10061482","","","","","2019-07-02 12:22:04","Sql in Pandas ""where id not in (select id from A)"" is possible?","<python-3.x><pandas>","1","3","","","","CC BY-SA 4.0","1"
"49330175","1","49330200","","2018-03-16 22:09:40","","1","34","<p>I've got a series of loops that calculate sales for different brands, different periods from different dataframes.
And here is the part of the code I want to optimize:</p>

<pre><code>#shname,dep,lname,y - come from itterables

var=shname+dep+'ytd'+lname
if dep == 'PY': ytd=data.loc[(Data.Brand==lname) &amp; (Data['PY_YTD?']==True),['Sales']].sum()
if dep == 'NY': ytd=data.loc[(Data.Brand==lname) &amp; (Data['NY_YTD?']==True),['Sales']].sum()
exec('%s=%d' % (var,ytd))
</code></pre>

<p>as you see there is a column name that happens to be different in dataframes depending on report year (PY - present, NY- next):</p>

<pre><code>Data['PY_YTD?']==True
Data['NY_YTD?']==True
</code></pre>

<p>so I simply used IF. However I wonder if there is a way to use some kind of function that calculates the column name based on currently iterated value</p>

<p>I imagine it should look something like this:</p>

<pre><code>def YEAR (y):
    if y = 'PY': return Data['PY_YTD?']==True
    if y = 'NY': return Data['NY_YTD?']==True

ytd=data.loc[(Data.Brand==lname) &amp; (YEAR(var)),['Sales']].sum()s
</code></pre>

<p>would appreciate your help</p>
","7796147","","","","","2018-03-16 22:12:10","use function or var to create a data frame column/index name","<python><python-3.x><function><pandas>","1","0","1","","","CC BY-SA 3.0","1"
"57420058","1","57420598","","2019-08-08 20:22:25","","0","34","<p>I have this line of code:</p>

<pre><code>df['datetime']= pd.to_datetime(df['date'])+ pd.to_timedelta(df['hour']/100,unit='hour')
</code></pre>

<p>and I am getting this error: </p>

<pre><code>invalid timedelta unit hour provided
</code></pre>

<p>the strange things are that it works on another PC but not in this PC.</p>
","654019","","","","","2019-08-08 21:25:34","Error from python panda timedelta function","<python-3.x><pandas>","1","4","","","","CC BY-SA 4.0","1"
"48953433","1","","","2018-02-23 17:40:17","","0","34","<p>I have a pandas datetime like this:</p>

<pre><code>col1   col2        col3   col4
aa     30-11-2017  ba     01-11-2017
aa     30-11-2017  bb     10-11-2017
ab     25-11-2017  bc     20-11-2017
ac     01-12-2017  bc     30-11-2017
</code></pre>

<p>My expected output is:</p>

<pre><code>col1   ba   bb   bc
aa     29   20   0
ab     0    0    5
ac     0    0    1
</code></pre>

<p>The logic is:
For each unique value in <code>col1</code>, compute the date difference (in days) between <code>col2</code> and <code>col4</code>, then assign it to the right column based on the unique value in <code>col3</code></p>

<p>My initial approach to this is a combination of <code>pd.groupby()</code> and <code>pd.assign()</code>
But can't seem to work it out. Appreciate any help.</p>
","5236124","","","","","2018-02-23 18:01:29","pandas transform dataframe and compute datetimedelta","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"57615224","1","","","2019-08-22 18:29:37","","0","33","<p>I have a dataframe <code>input_data</code>. I can't figure out how to select an individual row, based on two conditions. My dataframe looks as follows:</p>

<pre><code>        Date   Open   High    Low  Close     Volume  Name
0 2006-01-03  77.76  79.35  77.24  79.11    3117200   ABC
1 2006-01-03  51.70  52.58  51.05  52.58    7825700   DEF
</code></pre>

<p>I can select by date alone, or name alone, but get errors with: </p>

<pre><code>print(input_data.loc[(input_data['Date'] == pd.to_datetime('2006-01-03 00:00:00'))and (input_data['Name'] == 'ABC')]). 
</code></pre>

<p>I've tried to mimic the code from <a href=""https://stackoverflow.com/questions/50890844/select-individual-rows-from-multiindex-pandas-dataframe"">here</a>, <a href=""https://stackoverflow.com/questions/15315452/selecting-with-complex-criteria-from-pandas-dataframe"">here</a> and <a href=""https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/"" rel=""nofollow noreferrer"">here</a> but no luck. </p>

<pre><code>print(input_data.loc[input_data['Date'] == pd.to_datetime('2006-01-03 00:00:00')]) 
print(input_data.loc[input_data['Name'] == 'ABC'])

# Below line doesn't work
print(input_data.loc[(input_data['Date'] == pd.to_datetime('2006-01-03 00:00:00'))and (input_data['Name'] == 'MMM')]) 
</code></pre>

<p>Using the date and 'ABC' I want to end up with:</p>

<pre><code>0 2006-01-03  77.76  79.35  77.24  79.11    3117200   ABC
</code></pre>

<p>Simple answers that I can understand are preferred.</p>
","8519006","","5770501","","2019-08-22 18:31:41","2019-08-22 18:31:41","What syntax do I need to select an individual row in a Dataframe?","<python><python-3.x><pandas><dataframe>","0","3","","2019-08-22 18:33:37","","CC BY-SA 4.0","1"
"57296560","1","57296971","","2019-07-31 18:00:34","","2","33","<p>I’m very new to python and am trying really hard these last few days on how to go through a df row by row, and check each row that has a difference between columns dQ and dCQ. I just said != 0 since there could be a pos or neg value. Now if this is true, I would like to check in another table whether certain criteria are met. I'm used to working in R, where I could store the df into a variable and call upon the column name, I can't seem to find a way to do it in python. I posted all of the code I’ve been playing with. I know this is messy, but any help would be appreciated. Thank you!
I've tried installing different packages that wouldn't work, I tried making a for loop (I failed miserably), maybe a function? I’m not sure where to even look. I've never learned Python, I’m really doing my best watching videos online and reading on here.</p>

<pre><code>import pyodbc
import PyMySQL
import pandas as pd
import numpy as np
conn = pyodbc.connect(""Driver={ODBC Driver 17 for SQL Server};""
                      ""Server=***-***-***.****.***.com;""
                      ""Database=****;""
                      ""Trusted_Connection=no;""
                      ""UID=***;""
                      ""PWD=***"")
# cur = conn.cursor()
# cur.execute(""SELECT TOP 1000 tr.dQ, po.dCQ, 
tr.dQ - po.dCQ as diff FROM [IP].[dbo]. 
[vT] tr (nolock) JOIN [IP].[dbo].[vP] po 
ON tr.vchAN = po.vchCustAN WHERE tr.dQ 
!= po.dCQ"")
# query = cur.fetchall()
query = ""SELECT TOP 100 tr.dQ, po.dCQ/*, tr.dQ - 
po.dCQ as diff */FROM [IP].[dbo].[vT] 
tr (nolock) INNER JOIN [IP].[dbo].[vP] po ON 
tr.vchAN = po.vchCustAN WHERE tr.dQ != 
po.dCQ""
df = pd.read_sql(query, conn)
#print(df[2,])

cursor = conn.cursor(PyMySQL.cursors.DictCursor)
cursor.execute(""SELECT TOP 100 tr.dQ, po.dCQ/*, 
tr.dQ - po.dCQ as diff */FROM [IP].[dbo]. 
[vT] tr (nolock) INNER JOIN [IP].[dbo]. 
[vP] po ON tr.vchAN = po.vchCustAN 
WHERE tr.dQ != po.dCQ"")
result_set = cursor.fetchall()
for row in result_set:
    print(""%s, %s"" % (row[""name""], row[""category""]))


# if df[3] != 0:
#     diff = df[1]-df[2]
#     print(diff)
# else:
#     exit



# cursor = conn.cursor()
# for row in cursor.fetchall():
#     print(row)
#
# for record in df:
#     if record[1] != record[2]:
#         print(record[3])
#     else:
#         record[3] = record[1]
#         print(record)

# df['diff'] = np.where(df['dQ'] != df[""dCQ""])
</code></pre>

<p>I expect some sort of notification that there's a difference in row xx, and now it will check in table vP to verify we received this data's details. I believe i can get to this point, if i can get the first part working. Any help is appreciated. I'm sorry if this question is not clear, i will do my best to answer any questions someone may have. Thank you!</p>
","11809758","","2144390","","2019-07-31 18:15:04","2019-07-31 18:32:23","How to check df rows that has a difference between 2 columns and then send it to another table to verify information","<sql-server><python-3.x><pandas><pyodbc>","1","2","","","","CC BY-SA 4.0","1"
"57696494","1","","","2019-08-28 16:17:30","","1","33","<p>I have a CSV file that contains users and their questions and answers for prescreening questions on a job requisition.  There are cases where a given question can have multiple answers.  Below is how the CSV currently looks:</p>

<pre><code>User,RequisitionID,Question,Answer
user1,190004116,6162,7296
user2,190004086,6115,7260
user2,190004086,6117,7264
user2,190004086,6117,7265
user2,190004086,6117,7268
user2,190004086,6117,7269
user3,190005321,6321,4221
user3,190005321,6321,4322
</code></pre>

<p>Instead of the above format, I would like to have Answer data represented in separate columns (with as many columns as there are distinct answers per Question) grouped by User, RequisitionID and Question, like so:</p>

<pre><code>User,RequisitionID,Question,Answer1,Answer2,Answer3,Answer4
user1,190004116,6162,7296,,,
user2,190004086,6115,7260,,,
user2,190004086,6117,7264,7265,7268,7269
user3,190005321,6321,4221,4321,
</code></pre>

<p>I've tried using ""groupby"" as below, but I'm just not able to get the resulting Dataframe in the shape I want...</p>

<pre><code>reqPrscrAnsFileFiltered = reqPrscrAnsFileFiltered.groupby(['User','RequisitionID','Question']).Answer.apply(list)
reqPrscrAnsFileFiltered = pandas.DataFrame(reqPrscrAnsFileFiltered.tolist(), index=reqPrscrAnsFileFiltered.index)
</code></pre>

<p>Sorry, but I'm new to Python and Pandas, so any help would be great.</p>

<p>Well - we're getting there.  I implemented the pivot_table code snippet you provided in your comment but I'm afraid I'm not sure what you mean about collapsing the Multiindex.  Without doing so, the Dataframe output is as follows:    </p>

<pre><code>Answer,Answer,Answer,Answer
1,2,3,4
7296.0,,,
7260.0,,,
7264.0,7265.0,7268.0,7269.0
4221.0,4322.0,,  
</code></pre>

<p>Can you help me understand how to make it look like the desired output above in my post?  Specifically, I want to remove the row with the answer index numbers (1,2,3,4) and then add back in the missing columns of User, RequisitionID and Question to the left of the answer columns.  Apologies if this is elementary stuff...</p>
","4289176","","4289176","","2019-08-28 20:14:11","2019-08-28 20:14:11","Need to convert dataframe row data to columns based on grouping of columns","<python-3.x><pandas>","0","7","","2019-08-28 16:23:18","","CC BY-SA 4.0","1"
"57418843","1","","","2019-08-08 18:37:20","","-1","33","<p>I have DF as:</p>

<pre><code>code_range   CCS    CCS_Label   icode
'0112-0115'  232    Anesthesia  0112, 0115
'0118-0120'  232    Anesthesia  0118, 0120 
</code></pre>

<p>I need to generate the numbers between two values of icode(column) and append those values to df keeping other column-values same. And delete the old rows. Somewhat like expanding rows.
My excepted solution:</p>

<pre><code>code_range   CCS    CCS_Label   icode
'0112-0115'  232    Anesthesia  0112
'0112-0115'  232    Anesthesia  0113
'0112-0115'  232    Anesthesia  0114
'0112-0115'  232    Anesthesia  0115
'0118-0120'  232    Anesthesia  0118
'0118-0120'  232    Anesthesia  0119
'0118-0120'  232    Anesthesia  0120 
</code></pre>

<p>On spending good amount of time also I am not able to find the right approach to the problem. Please help to resolve the problem.</p>

<p>Thanks in Advance..</p>
","10136674","","10136674","","2019-08-09 06:05:21","2019-08-09 06:05:21","How to generate numbers missing between two numbers(having in a single cell) and append the generated values as new rows to pandas DataFrame","<python-3.x><pandas><append><rows><between>","1","4","","","","CC BY-SA 4.0","1"
"57741661","1","57741689","","2019-08-31 21:20:14","","1","33","<p>I have a dataframe containing multiple features. In the same dataset the features are grouped by 'id' column from 1 to 10. I need to count <strong>the number of rows of 'vout' which are > 0.2</strong> for <strong>each group id</strong> and save it for every single id.</p>

<pre><code>    id    freq    zr         zi           z        vout    
1   1   4641.60 0.010534    -0.002541   0.010687    0.63490
2   1   2154.40 0.010787    -0.000516   0.010786    0.63471
3   1   1000.00 0.011431    0.000674    0.011366    0.63451
4   1   464.16  0.012167    0.001527    0.012232    0.63432
</code></pre>

<p>the result must be something like this:</p>

<pre><code>id 1 -&gt; numbRows = 1304
id 2 -&gt; numbRows = 3234
...
id 10 -&gt;numbRows = 223
</code></pre>

<p>Thanks in advice.</p>
","11931010","","67579","","2020-07-22 10:42:19","2020-07-22 10:42:19","Counting how many rows satisfy a condition for each id","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"57420727","1","57421037","","2019-08-08 21:17:51","","0","33","<p>How can I use Pandas to do a loop and verify if the name in sheet1 is equal the name in sheet2?</p>
<p>After verify the name equal, the program should get a whole line of the sheet1 and sheet2 and save the new workbook.</p>
<p>The final result should a new workbook with union of the lines in each spreadsheet</p>
<p>I know how use Openpyxl, but I want to learn Pandas. In my researches I did not find answers.</p>
<h1>example sheet1:</h1>
<p><img src=""https://i.stack.imgur.com/8GtI6.jpg"" alt="""" /></p>
<h1>example sheet2:</h1>
<p><img src=""https://i.stack.imgur.com/JBLk1.jpg"" alt="""" /></p>
","10523522","","-1","","2020-06-20 09:12:55","2019-08-08 22:27:21","How can I use Pandas in Excel to get lines?","<python-3.x><pandas>","1","1","","2019-08-09 07:05:56","","CC BY-SA 4.0","1"
"49172988","1","","","2018-03-08 12:21:27","","0","33","<p>I've got some data out of the <a href=""https://getpocket.com/developer/docs/v3/retrieve"" rel=""nofollow noreferrer"">Pocket</a> API and the resulting JSON called <strong>list</strong> has some nested JSON within it. Sample below</p>

<pre><code>{'complete': 1,
 'error': None,
 'list': {'1992211110': {'authors': {'8683682': {'author_id': '8683682',
     'item_id': '1992211110',
     'name': 'Robert Kuttner',
     'url': 'http://www.nybooks.com/contributors/robert-kuttner/'}},
   'excerpt': 'What a splendid era this was going to be, with one remaining superpower spreading capitalism and liberal democracy around the world. Instead, democracy and capitalism seem increasingly incompatible.',
   'favorite': '0',
   'given_title': '',
   'given_url': 'http://nyrevinc.cmail20.com/t/y-l-klpdut-jduhlyklkl-d/',
   'has_image': '0',
   'has_video': '0',
   'is_article': '1',
   'is_index': '0',
   'item_id': '1992211110',
   'resolved_id': '1977788178',
   'resolved_title': 'The Man from Red Vienna',
   'resolved_url': 'http://www.nybooks.com/articles/2017/12/21/karl-polanyi-man-from-red-vienna/',
   'sort_id': 6,
   'status': '0',
   'time_added': '1520132694',
   'time_favorited': '0',
   'time_read': '0',
   'time_updated': '1520140351',
   'word_count': '4009'},
</code></pre>

<p>I've managed to get the whole results into a dataframe however there is some nesting of what looks like a dictionary called <strong>authors</strong>? I've managed to split this out into dictionaries with an index but can't figure out how to get that into a dataframe. Sample below of <strong>authors</strong>:</p>

<pre><code>{1: {'authors': {'8683682': {'author_id': '8683682',
    'item_id': '1992211110',
    'name': 'Robert Kuttner',
    'url': 'http://www.nybooks.com/contributors/robert-kuttner/'}}},
 2: {'authors': {'53525958': {'author_id': '53525958',
    'item_id': '2086463428',
    'name': 'Adam Tooze',
    'url': 'http://www.nybooks.com/contributors/adam-tooze/'}}},
 3: {'authors': {'3490600': {'author_id': '3490600',
    'item_id': '2090266893',
    'name': 'Adam Liaw',
    'url': ''}}},
 4: {'authors': {'75929933': {'author_id': '75929933',
    'item_id': '2091894678',
    'name': 'umair haque',
    'url': 'https://eand.co/@umairh'}}},
 5: {'authors': {'61177521': {'author_id': '61177521',
    'item_id': '2092663780',
    'name': 'Annalisa Merelli',
    'url': 'https://qz.com/author/amerelliqz/'}}},
 6: {'authors': {'52268529': {'author_id': '52268529',
    'item_id': '2092922221',
    'name': 'Aditya Chakrabortty',
    'url': 'https://www.theguardian.com/profile/adityachakrabortty'}}},
 7: {'authors': {'28083': {'author_id': '28083',
    'item_id': '2096294305',
    'name': 'Alana Semuels',
    'url': ''}}},
 8: {'authors': {'185472': {'author_id': '185472',
    'item_id': '2097100251',
    'name': 'TIM KREIDER',
    'url': ''}}},
 9: {'authors': {'2771923': {'author_id': '2771923',
    'item_id': '2098788948',
    'name': 'Richard Bernstein',
    'url': 'http://www.nybooks.com/contributors/richard-bernstein/'}}},
 10: {'authors': {'61111044': {'author_id': '61111044',
    'item_id': '2102383890',
    'name': 'Ephrat Livni',
    'url': 'https://qz.com/author/livniqz/'}}}}
</code></pre>

<p>Any help much appreciated, I am very new to python and pandas.</p>
","1509633","","","","","2018-03-09 09:47:37","Python: Converting dictionaries into pandas dataframe","<json><python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"57863805","1","","","2019-09-10 03:50:37","","0","33","<p>I'm trying to create an efficient MySQL query which includes one inner join, however it is taking a very long time to run. Currently, if I select * on both tables, and join them together using pd.merge in pandas, it is significantly quicker than running the inner join in MySQL.</p>

<pre class=""lang-py prettyprint-override""><code>q = '''
SELECT
    hr.date,
    hr.col2
    df.col3,
    df.col4
FROM
    table1 hr
INNER JOIN 
    table2 df ON
    DATE(df.date) = DATE(hr.date) AND
    df.name = hr.name
WHERE
    hr.date &gt;= ""2017-09-01"" AND
    LOWER(hr.state) IN ('nsw', 'vic', 'qld', 'wa', 'sa', 'tas', 'nt', 'act')
;
'''

data = pd.read_sql(q, con)

</code></pre>

<p>I have tried using indexes like suggested in other posts, but this hasn't helped. What am I doing wrong? Thanks</p>
","8903934","","","","","2019-09-10 03:57:07","Why does my inner join take longer in MySQL than selecting both tables and joining in Python?","<mysql><sql><python-3.x><pandas><pandasql>","0","2","","","","CC BY-SA 4.0","1"
"57861437","1","57861618","","2019-09-09 21:29:45","","1","33","<p>I'm trying to sum the values in count column based on place column and after that append values in sum column. </p>

<p>Output so far:</p>

<pre><code>         lat       lon       place  predict  count
0  51.375339 -0.390005      London        0     15
1  53.362981 -2.929597   Liverpool        0      2
2  51.375339 -0.390005      London        1      2
3  53.426872 -2.280006  Manchester        0      1
</code></pre>

<p>Desired Output:</p>

<pre><code>         lat       lon       place  predict  count  Sum
0  51.375339 -0.390005      London        0     15   17
1  53.362981 -2.929597   Liverpool        0      2    2
2  51.375339 -0.390005      London        1      2   17
3  53.426872 -2.280006  Manchester        0      1    1
</code></pre>

<p>Any help will be highly appreciated.</p>
","10450242","","10450242","","2019-09-10 09:05:07","2019-09-10 09:05:07","Sum a column based on another column and append that column in dataframe","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57701260","1","57701295","","2019-08-28 23:41:45","","2","33","<p>I have the following dataframe:</p>

<pre><code>                      hour  spike  spike_count
date_time           
2014-11-22 00:00:00     0     0     0
2014-11-22 01:00:00     1     1     0
2014-11-22 02:00:00     2     1     0
2014-11-22 03:00:00     3     1     0
2014-11-22 04:00:00     4     0     0
2014-11-22 05:00:00     5     0     0
2014-11-22 06:00:00     6     0     0
2014-11-22 07:00:00     7     0     0
2014-11-22 08:00:00     8     1     0
2014-11-22 09:00:00     9     0     0
2014-11-22 10:00:00     10    0     0
2014-11-22 11:00:00     11    1     0
2014-11-22 12:00:00     12    0     0
2014-11-22 13:00:00     13    0     0
2014-11-22 14:00:00     14    1     0
2014-11-22 15:00:00     15    0     0
2014-11-22 16:00:00     16    0     0
2014-11-22 17:00:00     17    0     0
2014-11-22 18:00:00     18    0     0
2014-11-22 19:00:00     19    1     0
2014-11-22 20:00:00     20    0     0
2014-11-22 21:00:00     21    0     0
2014-11-22 22:00:00     22    0     0
2014-11-22 23:00:00     23    1     0
</code></pre>

<p>I'd like to aggregate the number of spike into the spike_count column per hour (where the hour column is the hours on a 24 hr format). So my expected output would look like this:</p>

<pre><code>                      hour  spike  spike_count
date_time           
2014-11-22 00:00:00     0     0     0
2014-11-22 01:00:00     1     1     1
2014-11-22 02:00:00     2     1     2
2014-11-22 03:00:00     3     1     3
2014-11-22 04:00:00     4     0     0
2014-11-22 05:00:00     5     0     0
2014-11-22 06:00:00     6     0     0
2014-11-22 07:00:00     7     0     0
2014-11-22 08:00:00     8     1     4
2014-11-22 09:00:00     9     0     0
2014-11-22 10:00:00     10    0     0
2014-11-22 11:00:00     11    1     5
2014-11-22 12:00:00     12    0     0
2014-11-22 13:00:00     13    0     0
2014-11-22 14:00:00     14    1     6
2014-11-22 15:00:00     15    0     0
2014-11-22 16:00:00     16    0     0
2014-11-22 17:00:00     17    0     0
2014-11-22 18:00:00     18    0     0
2014-11-22 19:00:00     19    1     7
2014-11-22 20:00:00     20    0     0
2014-11-22 21:00:00     21    0     0
2014-11-22 22:00:00     22    0     0
2014-11-22 23:00:00     23    1     8
</code></pre>

<p>I don't know where to start or how to go about solving this. Any suggestions?</p>
","9431573","","","","","2019-08-28 23:48:41","Hourly aggregation of counts pandas","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"56851409","1","56851946","","2019-07-02 11:25:51","","0","33","<p>I have a table having duplicate rows in consecutive rows. Row having same 'id' should have duplicate data in other columns.But there are few rows in which data is not proper. Eg - </p>

<pre><code>id  Name    Age
1   Ram     12
1   Ram     10
2   Shyam   11
2   Yam     11
3   Ravi    23
3   Ravi    23
4   Harsh   34
4   Harsh   34
</code></pre>

<p>I need to know the columns in which the columns differ for consecutive rows.</p>

<p>Final output I need - </p>

<pre><code>id  Name    Age     DifferentColumn
1   Ram     12      
1   Ram     10      Age
2   Shyam   11
2   Yam     11      Name
3   Ravi    23 
3   Ravi    23
4   Harsh   34
4   Krish   54      Name,Age
</code></pre>

<p>I can use 'petl' or 'pandas' for this but what should be my approach?</p>
","4853331","","4853331","","2019-07-02 11:50:03","2019-07-02 12:03:58","Get list of columns in which data differs for consecutive rows","<python-3.x><pandas><dataframe><petl>","1","4","","","","CC BY-SA 4.0","1"
"57049510","1","57049605","","2019-07-16 03:08:22","","0","33","<p>I have CSV files inside the Region folder.</p>

<p>How to read only Sales files from the Region folder with Python Pandas?</p>

<p>I searched but did not find answer.</p>

<p>Please help me.</p>

<p>Thanks in advance.</p>

<pre><code>USA_Sales_20190716.csv
USA_Sales_20190715.csv
USA_Sales_20190714.csv
USA_Budget_20190716.csv
USA_Budget_20190715.csv
USA_Budget_20190714.csv

UK_Sales_20190716.csv
UK_Sales_20190715.csv
UK_Sales_20190714.csv
UK_Budget_20190716.csv
UK_Budget_20190715.csv
UK_Budget_20190714.csv

GER_Sales_20190716.csv
GER_Sales_20190715.csv
GER_Sales_20190714.csv
GER_Budget_20190716.csv
GER_Budget_20190715.csv
GER_Budget_20190714.csv
</code></pre>
","10808871","","","","","2019-07-16 03:25:27","How to read only Sales files from the Region folder?","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57653030","1","","","2019-08-26 06:51:33","","1","33","<p>I have a dataframe df. I want to set value of column A equal to 1 for all those rows where column i of dataframe is equal to 1 of this value in the list['S','T', 'U'].
I was doing something like this but it is not working. It is reducing my dataframe size.</p>

<pre><code>df.loc[df['A'] == 'S', 'i'] = 1
df.loc[df['A'] == 'T', 'i'] = 1
df.loc[df['A'] == 'U', 'i'] = 1
</code></pre>

<p>I also want to create new column c in dataframe df which has value equal to</p>

<pre><code>df['c'] = df['b']*df['a'] where a is not NULL
df['c'] = df['b'] where a is NULL
</code></pre>

<p>Can someone suggest an efficient way to do this.</p>
","11882601","","11882601","","2019-08-26 07:32:58","2019-08-26 09:04:03","how to make change value of one column based on on other column in dataframe","<python-3.x><pandas><loc>","0","3","","2019-08-26 09:02:11","","CC BY-SA 4.0","1"
"57160781","1","","","2019-07-23 09:19:53","","-1","32","<p>When I load a csv file, I receive a ""<code>KeyError</code>"" when setting the index. I see that the column names are not loaded as desired. For example:</p>

<pre><code>   cleaned_sql = cleaned_sql.set_index('a')
print(cleaned_sql.columns.tolist())
</code></pre>

<p>Looks like so (desired):</p>

<pre><code> ['a', 'b', 'c', 'd']
</code></pre>

<p>And the code not okay:</p>

<pre><code>col_names = pd.read_csv(in_file, nrows=0).columns
print(col_names)
</code></pre>

<p>Looks like so (NOT desired):</p>

<pre><code>Index(['""e"",""f"",""g""'])
</code></pre>

<p>The csv column row:</p>

<pre><code>""e"",""f"",""g""
</code></pre>

<p>Do I need to set another attribute in read_csv file? It's here I suspect the problem is. As you see from the print statement the result is wrapped in the list, in single quotes (so all 3 e,f,g are in a string together).</p>
","5257286","","5257286","","2019-07-23 09:59:31","2019-07-23 11:00:29","read_csv file loads column names as one long string","<python-3.x><pandas><csv>","1","4","","","","CC BY-SA 4.0","1"
"41718745","1","","","2017-01-18 11:53:13","","0","32","<p>I have a dataframe df</p>

<pre><code>                   c1     c2  
name       sample
person1    a1      aaa    AAA  
           b1      bbb    BBB  
           c1      ccc    CCC  
person2    d1      ...  
</code></pre>

<p>I want to apply any function to all index (level 1) values. Say, just to give an example, remove the last letter from the index (they are strings), by this function:</p>

<pre><code>def remove_last_letter(x)
    x = x[:-1]
    return x
</code></pre>

<p>to get this:</p>

<pre><code>                   c1     c2  
name       sample
person1    a       aaa    AAA  
           b       bbb    BBB  
           c       ccc    CCC  
person2    d       ...  
</code></pre>

<p>How to call such a function?</p>
","6592934","","6592934","","2017-01-18 13:53:09","2017-01-18 13:53:09","How to apply function to pandas index value","<python-3.x><pandas>","0","3","","2017-01-18 11:56:16","","CC BY-SA 3.0","1"
"57510795","1","57510836","","2019-08-15 13:57:28","","1","32","<p>I have tried extracting individual id's from the big data frame bining the price range count &amp; compute mean. Couldn't get a way to access price range from new_df for computing mean of bins, even tried to split and stack the price range but still could not access price range. Below is my code. Could someone please suggest?</p>

<pre><code>Sample data frame

Id          price    price_range                    
11111333    30.0    (0.0, 50.0]
11111333    34.0    (0.0, 50.0]
11111333    80.0    (50.0, 100.0]
11111333    25.0    (0.0, 50.0]
11111333    13.0    (0.0, 50.0]
11111333    17.0    (0.0, 50.0]
11111333    42.0    (0.0, 50.0]
11111333    20.0    (0.0, 50.0]
11111333    210.0   (200.0, 250.0]
22222111    30.0    (0.0, 50.0]
22222111    134.0   (100.0, 150.0]
22222111    1080.0  (1050.0, 1100.0]
22222111    25.0    (0.0, 50.0]
22222111    413.0   (400.0, 450.0]
22222111    117.0   (100.0, 150.0]
22222111    12.0    (0.0, 50.0]
22222111    60.0    (50.0, 100.0]
22222111    110.0   (100.0, 150.0]
</code></pre>

<pre><code>#generate bin range
x_range=np.arange(0,df[""Volume""].max()+50,50) 

#add new column price_range with values
df[""price_range""]=pd.cut(df[""Volume""],bins=x_range)

#get value counts of price 
new_df[""range_cnt""]=pd.DataFrame(df[""price_range""].value_counts())

new_df          
            range_cnt
(0.0, 50.0]     7
(50.0, 100.0]   1
(200.0, 250.0]  1

#split price range_cnt
out=new_df[""range_cnt""].str.split(',\s+', expand=True).stack()

(0.0, 50.0]    0    7
(50.0, 100.0]  0    1
(200.0, 250.0] 0    1

dtype: object

#When i try to access first row,could get only 7,instead of (0.0, 50.0]
out[1]
0    7
dtype: object
</code></pre>

<pre><code>Below is the expected format
Id          price_range         count   mean            
11111333    (0.0, 50.0]         7       25       
            (50.0, 100.0]       1       75
            (200.0, 250.0]      1       225

22222111    (0.0, 50.0]         3       25
            (50.0, 100.0]       1       75
            (100.0, 150.0]      3       125
            (400.0, 450.0]      1       425
            (1050.0, 1100.0]    1       1075
</code></pre>
","9433868","","","","","2019-08-15 14:00:00","How to split numerics bins and find mean of bins","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"56637289","1","","","2019-06-17 18:54:55","","0","32","<p>Full disclosure - I am a newbie so please be patient with me.
I have a data file. I need to sort by the zip_code column first - then I need to calculate the highest score per zip code.</p>

<pre><code>Fname  Lname  Area  Score
Amy    Doe    3    245
Jon    Doe    1    310
Jane   Doe    2    724
Brian  Doe    1    840
Gary   Doe    3    632
Jen    Doe    2   854
Jim    Doe    3   132
Rick   Doe    1   445
</code></pre>

<h1>My code:</h1>

<pre><code> import pandas as pd
 from pandas import DataFrame, pandas as pd

 file = pd.read_csv('test.dat',delimiter=',' )
 df = DataFrame(file, columns=['Fname','Lname','Score','zip_code'])
 df.sort_values(by=['Area','Score'], inplace=True)
 print(df)
</code></pre>

<hr>

<h2>The desired output would be something to this effect:</h2>

<pre><code>Fname  Lname  Area  Score
Brian  Doe    1   840--&gt;Winner!
Rick   Doe    1   445
Jon    Doe    1   310
Jen    Doe    2   854--&gt;Winner!
Jane   Doe    2   132
Gary   Doe    3   632--&gt;Winner!
Jim    Doe    3   132
Rick   Doe    3   445
</code></pre>

<hr>

<h2>This is what I get it:</h2>

<pre><code>    Fname  Lname  Score  Area
0     NaN    NaN    NaN   NaN
1     NaN    NaN    NaN   NaN
2     NaN    NaN    NaN   NaN
3     NaN    NaN    NaN   NaN
</code></pre>

<hr>

<p>I have not figured out how to sum up the column yet.
Can you please tell me what I am doing wrong?</p>
","11660039","","11660039","","2019-06-17 22:50:30","2019-06-17 22:50:30","Sort columns from a data file and calculate results","<python><python-3.x><pandas><sorting><pandas-groupby>","1","2","","2019-06-25 14:58:17","","CC BY-SA 4.0","1"
"57865616","1","57865686","","2019-09-10 07:04:27","","1","32","<p>I have two different dataframes, i.e.,</p>

<pre><code>firstDF = pd.DataFrame([{'mac':1,'location':['kitchen']}])
predictedDF = pd.DataFrame([{'mac':1,'location':['lab']}])
</code></pre>

<p>If the mac column value of predictedDF contains in mac column value of firstDF , then location column value of firstDF should extend the location column of predictedDF and the result of firstDF should be,</p>

<pre><code>firstDF
      mac    location
0     1      ['kitchen','lab']
</code></pre>

<p>I have tried with,</p>

<pre><code>firstDF.loc[firstDF['mac'] == predictedDF['mac'], 'mac'] = firstDF.loc[firstDF['location'].extend(predictedDF['location']), 'location']
</code></pre>

<p>Whereas the same returns,</p>

<pre><code>AttributeError: 'Series' object has no attribute 'extend'
</code></pre>
","5391663","","","","","2019-09-10 07:40:39","Python Pandas : Extend operation of a column if a condition matches","<python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"56855173","1","","","2019-07-02 14:55:19","","0","32","<p>I am trying to create sentences summarizing multiple countries stats using a World Bank file. I would like to use the data in the dataframe to create the sentences. </p>

<pre><code>df = pd.DataFrame({'country':['United States','Canada','Mexico'],'GDP':[2.9, 1.9, 2.0],'CPI':[2.4, 2.3, 4.9]}

def risky(val):
    if val &lt; 2.2:
        return ""at a high risk""
    else:
        return ""not at a high risk""

for i in df.columns.values().tolist():
    df[i] = df[i].apply(risky)

def text_app(country, ind):
    line1 = df[""country""].map(str) + "" is "" df[""GDP""].map(str) + "" of 
    defaulting.""`
</code></pre>

<p>I keep receiving the following error:</p>

<pre><code>""TypeError: can only concatenate str (not ""method"") to str""
</code></pre>

<p>Output from df[i].apply(risky) is as expected:</p>

<pre><code>country|    GDP|  CPI
United States|  not at a high risk| not at a high risk
Canada| at a high risk| not at a high risk
Mexico| at a high risk| not at a high risk
</code></pre>

<p>Expected Output is:</p>

<pre><code>""Canada is at a high risk of defaulting.""
""United States is not at a high risk of defaulting.""
""Mexico is at a high risk of defaulting.""
</code></pre>
","11729425","","4909087","","2019-07-02 15:31:32","2019-07-02 15:31:32","Translate Data from Multiple Dataframe Columns to Create A Sentence","<python-3.x><pandas><dataframe>","0","5","","","","CC BY-SA 4.0","1"
"58322623","1","58322798","","2019-10-10 12:11:31","","0","32","<p>I have a problem concerning comparing the values of two dataframes on a per day basis.
The dataframes contain df1 = minimum temperature values per day and df2 = maximum temperature values per day.</p>

<p>The dfs look like this (TS_TIMESTAMP is the index column):</p>

<p>df1:</p>

<pre><code>&gt; TS_TIMESTAMP              Date        TREND_VALUE             
&gt; 2019-04-03 18:48:10.970  2019-04-02   8.340000        
&gt; 2019-04-04 16:49:23.320  2019-04-03   7.840000           
&gt; 2019-04-05 13:19:33.550  2019-04-04   7.480000 
</code></pre>

<p>df2:</p>

<pre><code>&gt; TS_TIMESTAMP              Date        TREND_VALUE             
&gt; 2019-04-03 18:48:10.970  2019-04-02   19.340000        
&gt; 2019-04-04 16:49:23.320  2019-04-03   18.840000          
&gt; 2019-04-05 13:19:33.550  2019-04-04   18.480000   
</code></pre>

<p>I would like to calculate the difference between max_value and min_value per day with a function (to simply run the calculation with a number of different files).</p>

<p>This is what I came up with:</p>

<pre><code>def temp_diff (df1, df2):
    for row in df1, df2:
        if df1.Date == df2.Date:
            print (df2.TREND_VALUE - df1.TREND_VALUE)
</code></pre>

<p>if I run this function I get this Error Message for the if-statement: 
<strong>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</strong></p>

<p>I'm not sure how to change my def appropriately.</p>

<p>Thanks for your help!</p>
","11129825","","11129825","","2019-10-10 12:19:42","2019-10-10 14:57:00","Python Error: truth value of Series is ambigious in if-statement","<python-3.x><pandas><for-loop><if-statement>","2","0","","","","CC BY-SA 4.0","1"
"48799632","1","48799676","","2018-02-15 03:41:02","","1","32","<p>I have a data frame containing toy sales numbers and I'm looking for help on two things:</p>

<ol>
<li><p>Is it possible to combine the two line of code below into one. </p></li>
<li><p>Is it possible to print out entire row base on the index_num that is outputted? </p></li>
</ol>

<hr>

<pre><code>max = Toy['salenum'].max()
index_num = Toy[Toy['inv_change'] == max].index.tolist()
</code></pre>
","3534551","","7311767","","2018-02-15 03:54:33","2018-02-15 03:54:33","output row base on index","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"48572909","1","48575057","","2018-02-01 22:38:11","","0","32","<p>I have a pandas time series which contains cumulative monthly values.</p>

<p>If in a month on a certain date, the value is less than a certain number, I keep the first one and set everything through rest of month to 1000.</p>

<p>E.g.</p>

<pre><code>df:

 Date       cummulative_value
1/8/2017    -3
1/9/2017    -6
1/10/2017   -72
1/11/2017   500
1/26/2017   575
2/7/2017    -5
2/14/2017   -6
2/21/2017   -6
</code></pre>

<p>My cutoff value is -71 so in above example I need to achieve the following:</p>

<pre><code> Date       cummulative_value
1/8/2017    -3
1/9/2017    -6
1/10/2017   -72
1/11/2017   1000
1/26/2017   1000
2/7/2017    -5
2/14/2017   -6
2/21/2017   -6
</code></pre>

<p>On <code>1/10/2017   -72</code> the cumulative value was lower than -71 so we keep it but every value for rest of Jan 2017 is now set to 1000.</p>

<p>This <a href=""https://stackoverflow.com/questions/48571951/setting-cumulative-values-to-constant-after-it-reaches-threshold"">solution</a> sets all values to 1000 when condition is met. I need to keep the first value.</p>
","1243255","","","","","2018-02-02 02:58:35","Setting cumulative values to constant after it reaches threshold and preserving first value","<python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"57100361","1","","","2019-07-18 18:06:52","","0","32","<p>I have written the following function for a program that is supposed to search through an excel file and manipulate data frames, but the function is insanely slow and I am not sure how to make it more efficient. is there another way to iterate through excel sheets that works better than this?</p>

<pre><code>def read_masterfile(masterfile_path):
sheets_dict = pd.ExcelFile(masterfile_path).sheet_names
for sheet in sheets_dict:
    df = pd.read_excel(masterfile_path, sheet_name = sheet)
    print(sheet)
    print(df.columns)

user_input= input() 
masterfile_dir = (r""C:\Users\path\Desktop\July15\masterfile.xlsx"")
if user_input == 'y': 
   calculated = read_masterfile(masterfile_dir)
</code></pre>
","11777128","","11777128","","2019-07-18 18:19:15","2019-07-18 18:49:29","Making my function that iterates through excel sheets more efficient","<python-3.x><pandas><performance><for-loop>","1","0","","","","CC BY-SA 4.0","1"
"57197422","1","57197726","","2019-07-25 08:23:30","","0","31","<p>Given a dictionary: </p>

<pre><code>{0: 'Hello', 1: 'Bob', 2: '!', 3: 'Hello', 4: 'Bob', 5: '?', 6: 'Hello', 7: 'Bob', 8: '!', 9: 'Hello', 10: 'Jane', 11: '!', 12: 'Hello', 13: 'Rob', 14: '?'}
</code></pre>

<p>I wanted to extract these texts, inclusively, in between the pairs 'Hello' and '!' and ignore pairs 'Hello' and '?' but I am currently stuck. I just need a nudge in the right direction will do. </p>

<p>So far I have attempted to slice them and got the following result: </p>

<pre><code>{0: 'Hello', 2: '!', 3: 'Hello', 5: '?', 6: 'Hello', 8: '!', 9: 'Hello', 11: '!', 12: 'Hello', 14: '?'}
</code></pre>

<p>but now I have no clue how to pair the values. For example pairs 0,2 then pairs 7,8 then finally 9,11.</p>

<p>Here is the method I made</p>

<pre><code>def split(array):
    print(array)
    names = {'Hello','?','!'}
    lst = {key:value for key, value in array.items() if value in names}
    print(lst)
</code></pre>
","11834821","","11834821","","2019-07-25 08:24:13","2019-07-25 08:39:00","Is there a way/hint to extract dictionary in between in a given condition?","<python-3.x><pandas><text-extraction>","1","4","","","","CC BY-SA 4.0","1"
"57420489","1","57421433","","2019-08-08 20:57:04","","1","31","<p>I had a time series dataframe like below, but the records are not completed for each month. I would like to replicate every records with it's latest status and number till the last month.</p>

<pre><code>   Month      Client  Status     Revenue
0   2019-03-01     A      A           100
1   2019-04-01     A      T           null
2   2019-03-01     B      A           200
3   2019-05-01     B      A           200
4   2019-06-01     B      T           null
5   2019-03-01     C      A           150
6   2019-04-01     C      A           200
7   2019-05-01     C      T           null
8   2019-06-01     C      T           null           
</code></pre>

<p>The expected output would like below, thanks!</p>

<pre><code>     Month      Client  Status     Revenue 
0   2019-03-01     A      A           100 
1   2019-04-01     A      T           100 
2   2019-05-01     A      T           100 
3   2019-06-01     A      T           100 
4   2019-03-01     B      A           200 
5   2019-04-01     B      A           200 
6   2019-05-01     B      A           200 
7   2019-06-01     B      T           200 
8   2019-03-01     C      A           150 
9   2019-04-01     C      A           200 
10  2019-05-01     C      T           200 
11  2019-06-01     C      T           200
</code></pre>
","11729738","","","","","2019-08-08 22:50:21","Find the missing record and replicate it on a monthly basis","<python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"49514350","1","49514388","","2018-03-27 13:34:04","","-2","31","<p>Please help me on this, this should return value of Dollars where the insert is standard</p>

<p>'<a href=""https://i.stack.imgur.com/udVJP.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/udVJP.jpg"" alt=""enter image description here""></a>'</p>
","5894276","","5894276","","2018-03-27 13:35:12","2018-03-27 13:41:28","Grouping another col'n value from one col'n value","<python><python-3.x><pandas>","1","3","","2018-03-27 22:36:56","","CC BY-SA 3.0","1"
"56907470","1","","","2019-07-05 17:56:05","","0","31","<p>I have been playing with python and understanding the concept of copying a dataframe through the .copy function as opposed to just reassigning it to a variable.</p>

<p>Let's say we have the following data frame:
dfx:</p>

<pre><code>   Name        Score1   Score2  Score3        Score4
0  Jack            10  Perfect      10       Perfect
1  Jill            10       10      10  Not Finished
2  Jane            20       10      10             5
3   Tom  Not Finished       15      10             5

dfx2 = dfx.drop(""Score1"",axis = 1)
</code></pre>

<p>dfx2:</p>

<pre><code>   Name   Score2  Score3        Score4
0  Jack  Perfect      10       Perfect
1  Jill       10      10  Not Finished
2  Jane       10      10             5
3   Tom       15      10             5
</code></pre>

<p>running dfx again still returns the original dataframe</p>

<pre><code>   Name        Score1   Score2  Score3        Score4
0  Jack            10  Perfect      10       Perfect
1  Jill            10       10      10  Not Finished
2  Jane            20       10      10             5
3   Tom  Not Finished       15      10             5
</code></pre>

<p>Shouldn't the reassignment cause the column ""Score1"" be dropped from the original dataset as well?</p>

<p>However, running the following:</p>

<pre><code>dfx3 = dfx

dfx3

   Name        Score1   Score2  Score3        Score4
0  Jack            10  Perfect      10       Perfect
1  Jill            10       10      10  Not Finished
2  Jane            20       10      10             5
3   Tom  Not Finished       15      10             5

dfx3.loc[0,""Score4""] = ""BAD""

dfx3

   Name        Score1   Score2  Score3        Score4
0  Jack            10  Perfect      10           BAD
1  Jill            10       10      10  Not Finished
2  Jane            20       10      10             5
3   Tom  Not Finished       15      10             5

dfx
   Name        Score1   Score2  Score3        Score4
0  Jack            10  Perfect      10           BAD
1  Jill            10       10      10  Not Finished
2  Jane            20       10      10             5
3   Tom  Not Finished       15      10             5
</code></pre>

<p>does cause the original dataset to be modified.</p>

<p>Any explanation why a column drop does not modify the original dataset but an element change does change the original? and seems like any change to a column name in an assigned dataset also modifies the original dataset.</p>
","5231314","","13302","","2019-07-09 16:06:15","2019-07-09 16:06:15","Copying dataframes through variables vs copying through variables for columns","<python><python-3.x><pandas><dataframe>","1","2","","","","CC BY-SA 4.0","1"
"57298572","1","57298878","","2019-07-31 20:43:51","","1","31","<p>I need help with a function that accepts no input and returns a list representing the next row of data extracted from a dataframe</p>

<p>I have tried some iterators but this approach requires me to have an input parameter</p>

<pre><code>def get_next_data_as_list():
    out = list(data.iloc[i])
    i= i + 1
    return out

get_next_data_as_list()

Example output: [1619.5, 1620.0, 1621.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1.0, 10.0,
     24.0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 1615.0, 1614.0, 1613.0, 1612.0, 1611.0, 1610.0,
     1607.0, 1606.0, 1605.0, 1604.0, 1603.0, 1602.0, 1601.5, 1601.0, 1600.0, 7.0, 10.0, 1.0, 10.0, 20.0, 3.0, 20.0,
     27.0, 11.0, 14.0, 35.0, 10.0, 1.0, 10.0, 13.0]
</code></pre>
","7901426","","","","","2019-08-02 15:55:50","reading data from a dataframe row by row","<python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"56784165","1","56784267","","2019-06-27 04:59:15","","1","31","<p>In <a href=""https://stackoverflow.com/questions/42579908/use-corr-to-get-the-correlation-between-two-columns"">Use .corr to get the correlation between two columns</a></p>

<p>in answer with </p>

<pre><code>Top15['Citable docs per Capita'].corr(Top15['Energy Supply per Capita'])
</code></pre>

<p>and consulting pandas doc on .corr neither parameters nor example indicate you should put column to be correlated with as a parameter to .corr()
How do you know when and if you should can or should put data frame column reference inside a method like here for .corr?</p>
","725437","","","","","2019-06-27 05:09:10","How do you know when a dataframe column can / should be added to a method's parameters?","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"56871746","1","","","2019-07-03 13:54:34","","0","31","<p>I want to skip the final iteration of my for loop that i am running on a dataframe as it is giving incorrect results. This might be a very trivial problem and need help on this as i am new to python.I have my data spanning into millions of rows and a few hundred thousand ids.</p>

<p>I have a dataframe(df) with 3 columns ID,Event,Time. I am trying to compute another column TimeDiff, which is the difference between events for a particular id, can also be referred to as time to live for a particular event.</p>

<pre><code>ID|Event|Time|TimeDiff
1|x|hh|hh(y)-hh(x) 
1|y|hh|hh(z)-hh(y)
1|z|hh|Nan
2|x|hh|hh(y)-hh(x) 
2|y|hh|Nan
</code></pre>

<p>Above is the desired output but the approach i am trying gives me a value for Nan as well which is basically the time difference with the next event and next id which ideally should be Nan.</p>

<pre class=""lang-py prettyprint-override""><code>for i in df.Id.unique():
    df['TimeDiff'] = (df['Time'].shift(-1) - df['Time']).astype('timedelta64[h]')
</code></pre>

<p>Expected Result:</p>

<pre><code>ID|Event|Time|TimeDiff
1|x|hh|hh(y)-hh(x) 
1|y|hh|hh(z)-hh(y)
1|z|hh|Nan
2|x|hh|hh(y)-hh(x) 
2|y|hh|Nan
</code></pre>

<p>Actual Result:</p>

<pre><code>ID|Event|Time|TimeDiff
1|x|hh|hh(y)-hh(x) 
1|y|hh|hh(z)-hh(y)
1|z|hh|hh(x)-hh(z)
2|x|hh|hh(y)-hh(x) 
2|y|hh|Nan
</code></pre>

<p>If i am able to skip the final for loop iteration for my id no 1, i will be able to get the desired solution</p>
","4060982","","","","","2019-07-03 13:54:34","How to skip the last iteration of a for loop in a dataframe","<python-3.x><pandas>","0","3","","","","CC BY-SA 4.0","1"
"57748563","1","","","2019-09-01 18:37:37","","2","31","<p>I have a multiindex dataframe (index has 2 levels) and my variables as columns.</p>

<p>I'm trying to apply a function for each 1st level index. The idea is just to do a rolling standardization on the data itself:</p>

<pre><code>from sklearn.preprocessing import robust_scale, power_transform

df[list_feat_num] = df[list_feat_num].groupby(level='date').apply(lambda x: power_transform(x))
</code></pre>

<p>and obtained the folowing error:</p>

<pre><code>ValueError: Must have equal len keys and value when setting with an iterable
</code></pre>

<p><strong>Reproducing example</strong>:</p>

<pre><code>import numpy as np

iterable = [[1,2,3,4],['a','b','d','e','f','g','h']]
idx = pd.MultiIndex.from_product(iterable, names=['date', 'sub'])
df = pd.DataFrame(np.random.randn(28, 4), index =idx, columns=['var1', 'var2', 'var3', 'var4'])


df[['var1', 'var2']] = df[['var1', 'var2']].groupby(level='date').apply(lambda x: power_transform(x, method='yeo-johnson' ))
</code></pre>

<p>After investigating, the function is correctly applied, but everything is return back into a Serie, instead back to a multindex dataframe. Thus the error, as it cannot assign back to the original dataframe.</p>

<p>What can I do ? Is it related to the broadcast argument ?</p>

<p>Thanks a lot</p>
","10074708","","10074708","","2019-09-01 18:49:07","2019-09-01 19:00:26","Issue with Groupby.apply on multindex dataframe","<python-3.x><pandas><scikit-learn><pandas-groupby>","1","2","","","","CC BY-SA 4.0","1"
"56648149","1","","","2019-06-18 11:31:17","","0","31","<p>I have a main dataframe with 160,000 rows and 18 columns, one of which is email. I have a 2nd dataframe with emails some email and their corresponding validity status. I want to update the validity status in my main dataframe.</p>

<p>Thus I want to search for the location of each email in my 2nd dataframe inside the main dataframe, and then update the validity column.</p>

<p>What would be the most effective way to do that?</p>

<p>Thank you!</p>

<p>I tries using the code below. While it works, it is slow. Is there a more efficient method?</p>

<pre><code>for index, row in woodpecker.iterrows():
    main.loc[main['Email'] == row['email'],'status'] = row['status']
</code></pre>
","11324057","","","","","2019-06-18 11:33:21","How to locate a row in pandas dataframe based on one column and change corresponding values in other columns?","<python><python-3.x><pandas>","0","2","","2019-06-18 11:32:46","","CC BY-SA 4.0","1"
"57657427","1","","","2019-08-26 11:52:26","","0","31","<p>I have a dataframe like the below and I am trying to print the list of subjects for each unique ID. </p>

<pre><code> ID  Name    Subjects
  0  Tom    [maths,chem,history....]
  1  Harry  [biology,physics,maths...]
</code></pre>

<p>And then iterate over the length of the subject list to do different operations. </p>

<pre><code>    for Subjects in ID:   
        print(Subjects)
    for idx in range(len(Subjects)):
        -- Do operations ---
</code></pre>

<p>I did this:</p>

<pre><code>for df.Subjects in df.ID:
      print(df.Subjects) 
</code></pre>

<p>But this printed ID Numbers.</p>
","8382950","","","","","2019-08-26 12:06:01","Pandas: For each Unique ID, iterate over a list of strings and print it","<python-3.x><pandas><list><dataframe>","1","1","","","","CC BY-SA 4.0","1"
"57615440","1","57615941","","2019-08-22 18:45:30","","1","31","<p>In one column of a pandas <code>df</code>, I have values such as <code>Elgin (west/ouest) (123/456), Ont, Canada</code> and <code>West/Ouest, Ont, Canada</code> that I want to turn into <code>Elgin (west) (123), Ont, Canada</code> and <code>West, Ont, Canada</code> by removing all characters between <code>/</code> and <code>)</code> or between <code>/</code> and <code>,</code>.</p>

<p>My code:</p>

<pre><code>df_census1901['LOC2'] = df_census1901['LOC'].str.replace(r'/.*\)', ')')
df_census1901['LOC2'] = df_census1901['LOC2'].str.replace(r'/.*\,', ',')
</code></pre>

<p>The problem is it does a greedy cut into <code>Elgin (west), Ont, Canada</code></p>
","3635544","","","","","2019-08-22 19:54:03","How to find regex match between a special character and the first match of the second character?","<regex><python-3.x><string><pandas>","2","1","","","","CC BY-SA 4.0","1"
"49571564","1","49571765","","2018-03-30 09:09:11","","2","30","<p>I have a pandas Dataframe, called relevant_URT_data, that looks like this:</p>

<p>test MRN Number ResCat<br />12 AP Disbursements Payment Details URT INC3778700 0<br />33 AP Disbursements Payment Details NaN INC3783080 0<br />72 AP Disbursements Payment Details URT INC3782671 0<br />150 AP Quality Assurance Payment Status URT INC3778770 0<br />178 PR HBS Inquiry NaN INC3776742 1<br />192 AP Quality Assurance Payment Status NaN INC3778547 0<br />315 AP Quality Assurance Payment Status URT INC3780548 0<br />328 PR Accounting W-2 Form URT INC3782016 0<br />355 AP General Submit Invoice for Payment URT INC3781884 0<br />374 AP General Inquiry NaN INC3775944 0</p>

<p>I use the following code to group the data by test - please see image below.</p>

<pre><code>test_breakdown = relevant_URT_data[[""test"",""MRN"",""Number""]] \
        .groupby(""test"") \
        .agg({'MRN':'count', 'Number':'size'}) \
        .rename(columns={'MRN':'URT Use Count','Number':'Number'})
</code></pre>

<p><br />test URT Use Count Number&nbsp;<br />AP Connexxus Access Request 9 9 <br />AP Disbursements Payment Details 28 35 <br />AP General Inquiry 1 7 <br />AP General Submit Invoice for Payment 25 27 <br />AP General WebNow/Invoice Copies 0 4 <br />AP MyExpense Access Request 3 3 <br />AP MyExpense Grant Delegated Access 0 1 <br />AP MyExpense Inquiry 2 8 <br />AP Quality Assurance Payment Status 56 71 <br />Controller's Office General Medical Center 7 10 <br />PR Accounting W-2 Form 6 9 <br />PR HBS Inquiry 0 17 <br />PR HBS Timesheet Calculations 0 2 <br />PR Processing and Production Direct Deposit 2 2 <br />PR Processing and Production Payment Details 0 1 <br />PR Verification of Employment How to Request VOE 1 1 <br />PR Verification of Employment Written VOE 2 4</p>

<p>My goal is to add 2 more columns to the grouped dataset:</p>

<p>1) A column that shows 'URT Use Count' divided by 'Number' (I want the percentage of Number that is MRN)
  2) A column that shows ResCat == 0 divided by Number (I want the percentage of Number that has ResCat equal to 0)</p>

<p>Here is optimally what I would want the output to look like- the two new columns are displayed below:</p>

<p><br />test URT Use Count Number Percentage_Use Same_Day_Percentage<br />AP Connexxus Access Request 9 9 100% 55%<br />AP Disbursements Payment Details 28 35 80% 77%<br />AP General Inquiry 1 7 14% 92%<br />AP General Submit Invoice for Payment 25 27 92% 97%<br />AP General WebNow/Invoice Copies 0 4 0% 19%<br />AP MyExpense Access Request 3 3 100% 50%<br />AP MyExpense Grant Delegated Access 0 1 0% 50%<br />AP MyExpense Inquiry 2 8 25% 77%<br />AP Quality Assurance Payment Status 56 71 79% 88%<br />Controller's Office General Medical Center 7 10 70% 20%<br />PR Accounting W-2 Form 6 9 67% 20%<br />PR HBS Inquiry 0 17 0% 100%<br />PR HBS Timesheet Calculations 0 2 0% 45%<br />PR Processing and Production Direct Deposit 2 2 100% 99%<br />PR Processing and Production Payment Details 0 1 0% 15%<br />PR Verification of Employment How to Request VOE 1 1 100% 12%<br />PR Verification of Employment Written VOE 2 4 50% 22%</p>

<p>Any clarification on creating these calculated fields would be extremely helpful.</p>
","9382746","","9382746","","2018-04-02 00:15:03","2018-04-02 00:15:03","Assistance creating calculated columns with Pandas Groupby","<python-3.x><pandas><pandas-groupby>","1","4","","","","CC BY-SA 3.0","1"
"40960183","1","","","2016-12-04 15:25:41","","0","30","<p>I have a function f that I pass two <code>pandas.DataFrames</code>. I'm iterating through the columns of the first one. It contains index values of the second one. The index is a string, more particularly a MD5 string like <code>'1950abcbdf69bc4b6da8d950e87f538f'</code>. I use those indexes to retrieve rows of the second dataframe. Here's the code:</p>

<pre><code>def f(df_A, df_B):
    for row in df_A.itertuples():
        hash_index=row[1]
        fields_B = df_B.ix[hash_index].values  # &lt;== VERY SLOW
</code></pre>

<p>It works very fine on my laptop (Ubuntu 16.04.1 LTS, VM), but due performance issues, I moved to a server VM (Debian GNU/Linux 8 (jessie), I needed more RAM). The server uses:</p>

<pre><code>'3.5.2 (default, Dec  3 2016, 16:49:26) \n[GCC 4.9.2]'
numpy==1.11.2
pandas==0.19.1
</code></pre>

<p>My laptop has:</p>

<pre><code>'3.5.2 (default, Nov 17 2016, 17:05:23) \n[GCC 5.4.0 20160609]'
numpy==1.11.1
pandas==0.18.1
</code></pre>

<p>To mention the most relevant data. The big problem is, that the Server is way slower (factor 1000 or even more). In the code example I marked the line with ""VERY SLOW"". It takes the server 0.094 seconds to execute that line. <code>.loc[]</code> was even slower. Can you imagine a reason for this.</p>
","6172112","","6172112","","2016-12-04 16:05:45","2016-12-04 16:05:45","Pandas/ Numpy (v0.19.1/ v1.11.2) dataframe performance Issues when accessing by index","<performance><python-3.x><pandas><numpy><python-3.5>","1","0","","","","CC BY-SA 3.0","1"
"57263810","1","57264000","","2019-07-30 03:09:48","","0","30","<p>I have a <code>df['timestamp']</code> column which has values in format: <code>yyyy-mm-ddThh:mm:ssZ</code>. The dtype is object.</p>

<p>Now, I want to split the value into 3 new columns, 1 for day, 1 for day index(mon,tues,wed,..) and 1 for hour like this:</p>

<pre><code>Current:column=timestamp
yyyy-mm-ddThh:mm:ssZ

Desried:
New Col1|New Col2|New Col3
dd|hh|day_index
</code></pre>

<p>What function should I use?</p>
","11846355","","11012757","","2019-07-30 09:52:03","2019-07-30 09:52:03","Format datetime values in pandas by stripping","<python-3.x><pandas>","2","1","","","","CC BY-SA 4.0","1"
"56836977","1","","","2019-07-01 13:57:02","","0","30","<p>I have several dataframes that I have concatenated with pandas in the line:</p>

<pre><code> xspc = pd.concat([df1,df2,df3], axis = 1, join_axes = [df3.index])
</code></pre>

<p>In df2 the index values read one day later than the values of df1, and df3.  So for instance when the most current date is 7/1/19 the index values for df1 and df3 will read ""7/1/19"" while df2 reads '7/2/19'.  I would like to be able to concatenate each series so that each dataframe is joined on the most recent date, so in other words I would like all the dataframe values from df1 index value '7/1/19' to be concatenated with dataframe 2 index value '7/2/19' and dataframe 3 index value '7/1/19'.  When methods can I use to shift the data around to join on these not matching index values?</p>
","4293751","","5921693","","2019-07-01 17:17:11","2019-07-01 17:17:11","Python: DataFrame Index shifting","<python-3.x><pandas><dataframe>","1","1","","","","CC BY-SA 4.0","1"
"57052748","1","","","2019-07-16 08:07:16","","1","30","<p>I'm trying to parallelize the processing of big dataframe using <strong>Pool</strong> of <strong>multiprocessing</strong> module, in 64-bit architecture system. I splitted a dataframe using np.array_split(), called apply(), and compared resulting dataframes - one from batch and the other from parallel processing. 
And what I've found is that there are some difference between those two data frames. </p>

<p>I tested for DataFrame.mean(), DataFrame.median(), DataFrame.std(), and other lambda functions but only for DataFrame.std(), I could find the small difference. </p>

<p>The scale of differences about 10^-13. Is there any known issues about the floating-point precision when Pool of multiprocessing module is used for DataFrame.std()?</p>
","10982975","","","","","2019-07-16 08:07:16","Is there any precision issues for parallelization of pandas?","<python><python-3.x><pandas><dataframe><python-multiprocessing>","0","2","","","","CC BY-SA 4.0","1"
"49212264","1","49212282","","2018-03-10 18:00:31","","1","30","<p>any idea how i can create a column C based on the test of 3 columns in a dataframe?</p>

<p>so far i have</p>

<pre><code>df.loc[df['Negative'] &gt; df['Neutral'] and df['Negative'] &gt; df['Positive'], 
'C'] = 'Bad'
</code></pre>

<p>this gives me</p>

<p>ValueError: The truth value of a Series is ambiguous</p>
","8189236","","","","","2018-03-10 18:11:26","Pandas IF test to create new column","<python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"57197890","1","57198076","","2019-07-25 08:48:34","","1","30","<p>In Python 3, I'm trying to create an indicator column which indicates if two conditions hold for each contract in the data. </p>

<p>(1) If all the outstanding_balance's in for the contract are == 0, then the contract is <strong>Invalid</strong></p>

<p>(2) If the contract_maturity_date is earlier than the minimum date_report_created, then the contract is <strong>Invalid</strong>. </p>

<p>The data I have is as follows: </p>

<pre class=""lang-py prettyprint-override""><code>import pandas as pd

example_data = {'contract_no': [1,1,1,2,2,2],
                'date_report_created': ['2019-01-01', '2019-01-02', '2019-01-03', '2019-01-01', '2019-01-02', '2019-01-03'],
                'contract_maturity_date': ['2018-01-01', '2018-01-01', '2018-01-01', '2019-01-15', '2019-01-15', '2019-01-15'],
                'outstanding_balance': [0, 0, 0, 20, 0, 0]}
example_data = pd.DataFrame(example_data, columns = ['contract_no',
                                                     'date_report_created',
                                                     'contract_maturity_date',
                                                     'outstanding_balance'])
</code></pre>

<p>Which looks like this: </p>

<pre class=""lang-py prettyprint-override""><code>   contract_no date_report_created contract_maturity_date  outstanding_balance
0            1          2019-01-01             2018-01-01                    0
1            1          2019-01-02             2018-01-01                    0
2            1          2019-01-03             2018-01-01                    0
3            2          2019-01-01             2019-01-15                   20
4            2          2019-01-02             2019-01-15                    0
5            2          2019-01-03             2019-01-15                    0
</code></pre>

<p>And I want the data to look like this: </p>

<pre class=""lang-py prettyprint-override""><code>   contract_no date_report_created contract_maturity_date  outstanding_balance valid_contract_flag
0            1          2019-01-01             2018-01-01                    0             Invalid
1            1          2019-01-02             2018-01-01                    0             Invalid
2            1          2019-01-03             2018-01-01                    0             Invalid
3            2          2019-01-01             2019-01-15                   20               Valid
4            2          2019-01-02             2019-01-15                    0               Valid
5            2          2019-01-03             2019-01-15                    0               Valid
</code></pre>

<p>So far I've only been able to fulfill condition (1), and I'm unsure of how I can add the second condition into the logic. </p>

<pre class=""lang-py prettyprint-override""><code>import numpy as np
example_data['payment_information_in_database'] = np.where(example_data.groupby('contract_no')['outstanding_balance']
                                                                       .transform('sum') == 0, 'Invalid', 'Valid')
</code></pre>

<p>I'd be very grateful for any help with this problem!</p>
","9630271","","","","","2019-07-25 08:59:09","Pandas/Numpy: Using multiple conditional statements with Numpy where and transform","<r><python-3.x><pandas><numpy><pandas-groupby>","1","0","","","","CC BY-SA 4.0","1"
"57296876","1","57296910","","2019-07-31 18:25:54","","1","30","<p>I have a set, s, with values : </p>

<pre><code> s = {'K1', 'K2', 'K3', 'K6'}
</code></pre>

<p>And, a dataframe that looks like :</p>

<pre><code>df
   ID Name  Value
0  K1   N1   10.0
1  K2   N2   20.0
2  K3   N3   30.0
3  K4   N4   40.0
4  K5   N5   50.0
5  K6   N6   60.0
</code></pre>

<p>I want to add a new column to this dataframe called <code>Flag</code>. The row values of this new column would be <code>True</code> if the <code>ID</code> in that row is in the set else it would <code>False</code>. </p>

<p>So the output should look like :</p>

<pre><code>df
   ID Name  Value  Flag
0  K1   N1   10.0   True  
1  K2   N2   20.0   True  
2  K3   N3   30.0   True
3  K4   N4   40.0   False
4  K5   N5   50.0   False
5  K6   N6   60.0   True
</code></pre>

<p>I can do this using a  for-loop and marking the rows as True or False. But would like to know what is the cleanest and pandas specific way for doing this succinctly without any loops in the code. You can also use numpy, if necessary.</p>
","4996042","","6361531","","2019-07-31 20:20:31","2019-07-31 20:20:31","Add entries to pandas dataframe based on presence in a set","<python><python-3.x><pandas><dataframe>","1","0","","2019-07-31 18:34:18","","CC BY-SA 4.0","1"
"49013034","1","","","2018-02-27 16:06:00","","0","30","<p>Last time I tried to put a <code>nan</code> into a Pandas dataframe, it forced me to change the column from type <code>int</code> to <code>float</code>. </p>

<p>In SQL there is not an issue with with having a 'NULL' in a column of any type as far as I know. The dataframes I am working with often go in and out of SQL. </p>

<p>Now I have a dataframe with columns including <code>int</code>, <code>object</code> and <code>float</code> and need to create some code which programatically adds a few single rows where 6 out of 7 columns should contain nothing and only 1 out of 7 is assigned a value. </p>

<p>Is there some other standard 'NULL' thing in Pandas you can put in the columns that are not of type float?  </p>

<p>This time I definitely can't go and change the type of a column just to put an <code>nan</code> in it.</p>
","4288043","","","","","2018-02-27 16:23:32","Create a null / nan value in a Pandas row for a column which is not of type float64","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"57657888","1","57657941","","2019-08-26 12:24:55","","1","29","<p>I am have a dataframe as shown below:</p>

<pre><code>d = pd.DataFrame({'name':['bil','bil','bil','bil','jim', 'jim', 
'jim', 'jim'],'col2': ['acct1','law', 'acct1','law', 'acct1','law', 
'acct1','law'],'col3': ['a','b','c', 'd', 'e', 'f', 'g', 'h']
})

     col2 col3 name
0  acct1    a  bil
1    law    b  bil
2  acct1    c  bil
3    law    d  bil
4  acct1    e  jim
5    law    f  jim
6  acct1    g  jim
7    law    h  jim
</code></pre>

<p>I have tried convering it into below format using but not sure how to proceed after this:</p>

<pre><code>d = d.groupby(['name', 'col2'])['col3'].apply(lambda x: 
x.reset_index(drop=True)).unstack().reset_index()

   name   col2  0  1
0  bil  acct1  a  c
1  bil    law  b  d
2  jim  acct1  e  g
3  jim    law  f  h
</code></pre>

<p>My expected format is as show below:</p>

<pre><code>    acc1 law name
 0    a   b  bil
 1    c   d  bil
 2    e   f  jim
 3    g   h  jim
</code></pre>
","6903126","","","","","2019-08-26 12:39:25","pivot pandas dataframe while having multiple rows","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57505296","1","57505457","","2019-08-15 05:43:06","","1","29","<p>I am trying to group a dataframe based on the occurrence a variable. For example take this dataframe</p>

<pre><code>   | col_1 | col_2
---------------------
 0 | 1     | 1
 1 | 0     | 1
 2 | 0     | 1
 3 | 0     | -1
 4 | 0     | -1
 5 | 0     | -1
 6 | 0     | NaN
 7 | -1    | NaN
 8 | 0     | NaN
 9 | 0     | -1
 10| 0     | -1
 11| 0     | -1
</code></pre>

<p>I want to group variable based on the current occurrence of a variable in column_2 to a dataframe and get the next sequence into another dataframe and likewise till the end of dataframe while also ignoring NaN.</p>

<p>So the final output would be like:
ones_1 =</p>

<pre><code>   | col_1 | col_2
---------------------
 0 | 1     | 1
 1 | 0     | 1
 2 | 0     | 1
</code></pre>

<p>mones_1 = </p>

<pre><code> 3 | 0     | -1
 4 | 0     | -1
 5 | 0     | -1
</code></pre>

<p>mones_2 =</p>

<pre><code> 9 | 0     | -1
 10| 0     | -1
 11| 0     | -1
</code></pre>
","6465613","","","","","2019-08-15 06:10:10","Group rows based on the current occurance of a variable","<python-3.x><pandas><pandas-groupby>","2","0","","","","CC BY-SA 4.0","1"
"56717103","1","56717235","","2019-06-22 16:16:21","","0","29","<p>count the number of neg values in delay column using groupby </p>

<pre><code>merged_inner['delayed payments']=merged_inner.groupby('Customer Name')['delay'].apply(lambda x: x [x &lt; 0].count())  
</code></pre>

<p>the delayed payments col is showing null</p>
","8474261","","4685471","","2019-06-23 23:12:46","2019-06-23 23:12:46","i want to count the -ve in a col and put them in another colm using groupby in an col","<python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"57000181","1","","","2019-07-12 04:30:14","","1","29","<p>I am attempting to collect counts of occurrences of an id between two time periods in a dataframe. I have a moderately sized dataframe (about 400 unique ids and just short of 1m rows) containing a time of occurrence and an id for the account which caused the occurrence. I am attempting to get a count of occurrences for multiple time periods (1 hour, 6 hour, 1 day, etc.) prior a specific occurrence and have run into lots of difficulties. </p>

<p>I am using Python 3.7, and for this instance I only have the pandas package loaded. I have tried using for loops and while it likely would have worked (eventually), I am looking for something a bit more efficient time-wise. I have also tried using list comprehension and have run into some errors that I did not anticipate when dealing with datetimes columns. Examples of both are below.</p>

<pre class=""lang-py prettyprint-override""><code>## Sample data
data = {'id':[  'EAED813857474821E1A61F588FABA345', 'D528C270B80F11E284931A7D66640965', '6F394474B8C511E2A76C1A7D66640965', '7B9C7C02F19711E38C670EDFB82A24A9', '80B409D1EC3D4CC483239D15AAE39F2E', '314EB192F25F11E3B68A0EDFB82A24A9', '68D30EE473FE11E49C060EDFB82A24A9', '156097CF030E4519DBDF84419B855E10', 'EE80E4C0B82B11E28C561A7D66640965', 'CA9F2DF6B82011E28C561A7D66640965', '6F394474B8C511E2A76C1A7D66640965', '314EB192F25F11E3B68A0EDFB82A24A9', 'D528C270B80F11E284931A7D66640965', '3A024345C1E94CED8C7E0DA3A96BBDCA', '314EB192F25F11E3B68A0EDFB82A24A9', '47C18B6B38E540508561A9DD52FD0B79', 'B72F6EA5565B49BBEDE0E66B737A8E6B', '47C18B6B38E540508561A9DD52FD0B79', 'B92CB51EFA2611E2AEEF1A7D66640965', '136EDF0536F644E0ADE6F25BB293DD17', '7B9C7C02F19711E38C670EDFB82A24A9', 'C5FAF9ACB88D4B55AB8196DBFFE5B3C0', '1557D4ECEFA74B40C718A4E5425F3ACB', '68D30EE473FE11E49C060EDFB82A24A9', '68D30EE473FE11E49C060EDFB82A24A9', 'CAF9D8CD627B422DFE1D587D25FC4035', 'C620D865AEE1412E9F3CA64CB86DC484', '47C18B6B38E540508561A9DD52FD0B79', 'CA9F2DF6B82011E28C561A7D66640965', '06E2501CB81811E290EF1A7D66640965', '68EEE17873FE11E4B5B90AFEF9534BE1', '47C18B6B38E540508561A9DD52FD0B79', '1BFE9CB25AD84B64CC2D04EF94237749', '7B20C2BEB82811E28C561A7D66640965', '261692EA8EE447AEF3804836E4404620', '74D7C3901F234993B4788EFA9E6BEE9E', 'CAF9D8CD627B422DFE1D587D25FC4035', '76AAF82EB8C511E2A76C1A7D66640965', '4BD38D6D44084681AFE13C146542A565', 'B8D27E80B82911E28C561A7D66640965'  ], 'datetime':[ ""24/06/2018 19:56"", ""24/05/2018 03:45"", ""12/01/2019 14:36"", ""18/08/2018 22:42"", ""19/11/2018 15:43"", ""08/07/2017 21:32"", ""15/05/2017 14:00"", ""25/03/2019 22:12"", ""27/02/2018 01:59"", ""26/05/2019 21:50"", ""11/02/2017 01:33"", ""19/11/2017 19:17"", ""04/04/2019 13:46"", ""08/05/2019 14:12"", ""11/02/2018 02:00"", ""07/04/2018 16:15"", ""29/10/2016 20:17"", ""17/11/2018 21:58"", ""12/05/2017 16:39"", ""28/01/2016 19:00"", ""24/02/2019 19:55"", ""13/06/2019 19:24"", ""30/09/2016 18:02"", ""14/07/2018 17:59"", ""06/04/2018 22:19"", ""25/08/2017 17:51"", ""07/04/2019 02:24"", ""26/05/2018 17:41"", ""27/08/2014 06:45"", ""15/07/2016 19:30"", ""30/10/2016 20:08"", ""15/09/2018 18:45"", ""29/01/2018 02:13"", ""10/09/2014 23:10"", ""11/05/2017 22:00"", ""31/05/2019 23:58"", ""19/02/2019 02:34"", ""02/02/2019 01:02"", ""27/04/2018 04:00"", ""29/11/2017 20:35""]}
df = pd.dataframe(data)

df = df.sort_values(['id', 'datetime'], ascending=True)
# for loop attempt
totalAccounts = df['id'].unique()
for account in totalAccounts:
     oneHourCount=0
     subset = df[df['id'] == account]
     for i in range(len(subset)):
          onehour = subset['datetime'].iloc[i] - timedelta(hours=1)
          for j in range(len(subset)):

                    if (subset['datetime'].iloc[j] &gt;= onehour) and (subset['datetime'].iloc[j] &lt; sub):
                        oneHourCount+=1


#list comprehension attempt
df['onehour'] = df['datetime'] - timedelta(hours=1)
for account in totalAccounts:
     onehour = sum([1 for x in subset['datetime'] if x &gt;= subset['onehour'] and x &lt; subset['datetime']])
</code></pre>

<p>I am getting either 1) incredibly long runtime with the for loop or 2) an ValueError  regarding the truth of a series being ambiguous. I know the issue is dealing with the datetimes, and perhaps it is just going to be slow-going, but I want to check here first just to make sure.</p>
","11665508","","6622587","","2019-07-12 07:59:42","2019-07-12 19:41:41","Trouble obtaining counts using multiple datetime columns as conditionals","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57865331","1","57865434","","2019-09-10 06:42:52","","1","29","<p>I have a pandas dataframe with columns date, week_day, public_holiday and weekend.</p>

<pre><code> weekday Date           Public_Holiday? Weekend?
   5     2015-01-10              no      yes
   0     2015-01-12              no       no
   1     2015-01-13              no       no
   2     2015-01-14              no       no
   3     2015-01-15              no       no
   4     2015-01-16              no       no
   5     2015-01-17              no      yes
   6     2015-01-18              no      yes
   0     2015-01-19              no       no
   1     2015-01-20              no       no
   2     2015-01-21              no       no
   3     2015-01-22              no       no
   4     2015-01-23              yes      no
   5     2015-01-24              no      yes
   6     2015-01-25              no      yes
   1     2015-01-27              no       no
   2     2015-01-28              no       no
   3     2015-01-29              no       no
   4     2015-01-30              no       no
   5     2015-01-31              no      yes
   0     2015-02-02              no       no
   1     2015-02-03              no       no
   2     2015-02-04              no       no
   3     2015-02-05              no       no
   4     2015-02-06              no       no
   5     2015-02-07              no      yes
   6     2015-02-08              no      yes
   0     2015-02-09              yes      no
   1     2015-02-10              no       no
   2     2015-02-11              no       no
</code></pre>

<p>I need to add an additional column which has the long weekend flag. The output should look like the below.</p>

<pre><code>    long_weekend  weekday   Date          Public_Holiday? Weekend?
            0        5     2015-01-10              no      yes
            0        0     2015-01-12              no       no
            0        1     2015-01-13              no       no
            0        2     2015-01-14              no       no
            0        3     2015-01-15              no       no
            0        4     2015-01-16              no       no
            0        5     2015-01-17              no      yes
            0        6     2015-01-18              no      yes
            0        0     2015-01-19              no       no
            0        1     2015-01-20              no       no
            0        2     2015-01-21              no       no
            0        3     2015-01-22              no       no
            1        4     2015-01-23              yes      no
            1        5     2015-01-24              no      yes
            1        6     2015-01-25              no      yes
            0        1     2015-01-27              no       no
            0        2     2015-01-28              no       no
            0        3     2015-01-29              no       no
            0        4     2015-01-30              no       no
            0        5     2015-01-31              no      yes
            0        0     2015-02-02              no       no
            0        1     2015-02-03              no       no
            0        2     2015-02-04              no       no
            0        3     2015-02-05              no       no
            0        4     2015-02-06              no       no
            1        5     2015-02-07              no      yes
            1        6     2015-02-08              no      yes
            1        0     2015-02-09              yes      no
            0        1     2015-02-10              no       no
            0        2     2015-02-11              no       no
</code></pre>

<p>The regular weekends are not considered as long weekends. Only if friday or monday and in some cases if thursdays or tuesdays are holidays, the entire series is considered as longweekend. </p>

<p>Here is what I have tried below</p>

<pre><code>df['long_weekend'] = np.where((df['Public_Holiday?'] == 'yes') | (df['Weekend?'] == 'yes'), 1, 0)
df['weekday'] = df['Predicted_Date'].dt.dayofweek
df['long_weekend'] = np.where(((df['long_weekend'] == 1) &amp; (df['weekday'] == 4)) | (df['long_weekend'] == 1) &amp; (df['weekday'] == 0)), 'yes','no')
</code></pre>

<p>This gives me the following output which even has regular weekdays as 1.</p>

<pre><code>    long_weekend  weekday         Date   Public_Holiday? Weekend?
            1        5     2015-01-10              no      yes
            0        0     2015-01-12              no       no
            0        1     2015-01-13              no       no
            0        2     2015-01-14              no       no
            0        3     2015-01-15              no       no
            0        4     2015-01-16              no       no
            1        5     2015-01-17              no      yes
            1        6     2015-01-18              no      yes
            0        0     2015-01-19              no       no
            0        1     2015-01-20              no       no
            0        2     2015-01-21              no       no
            0        3     2015-01-22              no       no
            1        4     2015-01-23              yes      no
            1        5     2015-01-24              no      yes
            1        6     2015-01-25              no      yes
            0        1     2015-01-27              no       no
            0        2     2015-01-28              no       no
            0        3     2015-01-29              no       no
            0        4     2015-01-30              no       no
            1        5     2015-01-31              no      yes
            0        0     2015-02-02              no       no
            0        1     2015-02-03              no       no
            0        2     2015-02-04              no       no
            0        3     2015-02-05              no       no
            0        4     2015-02-06              no       no
            1        5     2015-02-07              no      yes
            1        6     2015-02-08              no      yes
            1        0     2015-02-09              yes      no
            0        1     2015-02-10              no       no
            0        2     2015-02-11              no       no
</code></pre>

<p>How can I get this working? Any help would be great. Thanks in advance.</p>
","10611339","","","","","2019-09-10 06:51:12","Creating a new column for long weekend with existing public holiday and weekend columns","<python><python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"48504771","1","48505119","","2018-01-29 15:33:57","","1","29","<p>I have usnparser data that outputs like below (arbitratry fields):</p>

<pre><code>{
 ""thisisafield"":""THISISAVALUE"",
 ""thisisanewf"": ""ThisIsANewVal"",
 ""LastField"":""LastValue""
}
{
 ""thisisafield"":""THISISAVALUE1"",
 ""thisisanewf"": ""ThisIsANewVal1"",
 ""LastField"":""LastValue1""
}
</code></pre>

<p>I am trying to read it with pandas like below:</p>

<pre><code>data = pathtomyfile
pd.read_json(data, orient='records')
</code></pre>

<p>I have also tried to use json like below:</p>

<p>data = json.dumps(pathtomyfile)
   pd.read_json(data,orient='records')</p>

<p>How can i get my data to a pandas dataframe? </p>
","6916973","","","","","2018-01-29 15:56:30","usnparser json-like data to pandas dataframe","<python><json><python-3.x><pandas>","1","1","","","","CC BY-SA 3.0","1"
"48738144","1","","","2018-02-12 00:52:30","","0","29","<p>I have a dataset like below:</p>

<pre><code>Process: matts.exe Pid: 900 Address: 0x7f6a0000
Vad Tag: Vad  Protection: PAGE_EXECUTE_READWRITE
Flags: Protection: 6

0x7f6a0000  c8 00 00 00 58 01 00 00 ff ee ff ee 08 70 00 00   ....X........p..
0x7f6a0010  08 00 00 00 00 fe 00 00 00 00 10 00 00 20 00 00   ................
0x7f6a0020  00 02 00 00 00 20 00 00 8d 01 00 00 ff ef fd 7f   ................
0x7f6a0030  03 00 08 06 00 00 00 00 00 00 00 00 00 00 00 00   ................

0x7f6a0000 c8000000         ENTER 0x0, 0x0
0x7f6a0004 58               POP EAX
0x7f6a0005 0100             ADD [EAX], EAX
0x7f6a0007 00ff             ADD BH, BH

Process: matts2.exe Pid: 910 Address: 0x7f6a0000
Vad Tag: Vad  Protection: PAGE_EXECUTE_READWRITE
Flags: Protection: 6

0x7f6a0000  c8 00 00 00 58 01 00 00 ff ee ff ee 08 70 00 00   ....X........p..
0x7f6a0010  08 00 00 00 00 fe 00 00 00 00 10 00 00 20 00 00   ................
0x7f6a0020  00 02 00 00 00 20 00 00 8d 01 00 00 ff ef fd 7f   ................
0x7f6a0030  03 00 08 06 00 00 00 00 00 00 00 00 00 00 00 00   ................

0x7f6a0000 c8000000         ENTER 0x0, 0x0
0x7f6a0004 58               POP EAX
0x7f6a0005 0100             ADD [EAX], EAX
0x7f6a0007 00ff             ADD BH, BH
</code></pre>

<p>How can I place this data into a pandas dataframe like below?</p>

<pre><code>Process    Pid   Address     Vad_Tag   Protection              Protection   Hex_out                                                                          Assembly_Out
matts.exe  900   0x7f6a0000  Vad       PAGE_EXECUTE_READWRITE  6            0x7f6a0000  c8 00 00 00 58 01 00 00 ff ee ff ee 08 70 00 00   ....X........p..   0x7f6a0000 c8000000         ENTER 0x0, 0x0
                                                                            0x7f6a0010  08 00 00 00 00 fe 00 00 00 00 10 00 00 20 00 00   ................   0x7f6a0004 58               POP EAX
                                                                            0x7f6a0020  00 02 00 00 00 20 00 00 8d 01 00 00 ff ef fd 7f   ................   0x7f6a0005 0100             ADD [EAX], EAX
                                                                            0x7f6a0030  03 00 08 06 00 00 00 00 00 00 00 00 00 00 00 00   ................   0x7f6a0007 00ff             ADD BH, BH

matts2.exe 910   0x7f6a0000  Vad       PAGE_EXECUTE_READWRITE  6            0x7f6a0000  c8 00 00 00 58 01 00 00 ff ee ff ee 08 70 00 00   ....X........p..   0x7f6a0000 c8000000         ENTER 0x0, 0x0
                                                                            0x7f6a0010  08 00 00 00 00 fe 00 00 00 00 10 00 00 20 00 00   ................   0x7f6a0004 58               POP EAX
                                                                            0x7f6a0020  00 02 00 00 00 20 00 00 8d 01 00 00 ff ef fd 7f   ................   0x7f6a0005 0100             ADD [EAX], EAX
                                                                            0x7f6a0030  03 00 08 06 00 00 00 00 00 00 00 00 00 00 00 00   ................   0x7f6a0007 00ff             ADD BH, BH
</code></pre>

<p>Currently I can read it in as a table but it places everything in a separate line. Every third blank line is what I am using as my delimiter but am still having problems with the shaping of the data. The hex and the assembly need to be a string format, i placed it in the table for brevity sake. Any help would be appreciated. </p>
","6916973","","","","","2018-02-12 00:57:52","abnormal data to pandas dataframe multiple types","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 3.0","1"
"57026732","1","57027033","","2019-07-14 10:54:46","","1","29","<p>I have a DataFrame that stores results from a regression, like this:</p>

<pre><code>feats = ['X1', 'X2', 'X3']
betas = [0.5, 0.7, 0.9]
ses = [0.05, 0.03, 0.02]
data = {
    ""Feature"": feats, 
    ""Beta"": betas,
    ""Error"":ses
}

data = pd.DataFrame(data)
</code></pre>

<p>It looks like this:</p>

<pre><code>   Beta  Error Feature
0   0.5   0.05      X1
1   0.7   0.03      X2
2   0.9   0.02      X3
</code></pre>

<p>I want to make a graph coefficients for each feature, the height being ""Beta"" and the error line being ""Error"".</p>

<p>Is there a way to get this working in Matplot? </p>

<p>I have tried error plot but maybe did it wrong or something. </p>
","","user5813071","","","","2019-07-14 11:32:27","Bar graph with standard errors from Dataframe?","<python-3.x><pandas><matplotlib><seaborn>","1","0","","","","CC BY-SA 4.0","1"
"57376173","1","","","2019-08-06 12:33:02","","1","29","<p>I have a dataframe with a column as below</p>

<p>Input:</p>

<pre><code>  CD

  Component Description_CAP YO
  Component Description_CAPE IO
  Component Description_CLOSE SO
  Component Description_CAT TO
  Component Description_CAPP TTO
  Component Description_CLOSE IUO
</code></pre>

<p>I have used lstrip, in which the ""C"" after Component_Description is getting deleted which wrong</p>

<pre><code>      df['CD'] = df['CD'].map(lambda x: x.lstrip('Component Description_'))
</code></pre>

<p>Expected Result:</p>

<pre><code>  CD

  CAP YO
  CLOSE SO
  CAT TO
  CAPP TTO
  CLOSE IUO
</code></pre>

<p>Actual Result I am getting</p>

<pre><code>       CD

       AP YO
       LOSE SO
       AT TO
       APP TTO
       LOSE IU
</code></pre>
","7905329","","7905329","","2019-08-06 12:36:33","2019-08-06 12:37:30","Removing extra character while using lstrip in pandas","<python><python-3.x><pandas><dataframe><strip>","2","0","","","","CC BY-SA 4.0","1"
"57411489","1","57411949","","2019-08-08 11:26:19","","-1","29","<p>getting these kind of errors while importing the file</p>

<pre><code>import pandas
data1 = pandas.read_csv(""zomato.csv"")
Traceback (most recent call last):
  File ""pandas\_libs\parsers.pyx"", line 1149, in pandas._libs.parsers.TextReader._convert_tokens
  File ""pandas\_libs\parsers.pyx"", line 1279, in pandas._libs.parsers.TextReader._convert_with_dtype
  File ""pandas\_libs\parsers.pyx"", line 1295, in pandas._libs.parsers.TextReader._string_convert
  File ""pandas\_libs\parsers.pyx"", line 1518, in pandas._libs.parsers._string_box_utf8
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 3: invalid continuation byte
</code></pre>
","10955686","","10295265","","2019-08-08 11:33:38","2019-08-08 11:51:19","Error in reading csv <UnicodeDecodeError>","<python-3.x><pandas>","1","1","","2019-08-10 05:22:27","","CC BY-SA 4.0","1"
"57374050","1","","","2019-08-06 10:31:09","","0","29","<p>I tried to load data from a csv file but i can't seem to be able to re-align the column headers to the respective rows for a clearer data frame.</p>

<p>Below is the output of</p>

<p><code>df.head()</code></p>

<pre><code>bookID,title,authors,average_rating,isbn,isbn13,language_code,# num_pages,ratings_count,text_reviews_count
</code></pre>

<p>0   1,Harry Potter and the Half-Blood Prince (Harr...
1   2,Harry Potter and the Order of the Phoenix (H...
2   3,Harry Potter and the Sorcerer's Stone (Harry...
3   4,Harry Potter and the Chamber of Secrets (Har...
4   5,Harry Potter and the Prisoner of Azkaban (Ha...</p>

<pre><code>import pandas as pd

file = 'C:/Users/user/Documents/Temporary data sets for practise only/books.csv'

df = pd.read_csv(file, sep ='/t')

df.head()
</code></pre>

<p><a href=""https://i.stack.imgur.com/CoxuK.png"" rel=""nofollow noreferrer"">Ref</a></p>
","11883882","","13302","","2019-08-19 20:26:17","2019-08-19 20:26:17","How to realign column headers with the respective rows after importing a csv data set","<python-3.x><pandas><jupyter-notebook><data-cleaning>","1","0","0","","","CC BY-SA 4.0","1"
"56626170","1","","","2019-06-17 07:01:41","","0","29","<p>I have a data frame df_train which has a column sub_division.</p>

<p>The values in the column is look like below</p>

<pre><code>ABC_commercial,
ABC_Private,
Test ROM DIV,
ROM DIV,
TEST SEC R&amp;OM
</code></pre>

<p>I am trying to 
1. convert anything starts with ABC* to a number (for ex: 1)
2. convert anything contains ROM and R&amp;OM to a number (for ex: 2)</p>

<p>Thanks in advance.</p>

<p>Expected result:</p>

<pre><code>1,
1,
2,
2,
2
</code></pre>
","9491554","","11450873","","2019-06-17 07:38:28","2019-06-17 08:17:50","Aggregate and Convert categorical data to numbers","<python><python-3.x><pandas><python-2.7>","3","1","","","","CC BY-SA 4.0","1"
"49516779","1","49517178","","2018-03-27 15:21:53","","0","28","<p>I am trying to conditionally change the value of a current row, based on the value of the Nth row below it. say for example I have a csv file that looks like:</p>

<pre><code>   trial
    ''
    ''
    ''
    ''
    ''
    ''
    ''
    'a'
    ''
    ''
    ''
    ''
    ''
    ''
    ''
    'a'
    ''
    ''
    ''
    ''
    ''
    ''
    ''
    'a'
</code></pre>

<p>Now,if the value of every 3rd row after the current row is 'a', then the null will be converted into 'a' from the current row to the 3rd row below.
as such:</p>

<pre><code>trial
''
''
''
''
'a'
'a'
'a'
'a'
''
''
''
''
'a'
'a'
'a'
'a'
''
''
''
''
'a'
'a'
'a'
'a'
</code></pre>

<p>my code is as follow:</p>

<pre><code>data =csv.reader(data)
next(data)

def convert(param):
    if param=='':
        value='a'
    else:
        value=''
    return value

for row in data:
    i=0
    for line in islice(data, i+3, None):
        print i
        print line
        print row
        if line==['a']:
            convert(row)
        print row
        i = i+1
</code></pre>

<p>however, the output is:</p>

<pre><code>0
[]
[]
[]
1
[]
[]
[]
2
[]
[]
[]
3
['a']
[]
[]
4
[]
[]
[]
5
[]
[]
[]
6
[]
[]
[]
7
[]
[]
[]
8
[]
[]
[]
9
[]
[]
[]
10
[]
[]
[]
11
[]
[]
[]
12
[]
[]
[]
13
['a']
[]
[]
14
[]
[]
[]
15
[]
[]
[]
16
[]
[]
[]
17
[]
[]
[]
18
[]
[]
[]
19
[]
[]
[]
20
[]
[]
[]
21
[]
[]
[]
22
[]
[]
[]
23
['a']
[]
[]
</code></pre>

<p>any idea on how to do this?</p>
","5760497","","","","","2018-03-27 15:41:30","Conditionally change the value of the current row, based on the value of Nth row below of a csv using python","<python><python-3.x><python-2.7><pandas><csv>","2","1","","","","CC BY-SA 3.0","1"
"57666971","1","","","2019-08-27 02:24:21","","0","28","<p>I am loading csv files to sql server table. EmpNo Ename ProdID Sales Value Int Varchar Int Int Float</p>

<p>But I am getting ProdID,Sales,Amount Values as 0 for blanks.But want to keep them as blank. I am using this code for keeping nulls.
Please help me. Thanks in advance.</p>

<pre><code>df = pd.read_csv(f, header=None,names=file_titles,low_memory=False)
df.columns = df.columns.str.strip()
df = df.fillna(value=' ')
</code></pre>

<p><strong>Source Data</strong>             </p>

<pre><code>EmpNo   Ename   ProdID  Sales   Amount
1   E1      10  120.00
2   E2  1   2   100.00
3   E3          
4   E4  3   3   353.00
5   E5      6   443.00
6   E6  4   8   533.00
</code></pre>

<p>Expected Output             </p>

<pre><code>  EmpNo Ename   ProdID  Sales   Amount
1   E1      10  120.00
2   E2  1   2   100.00
3   E3          
4   E4  3   3   353.00
5   E5      6   443.00
6   E6  4   8   533.00
</code></pre>
","10808871","","","","","2019-08-27 02:43:28","How to load blank values as blank for CSV value measures/int in python","<python><python-3.x><pandas>","1","3","","","","CC BY-SA 4.0","1"
"49099026","1","49099245","","2018-03-04 18:45:04","","0","28","<p><strong>Objective</strong>: I am looking to define a function which takes a single argument - a dictionary of column names and values - and returns a list of matching criteria from a Pandas data frame</p>

<p><strong>Details</strong>: I am looking to programmatically generate the following string</p>

<pre><code>data[(mydf.anchor_name == 'ing') &amp; (mydf.sales_qty ==8)]
</code></pre>

<p>Generates an example pandas dataframe:</p>

<pre><code>import pandas as pd
mydf = pd.DataFrame({'sales_qty' : pd.Series([4,8,10]),
                    'distance' : pd.Series([454.75,477.25,242.12]),
                    'signature' : pd.Series(['ab','cd','ab']),
                    'anchor_name' : pd.Series(['tec','ing','pol']),
                    'station_list' : pd.Series([['t1','t2','t3'],
                    ['4','t2','t3'],['t3','t2','t4']])
                    })
</code></pre>

<p>I have been trying to work with this code:</p>

<pre><code>data = mydf
params = {""anchor_name"": 'ing', ""sales_qty"": 8}
filters = [""{}"".format(k) for k in params]
t = tuple(params.values())
data += ""[df.""+ "" ) &amp; (df."".join(t).join(filters)+"")]""
</code></pre>
","1797250","","","","","2018-03-04 19:19:16","How to create queries from parameters?","<python-3.x><pandas>","1","1","","","","CC BY-SA 3.0","1"
"56646294","1","56646341","","2019-06-18 09:48:42","","0","28","<p>I have a data frame as below</p>

<pre><code>df = pd.DataFrame([['aa', 1], ['bb', 2], ['cc', 3]])

    0   1
0   aa  1
1   bb  2
2   cc  3
</code></pre>

<p>how can i add a list of lists <code>li = [['xx',11], ['yy',22], ['zz',33]]</code> to the data frame <code>df</code> so that each list inside the <code>li</code> will be added as a row . Expected output is as below:  </p>

<pre><code>    0   1   2   3
0   aa  1   xx  11
1   bb  2   yy  22
2   cc  3   zz  33
</code></pre>

<p>currently i am looping through indexes of sublist and adding them to <code>df</code></p>

<pre><code>for i in range(len(li[0])):
    df[str(df.shape[1])] = [x[i] for x in li]
</code></pre>

<p>is there any simpler way to do this without looping?</p>

<blockquote>
  <p><code>len(df)</code> always equals to <code>len(li)</code><br>
  all sublists in <code>li</code> are of the same length</p>
</blockquote>
","10419999","","","","","2019-06-18 10:26:15","How to add a List of list to an existing data frame as separate columns","<python><python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"56818168","1","","","2019-06-29 14:05:14","","0","28","<p>Let's consider the following simple csv file</p>

<pre><code>end_beat
2.989583333
1.989583333
2.989583333
</code></pre>

<h2>Pandas Approach</h2>

<p>When I try to use pandas to read this csv file with the following code</p>

<pre><code>import pandas as pd
df = pd.read_csv('debug.csv')
print(df['end_beat'][1])
</code></pre>

<p>I got </p>

<blockquote>
  <p>1.9895833330000001</p>
</blockquote>

<h2>Python csv library approach</h2>

<p>However, when I use another method to read the data, everything is fine</p>

<pre><code>import csv

with open('debug.csv') as f:
    df = csv.DictReader(f)
    next(df)['end_beat']
    print(next(df)['end_beat'])
</code></pre>

<blockquote>
  <p>1.989583333</p>
</blockquote>

<p>Only the result returned by pandas has this problem, how to get rid of the extra zeros?</p>
","9793316","","9793316","","2019-06-29 14:21:45","2019-06-30 08:02:37","Pandas is creating not existant extra digits in the end of a float number","<python-3.x><pandas><csv>","0","5","","","","CC BY-SA 4.0","1"
"57552856","1","57552902","","2019-08-19 08:18:47","","1","28","<p>I have two data frame which is like,</p>

<pre><code>df:
   building  level      site          mac_location  gw_mac_rssi
0  2b        2nd-floor  crystal-lawn  lab           {'ac233fc01403': -32.0, 'ac233fc015f6': -45.5, 'ac233fc02eaa': -82}
1  2b        2nd-floor  crystal-lawn  conference    {'ac233fc01403': -82, 'ac233fc015f6': -45.5, 'ac233fc02eaa': -82}  
</code></pre>

<p>I need to extract the keys of ""gw_mac_rssi"" as ""gw_mac"" and values of ""gw_mac_rssi"" as ""rssi"" which should be</p>

<pre><code>required_df:
   building  level      site          mac_location  gw_mac                                            rssi   
0  2b        2nd-floor  crystal-lawn  lab           ['ac233fc01403','ac233fc015f6','ac233fc02eaa']    [-32.0,-45.5,-82]
1  2b        2nd-floor  crystal-lawn  conference    ['ac233fc01403','ac233fc015f6','ac233fc02eaa']    [-82, -45.5, -82]
</code></pre>

<p>I have tried with,</p>

<pre><code>df['gw_mac'] = list(df['gw_mac_rssi'].keys())
</code></pre>

<p>whereas unable to get the required data frame.</p>
","5391663","","","","","2019-08-19 08:21:56","How to extract the keys and values of a dictionary as a separate column in a data frame?","<python-3.x><pandas><dictionary>","1","0","","","","CC BY-SA 4.0","1"
"57298611","1","","","2019-07-31 20:46:48","","1","28","<p>I am trying to get the result column to be the sum of the value column for all rows in the data frame where the country is equal to the country in that row, and the date is on or before the date in that row.</p>

<pre><code>Date        Country ValueResult
01/01/2019  France  10  10
03/01/2019  England 9   9
03/01/2019  Germany 7   7
22/01/2019  Italy   2   2
07/02/2019  Germany 10  17
17/02/2019  England 6   15
25/02/2019  England 5   20
07/03/2019  France  3   13
17/03/2019  England 3   23
27/03/2019  Germany 3   20
15/04/2019  France  6   19
04/05/2019  England 3   26
07/05/2019  Germany 5   25
21/05/2019  Italy   5   7
05/06/2019  Germany 8   33
21/06/2019  England 3   29
24/06/2019  England 7   36
14/07/2019  France  1   20
16/07/2019  England 5   41
30/07/2019  Germany 6   39
18/08/2019  France  6   26
04/09/2019  England 3   44
08/09/2019  Germany 9   48
15/09/2019  Italy   7   14
05/10/2019  Germany 2   50
</code></pre>

<p>I have tried the below code but it sums up the entire column</p>

<p><code>df['result'] = df.loc[(df['Country'] == df['Country']) &amp; (df['Date'] &gt;= df['Date']), 'Value'].sum()</code></p>
","11832565","","","","","2019-08-01 08:40:38","How do I create a new column in pandas which is the sum of another column based on a condition?","<python-3.x><pandas><numpy>","1","0","","","","CC BY-SA 4.0","1"
"57453947","1","","","2019-08-11 22:04:43","","-2","28","<p>I have the following pandas data frame and I only want to keep rows that <code>time = 0.1, 0.2, 0.3, 0.4 ... (every 0.1 second)</code></p>

<p><a href=""https://i.stack.imgur.com/TBNDM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TBNDM.png"" alt=""enter image description here""></a></p>

<p>I tried to use the floor function like below, but it doesn't work with pandas:</p>

<p><a href=""https://i.stack.imgur.com/TQG82.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TQG82.png"" alt=""enter image description here""></a></p>

<p>Any suggestions on how to achieve this? Thanks!</p>
","3993270","","","","","2019-08-11 23:27:54","pandas: extract every 10 rows based on a column value","<python-3.x><pandas>","2","3","","","","CC BY-SA 4.0","1"
"57455711","1","57456383","","2019-08-12 04:41:15","","0","28","<p>Can someone please explain the behavior of the following three numbers in Pandas? I'm trying to load the following values and have them correctly represented.</p>

<ul>
<li>954081199.495100000000000    => 9954081199.4951</li>
<li>9449546861.291050000000000   => 9449546861.29105</li>
<li>9752031802.626950000000000   => 9752031802.62695</li>
</ul>

<p>As the trailing 0s are removed the value as understood by Pandas changes. It seems as though the trailing digits are affecting the significance of the values. Simply truncating the value after the nth digit isn't feasible because the actual length of the non-zero values after decimal point isn't known.</p>

<p>Is there there something in Pandas that controls this behavior?
I've tried using the ""c"" engine but that gives the same output.</p>

<p>The data is being read from a text file.</p>

<p>Thanks.</p>

<pre><code>Loading sample_1.txt
Row   : Raw Value                      : Pandas Value
    0 : 954081199.495100000000000      : 954081199.4950998
    1 : 954081199.49510000000000       : 954081199.4950998
    2 : 954081199.4951000000000        : 954081199.4950998
    3 : 954081199.495100000000         : 954081199.4951
    4 : 954081199.49510000000          : 954081199.4951
    5 : 954081199.4951000000           : 954081199.4951
    6 : 954081199.495100000            : 954081199.4951
    7 : 954081199.49510000             : 954081199.4951
    8 : 954081199.4951000              : 954081199.4951
    9 : 954081199.495100               : 954081199.4951
   10 : 954081199.49510                : 954081199.4951
   11 : 954081199.4951                 : 954081199.4951
   12 : 9449546861.291050000000000     : 9449546861.291044
   13 : 9449546861.29105000000000      : 9449546861.291044
   14 : 9449546861.2910500000000       : 9449546861.291046
   15 : 9449546861.291050000000        : 9449546861.291046
   16 : 9449546861.29105000000         : 9449546861.291048
   17 : 9449546861.2910500000          : 9449546861.291048
   18 : 9449546861.291050000           : 9449546861.291048
   19 : 9449546861.29105000            : 9449546861.291048
   20 : 9449546861.2910500             : 9449546861.29105
   21 : 9449546861.291050              : 9449546861.29105
   22 : 9449546861.29105               : 9449546861.29105
   23 : 9752031802.626950000000000     : 9752031802.626955
   24 : 9752031802.62695000000000      : 9752031802.626955
   25 : 9752031802.6269500000000       : 9752031802.626951
   26 : 9752031802.626950000000        : 9752031802.626951
   27 : 9752031802.62695000000         : 9752031802.626951
   28 : 9752031802.6269500000          : 9752031802.626951
   29 : 9752031802.626950000           : 9752031802.626951
   30 : 9752031802.62695000            : 9752031802.626951
   31 : 9752031802.6269500             : 9752031802.62695
   32 : 9752031802.626950              : 9752031802.62695
   33 : 9752031802.62695               : 9752031802.62695
Done
</code></pre>

<p>Code to produce the above output</p>

<pre class=""lang-py prettyprint-override""><code>#!/usr/bin/env python3

import pandas

def main():
    file_name = 'sample_1.txt'
    print ('Loading ' + file_name)    
    content_df = pandas.read_csv(file_name, delimiter='|', header=None, engine='python', skipinitialspace=True,skiprows=0,skipfooter=0)
    num_rows = content_df.values.shape[0]

    with open(file_name, 'r') as f:
        lines_list = f.read().split('\n')

    f.close()
    rowcount = 0
    print('Row   : Raw Value' + ' '*22 + ': Pandas Value')
    while rowcount &lt; num_rows:
        value_list = lines_list[rowcount].split('|')
        print('{0:5d} : {1} : {2}'.format(rowcount, value_list[2].ljust(30, ' '), content_df.iloc[rowcount, 2]))

        # print('row: ' + str(content_df.iloc[rowcount, 1]) + ': ' + str(content_df.iloc[rowcount, 2]) + ': ' + str(value_list[2]))
        rowcount = rowcount +1

    print ('Done')


if __name__ == '__main__':
    main()
</code></pre>
","3869484","","","","","2019-08-12 06:11:47","Number of least significant '0' digits affects the value in Pandas","<python-3.x><pandas>","1","2","","","","CC BY-SA 4.0","1"
"57660673","1","","","2019-08-26 15:16:40","","1","28","<p>The documentation of the merge of dataframes state that with the suffixes parameter you can set the columnname suffixes for the left and right dataframe respectively (<a href=""https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html"" rel=""nofollow noreferrer"">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html</a>).</p>

<p>According to my example the suffixes are first sorted alphabetically.</p>

<pre><code>A = pd.DataFrame({'prodID':[0,1,2,3,4],'units':[5,6,7,8,9]})
B = pd.DataFrame({'prodID':[0,1,2,3,4],'units':[10,11,12,13,14]})

merged = pd.merge(B,A,on='prodID',suffixes={'_b','_a'})
print(merged)
</code></pre>

<p>Actual result:</p>

<pre><code>   prodID  units_a  units_b
0       0       10        5
1       1       11        6
2       2       12        7
3       3       13        8
4       4       14        9
</code></pre>

<p>What I expected to get:</p>

<pre><code>   prodID  units_b  units_a
0       0       10        5
1       1       11        6
2       2       12        7
3       3       13        8
4       4       14        9
</code></pre>

<p>I don't care about the order in which the columns are after the merge. But when I use the suffixes i want the left dataframe, in my example B, to get the left suffix, '_b'. And the right dataframe the right suffix. </p>

<p>A lot of bugs can occur due to the selection of the wrong column.</p>
","9961843","","9961843","","2019-08-27 09:42:30","2019-08-27 09:42:30","Merging of pandas DataFrames does not keep the order of suffixes","<python><python-3.x><pandas><merge>","0","2","","2019-08-26 15:18:53","","CC BY-SA 4.0","1"
"48796424","1","","","2018-02-14 21:20:52","","0","28","<p>I'm reading two db columns into pandas dataframe. It works fine but time data on Db is like this ""2018-01-18T00:00:00"". I just need to slice the year,month and day. I don't need time since its all 00:00 in db. How do we slice it? Thank you!</p>

<pre><code>tables_prices='''SELECT date, tryprice FROM Price'''

df=pd.read_sql_query(tables_prices, conn)    
x=df['date']   
y=df['tryprice']
</code></pre>
","9361569","","","","","2018-02-14 21:22:48","Slicing Time in Pandas DataFrame","<python-3.x><pandas><sqlite>","1","0","","","","CC BY-SA 3.0","1"
"57300279","1","57300306","","2019-08-01 00:06:40","","1","28","<p>I have a list:</p>

<pre><code>list_1 = ['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04']
list_2 = ['2PM', '3PM', '4PM']
</code></pre>

<p>I want to merge the list do that the new list looks like this:</p>

<pre><code>list = ['2000-01-01 2PM', 
        '2000-01-01 3PM', 
        '2000-01-01 4PM', 
        '2000-01-02 2PM', 
        '2000-01-02 3PM', 
        '2000-01-02 4PM',
        ................
        '2000-01-04 4PM']
</code></pre>

<p>Basically, for each value of <code>list_1</code>, I added every value of <code>list_2</code> separated by a space.</p>

<p>I did this using <code>Python</code></p>

<pre><code>for date in list_1:
  for hour in list_2:
     date +  "" "" + hour 
</code></pre>

<p>How do I do it using <code>Pandas</code>?</p>
","9161607","","","","","2019-08-01 00:47:18","Append value from one list to another one using Pandas","<python-3.x><pandas><dataframe>","2","0","","","","CC BY-SA 4.0","1"
"48721825","1","48721841","","2018-02-10 14:30:32","","1","27","<p>I have a datafame:</p>

<pre><code>import pandas as pd
df= pd.DataFrame({'date':['2017-12-31','2018-02-01','2018-03-01'],'type':['Asset','Asset','Asset'],'Amount':[1,0,0],'Amount1':[1,0,0],'Ted':[1,0,0]})
df
</code></pre>

<p>I want to delete rows where the first three columns are 0. I don't want to use the name of the column as it changes. In this case, I want to delete the 2nd and 3rd rows.</p>
","8300917","","","","","2018-02-10 15:11:36","delete rows based on first N columns","<python-3.x><pandas>","1","0","1","","","CC BY-SA 3.0","1"
"57865532","1","","","2019-09-10 06:58:17","","0","27","<p>Is there a way to compare 2 dataframes with multiple columns, and varies in length (<code>1386</code> vs <code>1383</code> in the below example)? And only output the rows which have the differences?</p>

<p>Example data:</p>

<pre><code>&gt; df_left  



      index        location       date
0         0        Adelaide       2019-01-01
1         1        Adelaide       2019-02-01
2         2        Adelaide       2019-03-01
3         3        Adelaide       2019-04-01
4         4        Adelaide       2019-05-01
...     ...            ...        ...
1381   1381  Western London       2019-03-01
1382   1382  Western London       2019-04-01
1383   1383  Western London       2019-05-01
1384   1384  Western London       2019-06-01
1385   1385  Western London       2019-07-01

[1386 rows x 2 columns]


&gt; df_right
                  location       date
0                 Adelaid        2019-01-01
1                 Adelaide       2019-02-01
2                 Adelaide       2019-03-01
3                 Adelaide       2019-04-01
4                 Adelaide       2019-05-01
...                   ...        ...
1378        Western London       2019-03-01
1379        Western London       2019-04-01
1380        Western London       2019-05-01
1381        Western London       2019-06-01
1382        Western London       2019-07-01

[1383 rows x 2 columns]
</code></pre>

<p>I tried this, but it does not yield the differences</p>

<pre><code>pd.concat([df_left_with_index_columns,df_right_with_index_columns], ignore_index=True, axis=1, join=""outer"")                                                                                                       
                        0          1                2          3
0                Adelaide  2019-01-01        Adelaide  2019-01-01
1                Adelaide  2019-02-01        Adelaide  2019-02-01
2                Adelaide  2019-03-01        Adelaide  2019-03-01
3                Adelaide  2019-04-01        Adelaide  2019-04-01
4                Adelaide  2019-05-01        Adelaide  2019-05-01
...                   ...         ...             ...         ...
1381       Western London  2019-03-01  Western London  2019-06-01
1382       Western London  2019-04-01  Western London  2019-07-01
1383       Western London  2019-05-01             NaN         NaT
1384       Western London  2019-06-01             NaN         NaT
1385       Western London  2019-07-01             NaN         NaT

[1386 rows x 4 columns]
</code></pre>
","618563","","","","","2019-09-10 08:04:04","Diff 2 dataframes with multiple columns and strings only output the differences","<python><python-3.x><pandas><dataframe>","0","7","","","","CC BY-SA 4.0","1"
"57502279","1","57502466","","2019-08-14 21:26:29","","1","27","<p>I am running a sql and output i am reading as pandas df. Now i need to convert the data  in to json and need to normalize the data. I tried to_json  but this give partial solution. </p>

<p>Dataframe output: </p>

<pre><code>| SalesPerson   | ContactID |
|12345  |Tom|
|12345  |Robin|
|12345  |Julie|
</code></pre>

<p>Expected JSON:</p>

<pre><code> {""SalesPerson"": ""12345"", ""ContactID"":""Tom"",""Robin"",""Julie""}
</code></pre>

<p>Please see below code which i tried.</p>

<pre><code>q = Select COL1,  SalesPerson   , ContactIDfrom table;


df = pd.read_sql(q, sqlconn)

df1=df.iloc[:, 1:2]


df2 = df1.to_json(orient='records')
</code></pre>

<h1>also to_json result bracket which i also dont need.</h1>
","11767117","","2538939","","2019-08-14 21:35:40","2019-08-14 21:45:42","How to convert dataframe output to json format and then Normalize the data?","<json><python-3.x><pandas>","1","2","","","","CC BY-SA 4.0","1"
"57667392","1","57667425","","2019-08-27 03:39:47","","-1","27","<p>dataframe <code>test_df</code> containing series <code>a b c d e</code> 
I need to count the number of each unique variable in a that has abc in e
then divide that number by the sum of <code>b</code> and <code>c</code> and output new dataframe
containing a d and g=sum of b and c</p>

<p>`</p>

<pre><code>test_df
a b c d e 
1 3 4 5 abc00 
2 6 5 3 12abc
1 3 1 4 5    &gt;&gt;&gt;&gt;this row will be left out 
1 4 7 3 sdabc
1 4 5 6 78abc
2 2 4 5 abc
3 2 3 4 kk abc
3 4 6 7 abc
2 6 7 9 abcd
1 7 4 1 abcabc
</code></pre>

<p>output will be </p>

<p>output_df
    a d g
    1 5 7
    2 3 11
    1 3 11
    1 6 9
    2 5 6
    3 4 5
    3 7 10
    2 9 13
    1 1 11</p>

<p>`</p>

<p><a href=""https://i.stack.imgur.com/TLPRA.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TLPRA.png"" alt=""image for input data here""></a></p>

<p><a href=""https://i.stack.imgur.com/5mJcs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5mJcs.png"" alt=""image for output data here""></a></p>

<p><a href=""https://i.stack.imgur.com/5mJcs.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/5mJcs.png"" alt=""enter image description here""></a></p>
","11977590","","11977590","","2019-08-27 03:54:50","2019-08-27 03:54:50","How to count the number of variable in a column in dataframe","<python-3.x><pandas><dataframe><data-visualization>","1","0","","","","CC BY-SA 4.0","1"
"58379388","1","","","2019-10-14 14:59:44","","0","27","<p>I am learning Pandas and am hitting a wall creating a dataFrame from the following input:</p>

<pre><code>tracked = {
        'Created': (df1.count()['Created']),
        'Closed': (df1.count()['Closed']),
        'Owner': (df1['Owner'].value_counts().to_dict()),
        'Resolution': (df1['Resolution'].value_counts().to_dict()),
        'Severity': (df1['Severity'].value_counts().to_dict())
    }
</code></pre>

<p>which creates:</p>

<pre><code>pp.pprint(tracked)
{u'Closed': '11', #numpy
 u'Created': '42', #numpy
 u'Owner': {u'foo.bar': 3, #dict
            u'FooBar': 6,
            u'BarFoo': 30,
            u'bar.foo': 3},
 u'Resolution': {u'FalsePositive': 1, u'TruePositive': 10}, #dict
 u'Severity': {1: 7, 2: 31, 3: 4}} #dict
</code></pre>

<p>my current DataFrame</p>

<pre><code>df4 = pd.DataFrame.from_dict({(i): tracked[i]
                                  for i in tracked.keys()},
                                 orient='index').transpose()
</code></pre>

<p>which creates:</p>

<pre><code>pp.pprint(df4)
                                               Owner                                  Resolution             Severity Closed Created
0  {u'foobar': 30, u'foo.bar': 3, u'bar.f...  {u'FalsePositive': 1, u'TruePositive': 10}  {1: 7, 2: 31, 3: 4}     11      42
</code></pre>

<p>my desired output would expand the dicts for Owner, Resolution and Severity over columns or rows.</p>

<p>I have tried a number of SO/Web solutions including trying to flatten the dict which returns a <code>AttributeError: 'int' object has no attribute 'split'</code>. </p>

<p>Any help would be greatly appreciated.</p>
","4713570","","4713570","","2019-10-14 15:06:20","2019-10-14 15:31:27","Pandas Creating a Dataframe from Dictionary which has Numpy array in it","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57664507","1","57665122","","2019-08-26 20:23:49","","0","27","<p>I have 2 dataframes; the first df_data looks like this:</p>

<pre><code>A   B   C
-----------------
0   0   1
1   4   1
1   10  1
2   8   1
2   9   1
3   7   1
3   4   0
4   8   1
5   8   1
6   7   1
9   17  1
9   13  1

...
</code></pre>

<p>The desired output (df_output) looks like this:</p>

<pre><code>Day Zt  Zr
-----------------
1   2   0
2   4   0
3   5   1
4   6   1
5   6   0
6   7   0
7   7   0
8   5   0
9   5   0
10  5   0
11  5   0
12  6   0

...
</code></pre>

<p>Basically I created a new dataframe (df_output) that only has the Days filled, which represents each day of a given month. I have been trying to figure out how to count occurrences of the values in df_data based on the following conditionals (these conditionals don't actually work since it is pulling from different df's):</p>

<pre><code>df_output['Zt'] = (df_data['A'] &lt;= df_output['Day']) &amp; (df_data['B'] &gt;= df_output['Day']) &amp; (df_data['C'] == 1).count()

df_output['Zr'] = (df_data['A'] &lt;= df_output['Day']) &amp; (df_data['B'] &gt;= df_output['Day']) &amp; (df_data['C'] == 0).count()
</code></pre>

<p>The logic for calculating Zt and Zr in more readable format is (if it were a function):</p>

<pre><code>def countZt():
   for each day in df_output['Day']:
   Zt = 0
      for each row in df_data:
         if (df_data['A'] &lt;= day) and (df_data['B'] &gt;= day) and (df_data['C'] == 1):
            Zt = Zt + 1
   return Zt

def countZr():
   for each day in df_output['Day']:
   Zr = 0
      for each row in df_data:
         if (df_data['A'] &lt;= day) and (df_data['B'] &gt;= day) and (df_data['C'] == 0):
            Zr = Zr + 1
   return Zr

</code></pre>

<p>You can see that the only difference between Zt and Zr is that one is counting those where the Bool (C) was True, and the other is counting where it was False.</p>

<p>The resources I've discovered on this subject only guide on how to make conditionals within the same dataframe, but there is a scarcity/absence of resources that show how to use the value of one dataframe to count the other. Any help pointing me in the right direction would be greatly appreciated! Thank you!</p>
","6799378","","6799378","","2019-08-26 20:58:19","2019-08-26 21:24:57","Count conditional occurrence of values in multiple cols of one df using another df","<python-3.x><pandas><numpy>","1","6","","","","CC BY-SA 4.0","1"
"57866967","1","57866993","","2019-09-10 08:36:27","","0","26","<p>I have two dataframes, when I use append concept the same does not return the required result.</p>

<pre><code>firstDF = pd.DataFrame({'mac':[1,3],'location':[['kitchen', 'kitchen', 'kitchen', 'kitchen', 'kitchen'],['conference']]})
predictedDF = pd.DataFrame({'mac':[2],'location':[['lab']]})

if predictedDF['mac'].isin(firstDF['mac']).any():
    pass
else:

    firstDF.append(predictedDF,ignore_index=True)
</code></pre>

<p>The same returns the result as,</p>

<pre><code>      mac                                       location
 0    1    [kitchen, kitchen, kitchen, kitchen, kitchen]
 1    3                                     [conference]
</code></pre>

<p>whereas I should get the result as,</p>

<pre><code> mac                                       location
 0    1    [kitchen, kitchen, kitchen, kitchen, kitchen]
 1    3                                     [conference]
 2    2                                            [lab]
</code></pre>
","5391663","","","","","2019-09-10 08:38:08","Python Pandas : Append does not return the required values","<python-3.x><pandas><append>","1","0","","","","CC BY-SA 4.0","1"
"58376541","1","58376564","","2019-10-14 12:12:57","","1","26","<p>I have the following dataframe</p>

<pre><code>import pandas as pd
 tmp = pd.DataFrame({'date':['2018-08-31','2018-07-30','2018-07-30','2018-07-31']})
</code></pre>

<p>I would like to create a new column in the <code>tmp</code> dataframe, which will be an increasing index, starting from 1 for the minimum <code>date</code> and it will increase as the date increases.</p>

<p>The output dataframe should look like this:</p>

<pre><code>         date  idx
0  2018-08-31    3
1  2018-07-30    1
2  2018-07-30    1
3  2018-07-31    2
</code></pre>

<p>Any ideas ?</p>
","5868293","","","","","2019-10-14 12:14:10","How to create an index column with groupby in pandas","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"49626346","1","","","2018-04-03 09:16:08","","1","26","<p>I have a dataframe where one of the columns contains strings. But they have some tab formatting for each of them. Below is a snippet of how it looks like</p>

<pre><code>formatted_line_items[1:3]
Out[393]: ['\t&lt;string1&gt;', '\t\t&lt;string2&gt;']
</code></pre>

<p>However when I write the dataframe using to_csv the formatting is lost. How can I write this to a csv file or excel file and still retain the formatting?</p>

<p>EDIT: I got to know that csv doesn't retain formatting so I used the pandas to_excel function but still no luck with the formatting.</p>
","4051357","","4051357","","2018-04-03 09:42:54","2018-04-03 10:56:48","tabs not getting displayed when writing dataframe to csv in pandas","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 3.0","1"
"57263801","1","","","2019-07-30 03:08:49","","1","26","<p>I have created a dataframe by reading the CSV file but I want to create a new DataFrame using the existing dataframe and put files into it.So my created dataframe looks like this:
<a href=""https://i.stack.imgur.com/Arrq4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Arrq4.png"" alt=""enter image description here""></a></p>

<p>The code is :</p>

<pre><code>import pandas as pd
df_csv = pd.read_csv('./mean_data.csv',names = ['A','B','C'] )
print(df_csv)
</code></pre>

<p>I am creating the new DataFrame using the below code:</p>

<pre><code>import pandas as pd

df_csv = pd.read_csv('./mean_data.csv',names = ['A','B','C'] )
df_we=pd.DataFrame(df_csv,columns=['TempOut','HiTemp','LowTemp','OutHum','WindSpeed','Rain'],index = df_csv['A'])
print(df_we)
</code></pre>

<p>But the output displays the value NaN . The out looks like this:
<a href=""https://i.stack.imgur.com/3tklE.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3tklE.png"" alt=""enter image description here""></a></p>
","11634498","","","","","2019-07-30 04:55:22","create dataframe using the values from csv file, (Displays NaN values)","<python-3.x><pandas><dataframe>","0","0","","","","CC BY-SA 4.0","1"
"56715385","1","56715438","","2019-06-22 12:16:29","","2","26","<p>I have two dataframes (df1 and df2) as follows:</p>

<pre><code>In [4]:df1
   Year  Annual Counts
0  1979          45345
1  1980          15381
2  1981          32171
3  1982          30288
4  1983          50573
In [5]:df2
   Year  CanESM2  GFDL-ESM2M  HadGEM2-ES365  IPSL-CM5A-MR  NorESM1-M
0  1984    10645       48143          57366         26979      37603
1  1985    15918       17178          34617         21304      31956
2  1986    51790       44111          50017         29233      61203
3  1987    34039       14504          23136         35848      34688
4  1988    68641       67681          24322         39591      34553
</code></pre>

<p>and I would like to combine both dataframes as follows:</p>

<pre><code>   Year  CanESM2  GFDL-ESM2M  HadGEM2-ES365  IPSL-CM5A-MR  NorESM1-M
0  1979    45345       45345          45345         45345      45345
1  1980    15381       15381          15381         15381      15381
2  1981    32171       32171          32171         32171      32171
3  1982    30288       30288          30288         30288      30288
4  1983    50573       50573          50573         50573      50573 
5  1984    10645       48143          57366         26979      37603
6  1985    15918       17178          34617         21304      31956
7  1986    51790       44111          50017         29233      61203
8  1987    34039       14504          23136         35848      34688
9  1988    68641       67681          24322         39591      34553
</code></pre>

<p>One simple solution I have:</p>

<pre><code>df1 = pd.DataFrame(file1)
df1_list = df1['Annual Counts'].tolist()
# empty lists
ext1=[] ; ext2=[] ; ext3=[] ; ext4=[] ; ext5=[]
df2 = pd.DataFrame(file2)
models = ['CanESM2','GFDL-ESM2M','HadGEM2-ES365','IPSL-CM5A-MR','NorESM1-M']
for idx,m in enumerate(models):
    ext+str(idx).append(df1_list)
    df2_mod = df2[m].tolist()
    ext+str(idx).extend(df2_mod)
</code></pre>

<p>Any suggestions if pandas have any function to perform this task without the need to create multiple lists and then extending them?</p>
","1801769","","","","","2019-06-22 12:43:10","Pandas dataframe columns extension using another dataframe column values","<python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"57746367","1","57746401","","2019-09-01 13:26:16","","3","26","<p>I have table Car as below:</p>

<pre><code>Car       Color 
Proton    White#Red
Perudua   Red
Proton    Black
Honda     Silver#Brown#Black
</code></pre>

<p>My expected output is filter and remain the row which the Color column contain ""#"" symbol only as below:</p>

<p>Expected ouput:</p>

<pre><code>Car       Color 
Proton    White#Red
Honda     Silver#Brown#Black
</code></pre>

<p>Anyone have ideas?</p>
","7892936","","","","","2019-09-01 13:30:50","How to use python pandas to return the row which the column contain # symbol?","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"56772319","1","56772436","","2019-06-26 12:00:23","","0","26","<p>I am trying to initialize multiple columns in pandas df using apply function.
I have a dataframe df as :</p>

<pre><code>A
dog
cat12
rat_1 wow
</code></pre>

<p>what I want is </p>

<pre><code>A          length   alphabet   digit
dog        3        3          0
cat12      5        3          2
rat_1 wow  9        6          1
</code></pre>

<p>This is the code I am trying:</p>

<pre><code>def calculate(x):
    a, l, d = 0, 0, 0
    for i in x:
        if i.isalpha():
            a += 1
        elif i.isnum():
            d += 1
        l += 1
    return l, a, d

df.loc[:, ['length', 'alphabet', 'digit']] = df['A'].apply(calculate)
</code></pre>

<p>This is not working since this format works only for a single column. I need a similar format so that I can add parameters just by adding them in calculate function. This won't increase the number of iterations over the string.</p>

<p>I am using python3 and pandas.</p>
","9099959","","","","","2019-06-26 12:22:02","Initialize multiple columns in a dataframe using mutiple operations","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57694178","1","57694236","","2019-08-28 13:57:35","","2","26","<p>In this previous question: <a href=""https://stackoverflow.com/questions/57693128/nesting-a-counter-within-another-dictionary-where-keys-are-dataframe-columns/57693171?noredirect=1#comment101831679_5769317"">Nesting a counter within another dictionary where keys are dataframe columns</a> , @Jezrael showed me how to nest a counter within another dictionary. </p>

<p>My dataframe has another column which is effectively a superset of the ID, and is not named in a way which allows for the SuperID to be logically derived from an ID. </p>

<pre><code>SuperID   ID      Code
E1        E1023   a
E1        E1023   b
E1        E1023   b
E1        E1023   b
E1        E1024   b
E1        E1024   c
E1        E1024   c
E2        E1025   a
E2        E1025   a
E2        E1026   b
</code></pre>

<p>Using the dictionary which was produced in the last stage,</p>

<pre><code>d = {k: v.value_counts().to_dict() for k, v in df.groupby('ID')['Code']}
print (d)

{'E1023': {'b': 3, 'a': 1}, 'E1024': {'c': 2, 'b': 1}, 'E1025 : {'a' : 2}, 
'E1026 : {'b' : 2}}
</code></pre>

<p>I would like to perform another level of nesting, where the SuperID is the key of the outer dictionary with the inner dictionary being the dictionary produced above, with IDs grouped by SuperID. So the dictionary should effectively be of the format:</p>

<pre><code>new_d = {k: v for k in df.SuperID, v in df.groupby('SuperID')[ID FROM d]}

{'E1': {'E1023': {'b':3, 'a':1}, 'E1024' : {'c':2, 'b': 1}...} 'E2': {'E1025: {'a' : 2}...}}
</code></pre>

<p>I would like to keep the original dictionary, produced by @Jezrael to allow me to perform an easy lookup by ID which I will need to do at a latter stage.</p>
","3058703","","3058703","","2019-08-28 14:01:35","2019-08-28 14:09:35","Nesting a dictionary within another dictionary, grouping by values in a Pandas Dataframe","<python-3.x><pandas><dictionary><pandas-groupby>","1","0","","","","CC BY-SA 4.0","1"
"56667214","1","","","2019-06-19 12:02:45","","-1","26","<p>My data doesn't contain columns when I am adding the columns in the data using <code>dataFrame.columns = ['XYZ','ABC']</code> then the 1st row is getting replaced by the column names.</p>
","10216243","","","","","2019-06-19 12:13:57","How to add columns in the data when columns are not present?","<python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"57022225","1","57022332","","2019-07-13 19:31:25","","0","25","<p>I have a data frame of messages from a social network. In this date frame, I created a new column without stop words with the use of a lambda function. As a result in this new column, the values ​​are inserted within a list. What I need is to get the values ​​within this list.</p>

<p>What I have:</p>

<pre><code>raw_data = {'CLASS':['1', '2', '3', '1', '2','3','2'],
        'MESSAGES': [['mama', 'said', 'home'],['dad', 'said', 'soccer', 'reality'], ['matrix', 'you'],
                     ['run', 'neo', 'free'], ['what', 'doing'], ['begnning', 'believe'],
                    ['choice', 'let', 'you', 'free', 'mind']]}
dfRaw = pd.DataFrame(raw_data, columns = ['CLASS','MESSAGES'])
</code></pre>

<p>What I need:</p>

<pre><code>clean_data = {'CLASS':['1', '2', '3', '1', '2','3','2'],
            'MESSAGES':['mama, said, home', 'dad, said, soccer, reality', 'matrix, you', 'run, neo, free', 'what, doing','begnning, believe','choice, let, you, free, mind']}
dfEndResult = pd.DataFrame(clean_data, columns = ['CLASS','MESSAGES'])
</code></pre>

<p>I read a topic right here on the Stack where the function was suggested:</p>

<pre><code>dfRaw.applymap(lambda x: x if not isinstance(x, list) else x[0] if len(x) else '')
</code></pre>

<p>but this function for me is not interesting because it is efficient when the list has only one element. In my case each cell has a different size list.</p>

<p>Thank you all for the help.</p>
","6278135","","","","","2019-07-13 19:47:33","How to get list values ​that are in rows from a dataframe","<python-3.x><pandas><list><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"56690343","1","","","2019-06-20 16:29:14","","1","25","<p>I have a <code>df</code>,</p>

<pre><code>code    y_m       day_1     day_2
13      201906    30        40
13      201906    10        20
14      201903    20        40
14      201903    20        30
</code></pre>

<p>I want to create a column</p>

<pre><code>df['delta'] = df['day_2'] - df['day_1']

code    y_m       day_1     day_2    delta
13      201906    30        40       10 
13      201906    10        20       10
14      201903    20        40       20
14      201903    20        30       10
</code></pre>

<p>then <code>groupby</code> <code>code</code> and <code>y_m</code>, sum the delta and get the group size into another dataframe at the same time; but</p>

<pre><code>df.groupby(['code', 'y_m']).size().reset_index(name = 'count')
</code></pre>

<p>only get the group size; so how to get aggregated <code>delta</code> as well into the same dataframe;</p>

<pre><code>code    y_m       delta    count
13      201906    20        2
14      201903    30        2
</code></pre>
","766708","","","","","2019-06-20 16:29:14","pandas groupby get aggregation on a column and group size into a dataframe","<python><python-3.x><pandas><dataframe><pandas-groupby>","0","3","","2019-06-20 16:33:26","","CC BY-SA 4.0","1"
"56742297","1","56742356","","2019-06-24 18:49:11","","-1","25","<p>My data set contains details of one company's stock. And i have calculated the %change in closing price of the stock for 2 consecutive days and that column is called ""Day_Perc_Change"". I have one more column called ""Trend"" which needs to be filled with constraints:
     'Slight or No change' for 'Day_Perc_Change' in between -0.5 and 0.5
     'Slight positive' for 'Day_Perc_Change' in between 0.5 and 1</p>

<pre><code>When i run combining these statements with 'and' i get 
</code></pre>

<p>ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().</p>

<pre><code>data.loc[data['Day_Perc_Change']&lt;0.5,'Trend'] = 'Slight or No change' 
data.loc[data['Day_Perc_Change']&gt;-0.5,'Trend'] = 'Slight or No change'
data.loc[data['Day_Perc_Change']&lt;1,'Trend'] = 'Slight positive' 
data.loc[data['Day_Perc_Change']&gt;0.5,'Trend'] = 'Slight positive'
</code></pre>
","11671222","","4685471","","2019-06-24 20:03:31","2019-06-24 20:03:31","how to combine two logical expressions and use both the conditional to fill a column","<python-3.x><pandas>","1","1","","2019-06-24 18:58:05","","CC BY-SA 4.0","1"
"57299179","1","57299263","","2019-07-31 21:39:14","","1","25","<p>My <code>df</code> is an hourly dataset given below:</p>

<pre><code>time                    Open
2017-01-01 00:00:00     5.2475
2017-01-01 01:00:00     5.2180
2017-01-01 02:00:00     5.2128
...., ....
2017-12-31 23:00:00     5.7388
</code></pre>

<p>I want to delete/remove the <code>row</code> if it matches the <code>Date</code> <code>Series</code> in this list:</p>

<pre><code>remove = ['2017-01-01','2017-05-21', '2017-09-19']
</code></pre>

<p>Please note that the data in <code>remove</code> is a single <code>Day</code> whereas the data in <code>df</code> is <code>hourly</code>. </p>

<p>I want to remove any <code>hourly</code> data that matches the <code>Day</code> in <code>remove</code></p>

<p><strong>What did I do?</strong></p>

<p>1: I tried <code>df2 = df[~df.time.str.startswith(remove)]</code> but it does not work and gives me float point error.</p>

<p>2: I also tried <code>df2 = df[~df.time.isin(a)]</code> but it only removes if it matches entirely not partially.</p>

<p>Could you please help me solve this?</p>
","9161607","","","","","2019-07-31 21:49:30","How to delete row if the data matches in list using Pandas","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"56657349","1","","","2019-06-18 21:32:37","","0","25","<p>I am comparing two excel files using pandas and monitoring certain columns to identify any changes that were made.  Currently, I am creating a boolean column to see if column A in df1 is equal to column A in df2, which gives me a new column of True/False values.  I do the same thing for column B in both the new and old dataframe. I am having trouble creating a ""master change"" column that will tell me if a change was made in either or both of the columns I am checking.</p>

<p>Right now, my ""master change"" column checks to see if the boolean column checking column A and B are equal.  If there is a change in only 1 column (ColA = True, ColB = False), my ""master change column correctly marks it as False, so I can later identify that as a row that needs to be updated.
However, when there is a change in BOTH columns (ColA=False, ColB=False), my ""master change"" column marks that row as True, as expected. </p>

<pre><code>old_data = [[1, 'red', 'short'], [2, 'blue', 'medium'],[3, 'green', 'long']]  
new_data = [[1, 'red', 'short'], [2, 'green', 'long'],[3, 'green', 'short']]

df1 = pd.DataFrame(old_data, columns=['ID_num', 'original_color', 'original_length'])
df2 = pd.DataFrame(new_data, columns=['ID_num', 'current_color', 'current_length'])

df_combined = pd.merge(df1, df2, on='ID_num', how='left')

df_combined['color_change'] = df_combined['current_color'] == df_combined['original_color']
df_combined['length_change'] = df_combined['current_length'] == df_combined['original_length']
df_combined['master_change'] = df_combined['color_change'] == df_combined['length_change']

df_combined['master_change']
</code></pre>

<p>This gives me a ""master_change"" column value of:    </p>

<p>True<br>
True<br>
False</p>

<p>How can I get pandas to give me a value of False in the ""master_change"" column if BOTH the color and length values have changed? (i.e., False, False --> False)</p>
","9301635","","","","","2019-06-18 21:44:56","How can I create a new column to track any changes between two columns in a dataframe?","<python><python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"56626347","1","56626530","","2019-06-17 07:12:13","","1","25","<p>I created a list out from what LDA Model did:
<em><code>lda_model.get_document_topics(bag_of_words)</code></em></p>

<p>The model consist of <strong>7 topics</strong> and gave me this result using list comprehension:</p>

<pre><code>[v for  v in lda_model_bigram.get_document_topics(bow_corpus_bigram)]
</code></pre>

<p>The list-to-DataFrame</p>

<pre><code>df = pd.DataFrame([[(0, 0.23410834), (1, 0.010244273), (2, 0.010266962), (3, 0.31661528), (4, 0.010282155), (5, 0.010329775), (6, 0.4081532)],
 [(0, 0.24538451), (3, 0.1353473), (6, 0.58342004)],
 [(0, 0.21097288), (1, 0.2306254), (3, 0.5263941)],
 [(0, 0.020534758), (1, 0.02050926), (2, 0.020555891), (3, 0.020502212), (4, 0.57683885), (5, 0.020568976), (6, 0.3204901)],
 [(2, 0.37945262), (4, 0.12737828), (6, 0.47517183)],
])
</code></pre>

<p>It looked like this:
<a href=""https://i.stack.imgur.com/zzsnq.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zzsnq.png"" alt=""Below""></a></p>

<p>My question is how can I align the values based on the first element of the tuple to make it look like the one below:</p>

<p><a href=""https://i.stack.imgur.com/bMIl6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bMIl6.png"" alt=""After""></a></p>
","10193760","","","","","2019-06-17 07:23:03","Moving the elements in the list to its desirable column","<python><python-3.x><pandas><lda>","1","1","","","","CC BY-SA 4.0","1"
"57702699","1","","","2019-08-29 03:44:11","","0","24","<p>Given the following Dataframe</p>

<pre><code>import pandas as pd                                                              
from collections import OrderedDict                                              


d = OrderedDict([ ('Date_Time', ['2016-01-18 00:00:00', '2016-01-18 12:00:00', '2016-01-19 00:00:00', '2016-01-19 12:00:00']),
           ('Symbol', ['AUD', 'AUD', 'AUD', 'AUD']),                              
           ('Hit',  [False, False, True, False]),                                 
            ] )                                                                   
df = pd.DataFrame.from_dict(d)                                                   
df = df.set_index('Date_Time')  

print(df)
</code></pre>

<hr>

<pre><code>                    Symbol    Hit
Date_Time                        
2016-01-18 00:00:00    AUD  False
2016-01-18 12:00:00    AUD  False
2016-01-19 00:00:00    AUD   True
2016-01-19 12:00:00    AUD  False
</code></pre>

<p>How would one go about passing the <strong><em>Date_Time</em> index and the following</strong> to a function only when <strong>Hit</strong> is <em>true</em> to then return the result of that same function to an other column ( say <em>Score</em> )</p>

<p>in this particular case in pseudo code :</p>

<pre><code>    my_func(dt1, dt2):
        #do something on dt1 and d2
        return True
</code></pre>
","415088","","","","","2019-08-29 04:24:28","Applying a function on specific Dataframe rows with datetime index as arguments","<python><python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"51970709","1","51970953","","2018-08-22 15:53:34","","1","24","<p>I have 2 dataframes like this</p>

<pre><code>frame1=pd.DataFrame(columns=['A','B','C'])


a=['d1','d2','d3']
b=['d4','d5']
tups=([('T1',x) for x in a]+
  [('T2',x) for x in b])

cols=pd.MultiIndex.from_tuples(tups,names=['Trial','Data'])
frame2=pd.DataFrame(columns=cols)
</code></pre>

<p>My goal is to have both DataFrames in one, and then add some rows of data. The resulting DataFrame would be like</p>

<pre><code>  Trial    A   B   C        T1         T2
  Data                  d1  d2  d3   d4  d5
0          1   2   3    4   5   6    7   8
1           ...     
...
</code></pre>

<p>That could somehow be achieved if I did </p>

<pre><code>frame2['A']=1
frame2['B']=2
frame2['C']=3
</code></pre>

<p>But this is not a clean solution, and I can't create the frame and then add data, for I would be required to at least insert manually the first row.</p>

<p>I tried</p>

<pre><code>frame3=frame1.join(frame2)

&gt;&gt;  A1   A2   A3 (T1, d1) (T1, d2) (T1, d3) (T2, d4) (T2, d5)
</code></pre>

<p>This I think is not multi column level.
My second trial</p>

<pre><code>tup2=([('A1',),('A2',),('A3',)]+[('T1',x) for x in a]+
  [('T2',x) for x in b])
cols2=pd.MultiIndex.from_tuples(tup2,names=['Trial','Data'])
data=[1,2,3,4,5,6,7,8]
frame20=pd.DataFrame(data,index=cols2).T 

 Trial  A1  A2  A3 T1       T2    
 Data   NaN NaN NaN d1 d2 d3 d4 d5      
0        1   2   3  4  5  6  7  8
</code></pre>

<p>This one works fine when trying to query it <code>frame20.loc[0,'A1'][0]</code> but if for example I do</p>

<pre><code>frame20['Peter']=1234
&gt;Trial  A1  A2  A3 T1       T2      Peter
 Data   NaN NaN NaN d1 d2 d3 d4 d5      
0        1   2   3  4  5  6  7  8   1234
</code></pre>

<p>being the column 'Peter' what I desire as opposed to for example A1, which is what I get.
My third trial</p>

<pre><code>tup3=(['A','B','C']+[('T1',x) for x in a]+
  [('T2',x) for x in b])
cols3=pd.MultiIndex.from_tuples(tup3,names=['Trial','Data'])
frame21=pd.DataFrame(data,index=cols3).T
</code></pre>

<p>returned exactly the same as the second one.</p>

<p>So, what I'm looking for, is a way to do</p>

<pre><code>pd.DataFrame(rows_of_data,index=alfa).T  #or
pd.DataFrame(rows_of_data,columns=beta)
</code></pre>

<p>where either alfa or beta are in a correct format.</p>

<p>Also, as a bonus, let's say I finally came up with a way to do</p>

<pre><code>finalframe=pd.DataFrame(columns=beta)
</code></pre>

<p>How do I have to use <code>concat</code>,<code>append</code> or <code>join</code> so I can add a random row of data such as <code>data=[1,2,3,4,5,6,7,8]</code> to my empty but perfectly created finalframe?</p>

<p>Thank you, best regards</p>
","7957633","","","","","2018-08-22 16:07:15","Create frame wih both single and multi-level columns and add data to it","<python-3.x><pandas><multi-level>","1","0","1","","","CC BY-SA 4.0","1"
"57657742","1","","","2019-08-26 12:13:30","","0","24","<p>I have the following code to process a large CSV file. The function <em>processRecord</em> is a a cpu-intensive multi-threaded function that sends some HTTP requests and stores the collected results in a database. I want to be able to process a maximum of n=25 different records at the same time by spawning a new process for each row. However, I can not figure out why the number of created processes keeps increasing on and on, and not stopping at the allowed limit! Any help would be strongly appreciated. Thanks!</p>

<pre><code>def processWebsites(largeCSV, chunksize= 10**3):

    nProcessWorkers = 25
    skipRows = 500
    iteration = 0
    exectuor = ProcessPoolExecutor(max_workers=nProcessWorkers)
    for chunk_df in pd.read_csv(largeCSV, chunksize=chunksize, usecols=[0, 1], header=None):
        print(""started_chunk: %s -- %s""%((iteration-1)*chunksize, iteration*chunksize))
        for (index, row) in chunk_df.iterrows():
            if (iteration == 0) and (index&lt;=skipRows): continue
            else:
                param = row[1]
                exectuor.submit(processRecord, (param))
        iteration = iteration + 1
</code></pre>
","2850116","","","","","2019-08-26 12:13:30","ProcessPoolExecutor creating more than allowed maximum workers","<python><python-3.x><pandas><multiprocessing>","0","2","1","","","CC BY-SA 4.0","1"
"49619508","1","","","2018-04-02 22:13:10","","0","24","<p>I started learning pandas and stuck at below issue:</p>

<p>I have two large DataFrames:
df1=</p>

<pre><code>ID                                 KRAS        ATM
TCGA-3C-AAAU-01A-11R-A41B-07       101         32
TCGA-3C-AALI-01A-11R-A41B-07       101         75
TCGA-3C-AALJ-01A-31R-A41B-07       102         65
TCGA-3C-ARLJ-01A-61R-A41B-07       87          54
</code></pre>

<p>df2=</p>

<pre><code>ID                                 BRCA1        ATM
TCGA-A1-A0SP                       54           65
TCGA-3C-AALI                       191          8
TCGA-3C-AALJ                       37           68
</code></pre>

<p>The ID is the index in both df. First, I want to cut the name of the ID to only the first 10 digits ( convert TCGA-3C-AAAU-01A-11R-A41B-07 to TCGA-3C-AAAU) in df1. Then I want to produce a new df from df1 which has the ID that exist in df2.</p>

<p>df3 should look:</p>

<pre><code> ID                                KRAS        ATM 
TCGA-3C-AALI                       101         75 
TCGA-3C-AALJ                       102         65
</code></pre>

<p>I tried different ways but failed. Any suggestions on this, please?</p>
","9586779","","9209546","","2018-04-02 22:23:42","2018-04-02 22:27:53","How to manipulate the index in one dataframe and filter for indices in another","<python><python-3.x><pandas>","2","0","","","","CC BY-SA 3.0","1"
"48553276","1","48553379","","2018-01-31 23:43:59","","1","24","<p>i am willing too add a new column with three consecutives 1 whenever rap is equal to one. The three consecutive 1 must be in the same year when rap is equal one and the two before from that one. New columns must be by id (i have a data panel).</p>

<p>df looks like this:</p>

<pre><code>id  year  rap  cohort  jobs  year_of_life  
1  2009    0     NaN      10      NaN       
1  2012    0     2012     12      0         
1  2013    0     2012     12      1         
1  2014    0     2012     13      2         
1  2015    1     2012     15      3         
1  2016    0     2012     17      4       
1  2017    0     2012     18      5         
2  2009    0     2009     15      0         
2  2010    0     2009     2       1         
2  2011    0     2009     3       2         
2  2012    1     2009     3       3         
2  2013    0     2009     15      4         
2  2014    0     2009     12      5         
2  2015    0     2009     13      6         
2  2016    0     2009     13      7         
</code></pre>

<p>expected output:</p>

<pre><code>id  year  rap  cohort  jobs  year_of_life  rap_new
1  2009    0     NaN      10      NaN       0  
1  2012    0     2012     12      0         0   
1  2013    0     2012     12      1         1
1  2014    0     2012     13      2         1
1  2015    1     2012     15      3         1
1  2016    0     2012     17      4         0
1  2017    0     2012     18      5         0
2  2009    0     2009     15      0         0
2  2010    0     2009     2       1         1
2  2011    0     2009     3       2         1
2  2012    1     2009     3       3         1
2  2013    0     2009     15      4         0
2  2014    0     2009     12      5         0
2  2015    0     2009     13      6         0
2  2016    0     2009     13      7         0
</code></pre>
","8627559","","9209546","","2018-03-02 03:45:21","2018-03-02 03:45:21","transforming a column of a dataframe according to a condition","<python><python-3.x><pandas><group-by>","2","0","","","","CC BY-SA 3.0","1"
"48690422","1","48690473","","2018-02-08 16:35:27","","0","24","<p>I have a dataframe with many columns but among them there are six columns named: people_emotions_anger, people_emotions_joy, people_emotions_surprise end so on.</p>

<p>Is there any clever way to extract only those columns which have prefix ""people_emotions""?</p>

<p>I know i can extract every single column and then merge them into new dataframe but maybe there is more efficient way?</p>
","8069542","","","","","2018-02-08 16:43:18","extracting columns with a certain name. dataframe python","<python-3.x><pandas><dataframe>","2","0","","","","CC BY-SA 3.0","1"
"57744076","1","","","2019-09-01 07:39:40","","0","24","<p>Any Idea for below,</p>

<pre><code>dictList = {'name':[""aparna"", ""pankaj"", ""sudhir"", ""Geeku"", ""feku""], 
    'degree': [""MTech"", ""BCA"", ""MTech"", ""MBA"", ""BCA""], 
    'score':[90, 40, 0, 98, 0]}
df = pd.DataFrame(dictList)
score = df.loc[ df['degree'] == 'MTech', 'score'].reset_index(drop=True)
# wherever a values is zero, copy from previous values 
for i in range(score.size):
    if score[i] == 0 and i !=0:
        score[i] = score[i-1]
df.loc[ df['degree'] == 'MTech', 'score'] = score
print(df)


Final Output :: 
    name degree  score
0  aparna  MTech   90.0
1  pankaj    BCA   40.0
2  sudhir  MTech    NaN  ---&gt; it should be 90.0, but index mismatch.
3   Geeku    MBA   98.0
4    feku    BCA    0.0
</code></pre>

<p>One way would be to preserve the old index and then do copy from prev value, Any better idea ?</p>
","4578652","","9403827","","2019-09-01 11:45:28","2019-09-01 11:45:28","Pandas doesn't place a value from series if index is not matching","<python-3.x><pandas>","1","5","","","","CC BY-SA 4.0","1"
"48633059","1","","","2018-02-05 23:17:20","","0","24","<p>I write a function to generate a plot for a given value of the variable 'Course'.</p>

<p>The code is:</p>

<p>def plot_course(course):</p>

<pre><code>tp = df[df['COURSE'] == course]
g = sns.FacetGrid(tp, hue=""SEX"",palette=""Set1"",hue_order=[""F"", ""M""])
g = g.map(sns.distplot, ""GRD_PTS_PER_UNIT"").add_legend()
g.fig.subplots_adjust(top=0.9)
g.fig.suptitle(course, fontsize=16)
</code></pre>

<p>And I then run:</p>

<p>plot_course('PSYCH350')</p>

<p>plot_course('ECON101')</p>

<p>plot_course('PHYSICS140')</p>

<p>plot_course('PHYSICS140')</p>

<p>The function works, but they are stacked vertically (like the screenshot), and I don't know how to put these plots into a grid of 2*2, or into one line.</p>

<p>Can someone help? Thank you!</p>

<p><a href=""https://i.stack.imgur.com/odpxi.png"" rel=""nofollow noreferrer"">they are stacked, and I want to organize them, for example, into one line</a></p>
","9319196","","","","","2018-02-05 23:17:20","python pandas - how to organize plots that are separately generated by a function","<python-3.x><pandas>","0","2","","","","CC BY-SA 3.0","1"
"57457439","1","57457600","","2019-08-12 07:54:38","","0","24","<p>I need to verify phone numbers as valid and invalid from my data.
I am using phonenumbers library in python </p>

<p>I have created a for loop which works but it is too slow so I was trying to use same for loop inside apply function but getting index error</p>

<pre><code>
    for i in range(len(df)):

        num = df.loc[i,'Primary Phone #']
        region = df.loc[i,'Override Address Country']
        try:
            output = phonenumbers.parse(num, region=region)
        except phonenumbers.NumberParseException:
            df.loc[i,'validation'] = False
        else:
            df.loc[i,'validation'] = phonenumbers.is_valid_number(output)
</code></pre>

<p>temp_data.apply(number_validation, axis = 0/1)</p>

<p>IndexingError: ('Too many indexers', 'occurred at index Work Order Code')</p>
","11040856","","","","","2019-08-12 08:07:43","Getting IndexingError while using custom function with apply in python","<python-3.x><pandas><apply>","1","0","","","","CC BY-SA 4.0","1"
"57295935","1","","","2019-07-31 17:11:12","","0","23","<p>My sample data looks like this.</p>

<pre><code>Date        14 Day Moving Average   Upper Band  Lower Band  Average_Price
2017-05-15  773.170797              1054.136298 492.205297  917.04
2017-05-16  773.170797              1054.136298 492.205297  897.29
2017-05-17  773.170797              1054.136298 492.205297  887.49
2017-05-18  773.170797              1054.136298 492.205297  902.17
2017-05-19  773.170797              1054.136298 492.205297  842.98
</code></pre>

<p>'Buy' if the stock price is below the lower Bollinger band</p>

<p>'Liquidate Short' if the stock price is between the lower and middle Bollinger band</p>

<p>'Hold Short/ Liquidate Buy' if the stock price is between the middle and upper Bollinger band</p>

<p>'Short' if the stock price is above the upper Bollinger band</p>

<p>How shall i proceed further?</p>
","10632473","","10632473","","2019-07-31 17:16:28","2019-07-31 17:16:28","Is there a way to find 'Call' function for time series data?","<python><python-3.x><pandas><time-series><logical-operators>","0","4","","","","CC BY-SA 4.0","1"
"56783595","1","","","2019-06-27 03:43:46","","0","23","<p>I have dataframe df with column x , column x have many values i want to plot bar . divde data in bins from 0 to 1  2to 3 ,4 to 5. each bin bar and percentage of each bar</p>

<p>i tried this now but i dont know how to give percentage of each bar</p>

<pre><code>out = pd.cut(xz, bins=[0,1, 2, 6,], include_lowest=True)
ax = out.value_counts(sort=False).plot.bar(rot=0, color=""b"", figsize=(6,4))
</code></pre>
","11327242","","","","","2019-06-27 03:43:46","How to bar plot with percentage of each bar","<python-3.x><pandas><matplotlib>","0","2","","2019-06-27 09:06:11","","CC BY-SA 4.0","1"
"57502031","1","57502111","","2019-08-14 21:04:12","","0","23","<p>My dataset <code>df</code> is <code>minute</code> based and looks like this:</p>

<pre><code>Time                    Open
2017-01-03 07:00:00     5.2475
2017-01-03 07:01:00     5.2475
2017-01-03 07:02:00     5.2475
2017-01-03 07:03:00     5.2475
2017-01-03 07:04:00     5.2475
2017-01-03 07:05:00     5.2475
2017-01-03 07:06:00     5.2475
.....
</code></pre>

<p>For each day, I want to start at <code>7:15:00</code> and not <code>7:00:00</code></p>

<p><strong>What did I do?</strong></p>

<p>I tried to remove the first 15 <code>rows</code> but then I would need to do it for each <code>day</code> which would be a lot of repeated work. </p>

<p>How can I identify a <code>day</code> and then for every <code>7 AM</code>, start at <code>7:15:00</code> using Pandas?</p>
","9161607","","","","","2019-08-14 21:26:13","How to remove 15 mins data from a single day hour using Pandas","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"50418561","1","50418695","","2018-05-18 20:06:31","","1","23","<p>For a sample DataFrame like,</p>

<pre><code>&gt;&gt;&gt; import pandas as pd
&gt;&gt;&gt; index = pd.date_range(start='1/1/2018', periods=6, freq='15T')
&gt;&gt;&gt; data = ['ON_PEAK', 'OFF_PEAK', 'ON_PEAK', 'ON_PEAK', 'OFF_PEAK', 'OFF_PEAK']
&gt;&gt;&gt; df = pd.DataFrame(data, index=index, columns=['tou'])
&gt;&gt;&gt; df
                          tou
2018-01-01 00:00:00   ON PEAK
2018-01-01 00:15:00  OFF PEAK
2018-01-01 00:30:00   ON PEAK
2018-01-01 00:45:00   ON PEAK
2018-01-01 01:00:00  OFF PEAK
2018-01-01 01:15:00  OFF PEAK
</code></pre>

<p>How to get all indexes for which <code>tou</code> value is not <code>ON_PEAK</code> but of row before them is <code>ON_PEAK</code>, i.e. the output would be:</p>

<pre><code>['2018-01-01 00:15:00', '2018-01-01 01:00:00']
</code></pre>

<p>Or, if it's easier to get all rows with <code>ON_PEAK</code> and the first row next to them, i.e</p>

<pre><code>['2018-01-01 00:00:00', '2018-01-01 00:15:00', '2018-01-01 00:30:00', '2018-01-01 00:45:00', '2018-01-01 01:00:00']
</code></pre>
","7818811","","7818811","","2018-05-18 20:18:47","2018-05-18 20:18:47","How to get all indexes which had a particular value in last row of a Pandas DataFrame?","<python-3.x><pandas><dataframe>","1","2","","","","CC BY-SA 4.0","1"
"49208499","1","49208572","","2018-03-10 11:24:15","","1","22","<p>I have a pandas series object, which contains hourly timeseries data of 10 days as:</p>

<pre><code>import pandas as pd
import numpy as np
# create index first
datetimeindex = pd.date_range('1/1/2018', periods = 24*10, freq = 'H')
ps = pd.Series(np.random.randn(len(datetimeindex)), datetimeindex)
</code></pre>

<p>Now, I want to create different groups out of it. Where each group should contain data between 23:00:00 hours and 02:00:00 hours of each two consecutive days. This grouping will result in 11 groups. I read <code>pandas.groupby()</code> but I find it difficult to get two consecutive days at once and extract required data. How should I proceed?</p>
","3317829","","","","","2018-03-10 11:41:31","How to get different groups from a pandas object with a timestamp range","<python><python-3.x><pandas><pandas-groupby>","1","2","","","","CC BY-SA 3.0","1"
"49570268","1","49570403","","2018-03-30 07:30:17","","-2","22","<p>I would like to set a <code>flag</code> column to <code>if salary &gt; 200 then flag='high' else 'low'</code>.</p>

<p><code>test1</code> is my DataFrame.</p>

<p>dataframe:</p>

<pre><code>policyID  salary           line

119736      100       Residential                     

206893      1000    Commercial

172534      70      Residential

785275       500     Residential
</code></pre>

<p>I'm using this code:</p>

<pre><code>for i in test1['salary']:
    if i &gt; 200:
       test1['flag']='high'
    elif i &lt; 200:
       test1['flag']='low'

print (test1) 
</code></pre>

<p>it's giving me flag = high only.</p>
","2897214","","146073","","2018-03-30 14:51:03","2018-03-30 14:51:03","wanted to create flag for salary in python","<python-3.x><pandas><if-statement>","2","3","","2018-03-30 14:32:34","","CC BY-SA 3.0","1"
"56952572","1","","","2019-07-09 12:26:49","","0","22","<p>I have a text file like this example:</p>

<pre><code>Name,SampleProbe
IRF2,231.14
IRF2,340.19
IRF2,139.57
PTEN,115.57
PTEN,240.4
PTEN,117.92
PRKACB,180.02
PRKACB,396.13
PRKACB,186.9
</code></pre>

<p>As you see every item in the first column is repeated 3 times and consequently for every item I have 3 values in the 2nd column. I am trying to make a sebset of this file (as a pandas dataframe) in which the first column is IRF2 and PTEN like expected results:</p>

<p>Expected results:</p>

<pre><code>IRF2    231.14
IRF2    340.19
IRF2    139.57
PTEN    115.57
PTEN    240.4
PTEN    117.92
</code></pre>

<p>to do so I wrote the following code:</p>

<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
plt.rcParams['figure.figsize'] = (20.0, 10.0)
plt.rcParams['font.family'] = ""serif""
df = pd.read_csv('data.txt')
df2 = df[[df['Name' == 'PTEN' AND 'Name' == 'IRF2']['SampleProbe']]]
</code></pre>

<p>It does not return the expected results. Do you know how to fix it?</p>
","10657934","","","user5861130","2019-07-09 13:00:49","2019-07-09 13:00:49","subsetting a csv file in pandas","<python-3.x><pandas><dataframe>","0","9","","2019-07-09 12:30:19","","CC BY-SA 4.0","1"
"48722049","1","","","2018-02-10 14:54:59","","1","22","<p>Hallo Stackoverflow community!!</p>

<p>I am trying to develop a small textual interface for my measurement apparatus. I have 3 inputs with current Time stamp: Input_1, Input_2, and Input_3. The Input_3 is an online measurement because I am measuring the value inside a liquid for e.g. pH or some concentration and should be maintained in specific values, in my case between 70 and 180 and all the inputs are stored separately into 3 different lists. </p>

<p>So I separated the Input_3 into Input_3_Low, Input_3_High and Input_3_Normal. The increase of Input_3 is dependent on the Input_1, for e.g. 15 g of Input_1, Input_3 will rise to 50 g/L, so if your concentration was 130 g/L by adding 30 g of Input_1, Input_3 will rise to 100 g/L so if your Input_3 before was 130 g/L then you have now 230 g/L. In order to maintain between 70 g/L and 180 g/L I have to add Input_2 as a compensation, for e.g. 1 unit of Input_2 corrects Input_3 by 50 g/L so by adding 2 units of Input_2, Input_3 will decrease from 230 g/L to 130 g/L. So the correction factor of Input_2 = 50</p>

<p>My question is how to put this calculation into the python script?</p>

<p>I will describe my example:</p>

<pre><code># These are already the Inputs calculated from the below calculation
time = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
Input_3 = [130, 130, 230, 230, 130, 110, 90, 80, 50, NaN, 120]
Input_1 = [NaN, 30, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN, NaN]
Input_2 = [NaN, NaN, NaN, 2, NaN, NaN, NaN, NaN, NaN, 12, NaN]
correction_factor = 50
(Input_1 / Input_2) ratio_1 = 0.1
(Input_2 / Input_3) ratio_2 = 0.3
</code></pre>

<p>I want to calculate:</p>

<pre><code>if Input_3 &gt; 180:
   correction = (Input_3[current] - Input_3[previous]) / correction_factor
   coverage = Input_2 * ratio_1
   Inp_2 = correction = coverage
   Input_2.append(Inp_2)

if Input_3 &lt; 70: # I will just set it to 120 g/L
   correction = 120 - Input_3
   Inp_1 = correction * ratio_2
   Input_1.append(Inp_1)
</code></pre>

<p>How to properly take the previous value from a list and append the results in the lists?</p>

<p>I am relying on you guys, any hello would be appreciated.</p>

<p>Cheers!</p>
","5681382","","","","","2018-02-10 14:54:59","setting a textual interface with python","<python-3.x><pandas><math><interface>","0","0","","","","CC BY-SA 3.0","1"
"57740687","1","57740728","","2019-08-31 18:31:17","","0","21","<pre><code>import pandas as pd
df= pd.DataFrame({'Data':['123456A122 119999 This 1234522261 1A1619 BL171111 A-1-24',
                                  '134456 dont 12-23-34-45-5-6 Z112 NOT 01-22-2001',
                                  'mix: 1A25629Q88 or A13B ok'], 
                          'IDs': ['A11','B22','C33'],
                          }) 
</code></pre>

<p>I have the following <code>df</code> as seen above. I am using the following to get only consequtive digits</p>

<pre><code>reg = r'((?:[\d]-?){6,})'
df['new'] = df['Data'].str.findall(reg) 

    Data    IDs new
0               [123456,119999, 1234522261, 171111]
1               [134456, 12-23-34-45-5-6, 01-22-2001]
2               []
</code></pre>

<p>This picks up many things I dont want like <code>171111</code> from <code>BL171111</code> and <code>123456</code> from <code>123456A122</code> etc</p>

<p>I would like the following output which only picks up 6 consequtive digits </p>

<pre><code>    Data    IDs new
0               [119999]
1               [134456]
2               []
</code></pre>

<p>How do I change my regex to so?</p>

<pre><code>reg = r'((?:[\d]-?){6,})'
</code></pre>
","","user11962395","","","","2019-08-31 18:36:39","make regex more specific in getting consequtive digits","<regex><python-3.x><string><pandas><text>","1","1","","","","CC BY-SA 4.0","1"
"56646426","1","56646578","","2019-06-18 09:55:22","","1","21","<p>I have a pandas data frame which has a column <code>'daysofweek'</code>. I am trying to write a simple if-else block to populate a new column <code>'weekend'</code> as 1 if daysofweek is 5 or 6. And, 0 otherwise. I am getting this error: <code>""ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().""</code></p>

<p>I tried changing 'or' to '|' but the error persists.</p>

<pre><code>if train['day of week'] == 5 | train['day of week'] == 6:
    train['weekend'] = 1
else:
    train['weekend'] = 0
</code></pre>

<p>I expect the <code>'weekend'</code> column to be populated with 0's and 1's but it is giving the error.</p>
","9799540","","3190076","","2019-06-18 10:01:30","2019-06-18 10:16:52","Getting ValueError while writing an if-else loop to populate a data frame column based on a condition","<python><python-3.x><pandas>","1","1","","","","CC BY-SA 4.0","1"
"48777854","1","48777933","","2018-02-14 00:25:36","","0","21","<p>I want to perform an action on a column in my pandas dataframe and I want to get back a list of information from another column of the rows that passed the action in the first place, Its a weird way of explaining it, please take a look at this diagram:</p>

<p><a href=""https://i.stack.imgur.com/7Yct0.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Yct0.png"" alt=""enter image description here""></a></p>

<p>How would I go about doing this?</p>
","9347541","","4270018","","2018-02-14 04:04:45","2018-02-14 04:04:45","How to return certain rows in pandas based on a calculation across a column","<python><python-3.x><pandas>","2","2","","","","CC BY-SA 3.0","1"
"56799621","1","56823486","","2019-06-28 00:06:25","","0","21","<p>Is there any way for me to take a DataFrame (originally in Pandas, sample it with different random seeds and use Dask to return several Dataframes (one per random seed).</p>

<p>My very primitive understanding of Dask is that I can take a Dataframe, split it and then make computations over it but I was wondering if I could use Dask to solve this issue.</p>
","11053294","","","","","2019-06-30 08:32:00","Use Dask to return several dataframes after computing over a single Dataframe","<python-3.x><pandas><dask>","1","0","2","","","CC BY-SA 4.0","1"
"57509861","1","","","2019-08-15 12:44:50","","-1","21","<pre><code>Column1.          Column2

Start1                   633

End.                       855

Start2.                     767

Start3.                      231

End.                           545

Start4.                      111

Start5                       243

End.                           333
</code></pre>

<p>Output</p>

<pre><code>Column1.          Column2

Start1                   633

End.                       855

Start3.                      231

End.                           545

Start5                       243

End.                           333
</code></pre>

<p>In column1 Start2 row drop because it's not end same as start4</p>
","11931675","","6361531","","2019-08-15 12:52:08","2019-08-15 13:28:36","I want to drop rows In my data frame base on column 1","<python-3.x><pandas>","2","0","","","","CC BY-SA 4.0","1"
"49760531","1","","","2018-04-10 18:20:11","","-1","20","<pre><code>import pandas as pd 
A=pd.read_csv(""C:/Users/amulya/Desktop/graves lab/Mani/HIDDEN WORDS/Hidden_word_4.csv"")
df1=pd.DataFrame(A)
B=pd.read_csv(""C:/Users/amulya/Desktop/graves lab/words.csv"")
df2=pd.DataFrame(B)
C=df1[df1.col1.isin(df2.col1)]``
df3=pd.DataFrame(C)
df3.to_csv(""C:/Users/amulya/Desktop/graves lab/Mani/HIDDEN WORDS/out4.csv"", encoding='utf-8', index=False)
</code></pre>

<h1>insteaad of naming the file each time, i want to iterate 20 files</h1>
","9605440","","355230","","2018-04-10 18:44:31","2018-04-10 18:48:34","how to iterate 20 .csv files and do some computation[the computation part,shown in code] on each and create sepaerate .scv output files in python?","<python><python-3.x><pandas><dataframe>","1","4","","","","CC BY-SA 3.0","1"
"57022840","1","","","2019-07-13 21:07:10","","1","20","<p>I have a data frame with 10 million rows as </p>

<p>ID    Speed</p>

<p>AB         34</p>

<p>AB         45</p>

<p>BC         60</p>

<p>BC         32  </p>

<p>BC         40</p>

<p>EF         90</p>

<p>Suppose I want to create a third column of Acceleration which takes either 0(deceleration) or 1(acceleration) on ID level.</p>

<p>In total I have more than 2000 unique ID's.</p>

<p>I tried this </p>

<pre class=""lang-py prettyprint-override""><code>for id in df['ID'].unique() :
    df.loc[df['ID'] == id,'speed']
    df['acceleration'] = df['speed'].apply(accl)
</code></pre>

<p>My accl function is as follows </p>

<pre class=""lang-py prettyprint-override""><code>def accl(a):
   for i in range(1,len(a))    :
        if a[i] &gt; a[i-1]:
            return 1
        else:
            return 0
</code></pre>

<p>After running the loop I am getting Memory issue.</p>

<p>How to solve this efficiently ? Any technique ? How to solve this using Numpy array if that's the solution? </p>

<p>Remember : I need the data on ID level so just succesive i+ 1 > i on df['speed'] will not work for getting Acceleration column.</p>
","10401786","","9081267","","2019-07-13 21:38:21","2019-07-13 21:38:21","Memory issue? How to get the desired output Efficiently?","<python-3.x><pandas><numpy>","0","1","","","","CC BY-SA 4.0","1"
"49519283","1","49519340","","2018-03-27 17:43:00","","1","20","<p>I have the following dataframe:</p>

<pre><code>df1:
id | country | state | amt1 | amt2 | var1 | var2
1 | US | TX | 20 | 40 | a | X
2 | US | CA | 30 | 900 | b  | Y
3 | US | CA | 40 | 230 | c | X
4 | US | TX | 80 | 670 | d | X
5 | US | NY | 20 | 120 | a | Y
6 | US | TX | 10 | 80 | c | X
</code></pre>

<p>This is an example dataframe - the original dataframe I have has multiple other columns, with different types of variables</p>

<p>I would like to create another dataframe which will contain aggregrates (sum/count/any other function) of any column from the 1st dataframe, per matching country, state</p>

<p>For this, I have already created the 2nd dataframe with groupings of the country, state:</p>

<pre><code>df2:
country | state | num
US | TX | 3
US | CA | 2
US | NY | 1
</code></pre>

<p>Now, I want to be able to add any column to df2, which will contain the sum of ""amt1"" from ""df1"" for the matching country &amp; state</p>

<p>I am able to do this using iterrows:</p>

<pre><code>for i, row in df2.iterrows():
    amt = df1[(df1.state == df2.state ) &amp; (df1.country == df2.country )].amt1.sum()
    df2.set_value(i, 'Amt ', amt)
</code></pre>

<p>However that is taking a lot of time, for a large dataset</p>

<p>Wanted to know if there is any alternative way of doing this efficiently, instead of using iterrows</p>
","9506443","","","","","2018-03-27 17:46:41","python pandas dataframe: create a column in 2nd dataframe with aggregate values acc to matching item from 1st dataframe","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 3.0","1"
"48494436","1","","","2018-01-29 04:31:55","","0","20","<p>Below code generate the first output as given below . I have multiple output as a dataframe for each variable like Age_Bin. </p>

<pre><code>    df1 = ( df.groupby(['Age_Bin','Outcome'])['Cat_Bin'] .size() .unstack(fill_value=0) .add_prefix('Outcome_') ) 

                     df_A = df1.assign(Total_cnt=lambda x: x.sum(1)).join(df1.div(df1.sum()).add_suffix('%')) 

        print (df_A)
</code></pre>

<p>First output:</p>

<pre><code>       Outcome   Outcome_0 Outcome_1 Total_cnt Outcome_0% Outcome_1% 
       Age_Bin 
         Age1          1       3        4         0.25      0.6 
         Age2          1       1        2         0.25      0.2 
         Age3          2       1        3         0.50      0.2
</code></pre>

<p>df_B is similar as df_A but for another variable. For example df_B given below</p>

<pre><code>print (df_B)

          Outcome   Outcome_0 Outcome_1 Total_cnt Outcome_0% Outcome_1% 
           LTM_Bin 
             LTM1          1       3        4         0.25      0.6 
             LTM2          1       1        2         0.25      0.2 
             LTM3          2       1        3         0.50      0.2
</code></pre>

<p>If I use concat function like below</p>

<pre><code>Df_final = pd.concat([df_A,df_B],axis=0)
</code></pre>

<p>I am getting an error due the first column structure in the first output . 
If I have output like second output then I believe it is easy to apply concat. </p>

<p>Second output :</p>

<pre><code>    Age_Bin Outcome_0 Outcome_1 Total_cnt Outcome_0% Outcome_1% 
    Age1        1         3       4           0.25      0.6 
    Age2        1         1       2           0.25      0.2
    Age3        2         1       3           0.50      0.2
</code></pre>
","3762120","","3762120","","2018-01-29 05:25:29","2018-01-29 05:25:29","How to order the pandas dataframe heading as proper dataset like the below second output","<python-3.x><pandas>","0","3","","","","CC BY-SA 3.0","1"
"56815669","1","","","2019-06-29 06:55:09","","0","19","<p>(Before I posted this question, I checked many other similar questions on SO but none of them answer my question.)</p>

<p>I have 2 dataframes, all with exact same index but different columns.</p>

<p>I want to create a new data frame with the exact same index as the other 3 but pick one specific column from each dataframe:</p>

<p>DF1</p>

<pre><code>           blah1       blah2        IMP1
Month1   0.80550    0.912814    1.135899
Month2  -4.61033    0.750013    1.093204
Month3   0.34241   -0.627297    2.035373
Month4   7.19570    0.039588    0.350060
Month5   3.86186   -0.208209   -0.939962
</code></pre>

<p>DF2</p>

<pre><code>            blah1       blah2        IMP2
Month1   0.580550    0.480814    1.135899
Month2  -1.961033    0.546013    1.093204
Month3   2.063441   -0.627297    2.035373
Month4   0.319570    0.058588    0.350060
Month5   1.318068   -0.802209   -0.369962
</code></pre>

<p>I would like to create a new DF like so:</p>

<p>DF3</p>

<pre><code>          IMP1          IMP2
Month1    1.135899      1.135899
Month2    1.093204      1.093204
Month3    2.035373      2.035373
Month4    0.350060      0.350060
Month5   -0.939962     -0.369962
</code></pre>

<p>Whats the most efficient way to do so? </p>
","1334713","","472495","","2020-06-17 12:12:37","2020-06-17 12:12:37","How to use specific columns from multiple dataframes to create a new dataframe?","<python-3.x><pandas><dataframe>","0","3","","2019-06-29 07:05:16","","CC BY-SA 4.0","1"
"56783920","1","","","2019-06-27 04:31:01","","1","19","<p>I have categorical variables based on states. </p>

<p>I want to create a <code>dynamic dataframe</code> with same name and data of filtered state only. </p>

<p>Like for <code>DataAL</code>, we will have all data of <strong>AL states only</strong>.</p>

<p><strong>Code 1:</strong></p>

<pre><code>l = []

for i in a:
    print(i)
    l[i] = df4[df4[""Provider State""].str.contains(i)]
    l[i] = pd.DataFrame(l[i])
    l[i].head()
</code></pre>

<blockquote>
  <p>TypeError: list indices must be integers or slices, not str</p>
</blockquote>

<hr>

<p><strong>Code 2:</strong></p>

<pre><code>l = []

for i in range(len(a)):
    print(i)
    l[i] = df4[df4[""Provider State""].str.contains(a[i])]
    l[i] = pd.DataFrame(l[i])
    l[i].head()
</code></pre>

<blockquote>
  <p>IndexError: list assignment index out of range</p>
</blockquote>
","11706323","","11175375","","2019-06-27 05:41:30","2019-06-27 05:41:30","Grouping categorical variable in dynamic dataframe in pandas","<python-3.x><pandas>","0","0","","","","CC BY-SA 4.0","1"
"49250561","1","","","2018-03-13 07:31:49","","0","19","<p>Good Morning,</p>

<p>I have the following dataset:</p>

<p>print(df)</p>

<pre><code>Product_code   Quantity_in_stock   
    01                4                     
    02                1                   
    01                2                   
    03                6                   
    02                9                    
...
</code></pre>

<p>I would like to create an additional column ""Average_quantity"", using Pandas, equals to the <strong>average ""Quantity_in_stock""</strong> of that <strong>specific</strong> <strong>product</strong>; for instance:</p>

<pre><code>Product_code   Quantity_in_stock    Average_quantity
    01                4                    3  
    02                1                    5
    01                2                    3
    03                6                    6
    02                9                    5
...
</code></pre>

<p>I tried:</p>

<pre><code>for i in df[""Product_code""]:
    df[""Average_quantity""] = np.mean(df[""Quantity_in_stock""])
</code></pre>

<p>But it reports the same average quantity of the whole set, not the one of that <strong>specific product code</strong></p>

<p>How Can I solve it?</p>
","8618380","","","","","2018-03-13 07:31:49","Subset and Loop to create a new column","<python-3.x><pandas><for-loop>","0","3","","2018-03-13 07:35:34","","CC BY-SA 3.0","1"
"57466069","1","","","2019-08-12 18:02:45","","0","19","<p>I have a Dataframe of the form:</p>

<pre class=""lang-py prettyprint-override""><code>
ID Address1 Address2 Address3
0  a        b        c
1  NaN      d        e
2  f        NaN      g
2  NaN      h        Nan
</code></pre>

<p>How could I select all rows that have (at least in one of them), a value in either Address1 column, or Address3?</p>

<p>In this particular example, it will select row 0,1 and 2, not 3 because both Address1 and Address3 are Nan.</p>

<p>Thanks!</p>
","11722667","","","","","2019-08-12 18:02:45","How do I select rows with at least a value in some selected columns?","<python><python-3.x><pandas><dataframe>","0","2","","2019-08-12 18:08:39","","CC BY-SA 4.0","1"
"56854510","1","","","2019-07-02 14:16:02","","0","18","<p>I have 3 attributes:</p>

<ul>
<li>a = 2018-10 (type = )</li>
<li>b = 2018-03 (type = )</li>
<li>c = 2 (type = int)</li>
</ul>

<p>I want to calculate an answer below (I expect the answer to be an integer):</p>

<pre><code>answer = math.floor((a - b) / c) 
</code></pre>

<p>Then I get the error message TypeError: must be real number, not MonthEnd. I want the answer to be an integer so I have unsuccesfully tried:</p>

<pre><code>answer = math.floor(int(a - b) / c) 
</code></pre>
","2466763","","","","","2019-07-02 14:16:02","TypeError: must be real number, not MonthEnd","<python-3.x><pandas>","0","3","","","","CC BY-SA 4.0","1"
"57419340","1","","","2019-08-08 19:18:00","","1","18","<p>My dataset <code>df</code> looks like this:</p>

<pre><code>DateTimeVal            Open 
2017-01-01 17:00:00    5.1532    
2017-01-01 17:01:00    5.3522 
2017-01-01 17:02:00    5.4535    
2017-01-01 17:03:00    5.3567    
2017-01-01 17:04:00    5.1512 
....
</code></pre>

<p>It is a <code>Minute</code> based data</p>

<p>The <code>Time</code> value starts from <code>17:00:00</code> however I want to only change the <code>Time</code> value to start from <code>00:00:00</code> as a <code>Minute</code> based data and up to <code>23:59:00</code> </p>

<p>The current <code>Time</code> starts at <code>17:00:00</code> and increments per <code>Minute</code> and ends on <code>16:59:00</code>. The total <code>row</code> value is <code>1440</code> so I can confirm that it is a <code>Minute</code> based <code>24 Hour</code> data </p>

<p>My new <code>df</code> should looks like this:</p>

<pre><code>DateTimeVal            Open 
2017-01-01 00:00:00    5.1532    
2017-01-01 00:01:00    5.3522 
2017-01-01 00:02:00    5.4535    
2017-01-01 00:03:00    5.3567    
2017-01-01 00:04:00    5.1512 
....
</code></pre>

<p>Here, we did not change anything except the <code>Time</code> part.</p>

<p><strong>What did I do?</strong></p>

<p>My logic was to remove the <code>Time</code> and then populate with new <code>Time</code></p>

<p>Here is what I did: </p>

<pre><code>pd.DatetimeIndex(df['DateTimeVal'].astype(str).str.rsplit(' ', 1).str[0], dayfirst=True)
</code></pre>

<p>But I do not know how to add the new <code>Time</code> data. Could you please help?</p>
","9161607","","","","","2019-08-15 02:34:31","How to join Minute based time-range with Date using Pandas?","<python-3.x><pandas><dataframe>","1","0","","","","CC BY-SA 4.0","1"
"57192270","1","57192354","","2019-07-24 22:58:56","","0","17","<p>I have several csv files name file1, file2, file3, etc. They all look like this (exactly identical, only the floats change):</p>

<pre><code>filename,    column1,  column2, ... columnN
asdfasd.jpg   23.23,    21.24,        1e-06
ersdadfsd.jpg 223.23,   1.23,         1
assd.jpg      23.23,    1e-08,       232.1
...
</code></pre>

<p>I would like to get an indentical looking table in which all fields contain the mean. How can this be done in an efficient way?</p>
","7762646","","","","","2019-07-24 23:15:54","How to merge several csv files averaging fields?","<python><python-3.x><pandas><csv>","1","3","","","","CC BY-SA 4.0","1"
"56929684","1","","","2019-07-08 06:51:36","","0","16","<p>Before you start.This is not just the matter of binning.
I want to group the data .</p>

<p>I want create a new column called 'PC' by classifying the 'user_id' as A or B or C based on their 'marks' from this dataframe.</p>

<pre><code>`user_id`   `session_id`    `is_correct` `is_attempted`

    1055898378  5cd473b8118de      0.0              1.0
    080941146   5cd46e6429063e     1.0              1.0
    1078950588  5cd473a329063e     1.0              1.3
    1080941146  5cd46e6429063e     1.0              1.0
    1116860923  5cd472d029063e     0.0              1.0
    1090133505  5cd2aad529063e     0.0              1.0
    1090324745  5cd31c0929063e     1.0              1.0
    1089085589  5cd4740329063e     0.0              1.0
    1090324745  5cd31c0929063e     0.0              1.0
    1092896346  5c816e341c1fba     1.0              1.0
</code></pre>

<p>To find 'marks' =  'is_correct'  /  'is_attempted' per user</p>

<pre><code>The rules to classify is: 
    A: if user have scored &gt;= 0.8
    B: if user have scored &gt;= 0.5 &amp;&amp; &lt; 0.8
    C: if user have scored &lt; 0.5
</code></pre>

<p>I expect the output to be like:</p>

<p><code>user_Id</code>      <code>session_id</code>        <code>marks</code>    <code>PC</code></p>

<p><code>1090133505</code>    <code>5cd2aad529063e</code>     <code>0.5</code>     <code>B</code></p>
","10010063","","10010063","","2019-07-08 07:29:46","2019-07-08 07:29:46","Creating a new column after aggregating data per group","<python-3.x><pandas><pandas-groupby>","0","6","","2019-07-08 06:57:20","","CC BY-SA 4.0","1"
"57748772","1","57748865","","2019-09-01 19:15:39","","0","16","<p>I opened a file in jupyter notebook using open() and read(). I used a for loop to find duplicates. It worked perfectly. </p>

<p>I opened the same file using pandas data frame. The for loop that I used before did not work. It did not produce any results.</p>

<p>that for loop in pandas does not print anything. </p>

<pre class=""lang-py prettyprint-override""><code>
duplicate_apps = []
unique_apps = []

for app in android:
    name = app[0]
    if name in unique_apps:
        duplicate_apps.append(name)
    else:
        unique_apps.append(name)

print('Number of duplicate apps:', len(duplicate_apps))
print('\n')
print('Examples of duplicate apps:', duplicate_apps[:15])
</code></pre>

<p>This loop worked perfectly when I used open() and reader(), but did not work when I opened the file using pandas dataframe.</p>
","11214091","","","","","2019-09-01 19:35:54","same for loop worked if opened file using open() & reader(), but didn't work for pandas dataframe","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57100430","1","","","2019-07-18 18:11:42","","0","16","<p>so Im using this statement to delete from my df some value with multiple conditions.</p>

<pre><code>Delete = df[ (df['Food'] == 'Ham') &amp; (df['Fruit'] == 'Beans') ].index
df.drop(Delete, inplace=True)
</code></pre>

<p>and I'm getting the message ""A value is trying to be set on a copy of a slice from a DataFrame""</p>

<p>I investigated and it seems I'm not doing the change in the original data frame, how can I achieve this?</p>

<p>Thanks</p>
","10906075","","","","","2019-07-18 18:11:42","Doing drop and this msg appear: A value is trying to be set on a copy of a slice from a DataFrame","<python-3.x><pandas>","0","3","","","","CC BY-SA 4.0","1"
"57261833","1","57261849","","2019-07-29 21:51:07","","0","15","<p>The code below appends two sub DataFrames df1 and df2 into one large dateframe:</p>

<pre><code>     df1 = pd.DataFrame({""a"":[1, 2, 3, 4], 
                     ""b"":[5, 6, 7, 8]}) 

     df2 = pd.DataFrame({""a"":[1, 2, 3], 
                ""b"":[5, 6, 7]}) 

     df1.append(df2) 
</code></pre>

<p>Gives:</p>

<p>ab<br>
  --<br>
0| 15<br>
1| 26<br>
2| 37<br>
3| 48<br>
0| 15<br>
1| 26<br>
2| 37  </p>

<p>Is it possible to split this appended DataFrame into the original sub DataFrames as shown below?</p>

<p>ab<br>
  --<br>
0| 15<br>
1| 26<br>
2| 37<br>
3| 48   </p>

<p>ab<br>
  --<br>
0| 15<br>
1| 26<br>
2| 37  </p>
","1686468","","1686468","","2019-07-29 21:54:39","2019-07-29 21:54:39","Is it possible to recover the original sub DataFrames after they have been appended into a larger DataFrame in Pandas?","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"56714459","1","56714506","","2019-06-22 09:56:50","","1","14","<p>I'm trying to build a multiIndex for a pandas DataFrame which is  to store time series data for several individuals. </p>

<p>I thought a good way to do this would be as follows:</p>

<pre><code>D1 = pd.date_range(start='1/1/2018', periods=2, freq='H')
D2 = pd.date_range(start='3/4/2018', periods=3, freq='H')
l1=[1,2]       # the individuals' numbers
l2 = [D1,D2]
l = list(zip(l1,l2))
M = pd.MultiIndex.from_tuples(l)
</code></pre>

<p>and the desired output would be a multiIndex of the form below:</p>

<pre><code>1    2018-01-01 00:00:00
     2018-01-01 01:00:00
2    2018-03-04 00:00:00
     2018-03-04 01:00:00
     2018-03-04 02:00:00
</code></pre>

<p>However, I'm getting <code>TypeError: unhashable type: 'DatetimeIndex'</code>. Any help would be appreciated.</p>
","3973834","","","","","2019-06-22 10:07:54","Creating a MultiIndex by Zipping DatetimeIndex is Causing Error","<python-3.x><pandas>","1","0","","","","CC BY-SA 4.0","1"
"57656151","1","","","2019-08-26 10:29:26","","0","14","<p>I have the following dataframe:</p>

<pre><code>    name    year0   id      year    month   val1    val2    col
0   Agency  2009    COM3    2019    April   0.02    0.04    2019April
1   Agency  2009    COM3    2019    May     0.00    0.00    2019May
2   Agency  2010    FRE5    2019    April   0.00    0.00    2019April
3   Agency  2010    FRE5    2019    May     0.05    0.07    2019May
</code></pre>

<p>I want to transform it from long to wide as:</p>

<pre><code>    name    year0   id      val1                 val2
                            2019April  2019May   2019April  2019May
0   Agency  2009    COM3    0.02       0.00      0.04       0.00
2   Agency  2010    FRE5    0.00       0.05      0.00       0.07
</code></pre>

<p>Write now I have the following code:</p>

<pre><code>df = df.pivot(index=None, columns='col', values=['val1', 'val2'])
</code></pre>

<p>But this drops the columns <code>name, year0, id, year, month</code>. I do want to drop <code>year</code> and <code>month</code> as col is just concatenation of both, but I want to keep <code>name, year0, id</code> as they are. How can I achieve that?</p>

<p>Please note that none of the columns can be used as index as they all repeat. However a combination of <code>name, year0, id</code> can be used. I tried the same:</p>

<pre><code>df = df.pivot(index=['name', 'year0', 'id'], columns='col', values=['val1', 'val2'])
</code></pre>

<p>But it gives me the following error:</p>

<pre><code>ValueError: Shape of passed values is (20011, 2), indices imply (3, 2)
</code></pre>

<p>I am new to pandas, so I may be missing something simple. Any help is appreciated! Thanks!</p>
","5181947","","5181947","","2019-08-26 10:41:37","2019-08-26 10:41:37","pandas pivot: keep some columns as they are","<python-3.x><pandas><dataframe>","0","4","","2019-08-26 10:42:01","","CC BY-SA 4.0","1"
"49324898","1","","","2018-03-16 15:55:18","","0","14","<p>so I have two data frames to start with in pandas.</p>

<p>One data frame has more columns than the other
In the larger one, some of those columns contain identical rows as those in the smaller frame.</p>

<p>I want to combine the two data frames so that the rows in both frames that are equal result in one row, and the rows that are NOT equal lead to a null.</p>

<p>Here is a picture of what I mean:</p>

<p><img src=""https://i.stack.imgur.com/OAhf4.png"" alt=""dataframe1""></p>

<p><img src=""https://i.stack.imgur.com/CADHs.png"" alt=""dataframe2""></p>

<p><img src=""https://i.stack.imgur.com/cVOPC.png"" alt=""resulting_data_frame""></p>

<p><strong>Setup</strong>  </p>

<pre><code>df1 = pd.DataFrame(
    [[1, 3, 'alpha'], [2, 4, 'beta']], columns=['a', 'b', 'features'])
df2 = pd.DataFrame([[5, 6], [1, 3]], columns=['u', 'v'])
</code></pre>

<p>How could this be done in pandas? How about in SQL?</p>
","9504049","","1000551","","2018-03-16 16:02:39","2018-03-16 16:02:39","Combining dataframes so that column entries in left side equal column entries in right side and if not return null","<python><python-3.x><pandas><dataframe><join>","1","0","","","","CC BY-SA 3.0","1"
"56741053","1","","","2019-06-24 17:03:21","","0","12","<p>My data set contains details of one company's stock.
And i have calculated the %change in closing price of the stock for 2 consecutive days and that column is called ""Day_Perc_Change"". I have one more column called ""Trend"" which needs to be filled with these constraints:
'Slight or No change' for 'Day_Perc_Change' in between -0.5 and 0.5
'Slight positive' for 'Day_Perc_Change' in between 0.5 and 1
'Slight negative' for 'Day_Perc_Change' in between -0.5 and -1
'Positive' for 'Day_Perc_Change' in between 1 and 3
'Negative' for 'Day_Perc_Change' in between -1 and -3
'Among top gainers' for 'Day_Perc_Change' in between 3 and 7
'Among top losers' for 'Day_Perc_Change' in between -3 and -7
'Bull run' for 'Day_Perc_Change' >7
'Bear drop' for 'Day_Perc_Change' &lt;-7</p>

<p>Hence I wrote a logic </p>

<pre><code>if -0.5&lt;data['Day_Perc_Change']&lt;0.5:
    data['Trend'] = 'Slight or No change'
elif 0.5&lt;data['Day_Perc_Change']&lt;1:
    data['Trend'] = 'Slight positive'
elif -1&lt;data['Day_Perc_Change']&lt;-0.5:
    data['Trend'] = 'Slight negative'
elif 1&lt;data['Day_Perc_Change']&lt;3:
    data['Trend'] = 'Positive'
elif -3&lt;data['Day_Perc_Change']&lt;-1:
    data['Trend'] = 'Negative'
elif 3&lt;data['Day_Perc_Change']&lt;7:
    data['Trend'] = 'Among top gainers'
elif -7&lt;data['Day_Perc_Change']&lt;-3:
    data['Trend'] = 'Among top losers'
elif data['Day_Perc_Change']&gt;7:
    data['Trend'] = 'Bull run'
else: 
    data['Trend'] = 'Bear drop'
</code></pre>

<p>Error i am getting is:</p>

<pre><code>ValueError: The truth value of a Series is ambiguous. Use a.empty, 
a.bool(), a.item(), a.any() or a.all().
</code></pre>
","11671222","","","","","2019-06-24 17:03:21","ValueError: The truth value of a Series is ambiguous. Fill column using if, else","<python-3.x><pandas><machine-learning>","0","2","","2019-06-24 17:04:31","","CC BY-SA 4.0","1"
"57693128","1","57693171","","2019-08-28 13:04:39","","1","10","<p>I have a Pandas dataframe with a format like:</p>

<pre><code>    ID      Code
    E1023   a
    E1023   b
    E1023   b
    E1023   b
    E1024   b
    E1024   c
    E1024   c
</code></pre>

<p>I would like to create a dictionary with the ID column as a key, with values from the Code column and its counts for a particular ID nested inside like:</p>

<pre><code>{'E1023' : {'a' : 1, 'b' : 3 } , {'E1024' : {'b' : 1, 'c' : 2}}} 
</code></pre>

<p>I understand that I can use a Counter on the Code column, but how do I do this such that it is grouped by the ID and then nested within a dictionary where the ID is key?</p>
","3058703","","","","","2019-08-28 13:07:09","Nesting a counter within another dictionary where keys are dataframe columns","<python-3.x><pandas><dictionary>","1","0","","","","CC BY-SA 4.0","1"
"56713124","1","","","2019-06-22 06:22:54","","0","9","<p>I am following an pandas instruction from <a href=""https://www.youtube.com/watch?v=2AFGPdNn4FM&amp;list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&amp;index=8"" rel=""nofollow noreferrer"">How do I filter rows of a pandas DataFrame by column value? - YouTube</a> which teach how to show movies with a duration  of at least 200 minutes </p>

<p>The data:</p>

<pre><code>#+BEGIN_SRC  python :results output  :session
# read a dataset of top-rated IMDb movies into a DataFrame
movies = pd.read_csv('../data/imdbratings.csv')
print(movies.head())
print('\n', movies.shape)
#+END_SRC

#+RESULTS:
: star_rating                     title              content_rating   genre  duration                                        actors_list
: 0          9.3  The Shawshank Redemption              R   Crime       142  [u'Tim Robbins', u'Morgan Freeman', u'Bob Gunt...
: 1          9.2             The Godfather                                 R   Crime       175    [u'Marlon Brando', u'Al Pacino', u'James Caan']
: 2          9.1    The Godfather: Part II                            R   Crime       200  [u'Al Pacino', u'Robert De Niro', u'Robert Duv...
: 3          9.0           The Dark Knight                       PG-13  Action       152  [u'Christian Bale', u'Heath Ledger', u'Aaron E...
: 4          8.9              Pulp Fiction                                      R   Crime       154  [u'John Travolta', u'Uma Thurman', u'Samuel L....
:
:  (979, 6)
</code></pre>

<p>Then construct a booleans to collect values whose duration greater than 200 minutes</p>

<pre><code>#+BEGIN_SRC  python :results output  :session
# create a list in which each element refers to a DataFrame row: True if the row satisfies the condition, False otherwise
booleans = [True if length &gt;= 200 else False for length in movies.duration]
print(len(booleans), booleans[:5])
#+END_SRC

#+RESULTS:
: 979 [False, False, True, False, False]
</code></pre>

<p>Please note that booleans only collected the bool values. 
However, magic hours follows</p>

<pre><code>#+BEGIN_SRC  python :results output  :session
# convert the list to a Series
is_long = pd.Series(booleans)
print(is_long.head())
print(movies[is_long])
#+END_SRC

#+RESULTS:
#+begin_example
0    False
1    False
2     True
3    False
4    False
dtype: bool
     star_rating                                          title  ...                                    duration                                        actors_list
2            9.1                         The Godfather: Part II  ...                              200  [u'Al Pacino', u'Robert De Niro', u'Robert Duv...
7            8.9  The Lord of the Rings: The Return of the King  ...      201  [u'Elijah Wood', u'Viggo Mortensen', u'Ian McK...
17           8.7                                  Seven Samurai  ...                                  207  [u'Toshir\xf4 Mifune', u'Takashi Shimura', u'K...
78           8.4                    Once Upon a Time in America  ...                  229  [u'Robert De Niro', u'James Woods', u'Elizabet...
85           8.4                             Lawrence of Arabia  ...                              216  [u""Peter O'Toole"", u'Alec Guinness', u'Anthony...
142          8.3              Lagaan: Once Upon a Time in India  ...           224  [u'Aamir Khan', u'Gracy Singh', u'Rachel Shell...
157          8.2                             Gone with the Wind  ...                           238  [u'Clark Gable', u'Vivien Leigh', u'Thomas Mit...
204          8.1                                        Ben-Hur  ...                                       212  [u'Charlton Heston', u'Jack Hawkins', u'Stephe...
445          7.9                           The Ten Commandments  ...                220  [u'Charlton Heston', u'Yul Brynner', u'Anne Ba...
476          7.8                                         Hamlet  ...                                        242  [u'Kenneth Branagh', u'Julie Christie', u'Dere...
630          7.7                                      Malcolm X  ...                                    202  [u'Denzel Washington', u'Angela Bassett', u'De...
767          7.6                It's a Mad, Mad, Mad, Mad World  ...            205  [u'Spencer Tracy', u'Milton Berle', u'Ethel Me...

[12 rows x 6 columns]
#+end_example
</code></pre>

<p>The passed-in booleans contains solely but the pure bool values, </p>

<p>How could <code>movies[is_long]</code> know the values are result from comparing with 200 minutes.</p>
","7301792","","","","","2019-06-22 06:22:54","Filter the DataFrame rows to only show movies with a 'duration' of at least 200 minutes","<python-3.x><pandas>","0","3","","2019-06-22 06:28:12","","CC BY-SA 4.0","1"